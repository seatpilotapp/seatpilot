Integer and Combinatorial Optimization **WILEY-INTERSCIENCE**  
# SERIES IN DISCRETE MATHEMATICS AND OPTIMIZATION  
# ADVISORY EDITORS  
RONALD L. GRAHAM  
*AT* & *T Laboratories. Florham Park. New Jersey, US.A.*  
JAN KAREL LENSTRA  
*Department of Mathematics and Computer Science,*  
*Eindhoven University of Technology, Eindhoven, The Netherlands*  
ROBERT E. TARJAN  
*Princeton University, New Jersey, and*  
*NEC Research Institute. Princeton, New Jersey. US.A.*  
A complete list of titles in this series appears at the end of this volume. Integer and Combinatorial Optimization  
GEORGE NEMHAUSER  
School of Industrial  
and Systems Engineering  
Georgia Institute of Technology  
Atlanta, Georgia  
LAURENCE WOLSEY  
Center for Operations Research and Econometrics  
Universite Catholique de Louvain  
Louvain-Ia-Neuve, Belgium  
A Wiley-Interscience Publication  
JOHN WILEY & SONS, INC.  
New York • Chichester • Weinheim • Brisbane • Singapore • Toronto *To our parents* **Preface**  
The explosion of new results in integer and combinatorial optimization that began about  
fifteen years ago inspired us to write a book that would unify theory and algorithms and  
could serve as a graduate text and reference for researchers and practitioners. We have  
been very excited about many of the new developments that have made it possible to solve  
large-scale integer programming problems and that have opened up new areas of research  
which surely will yield more robust and efficient algorithms. Little did we realize the  
enormity of the task. Both of us worked steadily on this project for more than four years.  
The end result was a manuscript of nearly 1400 typewritten pages which, although it does  
not come close to covering all of the literature, covers those topics that we believe  
constitute the most significant theoretical and algorithmic developments.  
Optimization means to maximize \(or minimize\) a function of many variables subject to  
constraints. The distinguishing feature of *discrete, combinatorial,* or *integer* optimization  
is that some of the variables are required to belong to a discrete set, typically a subset of  
integers. These discrete restrictions allow the mathematical representation of phenomena  
or alternatives where indivisibility is required or where there is not a continuum of  
alternati ves.  
Discrete optimization problems abound in everyday life. An important and widespread  
area of applications concerns the management and efficient use of scarce resources to  
increase productivity. These applications include operational problems such as the distri-  
bution of goods, production scheduling, and machine sequencing. They also include  
planning problems such as capital budgeting, facility location and portfolio selection, and  
design problems such as telecommunication and transportation network design, VLSI  
circuit design and the design of automated production systems. Discrete optimization  
problems also arise in statistics \(data analysis\), physics \(determination of minimum  
energy states\), cryptography \(designing unbreakable codes\), politics \(selecting fair election  
districts\), and mathematics \(as a powerful technique for proving combinatorial theorems\).  
Moreover, applications of discrete optimization are in a period of rapid development  
because of the widespread use of microcomputers and the data provided by information  
systems. This is particularly relevant in the manufacturing sector of the economy where  
increased competition and flexibility provided by new technology make it imperative to  
seek better solutions from larger and more complex sets of alternatives.  
This book is about the mathematics of discrete optimization, which includes the  
representation of problems by mathematical models and, especially, the solution of the  
models. The focus is on understanding the mathematical underpinnings of the algorithms  
that make it possible to solve \(exactly or approximately\) the large and complex models  
that arise in practical applications.  
Chapter I.l discusses problem formulation, which is important not only to demonstrate  
the scope of applications, but also because the structure of the formulation is of crucial  
vii viii Preface  
importance to solving the model. Chapter 1.1 gives a comprehensive treatment of this  
subject.  
The remainder of Part I presents mathematics and algorithms that are the foundations  
for the discrete optimization theory and techniques of Parts II and III. There are chapters  
on well-established subjects including linear programming \(Chapter 1.2\), graphs and  
networks \(Chapter 1.3\), and computational complexity \(Chapter 1.5\). The presentation of  
polyhedral theory \(Chapter 1.4\) begins with basic results from linear algebra and then  
emphasizes precisely those results that are essential to a fundamental understanding of the  
algebra and geometry of the convex hull of a discrete set. Chapter 1.6 gives new algorithms  
and results on linear programming and, in particular, establishes the fundamental  
connection between separation and optimization. Chapter I. 7 presents a modern treat-  
ment of the classical problem of solving linear equations in integers and also includes an  
introduction to the recent work on reduced bases for integer lattices.  
Parts II and III present basic approaches and algorithms for solving discrete optimiza-  
tion problems. Part II deals with general problems and those that contain some structure.  
These are the problems that are hard to solve but, for the most part, they are the ones that  
arise in practical applications.  
Chapters 11.1 and 11.2 treat the problem of describing the set of feasible solutions to an  
integer program by a set of linear inequalities. It begins with elementary ideas, but also  
includes a thorough development of advanced topics such as superadditive valid inequali-  
ties and the use of structure to obtain facet-defining inequalities. Objective functions for  
integer programs are introduced in Chapter 11.3 where the fundamental approaches of  
relaxation and duality are developed for the purpose of obtaining upper bounds on the  
optimal value. Most of the advanced material in these chapters has appeared only in  
research articles and monographs, but is essential for the development of future genera-  
tion algorithms for solving integer programs.  
Algorithms are presented in Chapters 11.4, 11.5 and 11.6. Chapter 11.4 presents classical  
branch-and-bound and cutting plane algorithms. Specialized algorithms that use varying  
degrees of structure to obtain exact or approximate solutions are presented in  
Chapters 11.5 and 11.6. Here we study and illustrate a number of techniques that, for the  
most part, have been developed over the last decade and are not covered in the currently  
available textbooks. These include strong cutting plane algorithms, primal and dual  
heuristic analysis, decomposition and reduced bases, and their applications to 0-1 integer  
programs, the traveling salesman problem and fixed-charge network flow problems.  
Part III treats highly structured combinatorial optimization problems for which elegant  
results are known. Chapter 111.1 studies polyhedra with integral extreme points. It includes  
classical results on total unimodularity and recent results on totally balanced, balanced,  
and perfect matrices and on the blocking and antiblocking theory of polyhedra. Chapters  
111.2 and 111.3 are on the classical combinatorial problems of matching and matroids,  
respectively. In both of these chapters the emphasis is on optimization algorithms,  
polyhedral combinatorics and duality. Chapter 111.3 also introduces the significant role of  
submodular and supermodular functions in combinatorial optimization.  
Notes appear at the end of each chapter. Their purpose is to reference our source  
materials, and to comment briefly on extensions and related topics that are not discussed  
in the body of the text. The citations and references are selective. With the exception of  
Chapter 1.1, in Part I our objective is to provide foundation material, and thus the notes  
are limited to a small number of references that cover the corresponding topics in much  
greater detail than is done here. However, in Parts II and III we have attempted to cite the  
original papers in which the material appears as well as some other influential works.  
The book can be used as a graduate text or for self-guided reading in several ways. Since  
we cannot imagine a reader who would want to undertake a straight cover-to-cover Preface ix  
reading and since our experience has shown that it is not possible to cover the whole book  
in even a two-semester, graduate level course, it is necessary to be selective in a first  
reading.  
For graduate students in mathematical programming, especially those planning to  
undertake research in discrete optimization, we suggest a full academic year course  
\(course AY\). Three one-semester options are\: a course emphasizing practical algorithms  
\(course PA\), a course emphasizing general theory \(course GT\), and a course in polyhedral  
combinatorics \(course PC\).  
Each course should begin with some exposure to Chapter 1.1 on model formulation,  
which is important not only to demonstrate the scope of applications, but also because the  
structure of the formulation is of crucial importance in solving the model.  
Chapters 1.2 and 1.3 are only for review, since it is wise for any reader of the book to have  
studied linear programming as a prerequisite. But a typical linear programming course,  
unfortunately, does not cover polyhedral theory. Therefore, all courses should cover  
Chapter 104. In course PA, just enough of the first four sections should be covered \(without  
proofs\) for the student to understand the concept of facets of polyhedra and the idea of  
Theorem 6.1 on the convex hull of a discrete set of points.  
The coverage of Chapter 1.5 on computational complexity will depend on the students'  
backgrounds and the instructor's taste; but at the very least, students in all courses should  
be introduced to the concepts of polynomial computation and NP-completeness. Simi-  
larly, students in all courses should be introduced to the concept of separation and the  
polynomial equivalence of separation and optimization \(Section 1.6.3\). This should be  
done very informally in course PA. Sections 1.6.2, *1.604* and 1.6.5 are independent reading  
and should be omitted in a first reading of the book. Chapter 1.7, and then Section 11.6.5,  
might be covered only in courses AY and GT if time permits at the end of the course. They  
can also be omitted in a first reading.  
Courses PA and GT focus on different parts of Part II. Course PC can omit Part II  
altogether, but would be more interesting if Sections 11.1.1, 11.1.2 \(first-half\), 11.2.1, 11.2.3,  
and 1I.6.3 also were included.  
The following sections from Part II are common to courses AY, PA, and GT\: 11.1.1,  
11.2.1, 11.2.2, 1I.3.l, 11.3.6, 11.3.7, 1104.1, 11.5.1, 11.5.2 and 11.5.3.  
Course PA should also cover Sections 1104.2, 11.504, 11.5.5, 11.6.1 \(knapsack problem\)  
and 11.6.2, and, if time permits, Sections 11.204 and 11.6.4. The instructor may find some  
time for the important class of problems and algorithms discussed in the later two sections  
by omitting or only sketching some proofs from the earlier sections.  
Course GT should also cover Sections 11.1.2 \(leaving out the subsection on bounded  
integer variables\), 11.1.3, 11.1.4, 11.1.5, 11.2.3, 11.3.2, and 11.3.3, and the first two sections of  
Chapter 111.1. If time permits, additional theoretical material could be selected from  
Sections 11.1.6, 11.1.7, 11.304, 11.3.5, or some algorithms could be studied from Sections  
11.4.3,11.5.4 and the first three sections of Chapter 11.6.  
With respect to Part II, course AY is the union of the material covered in courses PC  
and GT. From Part III, course AY should also cover sections 111.1.1, 111.1.2, 111.1.4 and the  
first three sections of Chapter 111.2. Any remaining time could be spent on either  
sections 111.1.5, 111.1.6, or the first few sections of Chapter 111.3.  
The material to be selected from Part III for course PC can vary according to taste. We  
suggest all of Chapter 111.1 except for Section 111.1.3, the first 3 sections of Chapter 111.2  
and the first 5 sections of Chapter 111.3.  
This book could not have been written without the tremendous support that we  
received from the Center for Operations Research and Econometrics \(CORE\) of the  
Universite Catholique de Louvain, and thus we are extremely grateful to Jacques Dreze,  
the founder and intellectual leader of CORE. x Preface  
We met at CORE in the winter of 1970. Nelnhauser spent the academic year 1969-1970  
at CORE and Wolsey presented a seminar on his early work on the group-theoretic  
approach to integer programming. Subsequently, Wolsey became a permanent member of  
CORE and Nemhauser returned to CORE for the period 1975-1977 as Research  
Director. During this period, the authors collaborated extensively on research in the  
analysis of heuristics and other topics in integer and combinatorial optimization stimu-  
lated by an active research group that included Jack Edmonds, Bob Bland, Guy de  
Ghellinck, Rick Giles, Bob Jeroslow, Tom Magnanti, Bill Pulleyblank, Mike Ball and  
Gerard Cornuejols. All of these people, as well as our Dutch neighbors, Jan-Karel Lenstra  
and Alexander Rinnooy Kan, contributed to our understanding of the subject and  
motivation to write a book.  
A NATO research grant made it possible for us to continue our research collaboration  
through the late seventies and early eighties, and we began to draft the manuscript  
earnestly during Nemhauser's fourth year at CORE in 1983-1984. During the writing of  
the book we benefitted from numerous discussions with our friends and professional  
colleagues including Egon Balas, Vasek Chvatal, Marshall Fisher, Martin Grotschel, Ellis  
Johnson, Manfred Padberg. Lex Schrijver, Jorgen Tind and Les Trotter. We are particu-  
larly grateful to Gerard Cornuejols who read Parts I and II and provided extensive  
comments and suggestions and to Bill Pulleyblank who did the same for Part III. We are  
also thankful for the comments we received on various drafts of the text from Jorgen Tind,  
Bob Jeroslow, Alan Goldman, Anton Kolen, Jan-Karel Lenstra, Lex Schrijver, Donna  
Crystal Llewellyn, Martin Dyer, Mike Todd, Jean-Philippe Vial and John Vande Vate.  
Our students in courses given at CORE, Cornell and Georgia Tech found typos and other  
mistakes that otherwise would have been missed\~ special thanks are due to Ronny Aboudi,  
Yves Pochet and Gabriele Sigismondi.  
The chores of deciphering our untidy handwritten drafts and of retyping endless  
revisions were done graciously and with utmost care and patience by the late Elizabeth  
Pecquereau, formerly a secretary at CORE. We are very sad that we will not be able to  
share the joy of seeing the final product with our dear friend Elizabeth. Fabienne Henry of  
CORE and Yvonne Kissi of Georgia Tech also did excellent jobs in typing parts of the  
manuscript. Sheila Verkaren of CORE always managed to spare some of Elizabeth's or  
Fabienne's time for our book, even though we were using far more than our fair share of  
CORE's secretarial resources.  
Over a period of four years, Ellen Nemhauser and Marguerite Wolsey were frequently  
ignored while their husbands spent evenings and weekends writing, and occasionally were  
imposed upon by a boarder who ate and slept at their house, but otherwise was too  
involved in mathematics to engage in civil conversation or to wash the dishes. We thank  
them for their love and patience and hope to make amends.  
GEORGE L. NEMHAUSER  
LAURENCE A. WOLSEY  
*Allan/a, Georgia, USA*  
*Louvain-Ia-Neuve, Belgium*  
*February, 1988.* **Contents**  
**PART I. FOUNDATIONS 1**  
**1.1 The Scope of Integer and Combinatorial Optimization 3**  
1. Introduction 3  
2. Modeling with Binary Variables I\: Knapsack, Assignment and Matching,  
Covering, Packing and Partitioning 5  
3. Modeling with Binary Variables II\: Facility Location, Fixed-Charge  
Network Flow, and Traveling Salesman 7  
4. Modeling with Binary Variables III\: Nonlinear Functions and  
Disjunctive Constraints 10  
5. Choices in Model Formulation 14  
6. Preprocessing 17  
7. Notes 20  
8. Exercises 22  
**1.2 Linear Programming 27**  
1. Introduction 27  
2. Duality 28  
3. The Primal and Dual Simplex Algorithms 30  
4. Subgradient Optimization 41  
5. Notes 49  
**1.3 Graphs and Networks 50**  
1. Introduction 50  
2. The Minimum-Weight or Shortest-Path Problem 3. The Minimum-Weight Spanning Tree Problem 4. The Maximum-Flow and Minimum-Cut Problems 5. The Transportation Problem\: A Primal-Dual Algorithm 6. A Primal Simplex Algorithm for Network Flow Problems 55  
60  
62  
68  
76  
7. Notes 82  
# 1.4 Polyhedral Theory 83  
1. Introduction and Elementary Linear Algebra 2. Definitions of Polyhedra and Dimension 3. Describing Polyhedra by Facets 4. Describing Polyhedra by Extreme Points and Extreme Rays 83  
85  
88  
92  
5. Polarity 98  
xi xii Contents  
6. Polyhedral Ties Between Linear and Integer Programs 104  
7. Notes 109  
8. Exercises 109  
1.5 Computational Complexity 114  
1. Introduction 114  
2. Measuring Algorithm Efficiency and Problem Complexity 117  
3. Some Problems Solvable in Polynomial Time 121  
4. Remarks on 0-1 and Pure-Integer Programming 125  
5. Nondeterministic Polynomial-Time Algorithms and.H9fJ Problems 127  
6. The Most Difficult *j\{9fJ* Problems\: The Class *j\{9fJcg* 131  
7. Complexity and Polyhedra 139  
8. Notes 142  
9. Exercises 143  
1.6 Polynomial-Time Algorithms for Linear Programming 1. Introduction 146  
2. The Ellipsoid Algorithm 147  
3. The Polynomial Equivalence of Separation and Optimization 4. A Projective Algorithm 164  
5. A Strongly Polynomial Algorithm for Combinatorial Linear Programs 146  
161  
172  
6. Notes 180  
1.7 Integer Lattices 182  
1. Introduction 182  
2. The Euclidean Algorithm 184  
3. Continued Fractions 187  
4. Lattices and Hermite Normal Form 189  
5. Reduced Bases 195  
6. Notes 201  
7. Exercises 202  
PART II. GENERAL INTEGER PROGRAMMING 203  
11.1 The Theory of Valid Inequalities 1. Introduction 205  
2. Generating All Valid Inequalities 3. Gomory's Fractional Cuts and Rounding 4. Superadditive Functions and Valid Inequalities 205  
217  
227  
229  
5. A Polyhedral Description of Super additive Valid Inequalities for  
Independence Systems 237  
6. Valid Inequalities for Mixed-Integer Sets 7. Superadditivity for Mixed-Integer Sets 242  
246  
8. Notes 254  
9. Exercises 256 Contents xiii  
11.2 Strong Valid Inequalities and Facets for Structured Integer Programs 259  
1. Introduction 259  
2. Valid Inequalities for the 0-1 Knapsack Polytope 3. 4. Valid Inequalities for the Symmetric Traveling Salesman Polytope Valid Inequalities for Variable U pper-Bound Flow Models 265  
270  
281  
5. Notes 290  
6. Exercises 291  
11.3 Duality and Relaxation 296  
1. Introduction 296  
2\. Duality and the Value Function 300  
3. Superadditive Duality 304  
4. The Maximum-Weight Path Formulation and Superadditive Duality 308  
5. Modular Arithmetic and the Group Problem 312  
6. Lagrangian Relaxation and Duality 323  
7. Benders'Reformulation 337  
8. Notes 341  
9. Exercises 343  
11.4 General Algorithms 349  
1. Introduction 349  
2. Branch-and-Bound Using Linear Programming Relaxations 355  
3. General Cutting-Plane Algorithms 367  
4. Notes 379  
5. Exercises 381  
11.5 Special-Purpose Algorithms 383  
1. Introduction 383  
2. A Cutting-Plane Algorithm Using Strong Valid Inequalities 386  
3. Primal and Dual Heuristic Algorithms 393  
4. Decomposition Algorithms 409  
5. Dynamic Programming 417  
6. Notes 424  
7. Exercises 427  
11.6 Applications of Special-Purpose Algorithms 433  
1. Knapsack and Group Problems 433  
2. 0-1 Integer Programming Problems 456  
3. The Symmetric Traveling Salesman Problem 469  
4. Fixed-Charge Network Flow Problems 495  
5. Applications of Basis Reduction 513  
6. Notes 520  
7. Exercises 526 xiv Contents  
**PART III. COMBINATORIAL OPTIMIZATION 533**  
**111.1 Integral Polyhedra 535**  
1. Introduction 535  
2. Totally Unimodular Matrices 540  
3. Network Matrices 546  
4. Balanced and Totally Balanced Matrices 562  
5 . Node Packing and Perfect Graphs 573  
6. Blocking and Integral Polyhedra 586  
7. Notes 598  
8. Exercises 602  
**111.2 Matching 608**  
1. Introduction 608  
2. Maximum-Cardinality Matching 611  
3. Maximum-Weight Matching 627  
4. Additional Results on Matching and Related Problems 636  
5. Notes 654  
6. Exercises 655  
**111.3 Matroid and Submodular Function Optimization 659**  
1. Introduction 659  
2. Elementary Properties 662  
3. Maximum-Weight Independent Sets 666  
4. Matroid Intersection 671  
5. Weighted Matroid Intersection 678  
6. Polymatroids, Separation, and Submodular Function Minimization 688  
7. Algorithms To Minimize a Submodular Function 694  
8. Covering with Independent Sets and Matroid Partition 702  
9. Submodular Function Maximization 708  
10. Notes 712  
11. Exercises 714  
# References 721  
# Author Index 749  
**Subject Index 755** Part I  
FOUNDATIONS 1.1  
The Scope of Integer  
and Combinatorial  
# Optimization  
1. INTRODUCTION  
Integer and combinatorial optimization deals with problems of maximizing or minimiz-  
ing a function of many variables subject to \(a\) inequality and equality constraints and  
\(b\) integrality restrictions on some or all of the variables. Because of the robustness of the  
general model, a remarkably rich variety of problems can be represented by discrete  
optimization models.  
An important and widespread area of application concerns the management and  
efficient use of scarce resources to increase productivity. These applications include  
operational problems such as the distribution of goods, production scheduling, and  
machine sequencing. They also include \(a\) planning problems such as capital budgeting,  
facility location, and portfolio analysis and \(b\) design problems such as communication  
and transportation network design, VLSI circuit design, and the design of automated  
production systems.  
In mathematics there are applications to the subjects of combinatorics, graph theory,  
and logic. Statistical applications include problems of data analysis and reliability. Recent  
scientific applications involve problems in molecular biology, high-energy physics, and  
x-ray crystallography. A political application concerns the division ofa region into election  
districts.  
Some of these discrete optimization models will be developed later in this chapter. But  
their number and variety are so great that we only can provide references for some of  
them. The main purpose of this book is to present the mathematical foundations of integer  
and combinatorial optimization models along with the algorithms that can be used to  
solve the problems.  
Throughout most of this book, we assume that the function to be maximized and the  
inequality restrictions are linear. Note that minimizing a function is equivalent to  
maximizing the negative of the same function and that an equality constraint can be  
represented by two inequalities. It is also common to require the variables to be nonnega-  
tive. Hence we write the linear mixed-integer programming problem as  
\(MIP\) max\{cx + hy\: Ax + Gy \~ b, x E Z\~, Y E R\~\},  
where Z\~ is the set of nonnegative integral n-dimensional vectors, R\~ is the set of  
nonnegative realp-dimensional vectors, and x = \(Xl, ... ,xn\) andy = \(Yb ... ,Yp\) are the  
3 4 1.1\. The Scope of Integer and Combinatorial Optimization  
variables or unknowns. An instance of the problem is specified by the data \(c, h, A, G, b\),  
with can n-vector, hap-vector, A an m x n matrix, G an m x p matrix and ban m-vector.  
We do not distinguish between row and column vectors unless the clarity of the presenta-  
tion makes it necessary to do so. This problem is called mixed because of the presence of  
both integer and continuous \(real\) variables.  
We assume throughout the text that all of the data sets are rational, that is, that each of  
the individual numbers is rational. Although in making this assumption we sacrifice some  
theoretical generality, it is a natural assumption for solving problems on a digital computer.  
The set S = \{x E Z\~, Y E R\~, Ax + Gy \:\:\:\:; b\} is called the feasible region, and an  
\(x, y\) E S is called a feasible solution. An instance is said to be feasible if S \* 0. The  
function  
z = cx + hy  
is called the objectivefunction. A feasible point \(XO, yO\) for which the objective function is  
as large as possible, that is,  
cxO + hyo \~ cx + hy for all \(x, y\) E S,  
is called an optimal solution. If \(XO, yO\) is an optimal solution, cxo + hyo is called the  
optimal value or weight of the solution.  
A feasible instance ofMIP may not have an optimal solution. We say that an instance is  
unbounded if for any OJ E R 1 there is an \(x, y\) E S such that cx + hy \> OJ • We use the  
notation z = 00 for an unbounded instance.  
In Section 1.4.6, we will show that every feasible instance ofMIP either has an optimal  
solution or is unbounded. This result requires the assumption of rational data. With  
irrational data, it is possible that no feasible solution attains the least upper bound on the  
objective function.  
Thus to solve an instance ofMIP means to produce an optimal solution or to show that  
it is either unbounded or infeasible.  
The linear \(pure\) integer programming problem  
\(IP\) max\{cx\: Ax \:\:\:\:; b, x E Z\~\}  
is the special case of MIP in which there are no continuous variables. The linear  
programming problem  
\(LP\) max\{hy\: Gy \:\:\:\:; b, y ERn  
is the special case of MIP in which there are no integer variables.  
In many models, the integer variables are used to represent logical relationships and  
therefore are constrained to equal 0 or 1. Thus we obtain the 0-1 MIP \(respectively 0-1 IP\)  
in which x E Z\~ is replaced by x E Bn, where Bn is the set of n-dimensional binary  
vectors.  
While there is no generally agreed-upon definition of a combinatorial optimization  
problem, most problems so named are 0-1 IPs that deal with finite sets and collections of  
subsets. The following is a generic combinatorial optimization problem. Let N =  
\{l, ... , n\} be a finite set and let c = \(Cb ... , cn\) be an n-vector. For F £; N, define c\(F\) =  
LjEF Cj. Suppose we are given a collection of subsets @P of N. The combinatorial optimiza-  
tion problem is 2. Modeling with Binary Variables I 5  
\(CP\) max\{c\(F\)\:F E BF\}.  
Some examples of combinatorial optimization problems will be given later in this chapter.  
This book is divided into three parts. This chapter is concerned with the formulation of  
integer optimization problems, which means how to translate a verbal description of a  
problem into a mathematical statement of the form MIP, Ip, or CPo The rest of Part I  
contains prerequisites, including linear programming, graphs and networks, polyhedral  
theory, and computational complexity, which are necessary for Parts II and III.  
Part II is concerned with the theory and algorithms for problems IP and MIP. Part III is  
devoted to some combinatorial optimization problems whose structure makes them  
relatively easy to solve.  
2. MODELING WITH BINARY VARIABLES I\: KNAPSACK, ASSIGNMENT  
AND MATCHING, COVERING, PACKING AND PARTITIONING  
An important and very common use of 0-1 variables is to represent binary choice.  
Consider an event that mayor may not occur, and suppose that it is part of the problem to  
decide between these two possibilities. To model such a dichotomy, we use a binary  
variable x and let  
I if the event occurs  
\{  
x = 0 if the event does not occur.  
The event itself may be almost anything, depending on the specific situation being  
considered. Several examples follow.  
The 0-1 Knapsack Problem  
Suppose there are n projects. Thejth project,\} = 1, ... , n, has a cost of aj and a value of Cj.  
Each project is either done or not, that is, it is not possible to do a fraction of any of the  
projects. Also there is a budget of b available to fund the projects. The problem of choosing  
a subset of the projects to maximize the sum of the values while not exceeding the budget  
constraint is the 0-1 knapsack problem  
Here the jth event is the \}th project. This problem is called the knapsack problem because  
of the analogy to the hiker's problem of deciding what should be put in a knapsack, given a  
weight limitation on how much can be carried. In general, problems of this sort may have  
several constraints. We then refer to the problem as the multidimensional knapsack  
problem.  
The Assignment and Matching Problems  
Another classical problem involves the assignment of people to jobs. Suppose there are n  
people and m jobs, where n \~ m. Each job must be done by exactly one person; also, each  
person can do, at most, one job. The cost of person\} doing job i is c ij. The problem is to  
assign the people to the jobs so as to minimize the total cost of completing all of the jobs.  
To formulate this problem, which is known as the assignment problem, we introduce 0-1 6 1.1\. The Scope of Integer and Combinatorial Optimization  
variables Xij, i = 1, ... , m,\} = 1, ... ,n corresponding to the ijth event of assigning  
person\} to job i. Since exactly one person must do job i, we have the constraints  
\(2.1\)  
n  
\)=1  
I Xi\) = 1 for i = 1, ... , m.  
Since each person can do no more than one job, we also have the constraints  
\(2.2\)  
m  
i=1  
I Xi\) \~ 1 for\} = 1, ... , n.  
It is now easy to check that if X E Bmn satisfies \(2.1\) and \(2.2\), we obtain a feasible solution  
to the assignment problem. The objective function is min L\~I L\}=I cijxij.  
In the assignment problem the m + n elements are partitioned into disjoint sets of jobs  
and people. But in other models of this type, we cannot assume such a partition. Suppose  
2n students are to be assigned to n double rooms. Here each student must be assigned  
exactly one roommate. Let the ijth event, i \<\}, correspond to assigning students i and\}  
to the same room; also suppose that there is a value of c ij when students i and\} are  
roommates. The problem  
\(2.3\) \{max2I1 I CijXij\: I Xki + I Xij = 1, i = 1, ... , 2n, X E Bn\(2n-I\)\}  
i=i \)=i+i k\<i j\>i  
is known as the perfect matching problem. We will see later that it is a generalization of the  
assignment problem. If the equality constraints in \(2.3\) are replaced by equal-to-or-Iess-  
than inequalities, then the problem is called the matching problem.  
Each of the above problems fits into the context of CPO In the knapsack problem, N =  
\{l, ... , n\} and F E\:JP if and only if LjEF aj \~ b. In the assignment problem, N = \{ij\: i =  
1, ... , m,\} = 1, ... , n\} and F E \:JP if and only if IF n \{i 1, ... , in\} I = 1 for all i and  
IF n \{l\}" ... ,m\}\} I \:\:\:\:; 1 for all\}.  
Set-Covering, Set-Packing, and Set-Partitioning Problems  
A common way of defining gji leads to important classes of combinatorial optimization  
problems known as set-covering, set-packing, and set-partitioning problems. Let M =  
\{l, ... , m\} be a finite set and let \{M\) for j EN = \{l, ... , n\} be a given collection of  
subsets of M. For example, the collection might consist of all subsets of size  
k, for some k \~ m. We say that F \~ N covers M if UjEF M j = M. In the CP known as the  
set-covering problem, gji = \{F\: F covers M\}. We say that F \~ N is a packing with respect to  
M if M j n Mk = 0 for all\}, kEF,\} \* k. In the CP known as the set-packing problem,  
gji = \{F\: F is a packing with respect to 1\\1\}. If F \~ N is both a covering and a packing, then  
Fis said to be a partition of M. In the set-covering problem, c\) is the cost of M\) and we seek  
a minimum-cost cover; in the set-packing problem, however, Cj is the weight or value of M j  
and we seek a maximum-weight packing.  
These problems are readily formulated as 0-1 IPs. LetA be the m x n incidence matrix  
of the family \(M\) for\} EN; that is, for i EM,  
I if\} E F  
\{  
Xj = 0 if\} \$. F. 3\. Modeling with Binary Variables II 7  
Then F is a cover \(respectively packing, partition\) if and only if x E En satisfies  
Ax \~ 1 \(respectively Ax \~ 1, Ax = 1\), where 1 is an m-vector all of whose components  
equal 1. We see, for example, that the set-packing problem is the special case of the 0-1 IP  
withA a 0-1 matrix \(i.e., a matrix all of whose elements equal 0 or 1\) and b = 1. Note that an  
assignment problem with m jobs and m people is a set-partitioning problem in which  
M = \{l, ... ,m, m + 1, ... ,2m\} and Mj for\} = 1, ... , m2 is a subset of M consisting of  
one job and one person.  
Many practical problems can be formulated as set-covering problems. A typical  
application concerns facility location. Suppose we are given a set of potential sites N = \{I,  
... , n\} for the location offire stations. A station placed at\} costs Cj. We are also given a set  
of communities M = \{I, ... , m\} that have to be protected. The subset of communities  
that can be protected from a station located at\} is M j • For example, M j might be the set of  
communities that can be reached from\} in 10 minutes. Then the problem of choosing a  
minimum-cost set of locations for the fire stations such that each community can be  
reached from some fire station in 10 minutes is a set-covering problem. There are many  
other applications of this type, including assigning customers to delivery routes, airline  
crews to flights, and workers to shifts.  
3. MODELING WITH BINARY VARIABLES II\: FACILITY LOCATION,  
FIXED-CHARGE NETWORK FLOW, AND TRAVELING SALESMAN  
The set-packing, set-partitioning, and set-covering models of the previous section illus-  
trated how we can use linear constraints on binary variables to represent relationships  
among the variables or the events that they represent. A packing constraint, Lj Xj \~ 1,  
states that at most one of a set of events is allowed to occur. Similarly, covering and  
partitioning constraints state, respectively, that at least one and exactly one of the events  
can occur. Here we show how more complex relationships can be modeled with binary  
variables, and we also formulate some models that use these relationships.  
The relation that neither or both events 1 and 2 must occur is represented by the linear  
equality X2 - XI = 0 in the binary variables Xl and X2. Similarly, the relation that event 2  
can occur only if event 1 occurs is represented by the linear inequality X2 - Xl \~ O. More  
generally, consider an activity that can be operated at any level y, 0 \~ y \~ u. Now suppose  
that the activity can be undertaken only if some event represented by the binary variable x  
occurs. This relation is represented by the linear inequality y - ux \~ 0 since X = 0 implies  
y = 0 and x = 1 yields the original constraint y \~ u. We now consider two models that use  
this relationship.  
Facility Location Problems  
These problems, as does our illustration of the set-covering model, concern the location of  
facilities to serve clients economically. We are given a set N = \{I, ... , n\} of potential  
facility locations and a set of clients 1= \{l, ... , m\}. A facility placed at\} costs Cj for\} EN.  
This problem is more complicated than the set-covering application because each client  
has a demand for a certain good, and the total cost of satisfying the demand of client i from  
a facility at\} is hi\). The optimization problem is to choose a subset of the locations at which  
to place facilities and then to assign the clients to these facilities so as to minimize total  
cost. In the uncapacitated facility location problem, there is no restriction on the number  
of clients that a facility can serve.  
In addition to the binary variable Xj = 1, if a facility is placed at\} and Xj = 0 otherwise,  
we introduce the continuous variable Yij, which is the fraction of the demand of client i 8 1.1\. The Scope of Integer and Combinatorial Optimization  
that is satisfied from a facility at \}. The condition that each client's demand must be  
satisfied is given by  
\(3.1\) L Yij = 1 for i E I.  
JEN  
Moreover, since client i cannot be served from\} unless a facility is placed at\}, we have the  
constraints  
\(3.2\) Yij - Xj\:\:\:\:\:; 0 for i E I and\} EN.  
Hence the uncapacitated facility location problem is the MIP  
min L CjXj + L L hijYij  
JEN iEI JEN  
subject to the constraints \(3.1\), \(3.2\) and x E Bn , Y E R,\:n.  
It may be unrealistic to assume that a facility can serve any number of clients. Suppose  
a facility located at\} has a capacity of Uj and the ith client has a demand of bi. Now we let  
Y ij be the quantity of goods sent from facility\} to client i and let h ij be the shipping cost per  
unit. To formulate the capacitated/acility location problem as an MIP, we replace \(3.1\) by  
\(3.3\)  
I Yij = hi for i E I,  
JEN  
and \(3.2\) by  
\(3.4\)  
L Yij - UjXj\:\:\:\:\:; 0 for\} EN.  
iEI  
The Fixed-Charge Network Flow Problem  
We are given a network \(see Figure 3.1\) with a set of nodes V \(facilities\) and a set of arcs d.  
An arc e = \(i,\}\) that points from node i to node\} means that there is a direct shipping  
route from node i to node\}. Associated with each node i, there is a demand hi. Node i is a  
demand, supply, or transit point depending on whether bi is, respectively, positive,  
negative, or zero. We assume that the net demand is zero, that is, LiEV b i = O. Each arc \(i,\}\)  
has a flow capacity U ii and a unit flow cost h ij.  
Let Y ij be the flow on arc \(i, i\). A flow is feasible if and only if it satisfies  
\(3.5\)  
Y E R'11  
\(3.6\)  
\(3.7\)  
Yo\:\:\:\:\:; uij for \(i,\}\) Ed  
L Yji - L Yij = bi for i E V.  
JEV JEV  
The constraints \(3.7\) are the flow conservation constraints. The problem  
\(3.8\) min\{ L hijYij\: Y satisfies \(3.5\), \(3.6\) and \(3.7\)\}  
\(i,j\)Esd  
is known as the networkflow problem. It will be discussed in Chapter 1.3. 3\. Modeling with Binary Variables II 9  
e = \(i,j\) j  
Figure 3.1  
The fixed-charge network flow problem is obtained by imposing a fixed cost of c ij if  
there is positive flow on arc \(i,\}\). Now we introduce a binary variable xij to indicate  
whether arc \(i,\}\) is used. The constraint Yij = 0 if Xij = 0 is represented by  
\(3.9\) Yij - uijxij \~ 0 for \(i,\}\) E stl.  
Hence we obtain the formulation  
\(3.10\) min\{ L \(cijxij + hijYij\)\: x E Bldl  
, Y E R'\:I satisfies \(3.7\), \(3.9\)\}.  
\(i,j\)Ed  
The fixed-charge flow model is useful for a variety of design problems that involve  
material flows in networks. These include water supply systems, heating systems, and road  
networks.  
The formulations of the traveling salesman problem given below provide another  
example of the use of binary variables in the modeling of logical relations. They also  
exhibit another important property of integer programming formulations, namely, that it  
may be appropriate to use an extraordinarily large number of constraints in order to  
obtain a good formulation.  
The Traveling Salesman Problem  
We are again given a set of nodes V = \{l, ... ,n\} and a set ofarcsstl. The nodes represent  
cities, and the arcs represent ordered pairs of cities between which direct travel is possible.  
For \(i,\}\) E stl, C ij is the direct travel time from city i to city\}. The problem is to find a tour,  
starting at city 1, that \(a\) visits each other city exactly once and then returns to city 1 and  
\(b\) takes the least total travel time.  
To formulate this problem, we introduce variables x ij = 1 if\} immediately follows i on  
the tour, x ij = 0 otherwise. Hence  
\(3.11\)  
The requirements that each city is entered and left exactly once are stated as  
\(3.12\) 2\: xij = 1 for\} E V  
u\: \(i,j\)Ed\) 10 1.1\. The Scope of Integer and Combinatorial Optimization  
and  
\(3.13\) I Xu = 1 for i E V.  
U\:\(i,j\)Ed\)  
The constraints \(3.11\)-\(3.13\) are not sufficient to define the tours since they are also  
satisfied by subtours; for example for n = 6, X12 = X23 = X31 = X4S = XS6 = X64 = 1 satisfies  
\(3.11\)-\(3.13\) but does not correspond to a tour \(see Figure 3.2\).  
One way to eliminate subtours is to observe that in any tour there must be an arc that  
goes from \{l, 2, 3\} to \{4, 5, 6\} and an arc that goes from \{4, 5, 6\} to \{l, 2, 3\}. In general, for  
any V C V with 2 \~ I VI \~ I VI - 2, the constraints  
\(3.14\) I Xu \~ 1  
\{\(i,j\)Ed\: iEU,jEV\\U\)  
are satisfied by all tours, but every subtour violates at least one of them. Hence the  
traveling salesman problem can be formulated as  
\(3.15\) min\{ I CijXU\: x satisfies \(3.1l\)-\(3.14\)\}.  
\(i,j\)Ed  
An alternative to the set of constraints \(3.14\) is  
\(3.16\) I x u \~ I V I - 1 for 2 \~ I V I \~ I V I - 2,  
\{\(i,j\)Ed\: iEU,jEU\}  
which also excludes all subtours but no tours.  
However, regardless of whether we use \(3.14\) or \(3.16\), the number of these constraints  
is nearly 21Vl. This huge number of constraints might motivate us to seek a more compact  
formulation. In fact, we will give such a formulation in Section 1.1.5. But we will argue that  
the compact formulation is inferior and we will show, in Parts II and III, that a very large  
number of constraints can frequently be handled successfully.  
4. MODELING WITH BINARY VARIABLES III\: NONLINEAR FUNCTIONS  
AND DISJUNCTIVE CONSTRAINTS  
In this section, we present two important uses of binary variables in the modeling of  
optimization problems. The first concerns the representation of nonlinear objective  
functions of the form Lj !j\(Yj\) using linear functions and binary variables. The second  
concerns the modeling of disjunctive constraints. In the usual statement of an optimiza-  
tion problem, it is assumed that all of the constraints must be satisfied. But in some  
applications, only one ofa pair \(or, more generally, k ofm\) constraints must hold. In this  
case, we say that the constraints are disjunctive.  
4  
\~----------------\~ 6  
Figure 3.2 4\. Modeling with Binary Variables III 11  
Piecewise Linear Functions  
A function of the formf\(Yb ... ,Yp\) = ''II\}=I jj\(yJ is said to be a separable function. Here we  
consider separable objective functions and suppose thatjj\(Yj\) is piecewise linear for each\}  
\(see Figure 4.1\). Note that an arbitrary continuous function of one variable can be  
approximated by a piecewise linear function, with the quality of the approximation being  
controlled by the size of the linear segments.  
Suppose we have a piecewise linear function f\(y\) specified by the points  
\(ai,f\(ai\)\} for i = 1, ... ,r. Then, any al \~ Y \~ ar can be written as  
r r  
Y = I Ai ai, I Ai = 1, A = \(AI, ••• , Ar\) E R\:.  
i=1 i=1  
The Ai are not unique, but if ai \~ Y \~ ai+1 and A is chosen so that Y = Aiai + Ai+lai+1 and  
Ai + Ai+1 = 1, then we obtainf\(y\) = A!\(aJ + Ai+1 f\(ai+I\)' In other words,  
\(4.1\) r r  
fey\) = I Aif\(ai\), I Ai = 1, A E R\:  
i=1 i=1  
if at most two of the A/S are positive and if Aj and Ak are positive, then k = \} - 1 or\} + 1.  
This condition can be modeled using binary variables Xi for i = 1, ... , r - 1 \(where  
Xi = 1 if ai \~ Y \~ ai+1 and Xi = 0 otherwise\) and the constraints  
Al \~XI  
Ai \~ X i-I + Xi for i = 2, . . . , r - 1  
\(4.2\)  
X E B r  
I  
-  
•  
Note that if xi = 1, then ;\\ = 0 for i =1= \{j,j + I\}.  
Piecewise linear functions that are convex \(concave\) can be minimized \(maximized\) by  
linear programming because the slope of the segments are increasing \(decreasing\) \(see  
Figure 4.2\). But general piecewise linear functions are neither convex nor concave, so  
binary variables are needed to select the correct segment for a given value of y.  
f\(y\)  
\~--------\~----\~----\~------\~--\~------\~y  
Figure 4.1 12 1.1\. The Scope of Integer and Combinatorial Optimization  
fey\)  
\~----------------------------------\~y  
Figure 4.2. A convex piecewise linear function.  
Disjunctive Constraints  
Disjunctive constraints arise naturally in many models. A simple illustration is when we  
need to define a variable equal to the minimum of two other variables, that is,  
y = min\(ul' U2\). This can be done with the two inequalities  
together with one of two inequalities  
A typical disjunctive set of constraints states that a point must satisfy at least k of m sets  
of linear constraints. The case of k = 1, m = 2 is shown in Figure 4.3, where the feasible  
region is shaded.  
Suppose pi = \{y ER\~\:Aiy \~ bi, y \~ d\} for i = 1, ... , m. Notethatthereisa vectorw  
such that, for all i, A iy \~ bi + w is satisfied for any y, 0 \~ y \~ d. Hence there is a y  
contained in at least k of the sets pi if and only if the set  
\(4.3\)  
y\~d  
Figure 4.3 4\. Modeling with Binary Variables III 13  
is nonempty. This follows since Xi = 1 yields the constraint A iy \~ bi while Xi = 0 yields the  
redundant constraints A iy \~ bi + ro.  
When k = 1, an alternative formulation is  
A iyi \~ Xibi for i = 1, ... , m  
yi \~ X id for i = 1, ... , m  
\(4.4\)  
X E Bm, y E R\~, yi E R\~ for i = 1, ... ,m.  
Now we claim that UZ!,l pi =1= 0 if and only if\(4.4\) is nonempty. First, given thaty E UZ!,l pi,  
suppose without loss of generality that y E pl. Then a solution to \(4.4\) is Xl = 1, Xi = 0  
otherwise, yl = y, and yi = 0 otherwise. On the other hand, suppose \(4.4\) has a solution  
and, without loss of generality, suppose Xl = 1 and Xi = 0 otherwise. Then we obtain  
yi = 0 for i = 2, ... , m and y = yl. Thus y E pl and UZ!,l pi.=I= 0.  
The models \(4.3\) and \(4.4\) are quite different formulations of the same problem. This  
choice of formulation is typical. A significant issue to be discussed in the next section is  
what constitutes a good formulation?  
A Scheduling Problem  
Disjunctive constraints arise naturally in scheduling problems where several jobs have to  
be processed on a machine and where the order in which they are to be processed is not  
specified. Thus we obtain disjunctive constraints of the type either "job k precedesjob\} on  
machine i" or vice versa.  
Suppose there are n jobs and m machines and each job must be processed on each  
machine. For each job, the machine order is fixed, that is, job\} must first be processed on  
machine\}\(l\) and then on machine\}\(2\), and so on. A machine can only process one job at  
a time, and once a job is started on any machine it must be processed to completion. The  
objective is to minimize the sum of the completion times of all the jobs. The data that  
specify an instance of the problem are \(a\) m, n, andpij for\} = 1, ... ,n and i = 1, ... ,m,  
which is the processing time of job\} on machine i, and \(b\) the machine order, \}\(l\), ... ,  
\}\(m\), for each job\}.  
Let tij be the start time of job\} on machine i. Since the \(r + l\)stoperation onjob\) cannot  
start until the rth operation has been completed, we have the constraints  
\(4.5\) t\}\(r+l\),\} \~ t\}\(r\),\} + P\}\(r\),\} for r = 1, ... , m - 1 and all\}.  
To represent the disjunctive constraints for jobs\} and k on machine i, let X ilk = 1 if job\}  
precedes job k on machine i andxi\}k = 0 otherwise where\} \< k. Thus 14 and  
1.1\. The Scope of Integer and Combinatorial Optimization  
Given an upper-bound OJ on tij - tik + Pij for all i, j, and k, we obtain the disjunctive  
constraints  
\(4.6\) tij - tik \~ -Pij + W\(l-Xijk\)  
tik - tij \~ -Pik + OJXijk for all i,j and k.  
Hence the problem is to minimize L\}=l tj\(m\),j subject to \(4.5\), \(4.6\), tij \~ 0 for all i andj and  
Xijk E \{a, 1\} for all i,j, and k.  
This model requires m G\) binary variables. In contrast to the integer programming  
models introduced previously, this mixed-integer programming model has not been  
successfully solved for values of m and n that are of practical interest. This formulation,  
which is based on \(4.3\), is cumbersome partly because of the large number of binary  
variables needed to represent the large number of disjunctions. Note that a formulation  
based on \(4.4\) would also have a large number of binary variables. In fact, a large number  
of binary variables may be unavoidable for this scheduling problem.  
Good formulations are essential to solving integer programming problems efficiently.  
In the next section, we will give some reasons why some formulations may be better than  
others; we will also suggest how formulations can be improved.  
5\. CHOICES IN MODEL FORMULATION  
We have formulated several integer optimization problems in this chapter to motivate the  
richness and variety of applications. Although a formulation may give insight into the  
structure of the problem, our goal is to solve the problem for an optimal or nearly optimal  
solution. As we have already indicated, most integer programming problems can be  
formulated in several ways. Moreover, in contrast to linear programming\:  
In integer programming, formulating a "good" model is of crucial importance to  
solving the model.  
Indirectly, the subject of "good" model formulation is a major topic of this book and is  
closely related to the algorithms themselves \(see Chapters 11.2 and 11.5\).  
A model is specified by the variables, objective function, and constraints. Typically,  
defining the variables is the first question addressed in formulating a model. Often the  
variables are chosen simply from the definition of a solution. That is a solution specifies  
the values of certain unknowns, and we define a variable for each unknown. Once the  
variables and an objective function have been defined, say in an IP, we can speak of an  
implicit representation of the problem  
max\{cx\: xES C Z\~\},  
where S represents the set of feasible points in Z\~. Now we say that 5. Choices in Model Formulation 15  
max\{ex\: Ax \~ b, X E Z\~\}  
is a valid IP formulation if S = \{x E Z1\: Ax \~ b\}.  
In general, when there is a valid formulation, there are many choices of \(A, b\), and it is  
usually easy to find some \(A, b\) that yields one. But an obvious choice may not be a good  
one when it comes to solving the problem. We believe that the most important aspect of  
model formulation is the choice of \(A, b\).  
The following example illustrates different representations of an S \~ Z1 by linear  
inequality and integrality restrictions.  
Example 5.1  
S = \{\(OOOO\), \(l000\), \(0100\), \(0010\), \(0001\), \(0110\), \(0101\), \(001l\)\} \~ B4.  
The reader can easily check that  
gives a valid formulation. Two other formulations that are easily established to be valid  
\(a\)  
are\:  
\(b\) \(c\) S = \{x E B4\: 2XI + X2 + X3 + X4 \~ 2\}  
S = \{x E B4\: 2XI + X2 + X3 + X4 \~ 2  
Xl + X2 \~  
Xl + X3 \~  
Xl + X4 \~ 1\}.  
We will see that, in a certain sense, formulation \(b\) is better than \(a\), and \(c\) is better than  
\(b\).  
How should we compare different formulations? Later we will see that most integer  
programming algorithms require an upper bound on the value of the objective function,  
and the efficiency of the algorithm is very dependent on the sharpness of the bound. An  
upper bound is determined by solving the linear program  
ZLP = \{max ex\: Ax \~ b, X E R1\}  
since P = \{x E R1 \:Ax \~ b\} 2 S. Now given two valid formulations, defined by \(A i, bi\)  
for i = 1, 2, let pi = \{X E R1\:A iX \~ bi\} and zLp = max\{ex\: X E Pi\}. Note that if pI \~ p2,  
then z Lp \~ Z\[p. Hence we get the better bound from the formulation based on \(A I, b I\) and  
we say that it is the better formulation. We leave it to the reader to check that in Example  
5.1, formulation \(c\) gives a better bound than \(b\), which, in turn, gives a better bound than  
\(a\).  
A striking example of one formulation being better than another, in the sense just  
described, is provided by the uncapacitated facility location problem. We obtain a  
formulation with fewer constraints than the one given in Section 3 by replacing \(3.2\) with  
\(5.1\) 2 Yij - mXj \~ 0 for all\} EN.  
iEI 16 1.1\. The Scope of Integer and Combinatorial Optimization  
When Xj = 0, \(5.1\) says that no clients can be served from facility\); and when Xj = 1, there  
is no restriction on the number of clients that can be served from facility\). In fact, by  
summing \(3.2\) over i E I for each\), we obtain \(5.1\). Although with x E Bn  
, \(3.2\) and \(5.1\)  
give the same set offeasible solutions, with x E R\~, \(3.2\) gives a much smaller feasible set  
than \(5.1\). Our ability to solve the formulation with \(3.2\) is remarkably better than with  
the more compact formulation that uses \(5.1\).  
We belabor this point because it is instinctive to believe that computation time  
increases and computational feasibility decreases as the number of constraints increases.  
But, trying to find a formulation with a small number of constraints is often a very bad  
strategy. In fact, one of the main algorithmic approaches involves the systematic addition  
of constraints, known as cutting planes \(see Part II\).  
A nice illustration of the suitability of choosing \(A, b\) with a very large number of rows  
concerns the traveling salesman problem. In Section 3, we gave two different sets of  
constraints, \(3.14\) and \(3.16\), for eliminating subtours. Both formulations contain a huge  
number of constraints, far too many to write down explicitly. Nevertheless, algorithms for  
the traveling salesman problem that solve these formulations have been successful on  
problems with more than 2000 cities. On the other hand, there is a more subtle way of  
eliminating subtours that only requires a small number of constraints.  
Let U E Rn  
-  
l and consider the constraints  
\(5.2\)  
Ifx E Bldl satisfies \(3.12\) and \(3.13\) and does not represent a tour, then x represents at least  
two subtours, one of which does not contain node 1. By summing \(5.2\) over the arc setd'  
of some subtour that does not contain node 1, we obtain  
\(5.3\) I Xij \~ 1.91' I· \(1 - lIn\).  
\(i,j\)Ed'  
Thus \(5.2\) excludes all subtours that do not contain node 1 and hence excludes all  
solutions that contain subtours.  
Now we prove that no tours are excluded by \(5.2\) by showing that for any tour there  
exists a corresponding U satisfying \(5.2\). In particular, we set Uj = k, where k is the position  
\(2 \~ k \~ n\) of node i in the tour. Now if xij = 0, Uj - Uj + nXij \~ n - 2, while if  
xij = 1, Uj = k and Uj = k + 1 for some k, and so Uj - Uj + nXij = n - 1. Hence \{x E Bldl\: x  
satisfies \(3.12\), \(3.13\), and \(5.2\)\} is the set of incidence vectors of tours.  
Now let pI = \{x E Rifl\: x satisfies \(3.12\), \(3.13\), \(3.16\)\} and p2 = \{x E Rif'\: x satisfies  
\(3.12\), \(3.13\), and \(5.2\) for some u\}. It is easy to see that p2 \$\: pl. For example, if n \~ 4,  
then U2 = U3 = U4 = 0 and X23 = X34 = X42 = \(n - l\)/n \> j satisfies \(5.2\) but not \(3.16\). In  
fact, it can be shown that pI \~ p2.  
We have emphasized the choice of constraints in obtaining a good formulation, given  
that the variables have already been defined, because for most problems this is the part of  
the formulation where there is the greatest freedom of choice. There are, however,  
problems in which the quality of the formulation depends on the choice of variables.  
In our formulation of network flow problems, we defined the variables to be the arc  
flows. However, in certain situations it is more advantageous to define variables that  
represent the flow on each path between two given nodes. Such a formulation involves  
many more variables but eliminates the need for some flow conservation constraints and  
can be preferable for finding integral solutions.  
We now give two radically different formulations of a production lot-sizing problem  
that depend on the choice of variables. The object is to minimize the sum of the costs of 6. Preprocessing 17  
production, storage, and set-up, given that known demands in each of T periods must be  
satisfied. For t = 1, ... , T, let dt be the demand in period t, and let Ct, Pt,and ht be the set-  
up, unit production, and unit storage costs, respectively, in period t.  
One formulation is obtained by defining Y t, S t as the production and end storage in  
period t and by defining a binary variable x t, indicating whether Y t \> 0 or not. This leads to  
the model  
\(5.4\)  
T  
minI \(PIYt + htS t + CtXt\)  
t=1  
Yl=d1 +S1  
St-I+Yt=dt+st fort=2, ... , T  
for t = 1, ... , T  
where w = 'LT\:\:1 dt is an upper bound on Y t for all t.  
A second possibility is to define q it as the quantity produced in period i to satisfy the  
demand in period t \~ i, and X t as above. Now we obtain the model  
\(5.5\)  
T t T  
minI I \(Pi + hi + hi+l + ... + ht- 1\)qit + I CtXt  
\(=1 i=1 \(=1  
for t = 1, ... , T  
qit \~ dtXi for i = 1, ... , T and t = i, ... , T  
In \(5.5\) if we replace x E BT by 0 \~ X t \~ 1 for all t, then the resulting linear program-  
ming problem has an optimal solution with x E jjT. But this is not necessarily the case  
for \(5.4\), which is the inferior formulation for soliving the problem by certain integer  
programming techniques. It is interesting to observe that \(5.5\) is a special case of the  
uncapacitated facility location problem. This can be seen by substituting Yit = q it/dt for all  
i and t \~ i.  
There is a similar result for the formulations \(4.3\) and \(4.4\) for finding a point that  
satisfies one of m sets oflinear constraints. In \(4.4\), one can replace the condition x E Bm  
with 0 \~ x \~ 1 and use linear programming to find a point in one of the pi. But this is not  
true for \(4.3\), which is therefore considered to be the inferior formulation.  
6\. PREPROCESSING  
Given a formulation, preprocessing refers to elementary operations that can be performed  
to improve or simplify the formulation by tightening bounds on variables, fixing values,  
and so on. Preprocessing can be thought of as a phase between formulation and solution. It  
can greatly enhance the speed of a sophisticated algorithm that might, for example, be  
unable to recognize the fact that some variable can be fixed and then eliminated from the  
model. Occasionally a small problem can be solved in the preprocessing phase or by 18 1.1\. The Scope of Integer and Combinatorial Optimization  
combining preprocessing with some enumeration. Although this approach had been  
advocated as a solution technique in the early development of integer programming,  
under the name of implicit enumeration, this is not the important role of these simple  
techniques. Their main purpose is to prepare a formulation quickly and automatically for  
a more sophisticated algorithm. Unfortunately, it has taken a long time for researchers to  
recognize the fact that there is generally a need for both phases in the solution of practical  
problems.  
Tightening Bounds  
We have seen that a common constraint in MIPs is Yj \~ Ujxj, where Uj is an upper bound  
on Yj and Xj is a binary variable. Provided thatxj E CO, 1\}, the tightness of the upper bound  
doesn't matter. But if we replace Xj E CO, 1\} by 0 \~ Xj \~ 1, it becomes important to have a  
tight bound. Suppose, for example, that the largest feasible value of Yj is u; \< U j and that  
there is a fixed costjj \> 0 associated with Xj' If Yj = u; in an optimal solution, and we use  
the constraint Yj \~ Ujxj, we will obtain Xj = u;/Uj \< 1. On the other hand, if we use the  
constraint Yj \~ U ;Xj' we obtain Xj = 1.  
In some cases, good bounds can be determined analytically. For example, in the lot-  
sizing problem, rather than using a common bound for each Y t, it is more efficient to use  
the bounds Yt \~ Cr.!\:\:t dJxt. In general, tight bounds can be determined by solving a linear  
program with the objective of maximizing Yj. Doing this for each variable with an upper  
bound constraint may be prohibitively time consuming, so a good compromise is to  
approximate the upper bounds heuristically.  
Example 6.1. We show a fixed-charge model in Figure 6.1 with the accompanying  
formulation\:  
= 1.46  
= 0.72  
- Y2 - Y3 + Ys =0  
Y6 = 0.32  
- Ys - Y6 + Y7 = 0  
where \(j\) is a large positive number because the arcs do not have capacity constraints.  
It is easy to tighten the bounds, giving  
Yl \~ 1.46xI, Y2 \~ 1.46x2  
Y3 \~ 0.72x3, Y4 \~ 0.72x4  
Ys \~ \(1.46 + 0.72\)xs  
Y6 = 0.32  
Y7 \~ \(1.46 + 0.72 + 0.32\)X7'  
In addition, we can set X6 = X7 = 1 because the flow into node 7 must use these arcs. 6\. Preprocessing 19  
1.46  
0.72  
Y2  
Y3  
3\~--\~  
0.32  
Figure 6.1  
Adding Logical Inequalities, Fixing Variables, and Removing Redundant Constraints  
Preprocessing of this sort is most useful for binary IPs. Consider a single inequality in  
binary variables, that is, S = \{x E Bn\: LjEN ajxj \:\:S; b\}. If aj \< 0, we can replace Xj by 1 - x;  
and obtain the constraint LjEN\:aj\>o ajxj + LjEN\:aj\<O I aj I xj \~ b - LjEN\:aj\<O aj. Thus without  
loss of generality, we can assume that aj \> ° for j EN. Now if LjEC aj \> b for C \~ N, we  
obtain the inequality  
\(6.1\) I Xj\:\:S; ICI - 1.  
jEe  
Obviously, the best inequalities of this type are obtained when LjEC\\\{k\} aj \:\:S; b for all k E C.  
Once some inequalities of this type have been obtained, it may be possible to combine  
some of them to fix variables. For example, XI + X2 \:\:S; 1 andxl + \(l - X2\) \~ 1 yield XI = 0.  
The application of these simple ideas is easy to see by considering an example.  
Example 6.2  
3x\~ + 2x\~ \:\:S; 3\)  
-4xl - 3xz - 3X3 \~ -6 \(4xi + 3xz + 3X3 \~ 4\)  
2Xl - 2Xl + 6X3\:\:S; 5 \(2Xl + 2Xl + 6X3 \~ 7\)  
xEB3.  
The first constraint yields Xl + x\~ \~ 1 or Xz + X3 \~ 1. The third constraint yields  
Xl + X3 \~ 1 or X3 \~ Xl. Combining these two yields Xl = 1. Now the first constraint is  
redundant and the second and third reduce to 4x\~ + 3X3 \~ 4 and 2xr + 6X3 \< 7. From  
these two, we obtain Xl + X3 \:\:S; 1 and Xl + X 3 \:\:S; 1, or Xl + X 3 = 1. Thus, by substitution, we  
can eliminate either X 1 or X 3. 20 1.1\. The Scope of Integer and Combinatorial Optimization  
Other simplifications of this type are considered as exercises.  
A second stage of preprocessing can be carried out after an upper bound has been  
obtained by linear programming. In particular, variables can be fixed by using the reduced  
prices that are obtained from a linear programming solution \(see Section 11.5.2\).  
7. NOTES  
Section 1.1.1  
Here we list bibliographies, other books, proceedings, and some of the main journals that  
contain a great deal of material on integer programming and/or combinatorial optimiza-  
tion. Four volumes of comprehensive bibliographies on integer programming have been  
prepared at Bonn University \[see Kastning \(1976\), Hausmann \(1978\) and von Randow  
\(1982, 1985\)\]. Each volume contains an alphabetical listing by authors, a subject classifica-  
tion, and a third part that enables one to find items by an author who is not listed first. The  
first volume contains items published through 1975 and includes 4704 entries classified  
under 41 subject headings. The last volume covers items published in the period 1981-  
1984 and contains 4751 entries classified under 50 subject headings. A much briefer, but  
annotated, bibliography is the subject ofO'hEigertaigh et al. \(1985\).  
Several books on integer programming and combinatorial optimization have appeared  
in the 1980s. In chronological order, these are Papadimitriou and Steiglitz \(1982\), Gondran  
and Minoux \(1984\), Lawler, Lenstra et al. \(1985\), Schrijver \(1986a\), and Grotschel, Lovasz,  
and Schrijver \(1988\). Papadimitriou and Steiglitz emphasize algorithms and computa-  
tional complexity from the point of view of computer scientists. Gondran and Minoux  
also stress algorithms and focus on problems associated with graphs. Lawler et al. is  
restricted to the traveling salesman problem, but we mention it here because of the  
prominent role played by the traveling salesman problem as a generic difficult combina-  
torial optimization problem. Schrijver gives an encyclopedic treatment of the theory of  
linear and integer programming from the polyhedral point of view. Grotschel et al. is a  
monograph whose subject matter is motivated by the consequences of ellipsoid algorithms  
in combinatorial optimization. It also contains information on algorithmic approaches to  
problems in geometric number theory. The applications of this branch of mathematics in  
discrete optimization have just begun to be investigated.  
Earlier general textbooks on integer programming are Hu \(1969\), Greenberg \(1971\),  
Garfinkel and Nemhauser \(1972a\), Salkin \(1975\), and Taha \(1975\). Lawler \(1976\)  
emphasizes the roles of network flows and matroids in combinatorial optimization.  
Christofides \(1975a\) studies a variety of combinatorial optimization problems associated  
with graphs. Johnson \(1980a\) is a monograph on integer programming theory that  
emphasizes subadditivity and group theory.  
Beale \(1968\) and Williams \(1978a\) are general texts on mathematical programming that  
are of some interest here because they emphasize modeling and problem formulation.  
General survey articles appeared early in the development of the field \[see Beale \(1965\),  
Balinski \(1965, 1967, 1970a\), Balinski and Spielberg \(1969\), Garfinkel and Nemhauser  
\(1973\), Geoffrion and Marsten \(1972\) and Geoffrion \(1976\)\]. Some recent surveys on  
combinatorial optimization are by Klee \(1980\), Pulleyblank \(1983\), Schrijver \(1983a\), and  
Grotschel \(1984\); Grotschel \(1985\) gives an annotated bibliography. More specialized  
surveys will be cited in the appropriate chapters.  
Numerous proceedings and study volumes have been devoted to integer and combina-  
torial optimization. These include Balinski \(1974\), Hammer, Johnson, Korte, and  
Nemhauser \(1977\), Balinski and Hoffman \(1978\), Hammer, Johnson, and Korte 7. Notes 21  
\(1979a,b\), Christofides, Mingozzi et al. \(1979\), Padberg \(1980a\), Hansen \(1981\), Pulley-  
blank \(1984\), and Monma \(1986\). The Hammer, Johnson, and Korte volumes and the  
book by Christofides et al. are collections of surveys. For the most part, the others are  
collections of research articles that complement the journals that contain a substantial  
number of papers on integer programming and combinatorial optimization.  
Some of the more prominent j ournals published in English are Mathematical Program-  
ming, Mathematical Programming Studies, Operations Research, Operations Research  
Letters, Annals of Operations Research, Networks, SIAM Journal on Algebraic and  
Discrete Methods, Discrete Mathematics, Discrete Applied Mathematics, Annals of  
Discrete Mathematics, Combinatorica, Journal of the Associationfor Computing Machin-  
ery, Management Science, Operational Research Quarterly, The European Journal of  
Operations Research, Naval Research Logistics Quarterly, lIE Transactions, and Trans-  
portation Science.  
The scope of each of these journals relative to their coverage of integer and combina-  
torial optimization is difficult to specify. A rough guideline is the following. The first five  
purport to cover the subject broadly, although there is unfortunately a dearth of papers on  
applications. The same can be said for Networks within its more narrowly defined scope of  
problems. The next five emphasize theory. The remainder contain some methodology  
oriented toward specific models and a few applications.  
The periodical Interfaces publishes an annual issue on successful case studies in  
operations research and management science. Some of these studies involve the use of  
integer programming techniques. Applications of integer programming are also discussed  
in journals of finance, marketing, production, economics, and the various branches of  
engineering.  
Sections 1.1.2-1.1.4  
Dantzig \(1957, 1960\) formulated several integer programming models and showed how a  
variety of nonlinear and nonconvex optimization problems could be formulated as  
mixed-integer programs. References on the models presented in these sections will be  
given in the notes for the chapters in which the models are discussed in detail. In  
particular, knapsack problems are considered in Sections 11.2.2 and 11.6.1, matching  
problems are discussed in Chapter 111.2, set covering is presented in Section 11.6.2 and  
Chapter II1.1, fixed-charge network problems are considered in Sections 11.2.4 and 11.6.4,  
and the traveling salesman problem is discussed in Sections 11.2.3 and 11.6.3.  
Section 1.1.5  
Strong formulations is one of the major themes of this book. See Williams \(1974, 1978b\)  
and Jeroslow and Lowe \(1984\) for a comparison of alternative formulations for some  
general integer programs.  
Systematic reformulation of knapsack problems was treated by Bradley et al. \(1974\).  
Formulation \(5.2\) appears in Miller et al. \(1960\). The strength of reformulation \(5.5\) was  
shown by Krarup and Bilde \(1977\), and that of the disjunctive formulation \(4.4\) was  
shown by Balas \(1979\). Many other citations will be made in the notes for Chapters 11.2,  
11.5, and 11.6.  
Section 1.1.6  
Preprocessing techniques are frequently attributed to folklore because the references are  
difficult to pin down. Bound tightening, variable fixing, and row elimination schemes  
used in mathematical programming systems are discussed in Brearley et al. \(1975\). 22 1.1\. The Scope of Integer and Combinatorial Optimization  
Preprocessing techniques that use boolean inequalities have been studied by Guignard  
and Spielberg \(1977, 1981\). Also see Guignard \(1982\), Johnson and Suhl \(1980\), Crowder,  
Johnson, and Padberg \(1983\), Johnson and Padberg \(1983\), and Johnson, Kostreva, and  
Suhl \(1985\).  
8\. EXERCISES  
1. Show that the integer program with irrational data max\{x 1 - \(2\)1/2X2\:  
Xl \~ \(2\)1/2x2, Xl \~ 1, X E Z\~\) has no optimal solution, even though there exist  
feasible solutions with value arbitrarily close to zero.  
2. The BST Delivery Company must make deliveries to 10 customers whose respective  
demands are dj for\} = 1, ... , 10. The company has four trucks available with  
capacities Lk and daily operating costs Ck for k = 1, ... ,4. A single truck cannot  
deliver to more than five customers, and customer pairs \{l, 7\}, \{2, 6\}, and \{2, 9\}  
cannot be visited by the same truck. Formulate a model to determine which trucks to  
use so as to minimize the cost of delivering to all the customers.  
3. An airline has fixed its daily timetable for flights between five cities. It now has the  
problem of scheduling the crews. There are certain legal limits on how much time  
each crew can work within any 24-hour period. The problem is to propose a crew  
schedule using the minimum number of crews in which each flight leg is covered.  
Formulate a generic problem of this type as a set covering problem.  
4\. The DuFour Bottling Company has two machines for its bottle production. The  
problem each year is to devise a maintenance schedule. Maintenance of each  
machine lasts 2 months. In addition, only half the workforce is available in July and  
August, so that only one machine can be used during that period. Monthly demands  
for bottles are dt , t = 1, ... , 12. Machine k, k = 1, 2, produces bottles at the rate of  
ak bottles per month but can produce less. There is also a labor constraint. Machine  
k requires h labor days to produce ak, and the total available days per month are L t  
for t = 1, ... , 12. Formulate the problem offinding a feasible maintenance schedule  
in which all demands are satisfied. Modify your formulation to handle the following  
objectives.  
i\) Minimize the sum of the monthly fluctuations in labor utilization.  
ii\) Minimize the largest monthly fluctuation.  
5. Integer and mixed-integer programming models are used on Wall Street to select  
bond portfolios. The idea is to pick a mix of bonds to maximize average yield subject  
to constraints on quality, length of maturity, industrial and government percentages,  
and total budget. Integrality arises because certain bonds only come in 100-unit lots.  
Formulate a model for this generic problem.  
6. A company has two products k = 1, 2, one factory, two distribution centers i = 1, 2,  
and five major clients\} = 1, ... , 5 whose product demands djk are known. The  
company must decide which products should be handled by each center and how  
each client should be serviced. The problem is to minimize total costs, where the  
costs include\:  
i\) a fixed costhk if product k is handled by distribution center i;  
ii\) fixed coStShjk if the demand of client\} for product k is satisfied by center i; and 8\. Exercises 23  
iii\) unit shipping costs C ijk per unit of product k shipped to client\} via center i.  
How does your model change if demands can be split between distribution centers?  
7. Formulate the traveling salesman problem using the variables Xijb where Xijk = 1 if  
\(i, \}\) is the kth arc of the tour and x ijk = 0 otherwise.  
8. a\) Given a graph G = \(V, E\) with weights We for e E E, formulate the following  
problems \(see Chapter 1.3 for some of the definitions\) as integer programs.  
i\) Find a maximum-weight tree.  
ii\) Find a maximum-weight s-t cut.  
iii\) Find a minimum-weight covering of nodes by edges.  
iv\) Find a maximum-weight cycle with an odd number of edges.  
v\) Find a maximum-weight bipartite subgraph.  
vi\) Find a maximum-weight eulerian subgraph.  
b\) Given a graph G = \(V, E\) with weights Cj for\} E V, formulate the following  
problems.  
i\) Find a maximum-weight clique.  
ii\) Find a minimum-weight dominating set \(a set of nodes U \~ V such that  
every node of V is adjacent to some node in U\).  
9. Suppose k trucks can be used to serve n clients from a single depot. Each client must  
be visited once. The time for truck k to travel from i to\} is C ijk. The tour of each truck  
cannot take longer than L k• Formulate the problem of finding a feasible schedule.  
10\. Consider the quadratic 0-1 knapsack problem  
By introducing a variable Yij to represent XiXj, reformulate the problem as a linear  
mixed-integer programming problem.  
11. Show that the BIP max\{cx\: Ax \~ b, x E En\} may be solved by solving the quadratic  
program  
max\{cx - MxT\(l - x\)\: Ax \~ b, 0 \~ Xj \~ 1 for all\}\},  
where M is a large positive number. Given A, b, c, how large should M be?  
12\. Let H E R'J!xn and C E R\~. Let \[!F be the collection of all the nonempty subsets of  
\{l, 2, ... , n\}. For FE \[!F define  
m  
z\(F\) = I IJ?ax h ij - I Cj.  
i=l JEE jEF  
i\) Show that the problem max\{z\(F\)\: FE \[!F\} can be formulated as the following  
integer program\: 24 1.1\. The Scope oflnteger and Combinatorial Optimization  
m n n  
max I I hijYij - I CjXj  
i=1 j=1 j=1  
m  
j=1  
I Y ij = 1 for i = 1, ... , m  
Y ij \~ Xj for i = 1 , ... , m and j = 1, ... , n  
ii\) Show that the problem max\{z\(F\)\: F E \~\) can also be formulated as the integer  
program\:  
m n  
max I U i-I CjXj  
i=1 j=1  
n  
Ui \~ hik + I \(hi\} - hiktXj for k = 0, ... , nand i = 1, ... , m  
j='  
U E R'J\},  
where a+ denotes max\(O, a\) and h iO = ° for i = 1, ... , m.  
13\. Consider the scheduling problem of Section 4 with only one machine. Each job has  
processing time Ph a deadline dh and a weight Wj \> 0.  
i\) Formulate the problem of finding a feasible schedule in which the weighted sum  
of completion times is minimized. Avoid using \(\)\) as in \(4.6\) by writing an exact  
expression for the finish time of job j.  
ii\) Give an alternative formulation using the variables Xjl' where Xjl = 1 if job j is  
completed at time t. \(Assume Ph dj are integers\).  
14\. Suppose the departure times of trucks A and B have to be scheduled. Each truck can  
leave at 1, 2, 3, or 4 p.m. Truck B cannot leave until at least 1 hour after truck A. Let  
Xi \(Yi\) = 1 if truck A \(B\) leaves at time i. Give two formulations of the feasible region  
and compare them.  
15\. Show that  
s = \{x E B4\: 97x, + 32x2 + 25x3 + 20X4 \~ 139\)  
= \{x E B4\: 2x I + X 2 + X 3 + X4 \~ 3\)  
X,+ X2+ X3  
XI + X3+  
XI+ X2+ +  
Which formulation do you think is most effective for solving max\{cx\: xES\}? 8\. Exercises 25  
16\. Consider the 0-1 feasible region  
s = \{ x E En\: j\~ ajXj ,,; b\} with aj, b E Zl for j E N.  
Formulate as an integer program the problem of finding weights cj, d E zl such that  
and d is minimized. Formulate and solve the example with  
17\. Consider the two formulations of the traveling salesman problem in Section 5. Show  
that PI C P2•  
18\. To show that \(4.4\) gives a tight formulation ofU7!l Pi when  
let  
and  
i\) Show that ify\* E U\~l Pi, there exists \(yi, x\) such that \(yi, y\*, x\) E T\*\*.  
ii\) Show that if \(yi, y\*, x\) E T\*\*, then y\* E U\~l Pi.  
iii\) Show that if\(yi, y\*, x\) E T\*, then y\* E conv\(U\~l Pi\)'  
iv\) What difficulties can arise if the polyhedra Pi are unbounded, that is, the  
constraints z \~ d are not present?  
19\. Given a linear inequality in 0-1 variables and the region  
s = \{x E B/N I/+/N2/. " ax· - " ax· \~ b\} 'L\)\) L\)\)  
JEN1 jEN2  
where aj \> 0 for j EN\} U N 2, write necessary and sufficient conditions for  
i\) S = 0,  
ii\) S = En,  
iii\) Xj = 0  
iv\) Xj = 1  
for all xES,  
for all xES, 26 1.1\. The Scope of Integer and Combinatorial Optimization  
v\) Xi + X\) \~ 1 for all xES,  
vi\) Xi \~ Xi for all xES, and  
vii\) Xi + Xi \~ 1 for all xES.  
20. If x E Bn, what is implied by  
i\) Xi + x\) \~ 1 and X i \~ xh  
ii\) Xi + Xi \~ 1 and Xi + Xi \~ 1, and  
iii\) Xi \~ x\) and Xj + Xk \~ 1 ?  
21. Use the results of Exercises 19 and 20 to solve the following problem without having  
recourse to enumeration\:  
max 2x, - 2X2 + 3X3 + lx4 + 2X5  
7x, + 3X2 + 9X3 - 2X4 + 2X5 \~ 7  
-6x, + 2X2 - 3X3 + 4X4 + 9X5 \~ -2  
xEB5. 1.2  
# Linear Programming  
**1.** INTRODUCTION  
The general linear programming problem is  
\(LP\) ZLP = *max\{cx\: Ax* \~ *b, x* E R\~\},  
where the data are rational and are given by the *m* x *n* matrix *A,* the 1 x *n* matrix c, and the  
*m* x 1 matrix *b.* This notation is different from that of Section I.1.1 but is preferable here  
because of its widespread use in linear programming. Recall that, as we observed in  
Section 1.1.1, equality constraints can be represented by two inequality constraints.  
Problem LP is well-defined in the sense that if it is feasible and does not have  
unbounded optimal value, then it has an optimal solution.  
A good understanding of the theory and algorithms of linear programming is essential  
for understanding integer programming for several reasons that can be summed up by the  
statement that "one has to learn to walk before one can run". Integer programming is a  
much harder problem than linear programming, and neither the theory nor the computa-  
tional aspects of integer programming are as developed as they are for linear program-  
ming. So, first of all, the theory of linear programming serves as a guide and motivating  
force for developing results for integer programming.  
Computationally, linear programming algorithms are very often used as a subroutine in  
integer programming algorithms to obtain upper bounds on the value of the integer  
program. Let  
\(IP\) ZIP = *max\{cx\: Ax* \~ *b, x* E Z\~\}  
and observe that *ZLP* \~ *ZIP* since Z\~ C R\~. The upper bound *ZLP* sometimes can be used to  
prove optimality for IP; that is, if XO is a feasible solution to IP and *cxo* = Z LP, then XO is an  
optimal solution to IP.  
A deeper connection between linear and integer programming is that corresponding to  
any integer programming problem there is a linear programming problem *max\{cx\: Ax*  
\~ *b, A IX* \~ b l  
, *X* E R\~\} that has the same answer as IP.  
Our presentation oflinear programming is by necessity very terse and is not intended as  
a substitute for a full treatment. The reader who has already studied linear programming is  
advised to scan this section to become familiar with our notation or, perhaps, to review an  
unfamiliar topic.  
In the next section, we consider the duality theory of linear programming which,  
among other things, provides necessary and sufficient optimality conditions. In the  
following two sections, we present algorithms for solving linear programs.  
27 28 1.2\. Linear Programming  
The simplex algorithms are used to prove the main duality theorem and also to show  
that every feasible instance of LP that is not unbounded has an optimal solution. But,  
more importantly, they are the practical algorithms that are part of linear programming  
software systems and many integer programming software systems as well. The perform-  
ance of simplex algorithms, observed over years of practical experience, shows that they  
are very robust and efficient. Typically the number of iterations required is a small  
multiple of m. Although there exist simplex algorithms that converge finitely, these are  
inefficient; and the ones used in practice can fail to converge. Moreover, there are  
examples which show that finitely convergent simplex algorithms may require an expo-  
nential number of iterations. But this bad behavior does not seem to occur in the solution  
of practical problems.  
Section 4 deals with subgradient optimization. There are convergent subgradient  
algorithms, but, as described, they are not finite. However, on certain classes of linear  
programs that arise in solving integer programs, they tend to produce good solutions very  
quickly.  
In Chapter 1.6, we consider two other linear programming algorithms. These have been  
deferred to a later chapter because some of the motivation for considering them concerns  
the theoretical complexity of computations, which is studied in Chapter 1.5.  
2\. DUALITY  
Duality deals with pairs of linear programs and the relationships between their solutions.  
One problem is called the primal and the other the dual.  
We state the primal problem as  
\(P\) ZLP = max\{ex\: Ax \~ b, x E R\~\}.  
Its dual is defined as the linear program  
\(D\) WLP = min\{ub\: uA \~ e, u E R'\:\}.  
It does not matter which problem is called the primal because\:  
Proposition 2.1. The dual of the dual is the primal.  
Proof To take the dual of the dual, we need to restate it as a maximization problem  
with equal-to-or-Iess-than constraints. Once this is done, the result follows easily. We leave  
the details to the reader. •  
Feasible solutions to the dual provide upper bounds on ZLP and feasible solutions to the  
primal yield lower bounds on WLP. In particular\:  
Proposition 2.2 \(Weak Duality\). ex\* \~ ZLP \~ WLP \~ u\*b.  
If x\* is primal feasible and u· is dual feasible, then  
Proof ex\* \~ u\*Ax\* \~ u\*b, where the first inequality uses u\*A \~ e andx\* \~ 0, and the  
second uses Ax· \~ band u\* \~ 0. Hence WLP \~ ex for all feasible solutions x to P, and  
ZLP \~ ub for all feasible solutions u to D, so that WLP \~ ZLP. •  
Corollary 2.3. If problem P has unbounded optimal value, then D is infeasible. 2. Duality 29  
Proof By weak duality, WLP ;\:\:\: *A* for all *A* E *R* I. Hence D has no feasible solution. •  
We now come to the fundamental result oflinear programming duality, which says that  
if both problems are feasible their optimal values are equal. A constructive proof will be  
given in the next section.  
Theorem 2.4 \(Strong Duality\). If ZLP or WLP is finite, then both P and D have finite  
optimal value and ZLP = WLP.  
Corollary 2.5. There are only four possibilities for a dual pair of problems P and D.  
i. ZLP and WLP arefinite and equal.  
ii. ZLP = 00 and D is infeasible.  
111. WLP = -00 and P is infeasible.  
iv. Both P and D are infeasible.  
A problem pair with property iv is max\{xi + *X2\: XI* - *X2* \~ -1, *-Xl* + *X2* \~ -1, *X* E R\~\} and  
its dual.  
Another important property of primal-dual pairs is complementary slackness. Let  
s = b - Ax ;\:\:\: 0 be the vector of slack variables of the primal and let *t* = uA - c ;\:\:\: 0 be the  
vector of surplus variables of the dual.  
Proposition 2.6. x/,lj\*= 0 for all *j,* and uisi= 0 for all i.  
If x\* is an optimal solution ofP and u\* is an optimal solution ofD, then  
Proof Using the definitions of s\* and t\*, we have  
cx\* = \(u\*A - t\*\) x\* = u\*Ax\* - t\*x\*  
*=* u\*\(b - s\*\) - t\*x\* = u\*b - u\*s\* - t\*x\*.  
By Theorem 2.4, cx\* = u\*b. Hence u\*s\* + t\*x\* = 0 with u\*, s\*, t\*, x\* ;\:\:\: 0 so that the result  
follows. •  
Example 2.1. The dual of the linear program  
ZLP = max *7xI* + *2X2*  
*-XI* + *2X2* \~ 4  
\(P\)  
*5Xl* + *X2* \~ 20  
*-2Xl* - *2X2* \~-7  
xER\~  
is  
\(D\)  
WLP = min *4uI* + *20U2* - *7U3*  
-u, + *5U2* - *2U3;\:\:\: 7*  
*2u* I + *U2* - *2U3* ;\:\:\: 2  
*u ERI.* 30 1.2\. Linear Programming  
It is easily checked that *x\** = \(if 1¥\) is feasible in P, and hence *Z* LP \~ *cx\** = 30ft-.  
Similarly, *u\** = \(n -if 0\) is feasible in D, and hence, by weak duality, ZLP \~ *u\*b* = 30n-.  
The two points together yield a proof of optimality, namely, *x\** is optimal for P and *u\** is  
optimal for D.  
Note also that the complementary slackness condition holds. The slack variables in P  
are *\(sT,* s\~, *sj\)* = \(0 0 6-&\), and the surplus variables in Dare *\(tT,* t\~\) = \(0 0\). Hence  
*xjtj=* 0 for\} = 1,2 and *u7s7=* 0 for *i* = 1,2,3. •  
It is important to be able to verify whether a system of linear inequalities is feasible or  
not. Duality provides a very useful characterization of infeasibility.  
Theorem 2.7 *\(Farkas' Lemma\). Either \{x* E R\~\: *Ax* \~ *b\}* =1= 0 *or \(exclusively\) there*  
*exists v ERr\: such that vA* \~ 0 *and vb* \< O.  
*Proof* Consider the linear program ZLP = *max\{Ox\: Ax* \~ *b, x* E R\~\} and its dual  
WLP = *min\{vb\: vA* \~ 0, *v* E *R'\:\}.* As *v* = 0 is a feasible solution to the dual problem, only  
possibilities i and iii of Corollary 2.5 can occur.  
i. ZLP = WLP = O. Hence *\{x* E R\~\: *Ax* \~ *b\}* =1= 0 and *vb* \~ 0 for all *v ERr\:* with *vA* \~ 0;  
iii. ZLP = WLP = -00. Hence *\{x* E R\~\: *Ax* \~ *b\}* = 0 and there exists *v ERr\:* with *vA* \~ 0  
and *vb* \< O. •  
There are many other versions of Farkas' Lemma. Some are presented in the following  
proposition.  
Proposition 2.8. *\(Variants of Farkas' Lemma\)*  
a. Either *\{x* E R\~\: *Ax* = *b\} =1=* 0, or *\{v* E *Rm\: vA* \~ 0, *vb* \< O\} =1= *0.*  
b. Either *\{x ERn\: Ax* \~ *b\} =1=* 0, or *\{v* E *R'\:\: vA* = 0, *vb* \< O\} =1= *0.*  
c. IfP = *\{r* E R\~\: *Ar* = a\}, either P \\ \{O\} *=1=* 0, or *\{u* E *Rm\: uA* \> O\} =1= *0.*  
3. THE PRIMAL AND DUAL SIMPLEX ALGORITHMS  
Here it is convenient to consider the primal linear program with equality constraints\:  
\(LP\) ZLP = *max\{cx\: Ax* = *b, x* E R\~\}.  
Its dual is  
\(DLP\) WLP = *min\{ub\: uA* \~ c, *u* E *Rm\}.*  
We suppose that rank\(A\) = m \~ n, so that all redundant equations have been removed  
from LP.  
Bases and Basic Solutions  
*LetA* = *\(a* b *a2,* ... , *an\)* where *aj* is the\}th column of *A.* Since *rank\(A\)* = *m,* there exists an  
*m* x *m* nonsingular submatrix *AB* = *\(aBI ,* ••• *,aBJ.* Let *B* = *\{B b* '" *,Bm\}* and let *N* = 3\. The Primal and Dual Simplex Algorithms 31  
\{l, ... , *n\}* \\ *B.* Now permute the columns of *A* so that *A* = *\(AB' AN\).* We can write *Ax* = *b*  
*asABxB* + *ANxN* = *b,* where *x* = *\(XB' XN\).* Then a solution to *Ax* = *b* is given by *XB* = *Aiib*  
*andxN* = O.  
Definition 3.1  
a. The *m* x *m* nonsingular matrix *A B* is called a *basis.*  
b. The solution *XB* = *Ai1b, XN* = 0 is called a *basic solution* of *Ax* = *b.*  
c. X *B* is the vector of *basic variables* and x N is the vector of *nonbasic variables.*  
d. If *Ai1b* \~ 0, then *\(XB' XN\)* is called a *basic primalfeasible solution andAB* is called a  
*primal feasible basis.*  
Now let C = *\(CB' CN\)* be the corresponding partition of c, that is, *cx* = *CBXB* + *CNXN,* and  
let *u* = *cBAIl* E Rm. This solution is complementary to x = *\(XB, XN\),* since  
*andxN* = O. Observe that *u* is a feasible solution to the dual ifand only if *cBAi/AN* - *CN* \~ O.  
This motivates the next definition.  
Definition 3.2. If C *BA 11A N* \~ C *N,* then *A B* is called a *dual feasible basis.*  
Note that a basis *AB* defines the point *x* = *\(XB' XN\)* = *\(Ai/b,* 0\) ERn and the point  
*u* = C *BA i1* E R *m* • *A B* may be only primal feasible, only dual feasible, neither, or both. Bases  
that are both primal and dual feasible are of particular importance.  
Proposition 3.1. *If AB is primal and dual feasible, then x* = *\(XB, XN\)* = *\(ABI b,* 0\) *is an*  
*optimal solution to* LP *and u* = *cBAB ! is an optimal solution to* DLP.  
*Proof* x = *\(AB1b,* 0\) is feasible to LP with value *cx* = *cBAj/b. u* = *cBA B !* is feasible in  
DLP and *ub* = *cBAB!b.* Hence the result follows from weak duality. •  
Changing the Basis  
We say that *two bases AB and AS' are adjacent* if they differ in only one column, that is  
IB \\ *B'* I = *IB'* \\ *B* I = 1. If *AB* and *A B ,* are adjacent, the basic solutions they define are  
also said to be adjacent. The simplex algorithms to be presented in this section work by  
moving from one basis to another adjacent one.  
Given the basis *A* B , it is useful to rewrite LP in the form  
ZLP = *cBAB!b* + *max\(cN* - *cBAB!AN\)XN*  
*LP\(B\)*  
*XB* + *A j/ANXN* = *Aj/b*  
It is simple to show that problems *LP\(B\)* and LP have the same set of feasible solutions and  
objective values.  
We now define some additional notation that allows us to state things more concisely.  
Let *AN* = *ABlAN'* b = *ABlb,* and *eN* = *CN* - *cBAi/AN* so that 32  
1.2\. Linear Programming  
*LP\(B\)*  
Also, for j EN, we let aj = ARlaj and Cj = Cj - CBa\) so that  
ZLP = cBb + max *L* CjXj  
JEN  
*LP\(B\)*  
XB + *L* ajxj = b  
JEN  
X B \~ 0, Xj \~ ° for *j* E *N.*  
Finally, we sometimes write the equations of *LP\(B\)* as  
XB; + *L* aijxj = bi for i = 1, ... , m,  
JEN  
that is, aj = \(alj, ... , amj\) and b = \(bb ... , bm \).  
Let CN = CN - cBAN be the reduced price vector for the nonbasic variables. Then, by  
Definition 3.2, dual feasibility of basis AB is equivalent to CN \~ 0.  
Now given the representation *LP\(B\),* we show how to move from one basic primal  
feasible solution to another in a systematic way.  
Definition 3.3\. A primal basic feasible solution x B = b, X N = ° is degenerate if b i = ° for  
some i.  
Proposition 3.2. Suppose all primal basic feasible solutions are nondegenerate. If AB is a  
primalfeasible basis and a*r* is any column of AN, then matrix \(AB' ar\) contains, at most, one  
primal feasible basis other than AB .  
Proof We consider the system  
XB + arxr = b  
\(3.1\)  
XB \~ 0, Xr \~ 0,  
that is, all components of XN except Xr equal zero.  
Case 1. ar \~ 0\. Suppose Xr = A \> 0. Then for all A\> ° we obtain  
- -  
xB = b - a  
A ;\:\:\: b\> O.  
*r*  
Thus for every feasible solution to \(3.1\) with Xr \> 0, we have XB \> ° so that AB is the only  
primal feasible basis contained in \(AB,ar\).  
Case 2. At least one component ofar is positive. Let  
\(3.2\) Ilr = mIn =-\: air\> = =-.  
1 • *fbi* - o\} *b*s  
air asr 3\. The Primal and Dual Simplex Algorithms 33  
Hence *b* - arAr \~ 0 and *b*s - asrAr = O. So we obtain an adjacent primal feasible basis AB\(r\)  
by deleting Bs from B and replacing it with r, that is, B\(r\) = B U \{r\} \\ \{Bs\}' Note that the  
nondegeneracy assumption implies that *b*i - airAr \> 0 for i =1= *S* so that the minimum in  
\(3.2\) is unique. Consequently, any basis AB with *B* = B U \{r\} \\ *\{k\}* for *k* E B \\ \{B s\} is not  
primal feasible. •  
The new solution is calculated by\:  
1. Dividing  
XBs + asrxr + I aSjXj = bs  
jEN\\\{r\}  
by asr , which yields  
\(3.3\)  
2\. Eliminating Xr from the remaining equations by adding -air multiplied by \(3.3\) to  
XB *j* + airXr + I aijXj = *b*i for i =1= *S*  
jEN\\\{r\}  
and eliminating Xr from the objective function.  
This transformation is called a pivot. It corresponds precisely to a step in the well-  
known Gaussian elimination technique for solving linear equations. The coefficient asr is  
called the pivot element.  
Corollary 3.3. Suppose AB is a primal feasible nondegenerate basis that is not dual  
feasible and c r \> O.  
a. If *ar* \~ 0, then ZLP = 00.  
b. If at least one component of *ar* is positive, then A B\(r\), the unique primal feasible basis  
adjacent to AB that contains a" is such that CB\(r\)XB\(r\) \> CBXB.  
Proof  
a. x *B* = b - a rA, x r = A, Xj = 0 otherwise is feasible for all A \> 0 and  
b.  
where the inequality holds since Ar defined by \(3.2\) is positive and c, \> 0 by  
hypothesis. •  
Primal Simplex Algorithm  
We are now ready to describe the main routine of the primal simplex method called Phase  
2. It begins with a primal feasible basis and then checks for dual feasibility. If the basis is 34 1.2. Linear Programming  
not dual feasible, either an adjacent primal feasible basis is found with \(in the absence of  
degeneracy\) a higher objective value or ZLP = 00 is established.  
Phase 2  
Step 1 \(Initialization\)\: Start with a primal feasible basis A B•  
Step 2 \(Optimality Test\)\: If *AB* is dual feasible \(i.e., *CN* \< 0\), stop. *XB* = b, *XN* = 0 is an  
optimal solution. Otherwise go to Step 3.  
Step 3 \(Pricing Routine\)\: Choose an r E N with cr \> O.  
a. Unboundedness test. *Ifar* \~ 0, ZLP = 00.  
b. Basis change. Otherwise, find the unique adjacent primal feasible basis A *B\(r\)* that  
contains *ar •* Let *B* \~ *B\(r\)* and return to Step 2.  
Note that in Step 3, we can choose any j EN with *Cj* \> O. A pricing rule commonly used  
is to choose r = arg\(maXjENCj\), since it gives the largest increase in the objective function  
per unit increase of the variable that becomes basic. But this computation can be time  
consuming when n is large, so that various modifications of it are used in practice.  
Theorem 3.4. Under the assumption that all basic feasible solutions are nondegenerate,  
Phase 2 terminates in a finite number of steps either with an unbounded solution or with a  
basis that is primal and dual feasible.  
Proof At each step the value of the basic feasible solution increases. Thus no basis can  
be repeated. Because there is only a finite number of bases, this procedure must terminate  
finitely. •  
When basic solutions are degenerate, and this happens often in practice, Proposition  
3.2 and Corollary 3.3 are not true. Consequently, the finiteness argument given in the  
proof of Theorem 3.4 does not apply.  
Note that when the basic feasible solution is degenerate, the arg\(min\) of \(3.2\) may not  
be unique. In this case, *\(AB'* a*r \)* contains more than one primal feasible basis adjacent to  
A *B,* and in Step 3b of the algorithm an arbitrary choice is made. A complication arises  
when Ar = 0 in \(3.2\) since each primal feasible basis in *\(AB'* ar\) defines the same solution,  
namely, x B = b and x N = O. A sequence of such degenerate changes of basis can, although it  
rarely happens in practice, lead back to the original basis. This phenomenon is called  
cycling.  
Two methods for eliminating the possibility of cycling are known. One involves a  
lexicographic rule for breaking ties in \(3.2\), and the other involves both the choice of *r* in  
Step 3 and a tie-breaking rule for \(3.2\). By eliminating cycling, these algorithms establish  
the finiteness of Phase 2 for any linear programming problem. Hence there are primal  
simplex methods for which Theorem 3.4 holds without a nondegeneracy assumption.  
Example 3.1  
ZLP = max *7Xl* + *2X2*  
*-Xl* + *2X2* + *X3* 4  
20 3\. The Primal and Dual Simplex Algorithms  
35  
*+* X5 = -7  
*x* \~O.  
*Step* 1 *\(Initialization\)\:* The basisA *B* = *\(a3, a4, al\)* with  
# 1  
*Ali* = \(00  
\~ 1\)  
o -1  
yields the primal feasible solution  
and xN = \(X2' xs\) = \(0 0\),  
Iteration **1**  
*Step* 2\:  
*AN* = *\(a" as\)* = *A.IA N* = \(-\~ J\).  
*eN* = *Cr cRAN* = \(2 0\) - \(0 0 *7\)AN* = \(-5 n  
Thus *LP\(B\)* can be stated as  
1  
- 4X2 + 22xs  
1  
X2 - 2X5  
X\~O.  
= 7!  
2  
= 2!  
2  
1  
+ Xl = 32  
*Step* 3\: The only choice for a new basic variable is X5. By \(3.2\),  
2! \}  
A.s = min -, 21, - = 1.  
# \{  
Hence X4 is the leaving variable. 36 1.2\. Linear Programming  
Iteration 2  
Step2\: All = \(\~ 1  
5  
2  
1  
5 !\} 4\),  
5  
X 2 is the entering variable.  
Step 3\: a2 = \(¥ -\~ As +- \(a2' a5, a,\).  
!\). By \(3.2\), ,12 = min\(,ts, -, '/5\) = 1¥. Hence X3 is the leaving variable.  
Iteration 3  
..i J.. 0\) 11 I'  
Step 2\: A Ii = 1 1 0 1  
\(  
,  
-11 11  
- - -\) \(3 16\) 0  
CN = \(C3, C4 = - IT - IT \~ .  
Hence *x* = \(Xl, X2, X3, X4, X5\) = \(if 1¥ 0 0 if\) is an optimal solution to LP, and  
U = cBAli = err -W 0\) is an optimal solution to DLP.  
We have shown that ifLP has a basic primal feasible solution, it either has unbounded  
optimal value or it has an optimal basic solution. It remains to show that if it has a feasible  
solution, then it has a basic feasible solution. This is accomplished by Phase 1 of the  
simplex algorithm.  
Phase 1. By changing signs in each row if necessary, write LP as max\{cx\: Ax = b, x E *R1\}*  
with b ;?; O. Now introduce artificial variables xf for i = 1, ... , m, and consider the linear  
program  
Za = max \{ - \~ x7\: Ax + Ixa = *b,* \(x, x a\) E R\~+m l  
*1. Lp*a is a feasible linear program for which a basic feasible solution xa = *b,* x = 0 is  
available. Hence Lpa can be solved by the Phase 2 simplex method. Moreover *Z* a \~ 0  
so that Lpa has an optimal solution.  
2. i\) A feasible solution \(x, x a\) to Lpa yields a feasible solution x to LP if and only if  
*x* a  
= O. Thus if Za \< 0, Lpa has no feasible solution with *x* a = 0 and hence LP is  
infeasible.  
ii\) If Za = 0, then any optimal solution to Lpa has *x* a = 0 and hence yields a feasible  
solution to LP. In particular, if all the artificial variables are nonbasic in some  
basic optimal solution to Lpa, a basic feasible solution for LP has been found.  
On the other hand, if one or more artificial variables are basic, it may be possible to  
remove them from the basis by degenerate basis changes. When this is not possible it can  
be shown that certain constraints in the original problem are redundant, and the equations 3\. The Primal and Dual Simplex Algorithms 37  
with basic artificial variables can be dropped. Again this leads to a basic feasible solution to  
LP.  
By combining Phases 1 and 2, we obtain a finite algorithm for solving any linear  
program. This establishes Theorem 2.4 and also Theorem 3.5\:  
Theorem 3.5  
a. If LP is feasible, it has a basic primal feasible solution.  
b. If LP has a finite optimal value, it has an optimal basic feasible solution.  
Example 3.1 \(continued\). We will use Phase 1 to construct the initial basis \(a3, a4, a *I\)*  
that we used previously. The Phase 1 problem is  
Za = max  
- Xl - x\~ - x\~  
*- XI* + *2X2* + *X3*  
*5xI* + *X2* + *X4*  
+ x\~  
- X5 4  
20  
*+* x\~ = 7  
Observe, however, that because *X* 3, *X4* are slack variables and b I and b*2* are nonnegative,  
the artificial variables *xf* and x\~ are unnecessary. Hence we can start with *\(x* 3, *X4, xD* as  
basic variables. Since - X\~ = -7 + *2x* I + *2x* 2 - *X* 5, the Phase 1 problem is  
*Z* a = max - 7 + *2x* I + *2X2*  
*- XI* + *2X2* + *X3* = 4  
*5xI* + *X2* + *X4* 20  
- X5 + X\~ = 7  
x ;?; 0, X3 ;?; o.  
Using the simplex algorithm \(Phase 2\) we introduce X I into the basis, and x\~ leaves. The  
resulting basis \(a3, a4, a 1\) is a feasible basis for the original problem.  
Dual Simplex Algorithm  
The primal simplex algorithm works by moving from one primal feasible basis to another.  
In contrast, the dual simplex algorithm works by moving from one dual feasible basis to  
another. This latter approach is useful when we know a basic dual feasible solution but not  
a primal one. This occurs, for example, when we have an optimal solution to a linear  
programming problem that becomes infeasible because additional constraints have been  
added.  
Proposition 3.6. Let An be a dual feasible basis with *b*s \< O.  
a. If as\} ;?; 0 for all *j* E N, then LP is infeasible.  
b. Otherwise there is an adjacent dualfeasible basis *AB\(r\),* where B\(r\) = B U \{r\} \\ \{Bs\}  
and r E N satisfies *a*sr \< 0 and 38 1.2\. Linear Programming  
*r* = arg \~ln . \{Cj \_ \}  
=-\: *a sj* \< 0 .  
*JEN asj*  
Proof  
a. XBs + LjEN asjXj = bs \< O. Hence if asj \~ 0 for all\} EN, every solution to Ax = b  
with Xj \~ 0 for all\} EN has XBs \< O.  
b. If Xr enters the basis and XBs leaves we have  
z = cBb + I CjXj - A\(XBs + I aSjxJ + Abs  
jEN jEN  
*=* cBb + Abs + I \(Cj - Aasj\)Xj - AxBs,  
jEN  
where A = \~r \~ O. The basis AB\(r\) is dual feasible since A \~ 0, Cj - A asj \~ Cj for all\}  
*a*sr  
with aSj \~ 0, and Cj - A aSj \~ 0 for all\} with aSj \< 0 by the choice of *r. •*  
Dual Simplex Algorithm \(Phase 2\)  
Step 1 \(Initialization\)\: A dual feasible basis A B .  
Step 2 \(Optimality Test\)\: If AB is primal feasible, that is, b = AI\} b \~ 0, then XB = band  
*x N* = 0 is an optimal solution. Otherwise go to Step 3.  
Step 3 \(Pricing Routine\)\: Choose an s with *b*s \< O.  
a. Feasibility Test. Ifasj \~ 0 for all\} EN, LP is infeasible.  
b. Basis change. Otherwise let  
*r* = arg \~in\{\~j *JEN asj*  
\: aSj \< O\}  
and *B\(r\)* = *B* U *\(r\)* \\ *\(Bs\)'* Return to Step 2 with *B* \<'- *B\(r\).*  
In contrast to the primal algorithm, in the dual simplex algorithm the objective  
function is nonincreasing. The magnitude of the decrease at each step is Icrbs/ars I. In the  
absence of dual degeneracy, c*r* \< 0 and the decrease is strict. As with the primal algorithm,  
it is possible to give more specific rules that guarantee finiteness. Such an algorithm is  
presented in Section IIA.3. A Phase 1 may be required to find a starting dual feasible basic  
solution.  
Example 3.1 \(continued\). We apply the dual simplex algorithm.  
Step 1 \(Initialization\)\: Consider the basis AB = \(a3, a2, as\), which is dual feasible since  
CN = \(cJ, \(4\) = \(-3 -2\).  
Iteration 1  
Step 2\: The basis is not primal feasible since XB = \(X3, X2, xs\) = \(-36 20 33\).  
Step 3\: The only possible choice is s = 1. We have all = -11, *a14* = -2, and min\(n, \~\) = rr.  
Hence XB, = x3leaves the basis, Xl enters the basis, andA B \~ \(ar, a2, as\). 3\. The Primal and Dual Simplex Algorithms 39  
Iteration 2. We have seen earlier that A *B* is primal and dual feasible and hence optimal.  
The Simplex Algorithm with Simple Upper Bounds  
It is desirable for computational purposes to distinguish between upper-bound constraints  
of the form *Xj* \~ h *j* and other more general constraints. Hence we consider the problem  
\(ULP\) *ZLP* = *max\{cx\: Ax* = *b,* ° \~ *Xj* \~ *hj* for j E \{l, ... , *n\}\).*  
Whereas the primal simplex algorithm described earlier would treat ULP as a problem  
with *m* + *n* constraints, the *simplex algorithm with upper bounds* treats it as a problem  
with m constraints.  
Now the columns of *A* are permuted so that *A* = *\(AB' ANI' AN\)'* where *AB* is a basis  
matrix as before, but the index set of the nonbasic variables N is partitioned into two sets  
Nl and N 2 . Nl is the index set of variables at their lower bound *\(Xj* = 0\), and N2 is the index  
set of variables at their upper bound *\(Xj* = h*j\).*  
Now we need to modify Definition 3.1.  
Definition 3.4  
a. The *m* x *m* nonsingular matrix *A B* is called a *basis.*  
b. For each partition Nt, N2 of N, we associate the *basic solution XB* =  
*AIl\(b* - *AN2 hN\)* = b - *AN2 hN2 ' XNI* = 0, *XN2* = *hN2.*  
c. If ° \~ *b* - *AN*2*hN*2 \~ *hB,* then *\(XB, XNI, XN\)* is a *basic primal feasible solution,* and  
*\(B,* Nt, *N 2\)* indexes a primal feasible basis.  
Now consider the dual of ULp,  
min *ub* + vh  
uA+v\~c  
*v* \~o,  
and let *v* = *\(VB, VNI, VN2\)* and c = *\(CB, CNI, CN\).* The dual basic solution complementary  
to *\(XB, XNI, XN2\)* is *\(u, VB, VNI, VN\)* = *\(cBAll,* 0, 0, *CN2* - *cBANJ* Observe that *\(u,* v\) is a  
feasible solution to the dual if and only *ifcNI* = *eNl* - *cBANl* \~ ° and *CN2* = *CN2* - *cB AN2* \~ 0.  
Proposition 3.7. *If \(AB , ANI' AN2 \) is primal and dual feasible, then x* = *\(XB, XNl, XNJ =*  
\(b -\~N *hN2,* 0, *hN\) is an optimal solution to ULP and \(u,* v*Bl ' VNl '* v*N2 \)* = \(c\~Bl, 0,0, *cN2*  
- C *BA N\) is an optimal solution to its dual.*  
The modifications to the simplex algorithms are straightforward. *BasesAB andAB"* are  
adjacent if \(i\) *IB* \\ *B'* I = *-IB'* \\ *B* I = 1 or \(ii\) *B* = *B',* and in both cases  
IN\~ \\ NIl + IN2 \\ N21 = 1. In the latter case, one nonbasic variable changes from its  
lower to its upper bound, or vice versa. It is then easy to write out the rules for the choice of  
entering and leaving variable, leading to primal and dual simplex algorithms for ULP.  
Note that these algorithms choose the same pivots as the standard simplex algorithms, so  
the advantage lies in handling a basis that is *m* x *m* rather than *\(m* + *n\)* x *\(m* + *n\).*  
Addition of Constraints or Variables  
After solving LP to optimality, it is common that one or more new constraints or  
columns have to be added. In Part II, we will discuss cutting-place algorithms that add a 40 1.2\. Linear Programming  
constraint cutting off the optimal solution ofLP; we will also discuss problems having such  
a large number of variables that we do not wish to introduce them all a priori.  
If LP has been solved by a simplex algorithm, there is a straightforward way to use the  
current optimal basisAB to solve the new problem. Suppose an inequality *Ll=l djxj* \~ *do* is  
added that is violated by the optimal solution *\(XB, XN\)* = *\(A1ib,* 0\). Now if *Xn+l* is the slack  
variable of the new constraint, then *B'* = *B* U *\{n* + 1\} indexes a new basis, and we obtain  
*LP\(B'\)\:*  
*XB +ANXN* = *b*  
*Xn+l* + *\(dN* - *dBAN\)xN* = *do* - *dBb*  
since  
We see immediately that this basis is dual feasible and that it is primal feasible in all but the  
last row, that is, *d Bb* \> *do.* It is therefore desirable to reoptimize using the dual simplex  
algorithm. Since the current solution is "nearly" primal feasible, it is likely that only a few  
iterations will be required.  
The procedure to be followed in adding new columns is dual to that described above.  
Given a new variable *Xn+l* with column \(\~\:\:\:\), we calculate its reduced price  
Cn+l = Cn+l - *cBA1ian+1* to check if the basis *AB* remains optimal. If *Cn+l* \~ 0, *AB* is still  
optimal and the solution is unchanged. If Cn+l \> 0, we can use the primal simplex  
algorithm as *A* B remains primal feasible.  
Example 3.1 \(continued\). We add the upper-bound constraint x I \~ 3, cutting off the  
optimal solution *x* = \("\* *WOO H\).* Let Xl + *X6* = 3, so that *X6* is the new basic  
variable. Starting from the optimal basis *AB* = *\(a2, as, at\),* we have *dB* = \(0 0 1\),  
*dN* = \(0 0\), *do* = 3, *andA*B , = *\(a2' as,* at, *a6\).*  
Iteration 1  
Step 2\: *XB'* = \(W H "\* -n\).  
Step 3\: x6leaves the basis  
min\{- 16j2\} = 8.  
'11 11  
Iteration 2  
Step 2\: *XB'* = G 6 3 \~\) \~ O. Hence *X* = \(3 \~ 0 \~ 6 0\) is an optimal solution to  
the revised problem. 4\. Subgradient Optimization 41  
Noting that the added constraint is an upper-bound constraint means that we can also  
reoptimize without increasing the size of the basis by using the dual simplex algorithm  
with upper bounds. In this case we have\:  
Iteration **1**  
so the basis is dual feasible.  
XB = \(40 11  
75  
11 36\). 11  
Because XB *I* = Xl\> hI, the basis is not primal feasible.  
The dual simplex algorithm then removes X 1 from the basis at its upper bound and  
calculates \(as above\) that X4 enters the basis.  
Iteration 2  
*AB* = \(a2' as, a4\), *ANI* = \(a3\), *AN2* = \(al\)'  
*CNI* = \(-1\) \~ 0, *CN2* = \(8\) ;\>- 0,  
so the basis remains dual feasible.  
X *B* = \(\~ 6 \~\). Because 0 \~ *x* \~ h *B,* the basis is primal feasible and hence optimal.  
4\. SUBGRADIENT OPTIMIZATION  
Here we consider an algorithm for solving linear programs whose roots are in non-  
linear, nondifferentiable optimization. Consider the linear program  
*I*  
C= min I *Ajdj*  
j=l  
*I*  
i=l  
I Aig *ij* = Cj for j = 1, ... , *n*  
o \~ Ai \~ hi for *i* = 1, ... , I.  
By duality it can be shown \(see Section II.3.6\) that this problem can be restated as  
Now to solve the inner optimization problem for fixed x, we can set Ai = 0 if  
di - Ll=l *gijXj* \> 0, and Ai = hi otherwise. Thus there are a finite number of candidate  
solutions Ak E R\~, k E K, where A7 E CO, hJ. So we can rewrite the problem as 42 1.2. Linear Programming  
or more generally as  
\(4.1\) ,= *max/\(x\),* xERn  
where  
\(4.2\) *I\(x\)* = min *\(aix* - *bi\)* and 1= \{l, ... *,m\}* is a finite set.  
*iEl*  
In other words a general linear program can be transformed to the nonlinear optimiza-  
tion problem \(4.1\), where typically m is much larger than n. In this section, we present an  
algorithm for problem \(4.1\).  
Figure 4.1 illustrates *I* given by \(4.2\) for *n* = 1. The heavy lines give *I\(x\),* and point B is  
the optimum solution *x\** with value' = *I\(x\*\).*  
We now develop an important property of the function!  
Definition 4.1. A function g\: Rn ..,. R I is *concave* if  
*g\(ax l* + \(1 - *a\)x2\)* \~ *ag\(xl\)* + \(1 - *a\) g\(X2\)* for all *Xl, x 2* ERn  
and all 0 \~ *a* \~ 1.  
Note that the definition simply states that the function is underestimated by linear  
interpolation \(see Figure 4.2\).  
This suggests the following proposition.  
Proposition 4.1. *Let/\(x\)* = *mini=!, ... ,* m *\(aix* - bJ *Then/\(x\) is concave.*  
# •  
\~----------------------------\~\~x  
Figure 4.1 4\. Subgradient Optimization 43  
/\(X\)  
\~--\~----------------\~--------\~X  
Figure 4.2  
An alternative characterization of concave functions is given by the following proposi-  
tion.  
Proposition 4.2. *A/unction g\:* R n  
\~ R I *is concave if and only iffor any x\** E Rn *there exists*  
*an s* E R n *such that g\(x\*\)* + *sex* - *x\*\)* \~ *g\(x\) for all x* ERn.  
The characterization is illustrated in Figure 4.3. Note that *s* is the slope of the  
hyperplane that supports the set *\{\(x, z\)* E R n  
*l*  
*+*  
*\: z* \~ *g\(x\)\}* at *\(x, z\)* = *\(x\*, g\(x\*\)\).*  
Comparing Figures 4.1 and 4.3, we see that in Figure 4.1 there is not a unique supporting  
hyperplane at the points *A, B,* and C, while for the smooth function *g* in Figure 4.3, the  
supporting hyperplane is unique at each point.  
Figure 4.4 illustrates Proposition 4.2 for *x* E R2. Contours of *\{x\: g\(x\)* = c\} are shown  
for different values of c along with the supporting hyperplane given by *sex* - *x\*\)* = O. By  
Proposition 4.2, if *x* satisfies *sex* - *x\*\)* \~ 0, then *g\(x\)* \~ *g\(x\*\).* In other words, if  
*g\(x\)* \> *g\(x\*\),* then *sex* - *x\*\)* \> O. Thus if we are at the point *x\** and want to increase *g\(x\),*  
we should move to a point *x'* with *sex'* - *x\*\)* \> O. One possibility is to move in a direction  
normal to the hyperplane *sex* - *x\*\)* = O. This direction is given by the vector *s,* which is,  
when *g* is differentiable at *x\*,* the *gradient vector \\l g\(x\*\)* = *\(ag\(x\*\)j ax* 1, ••• , *ag\(x\*\)j ax* n\) at  
*x* = *x\*.* It is well known that the gradient vector is the local direction of maximum increase  
of *g\(x\),* and *\\lg\(x\*\)* = 0 implies that *x\** solves *max\{g\(x\)\: x* ERn\}.  
The classical steepest ascent method for maximizing *g\(x\)* is given by the sequence of  
iterations  
\~----------\~\~----------------\~x  
x\*  
Figure 4.3 44  
1.2\. Linear Programming  
\~\~--g\(x\)=c=g\(x\*\)  
\~-+-+---g\(x\) =c+ 1  
Supporting hyperplane at x\*\: *8\(X* -x\*\) = 0  
\~---Normal direction atx\*  
\~-------------------------------------------------\~\~x1  
Figure 4.4  
With appropriate assumptions on the sequence of step sizes *\{eJf \},* the iterates \{Xl\} converge  
to a maximizing point.  
The potential problems that arise in applying this idea to a nondifferentiable concave  
function are illustrated in Example 4.1.  
Example 4.1  
The contours!\(x\) = c for c = 0, -1, and -2 are shown in Figure 4.5.  
f\(x\) = -2  
8 2 = \(1,2\)  
f\(x\) = -1  
f\(x\) = 0  
*sl* = \(1, - 2\)  
Figure 4.5 4\. Subgradient Optimization 45  
In addition, at the point *x\** = \(-2 0\) we show the supporting hyperplanes Si\(X - *x\*\)* = 0,  
for *i* = 1, 2 where s 1 = \(l - 2\) and S2 = \(l 2\).  
Now consider what happens when we move from *x\** in the direction *s* 1. We have  
*f\(x\** + 8s *1\) =f\(-2* + 8, 0 - 28\) = min\{2 - 8, - 2 + 58, - 2 - 3 8\}  
= - 2 - 3 8 for all 8 \~ O.  
*Hencef\(x\** + 8s *l \) \<f\(x\*\)* for all 8\> O. Similar behavior is observed for S2.  
The example illustrates the nonuniqueness of the supporting hyperplanes and also  
shows that a direction normal to a supporting hyperplane may not be a direction of  
increase.  
There is, however, an alternative point of view, which provides the intuitive justifica-  
tion for moving in a direction normal to any supporting hyperplane at *x\*.* As we have  
already noted, if *s\(x* - *x\*\)* = 0 is *any* supporting hyperplane at *x\*,* then any point with a  
larger objective value than *x\** is contained in the half-space *s\(x* - *x\*\)* \> O. Now it is a  
simple geometric exercise to show that if *x* is an optimal solution, a small move in the direc-  
tion s gives a point that is closer to *x.* In particular, there exists 8 such that for any 0 \< 8 \< 8,  
IIx - *\(x\** + 8s\)1I \< IIx - x\*11  
\(see Figure 4.6.\). The notation *\\lull, u* ERn, represents the euclidean distance from 0 to *u,*  
that is, .J uT u.  
We now formalize the discussion given above.  
Definition 4.2. g\(x\) - g\(x\*\) for all x E Rn.  
rfg\: Rn \~ Rl is concave, s ERn is asubgradientofg atx\* ifs\(x-x\*\) \~  
Definition 4.3. The set *ag\(x\)* = *\{s* ERn\: *s* is a subgradient of *g* at *x\}* is called the  
*subdifferential* of *g* at *x.*  
Note that by Proposition 4.2, *ag\(x\) =1= 0.*  
Proposition 4.3. *and only if 0* E *ag\(x\*\).*  
*Ifg is concave on* Rn, *x\* is an optimal solution ofmax\{g\(x\)\: x* ERn\} *if*  
x\*+ 8s  
A  
*X*  
Figure 4.6 46 1.2\. Linear Programming  
*Proof* 0 E *ag\(x\*\)* if and only ifO\(x - *x\*\)* \~ *g\(x\)* - *g\(x\*\)* for all *x* ERn if and only if  
*g\(x\)* \~ *g\(x\*\)* for all *x* ERn. •  
Now we characterize the subdifferential *off\(x\)* given by \(4.2\).  
Proposition 4.4. *Letf\(x\)* = mini=I. ... ,m *\(dx* - *bi\) and let I\(x\*\)* = *U\:f\(x\*\)* = *dx\** - bJ  
*1. ai is a subgradient off at x\* for all i* E *I\(x\*\).*  
*2. af\(x\*\)* = \{s ERn\: S = *LiEI\(x\*\) Aiai, LiEI\(x\*\) Ai* = 1, *Ai* \~ *Ofor i* E *I\(x\*\)\}.*  
*Proof*  
1. Ifi E *I\(x\*\),* then *ai\(x* - *x\*\)* = *\(aix* - *bi\)* - *\(aix\** - *bi\)* \~ *f\(x\)* - *f\(x\*\)* for all *x* ERn, so  
that *ai* E *af\(x\*\).*  
2\. A proof is obtained by using statement 1 of Proposition 4.4 along with the Farkas  
lemma. •  
The following algorithm can use any subgradient at each step, but for computational  
purposes one of the extreme directions *a*i will be chosen.  
The Subgradient Algorithm for \(4.1\)  
*Step 1 \(Initialization\)\:* Choose a starting point Xl and let *t* = 1.  
Step 2\: Given xt, choose any subgradient *Sf* E af\(xt\). If st = 0, then xt is an optimal  
solution. Otherwise go to Step 3.  
Step 3\: Let xt+l = *Xl* + Otst for some Ot\> O. \(Procedures for selecting Ot are given below.\)  
Let t \~ t + 1 and return to Step 2.  
Two schemes for selecting \{Ot\} are the following\:  
i. A divergent series\: L\~I Ot -+ 00, Ot -+ 0 as *t* -+ 00.  
ii. A geometric series\: at = aopt, or at = [[ - J\(xt\)\]pt/llstjj2 where ° \< p \< 1 andfis a  
target, or upper bound on the optimal value' of\(4.1\).  
Series i is satisfactory theoretically, since it converges to an optimal point. But in  
practice the convergence is much too slow. Series ii, which is recommended in practice, is  
less satisfactory theoretically. The convergence is "geometric", but the limit point is only  
an optimal point if the initial choices of *\(0*o, *p\)* or \(j, *p\)* are sufficiently large. In practice,  
appropriate values can typically be found after a little testing, and step sizes closely related  
to a geometric series of type ii will be used in our applications of the subgradient algorithm  
in Part II.  
Ideally the subgradient algorithm can be stopped when, on some iteration t, we find  
*st* = 0 E *af\(x*t \). However, in practice this rarely happens, since the algorithm just chooses  
one subgradient st and has no way of showing 0 E *af\(xt\)* as a convex combination of  
subgradients. Hence the typical stopping rule is either to stop after a fixed number of  
iterations or to stop if the function has not increased by at least a certain amount within a  
given number of iterations. 4\. Subgradient Optimization  
47  
Example 4.2. Consider maxlf\(x\)\: x E *R2\},* where  
f\(x\) = min\{h\(x\)\: *i* = 1, ... , 5\}  
and  
fl\(X\) = Xl - 2X2 + 4  
f2\(X\) = - 5x 1 - X2 + 20  
f3\(X\) = 2XI + 2X2 - 7  
f4\(X\) = Xl  
fs\(X\) = X2·  
We apply the subgradient algorithm with Ot = \(0.9Y and initial point Xl = \(0 0\). The  
results of25 iterations are shown in Table 4.1, in which the last column, *i\(t\),* gives the index  
of the function that defines the subgradient. The best solution of value 2.30 is found at  
iteration 13. The optimal solution is \(Xl X2\) = \(\~ \~\) of value \~ = 2.353.  
Table 4.1.  
*XII* x\~ I\(x*l \) pI i\(t\)*  
1 0.000 0.000 -7.000 0.900 3  
2 1.800 1.800 0.200 0.810 3  
3 3.420 3.420 -0.520 0.729 2  
4 -0.225 2.691 -2.068 0.656 3  
5 1.087 4.003 -2.919 0.590 1  
6 1.678 2.822 0.033 0.531 1  
7 2.209 1.759 0.937 0.478 3  
8 3.166 2.716 1.455 0.430 2  
9 1.013 2.285 -0.402 0.387 3  
10 1.788 3.060 -0.332 0.349 1  
11 2.137 2.363 1.411 0.314 1  
12 2.451 1.735 1.372 0.282 3  
13 3.016 2.300 2.300 0.254 5  
14 3.016 2.554 1.907 0.229 1  
15 3.244 2.097 1.681 0.206 2  
16 2.215 1.891 1.212 0.185 3  
17 2.585 2.262 2.062 0.167 1  
18 2.752 1.928 1.928 0.150 5  
19 2.752 2.078 2.078 0.135 5  
20 2.752 2.213 2.213 0.122 5  
21 2.752 2.335 2.083 0.109 1  
22 2.862 2.116 2.116 0.098 5  
23 2.862 2.214 2.214 0.089 5  
24 2.862 2.303 2.256 0.080 1  
25 2.941 2.144 2.144 0.072 5 48 1.2\. Linear Programming  
\~--------------------------------\~----\~\~Xl  
Figure 4.7  
We can also view the problem as one of finding *\(x* 1, *x* 2\) such that the smallest slack  
variable Y i of the constraints  
4  
+ Y2 *20*  
+ Y3 - 7  
*+* Y4 0  
*+* Ys = 0  
is as large as possible \(see Figure 4.7\). With this geometry, each subgradient step is in the  
direction of the normal to the constraint whose slack variable is smallest.  
Because the magnitudes of the constraint coefficients are different, the five subgra-  
dients have different magnitudes which can substantially bias the progress of the algo-  
rithm. This suggests the use of normalized subgradients slllsil in the subgradient algo-  
rithm. For Example 4.2, this gives the iterations shown in Table 4.2. Note that more rapid  
convergence is achieved using normalized subgradients.  
Finally suppose that *x* ERn must satisfy some linear constraints, say *x* E C. Thus we  
have the problem  
\(4.3\) 11 = max\{f\(x\)\: *x* E C\}, where/ex\) = . min *\(aix* - bJ.  
/=1, ... ,m  
The subgradient algorithm for \(4.3\) is as before, except that Step 3 is modified to maintain  
feasibility. 5\. Notes 49  
Table 4.2.  
xi x\~ f\(xt \) pt *i\(t\)*  
1 0.000 0.000 -7.000 0.900 3  
2 0.636 0.636 -4.454 0.810 3  
3 1.209 1.209 -2.163 0.729 3  
4 1.725 1.725 -0.101 0.656 3  
5 2.189 2.189 1.754 0.590 3  
6 2.606 2.606 1.394 0.531 1  
7 2.844 2.131 2.131 0.478 5  
8 2.844 2.609 1.626 0.430 1  
9 3.036 2.224 2.224 0.387 5  
10 3.036 2.611 1.813 0.349 1  
11 3.192 2.300 1.739 0.314 2  
12 2.885 2.238 2.238 0.282 5  
13 2.885 2.520 1.844 0.254 1  
14 2.998 2.293 2.293 0.229 5  
15 2.998 2.522 1.954 0.206 1  
16 3.090 2.338 2.211 0.185 2  
17 2.909 2.301 2.301 0.167 5  
18 2.909 2.468 1.972 0.150 1  
19 2.976 2.334 2.308 0.135 1  
20 3.036 2.213 2.213 0.122 5  
21 3.036 2.335 2.335 0.109 5  
22 3.036 2.444 2.148 0.098 1  
23 3.080 2.356 2.243 0.089 2  
24 2.993 2.339 2.316 0.080 1  
25 3.029 2.267 2.267 0.072 5  
Step 3'\: Let *yt+l* = *Xl* + Ot*st* for some Ot \> ° and let *xt+1* = arg minxEc /Ix \_ *yt+lll.*  
In other words, *Xt+1* is the projection of *yt+1* onto the feasible region C. A typical  
application is to have C = R\~, in which case *Xj+1* = max\(x; + OtS;, 0\) for j = 1, ... , n  
5. NOTES  
Sections 1.2.1-1.2.3.  
Chvatal \(1983\) gave a modern and comprehensive treatment oflinear programming, with  
the exception of the significant post-1983 developments covered in Sections 1.6.2-1.6.4.  
Some earlier books are Charnes and Cooper \(1961\), Dantzig \(1963\), Gass \(1975\), Hadley  
\(1962\), and Murty \(1976\).  
Section 1.2.4  
The use of subgradient directions in the solution of large-scale linear programs that arise  
from combinatorial optimization problems was instigated by Held and Karp \(1970, 1971\)  
in a study of the traveling salesman problem. Held et al. \(1974\) investigated the behavior of  
a subgradient algorithm in a variety of combinatorial problems. A theoretical analysis of  
the convergence of subgradient algorithms is given by Goffin \(1977\). Subgradients and  
subgradient algorithms are also discussed by Grinold \(1970, 1972\), Camerini et al. \(1975\),  
Shapiro \(1979a, b\), and Sandi \(1979\). **1.3**  
# Graphs and Networks  
1. INTRODUCTION  
In this section we give the terminology and some elementary results of graph theory. For  
our purposes the language of graphs is nearly as important as the results, which are  
elementary and given without proof.  
In the remaining sections, we define some classical optimization problems on graphs  
and present algorithms to solve them. All of these problems are linear programming  
problems and, excluding the minimum-weight spanning tree problem, are in the class of  
linear programming problems known as *network flow problems.* Their structure makes it  
possible to solve them by special-purpose algorithms that are more efficient than the  
simplex method.  
These problems are of interest to us because they frequently arise as subproblems in the  
solution of integer programs. The algorithms presented in the following sections are  
examples of classes of algorithms that are used to solve some of the problems considered in  
Parts II and III. We will introduce the ideas of recursive, greedy, augmenting, primal-dual,  
and specialized simplex algorithms. So this chapter also has the pedagogical objective of  
introducing different algorithmic approaches in a simple setting. To explain the basic ideas  
succinctly, we have deliberately chosen to present simple, rather than efficient, versions of  
the algorithms. Thus, in this chapter, the reader should not necessarily expect the  
algorithmic details that yield efficient implementations.  
A *graph* G = *\(V, E\)* consists of a finite, nonempty set *V* = \{l, 2, ... , *m\}* and a set  
*E* = *reb e2,* ... *,en\}* whose elements are subsets of *V* of size 2, that is, *ek* = \(i,\}\), where  
i,\} E *V.* The elements of *V* are called *nodes,* and the elements of *E* are called *edges.* Thus  
graphs are a mechanism for specifying certain pairs of a set.  
Graphs can be represented pictorially in *R2* by points and lines. The points or nodes are  
placed arbitrarily in the plane, and a line connects points i and\} if *e* = \(i,\}\) E *E.* A graph  
with five nodes and seven edges is shown in Figure 1.1.  
Graphs are useful models for many of the problems considered in combinatorial  
optimization. We have used graphs informally in Chapter 1.1 to model network flow  
problems, the traveling salesman problem, and so on. Generic examples of graph models  
are derived from transportation and communication networks. Here *V* is a set of cities,  
and *E* consists of those pairs of cities that are connected by a direct transportation or  
communication link. Another set of generic examples concerns relationships between  
objects. For example, Vis a set of people; and *E* are those pairs that are married, or of the  
same sex, religion, and so on. The list of examples could go on and on. We are just going to  
give one more that relates directly to some examples discussed in Chapter 1.1.  
A graph G = *\(V, E\)* is called *bipartite* if there is a partition of *V* into disjoint sets  
VI and *V2* such that each edge joins a node in VI to a node in *V2* \(see Figure 1.2\). Bipartite  
50 1. Introduction  
51  
e3 4  
*e7*  
e1 e5 5  
*e6*  
2 e4 3  
Figure l.1. *V* = \(I, 2, 3,4, 5\) and *E* = *\{el* = \(I, 2\), *e2* = \(\\, 3\), *e3* = \(\\, 4\), *e4* = \(2,3\), *es* = \(3, 4\), *e6* = \(3,5\),  
*e,* = \(4, 5\)\}.  
graphs arise in many applications. For example, in the assignment problem, *VI* is the set of  
workers, *V2* is the set of jobs, and \(i, *j\)* E *E* if and only if worker *i* can do job *j.* In facility  
location problems, VI is the set of customers, *V2* is the set offacilities, and \(i, *j\)* E *E* if and  
only if facility *j* can serve customer *i.*  
Unless otherwise specified, we assume that the edges are distinct and if *e* = *\(i,j\),* then  
*i* \* *j.* Such graphs are called *simple.*  
We say that *ei* E *E meets* or is *incident to v* E *V* or that *v* is an *endpoint* of *ei* if *v* E *ei'*  
One way to represent a graph is by its *m* x *n node-edge incidence matrix A* = *\(aij\),* where  
*au=g*  
if *ej* is incident to node *i*  
otherwise.  
The incidence matrix of the graph of Figure 1.1 is  
*el e2 e3 e4 es e6 e7*  
1 1 1 0 0 0 0 1  
1 0 0 0 0 0 2  
*A=* 0 1 0 1 1 0 3  
0 0 1 0 1 0 4  
0 0 0 0 0 5  
Note that each column of *A* contains exactly two l's. The number of l's in row *i* equals the  
number of edges incident to node *i* and is called the *degree* of node i. The set of edges  
incident to node i is denoted by J\(i\). We have 0 \~ I J\(i\) I \~ *m* - 1 for all i E *V.* A graph is  
called *complete* if it contains all possible edges, that is, I J\(i\) I = *m* - 1 for all i E *V.*  
Figure 1.2 52 1.3\. Graphs and Networks  
Another way to represent a graph is by its *m* x *m* adjacency matrix *A'* = *\(aij\),* where  
*a'.* = \{ 1 if *\(i,\}\)* E *E*  
*I\)* 0 otherwise.  
The adjacency matrix for the graph of Figure 1.1 is  
0 1 1 0  
0 1 0 0  
*A'=* 1 0 1  
1 0 0 1  
0 0 0  
The *complement* of G = *\(V, E\)* is G = *\(V, E\),* where *E* = *\{e\: e* \$. *E\}.* The complement  
of the graph of Figure 1.1 is shown in Figure 1.3.  
For *U* r;;. *V,* let *E\(U\)* = \{\(i,\}\)\: *\(i,\}\)* E *E, i* E *U,\}* E *U\}. E\(U\)* is the set of edges with  
both endpoints in *U.* If *V'* r;;. *VandE'* r;;. *E\(V'\),* then G' = *\(V\: E'\)* is said to be a *subgraph*  
of G = *\(V, E\).* G' is a *spanning subgraph* if *V'* = *V.* G' is the *subgraph induced by V'* if  
*E'* = *E\(V'\).* Figure 1.4 gives the subgraph induced by *V'* = \{l, 2, 3, 4\} of the graph of  
Figure 1.1.  
Two of the most important definitions that we need are paths and cycles. To define  
these terms, we need another definition. A node sequence *va,* v I\> ••• , V *k\> k* \~ 1, is called a  
*VO-Vk walk* if *\(Vi-I\> Vi\)* E *E* for *i* = 1, ... *,k.* Node *Vo* is called the *origin,* node *Vk* is called  
the *destination,* and nodes \{VI\> ••• , *Vk-l\}* are *intermediate nodes.* We can also represent a  
walk by its edge sequence *el\> e2,* ... , *ek,* where *ei* = *\(Vi-I\>* V;\) for *i* = 1, ... *,k.* The *length*  
of the walk *Vo, VI\>* ... , *Vk* or *el\>* ... *,ek* is *k,* the number of edges in it. A walk is called a  
*path* if there are no node repetitions. In the graph of Figure 1.1, 1, 3,4, 5 is a 1-5 path. Its  
edge sequence is *e2, es, e7.* A *VO-Vk* walk is said to be *closed* ifvo = *Vk.* A closed walk is said to  
be a *cycle* if *k* \~ 3 and *va'* VI\> ••• , *Vk-l* is a path. In the graph of Figure 1.1,1,3,5,4,1 is a  
cycle oflength 4. A graph is said to be *acyclic* if it does not contain any cycles.  
Let *w* be a *VO-Vk* walk with node repetitions. Consider a subsequence of nodes *Vi, Vi+\),*  
... , *Vj* = *Vi* that contains no node repetitions other than the beginning and end nodes.  
\(The subsequence is a cycle unless it contains three nodes\). By deleting *Vi+1\>* ... , *Vj* from *w*  
we obtain a *Vo-* V *k* walk of smaller length. And by deleting all such subsequences, we obtain  
a *VO-Vk* path. Referring to Figure 1.1, by deleting the indicated subsequences from  
1, 3, 4, 1, 2, 3, 4, 3, 5  
'-.,---I '---v---/  
we obtain the 1-5 path 1, 2, 3, 5.  
4  
5  
Figure 1.3 1. Introduction 53  
4  
2  
Figure 1.4  
Proposition 1.1. *There* is *a unique partition of the nodes of a graph* G *into subsets* VI,  
... , *Vp with the property that nodes i and j are in the same subset if and only if* G *contains*  
*an i-j path.*  
Let *Vk* be a subset of the partition. The subgraph *Gk* = *\(Vb E\(Vk \)\)* is called a *component*  
of G. G is said to be *connected* if it has one component. This means that there is a path  
between each pair of nodes. The graph of Figure 1.1 is connected. The graph of Figure 1.3  
has two components defined by *V j* = \{l, 2, 4, 5\} and *V2* = \{3\}. When a component con-  
tains only one node, that node is said to be *isolated.*  
An acyclic graph is called a *forest.* A connected forest is called a *tree.* The subgraph  
obtained by deleting edges *\{e* 4, *e* 5, *e* 6\} from the graph of Figure 1.1 is a spanning tree \(see  
Figure 1.5\).  
The following proposition gives four useful characterizations of trees.  
Proposition 1.2. *Let* G = *\(V, E\) be a graph on m nodes. The following statements are*  
*equivalent.*  
1. G *is a tree.*  
*2\. There is a unique path between each pair of nodes in* G.  
3. G *contains m* - 1 *edges and is connected.*  
4\. G *contains m* - 1 *edges and is acyclic.*  
5. G *is connected and acyclic.*  
Trees are minimal \(with respect to the number of edges\) connected graphs. A *leaf* of a  
graph is a node of degree 1. It is easy to show that every component of a forest with at least  
two nodes contains at least two leaves.  
Corollary 1.3  
a. *IfG =\(V,E\) isatreeand e'* \$. *E, then* G' = *\(V, E* U *\{e'\}\) contains exactly one cycle.*  
b. *IfC is the edge set of the cycle ofG' and e\** E C \\ *\{e'\},* G\* = *\(V, E* U *\{e'\}* \\ *\{e\*\}\) also*  
*is a tree.*  
4  
2 3  
Figure 1.5 54 1.3\. Graphs and Networks  
A walk is called *odd* or *even* according to whether its length is odd or even. The  
following proposition characterizes bipartite graphs.  
**Proposition 1.4.** *A graph is bipartite if and only ifit has no odd cycles.*  
An important generalization of graphs is directed graphs. A *directed graph* or *digraph*  
qz; = *\(V,* d\) consists ofa finite, nonempty set *V* = \{I, ... , *m\}* and a setd = *\{eb e2,* ... , *en\}*  
whose elements are *ordered* subsets of *V* of size 2 called *arcs.* \(Note that we use *e* for both  
an edge of a graph and an arc of a digraph.\) In a digraph, \(i,\}\) and \(j, *i\)* are different  
elements and we may have neither, one, or both of these elements. In the pictorial  
representation of a digraph, arrows are used to indicate order. Figure 1.6 gives a digraph.  
Digraphs are useful for modeling one-way relationships. For example, it is possible to  
go directly from intersection *i* to intersection\} directly \(by a one-way street\) but not  
conversely, *i* is the father of\} but not conversely, and so on.  
By removing the directions from the arcs of a digraph qz;, that is, replacing the arcs by  
edges and removing any edge duplications, we obtain a graph G that is said to *underlie* qz;.  
The *node-arc incidence matrix* of a digraph qz; with *m* nodes and *n* edges is the *m* x *n*  
matrix *A* with  
aij\~ \{ -\~  
if *ej* = *\(k, i\)* for some *k* E *V* \\ \{i\}  
if *ej* = \(i, *k\)* for some *k* E *V* \\ \{i\}  
otherwise.  
The node-arc incidence matrix of the graph of Figure 1.6 is  
*el e2 e3 e4 es e6 e7 eg e9*  
-1 -1 -1 0 0 0 0 1 0 1  
1 0 0 -1 0 0 0 0 2  
*A=* 0 1 0 1 -1 -1 -1 0 0 3  
0 0 1 0 0 1 0 -1 -1 4  
0 0 0 0 0 0 0 5  
The node sequence vo, Vb ... , Vb *k* "'" 1, is a *VO-Vk directed walk* in qz; = *\(V,* d\) if  
\(Vi-b *Vi\)* Ed for *i* = 1, ... *,k.* The walk is called a *VO-Vk directed path* if there are no node  
repetitions and is called a *directed cycle* if *k* "'" 2, and the only node repetition is Vo = *Vk.* In  
Figure 1.6,1,3,4,1 is a directed cycle, but 1, 4, 3, 5 is not a directed path since \(4,3\) \$. d.  
5  
2  
Figure 1.6. *V* = \{t, 2, 3, 4, 5\} andst = eel = \(I, 2\), *e2* = \(1,3\), *e\)* = \(1, 4\)  
*e4* = \(2,3\), *e,* = \(3, 2\), *e6* = \(3, 4\),  
*e7* = \(3,5\), *es* = \(4, 1\), *e9* = \(4, 5\)\}. 2\. The Minimum-Weight or Shortest-Path Problem 55  
By deleting the cycles from a *vo-v k* directed walk with *v k* '\*' *va,* we obtain a *vo-v k* directed  
path \(see Figure 1. 7\). Note that the figure does not unambiguously specify the walk.  
Generally when we deal with digraphs we use the term *path* to mean a directed path.  
However, there are times when we need to distinguish between a directed path in *f!iJ* and a  
path in its underlying graph. Then we use the term *dipath* to refer to the directed path in *f!iJ.*  
The same terminology applies to cycles.  
A directed graph *f!iJ* is called *strongly connected* if there is a directed path between each  
pair of nodes. When we say that *f!iJ* is connected, we mean that the underlying graph is  
connected.  
A digraph is called a *tree* if the underlying graph is a tree. A subgraph of *f!iJ* that is a tree  
and spans *f!iJ* is called a *spanning tree.* A tree is called a *branching* if there is a node called  
the *root* such that there is a directed path from the root to every other node. If the root *r* is  
specified a priori, we will refer to an r-branching or branching with specified root *r.*  
2. **THE MINIMUM-WEIGHT OR SHORTEST-PATH PROBLEM**  
One of the simplest and most widely applicable combinatorial optimization problems is  
the minimum-weight or shortest-path problem. An instance of the shortest-path problem  
is given by a digraph *f!iJ* = *\(V, A\),* a function *w* \: *d* .... *R* I \(where *We* is the weight of arc *e\),*  
and designated origin and destination nodes 1 and *m,* respectively. The weight of a *1-m*  
path is the sum of the arc weights over all arcs in the path. \(All paths considered here are  
directed.\) The problem is to find a *1-m* path of minimum weight. Such a path is generally  
called a *shortest path,* but it may not be a minimum-length path unless all arcs have equal  
weight. Clearly iff!iJ is strongly connected, there is a shortest path since no path can contain  
more than *m* - 1 arcs where I *VI* = *m.*  
A generic example of the shortest-path problem is to find a minimum cost route  
between two cities where, if *e* = *\(i,* i\), then *We* is the cost of a direct route between nodes *i*  
and\}. We will encounter many other examples throughout the text, including the finding  
of shortest paths as a subroutine in the solution of more complex problems.  
We first consider the special case in which all arc weights are nonnegative, that is,  
*w\: d* .... *Rl.* Thus, if *p* is a 1-*m* path contained in a 1-*m* walk *p\:* then the weight of *p* is not  
greater than the weight of *p\:*  
The algorithm we present for solving this problem actually solves the slightly more  
general problem of finding minimum-weight paths from node 1 to all other nodes. It is  
based on the following fundamental property of minimum-weight paths.  
*Vg*  
Figure 1.7. Directed walk\: *Yo,* Vb *V2, VIO, V\), VI, V2, V\), V4, V5, V7, Vs, V9, V5, V6.* Directed path\:  
\~ \~. 56 1.3\. Graphs and Networks  
Proposition 2.1. *Suppose k is an intermediate node on a minimum-weight l-i path Pi.*  
*Then the l-ksubpathpk a/pi is a minimum-weight l-kpath.*  
*Proof* Let *w\(P\)* be the weight of path *p.* The proof is by contradiction. So we suppose  
thatlh is a *l-k* path and *w\(h\)* \< *W\(Pk\)* \(see Figure 2.1\).  
Let *Pi* = *\(Ph Pk;\).* Then *Pi* = \(fib *Pki\)* is a *l-i* walk and  
This is a contradiction because *Pi* contains a *l-i* path *Pi* and *W\(jJi\)* ",;; *W\(fii\)* \< *W\(Pi\)' •*  
Now let *g\(i\)* be the weight of a minimum-weight l-i path and define *g\(l\)* = 0.  
Dijkstra's Minimum-Weight Path Algorithm  
*Step* 1 *\(Initialization\)\: g\(1\)* = 0, *U* = \{l\}, *h\(j\)* = *Wlj* if\(1,j\) *E.st1, h\(j\)* = 00 otherwise.  
*Step* 2\: Let *i* = arg\(minjlw *h\(j».* If the minimum is not unique, select any *i* that achieves  
the minimum. Set *U .... U* U \{i\} and *g\(i\)* = *h\(i\).* If *U* = *V,* stop.  
*Step* 3\: For allj \$. *U* with *\(i,j\) E.st1, h\(j\) .... min\(g\(i\)* + *Wij' h\(j».* Return to Step 2.  
As stated, the algorithm determines only the weights of paths. To determine the path,  
we simply keep a record of the node beforej on the path that has weight *h\(j\).* Thus, in the  
initialization, we let *p\(j\)* = 1 if \(1, j\) *E.st1* and j otherwise, and in Step 3 we set *p\(j\)* to *i* if  
*h\(j\)* = *g\(i\)* + *w ij.* Thus when the algorithm terminates, *p\(j\)* is the node before j on some  
minimum-weight I-j path.  
Theorem 2.2. *Dijkstra's algorithm is correct.*  
*Proof* The proofis inductive. The induction hypothesis is that after *t* passes through  
Step 3, *g\(j\)* is correct for allj E *U,* and *h\(j\)* is the weight of a minimum-weight I-j path  
restricted to having intermediate nodes in the set *U.* This is true initially with *U* = \{l\} and  
*g\(1\)* = 0.  
From the induction hypothesis, *h\(j\)* \~ *g\(j\)* for allj \$. *U.* Suppose now that *h\(i\)* \> *g\(i\),*  
where *i* is as defined in Step 2. Then the minimum-weight l-i path must contain some  
intermediate node not in *U.* Let *k* \$. *U* be the first such node. Then by Proposition 2.1 the  
subpath from 1 to *k* must be a minimum-weight *l-k* path so that its weight is *g\(k\).* But this  
*l-k* path contains only intermediate nodes in *U.* Thus *h\(k\)* = *g\(k\)* ",;; *g\(i\)* \< *h\(i\),* contra-  
dicting the choice *ofi.* Hence *h\(i\)* = *g\(i\).*  
To see that for j \$. *U* U \{i\}, *h\(j\)* now represents the weight of a minimum weight I-j  
path with intermediate nodes in *U* U \{i\}, it suffices to observe that any such path either  
remains as before or contains *i* as its last node, in which case *h\(j\)* = *g\(i\)* + *wij. •*  
Figure 2.1 2\. The Minimum-Weight or Shortest-Path Problem 57  
In order to consider the number of computations required in Dijkstra's algorithm and  
other algorithms to be given later, we need to introduce some new notation. Given  
*functionsf\(n\)* andg\(n\) from zl to Zl, we saythatf\(n\) is *O\(g\(n»* if there is a constant c \> 0  
and *n'* E Zl such *thatf\(n\).-,s; cg\(n\)"* for all *n* \~ *n'.* Thus, for example if  
*f\(n\)* = *7.2n3* + *4n 2* + *9n* + 4,  
*thenf\(n\)* is *O\(n3\).* In other words, the "big 0" notation allows us to approximateffrom  
above by a simpler function cg with c unspecified.  
*Letf\(m\)* be the maximum number of basic operations \(additions and comparisons\)  
required by Dijkstra's algorithm on a graph with *m* nodes. At each step of the algorithm,  
I *U* I is increased by 1. When I *V* \\ *U* I = *k,* 1 .-,s; *k* .-,s; *m* - 1, Step 3 requires no more than *k*  
additions and comparisons, and Step 2 requires finding the minimum of *k* numbers.  
Hence *fim\)* is bounded by c Lr\~l *k* for some constant c. Thus Dijkstra's algorithm is  
*O\(m2\).*  
The efficiency ofthe algorithm can be seen by observing that each arc is examined only  
once. Note that a slight improvement can be obtained by including in *U* at Step 2 all nodes  
for which the minimum is achieved.  
*Example 2.1.* We determine minimum-distance paths from Chicago to nine other  
midwestern cities. The distances shown in Table 2.1 are miles/lO, and *W ij* = *Wji* for all *i* '\*' *j.*  
Table 2.2 gives the *h\(j\)* andp\(j\) at each iteration if they have changed from the previous  
iteration. An asterisk indicates that *h\(j\)* = *g\(j\).*  
Table 2.1.  
2 3 4 5 6 7 8 9 10  
l. Chicago 96 105 50 41 86 46 29 56 70  
2. Dallas 78 49 94 21 64 63 41 37  
3. Denver 60 84 61 54 86 76 51  
4. Kansas City \(MO\) 45 35 20 26 17 18  
5. Minneapolis 80 36 55 59 64  
6. Oklahoma City 46 50 28 8  
7. Omaha 45 37 30  
8. St. Louis 21 45  
9. Springfield \(MO\) 25  
10. Wichita  
Table 2.2.  
Iteration 2 3 4 5 6 7 8 9 10  
° \(00,2\) \(00,3\) \(00,4\) \(00, 5\) \(00,6\) \(00, 7\) \(00, 8\) \(00,9\) \(00, 10\)  
1 96, 1 105,1 50,1 41,1 86,1 46,1 29,1\* 56,1 70,1  
2 92,8 \* 79,8 50,8  
3 \*  
4 100,7 \* \*  
5 91,9 78,9 68,4\*  
6 76,10\*  
7 \*  
8 \* 58 1.3\. Graphs and Networks  
8 5  
2  
Figure 2.2  
Figure 2.2 gives the solution.  
It is easy to see that the algorithm can fail when there are negative arc weights. An  
example is shown in Figure 2.3. The algorithm would set *g\(3\)* = 3 at iteration 1, but  
*g\(3\)* = *W2.3* + *g\(2\)* = - 2 + 4 = 2. In particular, it is not valid to set *g\(i\)* = *h\(i\)* just because  
*h\(i\)* is the smallest value of *h\(j\)* for *j* \$. *U.*  
However, if the graph does not contain any cycles of negative weight, the algorithm can  
be modified to treat negative arc weights. The essential modification is that none of the  
*h\(j\)* are set equal to *g\(j\)* until *m* iterations have taken place.  
Bellman-Ford Minimum-Weight Path Algorithm  
*Step* 1 *\(Initialization\)\:* hO \(1\) = 0, hO *\(j\)* = 00 for *j* E *V* \\ \{l\}, *k* = 1.  
*Step* 2\: For allj E *V,*  
*Step* 3\: If *hk\(j\)* = *hk-1\(j\)* for all *j* E *V,* then *g\(j\)* = *hk\(j\)* for all *j* E *V.* Otherwise if  
*k* \< *m, k* \<-- *k* + 1 and return to Step 2. If *k* = *m,* the graph contains a cycle of negative  
weight.  
Theorem 2.3. *The Bellman-Ford algorithm is correct.*  
*Proof* We claim that *hk\(j\)* is the weight of a minimum-weight 1-*j* walk containing no  
more than *k* arcs. This is trivially true for *k* = o. Suppose it is true for *k* - 1. At iteration *k,*  
we consider all possible ways of adding an arc *\(i,j\)* to the end ofa minimum-weight *l-i*  
walk containing no more than *k* - 1 arcs, and then we compare the weights of these walks  
to the weight of a minimum-weight *I-j* walk containing *k* - 1 or fewer arcs. Thus, by  
2  
3  
Figure 2.3 2. The Minimum-Weight or Shortest-Path Problem 59  
2 -2 4 3 6  
8  
10  
4  
12  
-4  
4  
3 -1 5 2 7  
Figure 2.4  
enumeration, *hk\(j\)* is the weight of a minimum-weight 1-\) walk containing no more than  
*k* arcs. Now if *hm\(j\)* = *hm-1\(j\)* for all\) E *V,* then *hk\(j\)* = *hm\(j\)* for all *k* \> *m.* Hence the  
minimum-weight 1-\) walk is of bounded weight for all\) E *V.* This implies that *I\}j\)* contains  
no cycles of negative weight so that *hm-1\(j\)* is the weight ofa minimum-weight 1-\) path  
containing *m* - 1 or fewer arcs. But since any 1-\) path contains no more than *m* - 1 arcs,  
*hm-1\(j\)* = *g\(j\).* On the other hand, if there exists a\)\* such that *hmU\*\)* \< *hm-1U\*\),* there is a  
1-\)\* walk containing *m* arcs that has lower weight than any 1-\)\* walk containing *m* - 1  
arcs. Hence this walk contains a cycle of negative weight. •  
To find a minimum-weight path or a negative-weight cycle, we use the bookkeeping  
scheme proposed above for Dijkstra's algorithm. In other words if *h\\\)* = *wi\}* + *hk-1\(i\),*  
then we set *pk\(j\)* = *i.* To avoid having cycles of zero weight, set *pk\(j\)* = *pk-l\(j\)* whenever  
*hk\(j\)* = *hk-1\(j\).*  
At each of the *m* steps of the algorithm, we do an addition for each of the *n* arcs and then  
for each node take the minimum over *m* numbers. Hence the number of computations is  
*cm\(n* + *m\),* where c is a constant. In the case of a complete digraph, the number of  
computations is *O\(m3\).*  
Thus the price we pay for being able to deal with negative arc weights in the absence of  
negative-weight cycles is an increase in computation time by a factor of *m.* Although the  
algorithm is able to detect a negative-weight cycle, it is unable to find a minimum-weight  
path in this case. The general minimum-weight path problem is much more difficult.  
*Example* 2.2\. The numbers on the edges of the digraph of Figure 2.4 are the weights. The  
problem is to find minimum-weight paths from node 1 to all other nodes or to detect a  
negative-weight cycle. Table 2.3 gives *hk\(j\)* and *pk\(j\)* for *k* = 1, ... , 7.  
The solution is shown in Figure 2.5.  
Table 2.3.  
Iteration 2 3 4 5 6 7  
0 \(0, 1\) \(00,2\) \(00, 3\) \(00,4\) \(00, 5\) \(00,6\) \(00, 7\)  
1 \(8, 1\) \(4, 1\)  
2 \(14, 3\) \(3,3\)  
3 \(7,5\) \(5,5\)  
4 \(10,6\) \(6,7\)  
5 \(9,6\)  
6 \(7,4\)  
7 No change 60  
2  
-2  
4 3  
1.3\. Graphs and Networks  
6  
4  
3  
-1  
Figure 2.5  
5 2  
7  
3. THE MINIMUM-WEIGHT SPANNING TREE PROBLEM  
Spanning trees are used in the design of communication networks in which each node  
must be able to communicate with every other node. If the communication links are  
expensive, then it is desirable to have just one path between each pair of nodes so that the  
resulting network is a spanning tree.  
Given a connected graph G = *\(V, E\),* let *E* be those pairs of nodes that can be joined  
directly by a communication link. The weight of an edge *e* E *E* is *W e* \~ O. The problem is  
to build a spanning tree of G of minimum weight, where the weight of a tree  
*T* = *\(V, E \(T\)\), E\(T\)* s *E,* is *LeEE\(D We.*  
lt is easy to build a spanning tree from a connected graph. We scan the edges in any  
order, say *e!, e2,* ... *,en,* and include *ei* in the tree if and only if it does not create a cycle  
with those edges already chosen from *\{e!,* ... , *ei-l\}.* More precisely, we have  
Algorithm for Constructing a Spanning Tree  
*Step* 1 *\(Initialization\)\:* Edge ordering *e!, e2,* ... , *en, EO* = 0, *k* = 1.  
*Step* 2\: If *H* = *\(V, E k-1* U *\{ek\}\)* is acyclic, then *Ek* = *E k-1* U *\{ek\}.* Otherwise *Ek* = *E k-1•*  
*Step* 3\: If *IEk* I = *m* - 1, stop, *\(V, Ek\)* is a spanning tree. Otherwise *k .... k* + 1, and return  
to Step 2.  
To execute Step 2, we keep track of the components of *\(V, E k-1\).* Then *ek* is included if  
and only if it joins two nodes that are in different components of *\(V, E k- 1\).* Thus each time  
we add an edge, the number of components is decreased by 1.  
Now to find a minimum-weight spanning tree we simply order the edges according to  
increasing weight. Thus Step 1 is replaced by\:  
*Step* l' *\(Initialization\)\:* Edge ordering *e!, e2,* ... *,en* such that *wee!\)* \~ *w\(e2\)* \~ ...  
\~ *ween\). EO* = 0, *k* = 1.  
The algorithm consisting of Steps 1', 2, and 3 is called a *greedy algorithm* because at  
each iteration the edge of least weight is considered and included in the tree if it does not  
create a cycle. The greedy algorithm does what is locally best without regard to future  
consequences.  
We now show that the greedy algorithm produces a minimum-weight spanning tree.  
However, for most combinatorial optimization problems, greedy algorithms are merely  
heuristics for finding a good feasible solution \(see Section 11.5.3\). 3\. The Minimum-Weight Spanning Tree Problem 61  
# "  
*T\* T*  
Figure 3.1  
Theorem 3.1. *The greedy algorithm produces a minimum-weight spanning tree.*  
*Proof* Suppose the greedy algorithm produces the tree ro = *\(V, EO\)* and ro is not  
optimal. Let *T\** = *\(V, E\*\)* be an optimal tree with the property that I *E\** \\ *EO* I is minimum  
over all optimal trees. Note that *E\** \\ *EO* \:f= 0 and *EO* \\ *E\** \:f= 0. Let *eO* be a smallest-weight  
edge in *EO* \\ *E\*.* Consider the set of edges *E\** U *\{eO\},* which, by Corollary 1.3, contains a  
unique cycle. Let C be the edge set of the cycle. Now by Corollary 1.3, there is an edge  
*e\** E C \\ *EO* such that the graph *\(V, E\** U *\{eO\}* \\ *\{e\*\}\)* is a tree, say *t* \(see Figure 3.1\).  
Moreover, *t* is an optimal tree, since *w eO* \~ *We',* where the inequality holds because the  
greedy algorithm selected *eO.* Finally I *it* \\ *EO* I = I *E\** \\ *EO* I - 1, which contradicts the  
choice of *T\*.* So ro is optimal. •  
Unless G is a sparse graph, that is, contains a very small number of edges, the dominant  
step of the greedy algorithm with respect to the number of computations is Step 1'. Since it  
takes *n* log *n* computations to order the edges by increasing weight, the total number of  
computations is *O\(n* log *n\).* There are, in fact, more efficient greedy-like algorithms as  
well as others designed specifically for sparse graphs.  
The greedy algorithm is still applicable if the graph contains edges with negative weight.  
It also applies to the problem of finding a maximum-weight spanning tree. Here we order  
the edges by decreasing weight. Note that if there are some edges of negative weight, we  
might prefer to solve the problem of finding a maximum-weight acyclic subgraph. To  
solve this problem, we simply terminate the greedy algorithm as soon as the last edge of  
positive weight has been considered.  
*Example* 3.1. A minimum-weight spanning tree for the graph of Example 2.1 is shown in  
Figure 3.2. After including the two edges of weight 21, edges \(9, 10\) and \(4,8\) are skipped  
because they would create cycles. Several other edges are skipped before the final edge  
\(3, 10\) is included. The example suggests why a full sort is not needed. Note that after a tree  
has been found on *V'* = \{2, 4, 6, 7, 8, 9, 1O\}, only edges that are incident to n, 3, 5\} need to  
be considered.  
5  
29  
6  
2  
Figure 3.2 62 1.3. Graphs and Networks  
4. THE MAXIMUM-FLOW AND MINIMUM-CUT PROBLEMS  
Network flow problems were introduced in Section 1.1.3. In the general linear minimum  
cost network flow problem, we are given a digraph *fliJ* = *\(V, d\),* a function *d\: d* .... R\~ where  
*di\)* is called the *capacity* of arc *\(i,* i\), a function *w\: d* .... *Rn* where *wi\)* is the *unit cost* offlow  
on arc *\(i,* i\), a function *b\: v .... R m* where *b;* is called the *supply* at node *i \(b;* \< 0 is called a  
*demand\),* and *L;Ev b;* = O. *Afeasibleflow* in *fliJ* is an *x\: d* --\> R\~ that satisfies  
\(4.1\) I *Xi\)* - I *Xj;* = *bi* all *i* E *V*  
*jEo-\(i\) jEo-\(i\)*  
\(4.2\) Xij\~dij all *\(i,\}\) Ed,*  
where *c5+* \(i\) = \{j\: *\(i,\}\) Ed\}* and *c5-* \(i\) = \{j\: *\(j,* i\) *Ed\}.*  
The equations \(4.1\) express the *node conservation* relations indicating that flow out-  
flow in = supply, and \(4.2\) indicates that the flow in each arc has a specified upper bound.  
When there is no upper bound on *xij,* we take *dij* = 00.  
The *general minimum-cost network flow problem* is to find a feasible flow that  
minimizes the objective function  
\(4.3\)  
We will consider this problem in Section 6.  
An important special case is the *transportation problem.* Here *CZiJ* = *\(VI* U *V2, d\)* is  
bipartite and *bi* \> 0 for all *i* E *VI* and *bi* \< 0 for all *i* E *V2•* We will study the transportation  
problem in Section 5.  
In this section we consider the *maximum-flow problem.* Two nodes sand *t,* called the  
*source* and *sink,* respectively, are specified, *bi* = 0 for all *i* E *V, wlS* = - 1, *wi\)* = 0 other-  
wise, and *dts* = 00. In other words, the problem is to find a feasible flow that maximizes the  
flow on arc *\(t,* s\) with no exogenous supplies or demands. Observe that any feasible flow  
that maximizes *XIS* will have *X;s* = 0 for *i* '\*' *t* and *Xlj* = 0 for\} '\*' *s* \(see Figure 4.1\). Hence  
*XIS* = *LjEo-\(s\) Xsj* = *LiEo-\(t\) Xi/.*  
SO, stated in its customary form, the maximum-flow problem is to maximize the flow  
out of the source or, equivalently, the flow into the sink, subject to the constraints of flow  
out equal to flow in for all the other nodes. Thus the maximum-flow problem asks the  
Figure 4.1  
0+-\) -----+-\(\)  
Figure 4.2 4\. The Maximum-Flow and Minimum-Cut Problems 63  
Figure 4.3  
question of how much flow can be sent from the source to the sink subject to conservation  
at the nodes and capacities on the arcs.  
We now introduce the minimum-cut problem. Let *\(U, U\)* be a partition of *V* such that  
s E *Uandt* E *U.* The set of arcs *J+\(U\)* = *\{\(i,j\)* Ed\: *i* E *U,j* E *U\}iscalledans-tcut* \(see  
Figure 4.2\). The *capacity* of the cut 15+\( *U\)* is \~\(i,j\)EO+\(U\) *dij.* The *minimum-cut problem* is to  
find a cut of minimum capacity.  
It is apparent from Figure 4.2 that all flow from s to *t* must pass through the arcs of  
J+\( *U\).* Hence for any feasible flow, we have  
*Xts* \~ *L dij* for all *s-t* cuts *U,*  
*\(i,j\)Eo+\(U\)*  
and, in particular,  
\(4.4\)  
max *Xts* \~ min *L dij.*  
*x* feasible *\(U\:sEU,trf-U\) \(i,j\)Eo+\(U\)*  
The algorithm we present in this section finds a maximum flow and minimum cut for  
any maximum-flow problem and also proves the following two theorems.  
Theorem 4.1. *The value of a maximum flow equals the capacity of a minimum cut.*  
Theorem 4.2. *If all of the arc capacities are integer-valued, then there is a maximum flow*  
*x* E z\:..  
An important concept in finding a maximum flow is that of an augmenting path. Given  
a flow *x,* we say that arc *\(i,j\)* is *saturated* ifxij = *dij.* Let *X* be any feasible flow and letp be  
the arcs of an *s-t* path with no saturated arcs. Then *min\(i,j\)EP \(dij* - *xij\)* = 6. \> 0, and *x* is  
not a maximum flow because we can increase *Xts* by 6. by increasing *xij* by 6. for all  
*\(i,j\)* E *p.* Ifno such path exists, *x* is said to be a *blockingflow.* A blocking flow may not be  
maximum.  
*Example* 4.1. In Example 4.1 \(see Figure 4.3\), the numbers on arc *\(i,j\)* are the pair  
*\(xij, dij\).1t* is easy to check that each of the four *s-t* paths contains a saturated arc. But we  
can increase *X ts* by 1 as shown in Figure 4.4, to obtain the flow given in Figure 4.5. The arc  
from 2 to 4 in Figure 4.4 indicates that we have returned to node 2 the unit of flow  
previously shipped from 2 to 4.  
Figure 4.4 64 1.3\. Graphs and Networks  
Figure 4.5  
Note that the flow in Figure 4.5 is maximum because *U* = \{l, 3, 4\} generates the cut  
*\<5+\( U\)* = \{\(l, 2\), \(3, 5\), \(4, 6\)\} of capacity 5 \(see Figure 4.6\).  
Given a flow *x,* define the digraph fi.iJ\(x\) = *\(V, d\(x\)\)* by  
*d\(x\)* = *\{\(i,\)\)\: \(i,\)\)* Ed, *Xij* \< *dij\}* U \{\(i,\)\)\: *\(j,* i\) Ed, *Xji* \> O\}  
*= dj\(x\)* U *dr\(x\).*  
We say that *dj\(x\)* is the set of *forward arcs* and *dr\(x\)* is the set of *reverse arcs.*  
Corresponding to Figure 4.3, we obtain the graph shown in Figure 4.7.  
An *s-t* path in *fi.iJ\(x\)* is called an *augmenting path* with respect to *x.*  
Proposition 4.3. *A feasible flow x* is *not maximum, if there is an augmenting path with*  
*respect to x.*  
*Proof* Let *p* be the set of arcs in an augmenting path and let *PI* = *p* n d *j\(x\)* and  
*Pr* = *P* n *dr\(x\).* Let  
6 = min\{ min *\(du* - *xu\),* min *Xji\}'*  
*\(i,j\)EPf \(i,j\)Ep,*  
By the definition of *dj\(x\)* and *dr\(x\),* 6 \> O. We claim that by increasing *Xu* by 6 for all  
*\(i,\)\)* E *PI* and decreasing *xji* by 6 for all *\(i,\)\)* E *p" X Is* increases by 6. By choice of 6, the  
capacity constraints are still satisfied. Also the flow out of s increases by 6, and the flow  
into *t* increases by 6, so *XIS* increases by 6. Now consider a node\) on the path. If the arcs in  
and out of\) are both forward \(reverse\) arcs, then the flow in and the flow out of i goes up  
\(down\) by 6. On the other hand, ifone of the arcs is a forward arc and the other is a reverse  
arc, there is no change of flow in or out. Hence conservation of flow is maintained. •  
2  
2  
Figure 4.6 4. The Maximum-Flow and Minimum-Cut Problems 65  
Figure 4.7  
We will prove the converse of Proposition 4.3 by showing that when no augmenting  
path exists, there is a cut of capacity *XIS'* Before doing so, we present a simple algorithm for  
finding an augmenting path if one exists.  
In the algorithm, nodes\) '\* *s* get a label of the form *\(p\(j\),* ll\), where *p\(j\)* is the node  
from which\) receives flow and II is the amount of flow sent *fromp\(j\)* to\}. The source *s* is  
initialized with the label *\(s,* \(0\) which means that *s* can receive any amount of flow  
exogenously. Figure 4.8 shows the labeling for forward and reverse arcs on *.@\(x\).*  
The labeling can also be done directly on the original graph, which is what we do in the  
algorithm given below, as shown in Figure 4.9.  
Augmenting Path Algorithm  
*Step 1 \(Initialization\)\: x* = ° \(or any feasible flow\). Source is labeled \(s,oo\). All nodes are  
unscanned, and all nodes except *s* are unlabeled. Let *i* = *s.*  
*Step* 2 *\(Scan node i\)\:* For all\} such that *\(i,\)* E *d\(x\), xij* \< *dij,* and\) is unlabeled, label\)  
\(i, min\(ll, *dij* - *Xi\)'* For all\) such that *\(j,i\)* E *d\(x\),\)* is unlabeled, and *Xji* \> 0, label  
\} \(i, min\(ll, *xji»'* Node *i* is scanned.  
*Step* 3\: Ifthe sink is labeled, go to Step 4. If not, choose a labeled and unscanned node *i*  
and go to Step 2. Ifnone exists, the current flow is maximum.  
*Step* 4\: Suppose *t* has the label *\(P\(t\),* ll\}. An augmenting path has been found. Use the first  
element of each label to trace the path back to *s.* Increase the flow by II on all forward  
arcs of the path, and decrease the flow by II on all reverse arcs. Erase all labels and  
return to Step 1.  
Note that to find one augmenting path, the number of computations is proportional to  
*n,* since each arc is considered no more than once.  
Forward arc *\(xij* \< *dii\)*  
*\(P\(i\),* 6\) \(i, min\(6, *dij* - *Xij»*  
C\)----------\~.Q\)  
*\(Xij, dij\)*  
Reverse arc *\(Xii\> 0\)*  
*\(P\(i\),6\) f\:\\/'* \(i, min\(6, *Xii»*  
\~------------------\~.-CZ\)  
Figure 4.8 66 I.3. Graphs and Networks  
*\(P\(i\),* ,0,\) \(i, min\(,0" *dij* - *Xij\)\)* if *xij* \< *dij*  
01---------..· 0  
*\(Xij, dij\)*  
*\(P\(i\),* ,0,\) \(i, min\(,0" *Xj;\)\)* if *xji* \> 0  
# 0\~·-----\~Q\)  
*\(Xji, dji\)*  
Figure 4.9  
*Example* 4.1 *\(continued\).* The algorithm is applied to the example shown in Figure 4.3.  
The labels are shown in Figure 4.10. Nodes are scanned in the order \(1,3,4,2,5,6\). The  
augmenting path has been shown in Figure 4.4. We now label again as shown in Figure  
4.11.  
Nodes 1,3, and 4 are scanned and no further labeling is possible. Now observe that the  
cut generated by the set oflabeled nodes *U* = \{l, 3, 4\}, that is, £5+\( *U\)* = \{\(1, 2\), \(3, 5\), \(4, 6\)\},  
has capacity equal to 5 so that this flow is maximum.  
Theorem 4.4. *A feasible flow x* is *maximum if and only if there* is *no augmenting path*  
*with respect to x.*  
*Proof* We have already shown \(Proposition 4.3\) that the existence of an augmenting  
path implies that the flow is not maximum. Now suppose there is no augmenting path and  
let *U* = \{i E *V\:* i is scanned in the augmenting\~ath algorithm\}. Then *s* E *U,*  
*t* \$. *u, xij* = *dij* for all *\(i,j\)* E *£5+cU\),* and *xij* = 0 if i E *U* andj E *U.* Hence the flow into  
node *t* equals *LU,j\)E&'\(U\) xij* = *LU,j\)E&'\(U\) dij.* In other words, we have shown that if the  
algorithm does not find an augmenting path, it defines a cut of capacity equal to the flow  
into node *t. •*  
Note that we have also proved Theorem 4.1, which also can be proved by linear  
programming duality. Theorem 4.2 also is a consequence of the augmenting path  
algorithm. The flow change in Step 4 either equals *x ij* for some \(i, *j\)* E *sti* with positive flow  
or *dij* - *xij* for some *\(i,j\)* E *sti* with *xij* \< *dij.* Hence if we begin the algorithm with any  
integral flow, we terminate with an integral maximum flow when all of the arc capacities  
are integral.  
Therefore, when the capacities are integral, the number of augmentations is bounded  
above by the value of the maximum flow. In fact the bound can be achieved with a poor  
\(4, 1\) \(3,4\)  
\(1,6\) \(2, 1\)  
Figure 4.10 4. The Maximum-Flow and Minimum-Cut Problems 67  
\(3,3\)  
0,5\}  
Figure 4.11  
choice of augmenting paths. In the example of Figure 4.12, each of the arcs except \(2,3\)  
has capacity *K,* where *K* is a large positive integer. The maximum flow *of2K* can be found  
with augmentations of *K* along the paths *s,* 2, *t* and *s,* 3, *t.* On the other hand, it is possible  
to send one unit of flow along the augmenting paths *s,* 2, 3, *t,* then one unit along the  
augmenting path *s,* 3, 2, *t,* and so on. To achieve the maximum flow in this way requires  
*2K* augmentations.  
Fortunately, a very natural way of selecting a next node to be scanned in the augment-  
ing path algorithm yields a bound on the number of augmentations that is independent of  
the capacities.  
**Proposition** 4.5. *If at each step of the augmenting path algorithm a shortest-length*  
*augmenting path is found, then the number of augmentations is bounded by mn.*  
Although we omit the details, the essential idea of the proofis to show that after, at most,  
*n* augmentations, the length of an augmenting path increases.  
Note that we don't need a general shortest-path algorithm to find an augmenting path  
with the fewest number of arcs. We simply use *breadth-first search* to choose the next node  
to be scanned. That is, after *s* is scanned, all labeled nodes\} with *\(s,\}\)* Ed are scanned.  
These are the labeled nodes of distance 1 from *s.* In general, all labeled nodes of distance *k*  
from *s* are scanned before any of distance *k* + 1 from *s.*  
Figure 4.12 68 1.3. Graphs and Networks  
There is another class of algorithms for the maximum-flow problem that are not based  
on augmenting paths, and some of these have a smaller bound on the number of  
computations than the breadth-first algorithm for finding augmenting paths. We will not  
give the details here. The basic idea is that a set of, at most, *n* - 1 blocking flows are found  
and then combined into a maximum flow.  
5. **THE TRANSPORTATION PROBLEM\: A PRIMAL-DUAL ALGORITHM**  
The transportation problem introduced in the previous section can be formulated as a  
minimum-cost flow problem on a bipartite digraph gz; = *\(Vj* U *V2, si\),* where Vj = \{l, ... ,  
*m* j\} is the set of sources, *V2* = *\{m* j + 1, ... , *m\}* is the set of sinks, and *si =*  
*\{\(i,\}\)\: i* E *V\),\}* E *V2\}.* Thus we make the assumption, without loss of generality, that there  
is an arc from each supply node to each demand node. The unit shipping cost from i E Vj  
to\} E *V2* is *wij.* Thus if there is really no arc from i to\}, we take *wij* to be very large. Node  
i E V j has a positive integral supply *ai,* and node\} E *V2* has a positive integral demand of  
*bj •* The flow out of a source is required to equal its supply, and the flow into a sink must  
equal its demand. Thus a necessary condition for feasibility is *LiEV, ai* = *LjEV, b j •*  
The *transportation problem* is to find a flow *x* E R\~, *n* = lsi I, that satisfies the supply-  
and-demand conservation equations at minimum cost. It can be formulated as the linear  
program  
\(5.1\)  
minI I *WijXij*  
iEV, *jEV,*  
I *Xij* = *ai* for i E Vj  
*jEV,*  
I *Xij* = *bj* for\} E *V2*  
*iEV,*  
xER\~.  
Note that the problem remains unchanged by adding a constant to all of the *wij,* so there is  
no loss of generality in assuming *W ij* \~ 0 for all i and *j.*  
When *ai* = *bj* = 1 for all *i* and *j* and *m* = *2m\),* \(5.1\) is the *assignment problem* \(see  
Section 1.1.2\).  
It is easy to accommodate some variations of the transportation problem in the  
formulation \(5.1\). For example, if *LiEV, ai* \> *LjEV, b j* and the source node constraints are  
*LjEV, xij* \~ *ai,* then we add a "dummy" sink with demand *LiEV, ai* - *LjEV, b j* and set the  
unit shipping costs to zero for arcs from Vj to the dummy node.  
The dual of\(5.1\) is  
max I *aiUi* + I *bjvj*  
\(5.2\) iEV, *jEV,*  
The complementary slackness conditions for this pair oflinear programs are  
or  
\(5.3\) 5\. The Transportation Problem\: A Primal-Dual Algorithm 69  
Figure 5.1  
where *wij* = *wij* - *Ui* - *Vj.* Thus *\(u, v\)* E *R m* is dual feasible if  
*\(S.4\)*  
\(We implement *wij* "very large" simply by assuming *wij* \> 0; then, by complementarity,  
*xij* = 0.\) Thus *x* E R\~ and *\(u, v\)* E *R m* are optimal solutions to the primal and dual if they  
satisfy *\(S.3\), \(S.4\),* and  
*\(S.S\)* I *Xij* = *ai* for *i* E *Vb* I *Xij* = *hj* for\} E *V2•*  
*JEV,* iEVI  
The *primal-dual* algorithm for the transportation problem maintains *\(S.3\), \(S.4\),* and  
*\(S.6\)* I *xij";\:; ai* for *i* E *Vb* I *Xij";\:; hj* for\} E *V2•*  
*JEV,* iEVI  
It is easy to find an initial solution that satisfies these conditions. For example, take  
*u?* = *minjEv, wij* for *i* E *VI* and *vJ* = miniEVI *\(wij* - *u?\)* fori E *V2 ,* and *XO* = O. At each  
major iteration the algorithm increases *LiEVI LjEV, xij* by an integer and stops when *\(S.S\)* is  
satisfied.  
Given *W,* to see whether *\(S.S\)* can be satisfied, we consider the problem of maximizing  
*LiEVI LjEV, xij* subjectto *\(S.3\), \(S.6\),* and *X* E R\~. This is *ans-t* maximum-flow problem on  
the digraph \~\(w\) = *\(VI* U *V2* U *\{s,* t\}, dew»\~, where  
*dew\)* = \{\(i,\}\) Ed\: *wij* = O\} U *\{\(s,* i\)\: i E *VI\}* U *\{\(j, t\)\:\}* E *V2\}.*  
The capacity of arc *\(s,* i\) is *ai* for *i* E Vb and the capacity of arc *\(j, t\)* is *hj* for\} E *V2•* All  
other arcs have "very large" capacities. If the maximum flow equals *LiEVI ai,* we have  
found an optimal solution. Ifnot, we change the dual variables.  
Consider the status of the node labels when the maximum-flow algorithm terminates  
\(see Figure *S.1\).* Note that *ifi* E T1 and\} E *V2* \\ *V;,* then *wij* \> 0 otherwise, we could label  
\} from *i.* Also if *i* E *VI* \\ T1 and\} E *V;,* then *xij* = 0 otherwise, we could label *i* from\}. 70 1.3\. Graphs and Networks  
We now change the dual solution as follows. Let *h* = miniEVi,jEV,\\V; *Wij* \> 0 and define  
new values for the dual variables by  
\(5.7\)  
Hence the new reduced costs are  
\(5.8\)  
*i* E *V;,* \} E *V2* \\ rz  
*i* E *V j \\ V;,* \} E rz  
otherwise.  
By the choice of *h,* dual feasibility is maintained. Complementary slackness is main-  
tained, since for *i* E *V;* and\} E *V2* \\ rz we have *Xij* = 0 by definition of *@\(w\),* and for  
*i* E *V;* \\ Vj and\} E rz we have *Xij* = 0 by the labeling rules.  
The important outcome is that Wi'\}' = 0 for some *i\** E *V;* and\}\* E *V2* \\ rz so that at  
least one new arc from *i* E *V;* to\} E *V2* \\ rz is added to *@\(w\)* to obtain @\(W\). Some arcs  
may also be deleted from *i* E Vj \\ *V;* to\} E rz. Now we can transfer the final labels from  
*@\(w\)* to @\(W\) and continue with the maximum-flow algorithm, with the assurance that at  
least one node in *V2* \\ rz will be labeled. This proves that after, at most *m* - *m* j such dual  
changes, the maximum flow will be increased by at least one unit. Thus the whole process  
is applied, at most, *LiEV,* ai times.  
Primal-Dual Algorithm for the Transportation Problem  
*Step 1 \(Initialization\)\: t* = 0, *XO* = 0, *u7* = *minjEv, Wij* for *i* E *Vb vJ* = miniEv, *\(Wi\)* - *u7\)* for  
\} E *V2,* and *wi\}* = *wZ* - *u7* - *vJ* for all *i* and\}.  
*Iteration t*  
*Step* 2\: Solve the maximum-flow problem over *@\(wl\).* Let *xij* be the flow from *i* E Vj to  
\} E *V2* for all *i* and\}.  
*Step* 3\: If the maximum flow equals *LiEV,* ai, then *Xl* = \(x\~j\) is an optimal solution.  
Otherwise, adjust the dual variables and reduced costs using \(5.7\) and \(5.8\). Keep the  
labels from the solution of the maximum-flow problem and return to Step 2 with  
*t\<-t+1.*  
We have already proved the following theorem.  
Theorem 5.1. *The primal-dual algorithm solves the transportation problem with, at*  
*most, LiEV,* ai *applications of the maximum-flow routine,*  
Corollary 5.2. *There is an integral optimal solution to the transportation problem.*  
*Proof* At each iteration, the solution *Xl* is obtained as the solution to a maximum-  
flow problem with integral capacities and hence is integral. • 5. **The Transportation Problem\: A Primal-Dual Algorithm** 71  
*Example* 5.1  
3 7 3 8 1; \) 6 12 5 7 *a* = \(4 5 3 5\)  
w\~ \( \~  
8 3 4 8 2 ' *b* = \(3 3 6 2 1 2\)  
11 6 10 5 10 9  
*UO* = \(3 5 2 5\), VO = \(0 0 1 0 2 0\), and  
0 3 0 3  
6 0 2 4  
n 1 6 0 0  
# w"\~G  
4 0 3  
Solving the maximum-flow problem on 0J\(WO\) yields  
-/ -/ -/ -/  
0 0 1 1  
*/c* 3 0 0 D *xO=* -/ \~  
0 3 0 0  
-/ 0 0 0 0 0  
Rows and columns corresponding to labeled nodes are noted with a check mark. Hence  
*h* = W;6 = 2, *u l* = \(5 7 2 7\), VI = \(-2 -2 1 -2 0 0\),  
-/ -/  
o 103  
1 400  
804 6  
2 0 3  
and  
-/  
# x'\~/u  
-/ 0  
-/ -/  
3 0 0 0  
0 0 1 1  
0 0 0 D0 3 0 0 72 1.3. Graphs and Networks  
N *h* -I -I 2  
ow = *W22* = *W42* = 1, *U* = \(5 8 2 *8\), v2* = \(-3 -2 1 -3 -1 0\),  
0 1 4  
8 0 5 7  
n 0 3 0 0  
# w'\~u  
0 0 3  
and  
./ ./ ./ ./ ./  
0 0 1 1  
*'C* 2 0 0 0  
0 3 0 0  
n *x 2=./* \~  
./ 0 0 0  
N *h* -2 -2 3  
ow = *W* 13 = *W* 43 = 1, *U* = \(6 9 2 *9\), v3* = \(-4 -3 1 -4 -2 -1\),  
0 0 1 4 r\} 0 2 0 0  
9 0 6 8  
# w'\~o  
0 0 0 3  
and we obtain an optimal flow given by  
2 0 0 0  
0 3 0 0  
n 0 0 1 1  
# x'\~o  
3 0  
When the total supply is large, there is a simple way to reduce the maximum number of  
possible augmentations from LiEVI *ai* to *m* \[log2 \(maxi,j *\(ai,bj\)\)\].* The technique is called  
*scaling.* An integer *a* \< *2k* can be written as *a* = *L7.\:6* 6i 2i  
, where 6i E \{O, 1\} for *i* = 1, ... ,  
*k* - 1. The *binary representation* of *a* is the string *\(6k-1 6k-2* ... 6o\). The scaling technique  
represents each supply and demand in binary. If2k  
-  
1 \<s; *maXi,j \(ai, bj\)* \< 2\\ then the length  
of each string is *k.* Hence in Example 5.1, *al* = 100, *a2* = 101, *a3* = 011, and so on.  
We now consider an approximate problem with supply-and-demand vectors *\(aD, bD \)*  
in which only the leading digit is considered, that is, *aD* = 1 if *a.* \~ *2k- 1*  
*, aO* = 0 other-  
*I I I*  
wise, or *a?* = *\[aJ2k-Ij* for *i* E *VI* and *bJ* = *\[b\)2k-Ij* for *j* E *V2 .* In example 5.1, *k* = 3, *aU =*  
\(1 1 0 1\) and *bO* = \(0 0 1 0 0 0\). As the example shows, supply and demand  
may now be unequal. Without loss of generality, assume LiEVI *a7* \~ *LjEV, bJ,* so there may  
be a need for a dummy sink node. Since LiEVI *a?* + *LjEV, bJ* \<s; *m,* the first approximation  
can be solved with *m* or fewer augmentations. Suppose the solution is *\(XO, WO\),* where  
*XO* E Z,\:, does not include shipments to the dummy sink.  
We now begin the next approximation with the optimal reduced costs *WO,* the flow *2xo,*  
and the supplies and demands *a\}* = *\[aJ2k-2 j* for *i* E *VI* and *b\)* = *\[b\)2k-2j* for *j* E *V2 .* Since 5\. The Transportation Problem\: A Primal-Dual Algorithm 73  
\~iEVI *a?* \~ \~jEV, *bJ,* we have \~iEVI x\~ = *bj* for all\} E *V2•* Thus the *unsatisfied* supplies and  
demands are  
*ti\)* = *a\)* - 2 2\: x\~.;;;; 3 for *i* E *VI*  
*jEV,*  
and  
Hence \~jEV2 *bJ* .;;;; *m,* so no more than *m* augmentations are required, other than the trivial  
ones to the dummy source or sink.  
The procedure continues in this way. In the pth approximation, af-I =  
*\[a;/2k*  
*-Pj* for *i* E *VI* and *bj-I* = *\[bj /2k*  
*- Pj* for\} E *V2•* The primal solution from the previous  
approximation is doubled to get the unsatisfied supply and demand, at least one of which  
does not exceed *m.* The dual variables are kept from one iteration to the next. The  
procedure is applied *k* = \[log2 \(maxi,j *\(ai, bj »\]* times to find an optimal solution.  
*Example* 5.1 *\(continued\).* We apply the scaling technique to solve this problem. The  
initial supplies and demands are *aO* = \(1 1 0 1\) and *bO* = \(0 0 1 0 0 0\), so to  
accommodate the imbalance, we add a dummy sink with a demand of 2 and costs of  
Wi7 = 0 for all *i.* An optimal solution is given by *UO* = \(0 0 -4 0\),  
*VO* = \(0 0 7 0 0 0 0\),  
3 0 3 8 5  
wo\~ \( ;  
12 0 8 12 6  
n 6 5 5 7 11  
11 6 3 5 10 9  
and  
0 1 0 0 0  
0 0 0 0 0  
D· 0 0 0 0 0  
# \~\~G  
0 0 0 0 0  
The initial flow for the second approximations is *2xo,* and the supplies and demands are  
\~ven by *al* = \(2 2 1 2\) and *b l* = \(1 1 3 1 0 1\). Thus *til* = \(0 2 1 2\) and  
*bl* = \(1 1 1 1 0 1\). Now there are five units of unsatisfied supply. Since  
\~iEVI *a\)* = \~jEV2 *bJ,* no extra source or sink is needed. 74 1.3\. Graphs and Networks  
An optimal solution to the second approximation is *u l* = \(0 7 -4 3\),  
VI = \(0 1 5 0 2 -2\),  
o 0  
# 020  
9 0 6  
o 0 0  
4  
# o  
8  
3  
and  
# o  
1  
# o  
# o  
1  
# o  
# o  
# o  
# o  
1  
# o  
# o  
# o  
# o  
The initial flow for the third approximation is *x* = 2XI, and *a2* = *a =*  
\(4 5 3 5\), *b2* = *b* = \(3 3 6 2 1 2\), *il2* = \(0 1 1 1\), and .52 = \(1 1 0  
o 1 0\). Hence there are three units of unsatisfied supply. No dual variable change is  
required, and we immediately obtain an optimal solution given by  
# o  
1  
# o  
2  
2  
# o  
3  
# o  
# o  
# o  
2  
# o  
1  
# o  
# o  
There is another interpretation and implementation of the primal-dual algorithm that  
is also of interest. Note that for any *u* and v, the instances of the transportation problem  
with cost matrix W = *\(wij\)* and W = *\(wij* - *Ui* - *Vj\)* have the same optimal solutions, since  
for any feasible *x* we have  
*L L wijxi\}* - *L L WijXij* = *L L \(Ui* + *Vj\) xi\}*  
*iEV\\ JEV, iEV\\ JEV2 iEV\\ JEV,*  
*= L Uiai* + *L vjbj.*  
*iEV\\ JEV,*  
The dual part of the primal-dual algorithm eventually finds a matrix *W* \~ 0 such that  
there is a feasible solution *x* of cost *LiEV\\ LjEV2 W ijX i\}* = O. Since zero is a lower bound on the  
cost of any solution with *W* \~ 0, such a solution must be optimal. The primal part of the  
algorithm uses maximum flow to find the solution of cost zero when one exists. In other  
words, with respect to the matrix w, all flow is sent over paths of zero cost.  
We want to point out that this can be achieved by a different implementation that uses  
a minimum-cost path algorithm to calculate the dual variables. Consider the digraph  
@ = *\(VI* U *V2* U *\{s,* t\}, *.s!1\),* where there is a directed arc from the source s to each node in  
VI, a directed arc from each node in *V2* to the sink *t,* and arcs *\(i,j\), i* E *VI,j* E *V2* if it is  
possible to ship directly from *i* to *j.* Arcs going out of the source or into the sink have zero  
cost and a capacity equal to the corresponding supply or demand. Arc *\(i,j\),*  
*i* E *VI,j* E *V2,* has cost *wi\}* and infinite capacity. 5\. The Transportation Problem\: A Primal-Dual Algorithm  
75  
Minimum-Cost Path Augmentation Algorithm  
*Step* 1 *\(Initialization\)\: k* = 0, *XO* = 0, f00 = f0.  
*Step* 2 *\(Iteration k\)\:* Let the current flow be *Xk.* Find a minimum-cost path from *s* to *t* of  
the form *\(s, iO,* ... *,l, t\).* Let  
Let  
L2 = *min\{xt\:* \(j, *i\)* is on the path with *i* E *VI,* and\) E *V2\}.*  
Ifno arcs from\) E *V2* to *i* E *VI* are on the path, let L2 = 00. Let L = min\(LI, *L 2\).*  
*Step* 3 *\(Flow augmentation\)\:* Increase the flow in *\(s, iO\)* and \(l, *t\)* by L. For all arcs *\(i,\)\)*  
on the path with *i* E *VI* and\) E *V2,* increase the flow by L. For all arcs \(j, *i\)* on the path  
with *i* E *VI* and *j* E *V2,* decrease the flow in \(i, *j\)* by L. If the new flow *Xk+1* satisfies  
x\~tl = *ai* for all *i* E *V\),* stop. *Xk+1* is an optimal solution.  
*Step* 4 *\(Arc and cost change\)\:* Add \(j, *i\),\)* E *V2, i* E *V\),* to the graph ifxt = ° andxt+1 \> 0,  
and assign it the cost *\(-wij\).* Delete *\(s, iO\)* ifx\~ibl = *aiD,* delete \(l, *t\)* ifxfoil = *bjD,* and delete  
\(j,i\)\)E *V2,iE V1 ifxt+1 =0.k .... k+* 1.  
Each time a minimum-cost path is found, we can interpret the costs on the nodes of that  
path as the incremental values of dual variables such that when the dual change  
*wij* - *Uj* - *Vj* is made, the cost of the path is reduced to zero. By augmenting over  
minimum-cost paths, the flow at iteration *k* is the minimum-cost solution to the  
transportation problem with supplies *LjEV, xt* for *i* E *VI* and demands *LiEV, xt* for *j* E *V2.*  
In fact it is possible to implement the primal-dual approach given above to produce the  
same augmentations as those determined by minimum-cost paths.  
*Example* 5.1 *\(continued\).* We find an optimal solution by finding minimum-cost path  
augmentations. Table 5.1 shows the paths, quantity of flow, and cost per unit of flow for  
each augmentation. Arcs from *V2* to *VI* are noted by overbars.  
Table 5.1.  
Augmentation Path Flow Cost  
1 *\(s,* 3\), \(3, 1\), \(1, *t\)* 3 2  
2 *\(s,* 1\), \(1, 2\), \(2, *t\)* 3 3  
3 *\(s,* 1\), \(1, 4\), \(4, *t\)* 1 3  
4 *\(s,* 4\), \(4, 4\), \(4, *t\)* 1 5  
5 *\(s,* 2\), \(2, 1\), \(1, 3\), \(3, 6\), \(6, *t\)* 2 5  
6 *\(s,* 2\), \(2, 1\), \(1, 3\), \(3, 3\), \(3, *t\)* 1 6  
7 *\(s,* 2\), \(2, 5\), \(\~ 1 7  
8 *\(s,* 4\), \(4, 4\), \(4, 1\), \(1, 6\), \(6, 3\), \(3,3\), \(3, *t\)* 1 8  
9 *\(s,* 2\), \(2,3\), \(2,1\), \(1, 6\), \(6,3\), \(3, 3\), \(3, *t\)* 1 9  
10 *\(s,* 3\), \(3, 3\), \(3, *t\)* 3 20 76  
1.3\. Graphs and Networks  
The optimal solution found is  
2  
1  
# o  
# o  
# o  
# o  
3  
3  
o 0  
o 1  
o 0  
2 0  
The primal-dual method is readily extended to handle the general minimum-cost flow  
problem. However, in the next section we give a primal simplex algorithm that seems to be  
more practical for solving large-scale minimum-cost network flow problems.  
6\. A PRIMAL SIMPLEX ALGORITHM FOR NETWORK FLOW PROBLEMS  
The simplex method works very efficiently on network flow problems because the basis  
matrices have a very simple structure that greatly simplifies the calculations required in  
the pivot operations. Graphically, the arcs corresponding to basic variables induce  
subgraphs that are spanning trees. The trees provide a very simple way of calculating  
primal and dual solutions and the other quantities needed to do simplex pivots.  
Let *f!lJ* = *\(V, d\),* where *V* = \{l, ... *,m\}* and .14 = *\{el\>* ... *,en\}* be the connected digraph  
of an instance of a network flow problem, and let *A* be the coefficient matrix of the  
conservation equations \(4.1\). Note that *A* = *\(a ij\)* is the node-arc incidence matrix of *f!lJ,* that  
is, if *ej* = *\(k, I\)* then *atq* = -1, *a/j* = 1, and *aij* = 0 otherwise.  
Proposition 6.1. *If A is the node-arc incidence matrix of a connected digraph f!lJ with m*  
*nodes, then* rank\(A\) = *m* - 1 \(see Definition 1.3 of Section I.4.1\).  
*Proof* 1\:7!1 *a ij* = 0 for all *j,* hence rank\(A\) \< *m.*  
To show that rank\(A\) = *m* - 1, let *T* = *\(V,* .14 '\) be a spanning tree of *f!lJ* and let *AT* be the  
*m* x *\(m* - 1\) incidence matrix of *T.* The idea of the proof is to permute the rows and  
columns of *AT* so that the *\(m* - 1\) x *\(m* - 1\) submatrix consisting of the first *m* - 1 rows is  
lower triangular with the magnitude of each diagonal element equal to 1.  
Let *i* 1 be a leaf of *T* so that the row of *A T* corresponding to *i* 1 is a unit vector or its  
negative. Put row *i* 1 and the column corresponding to the arc *ei1* incident to node *i* 1 as the  
first row and column, respectively. Then delete node *i* 1 from *T.* The resulting graph is  
again a tree and thus contains a leaf, say *i2 .* Now the row corresponding to *i2* contains, at  
most, two nonzero elements, one corresponding to an arc *ei2* \* *ei1,* and ifthere is another it  
corresponds to *ei1•* Hence by putting row *i2* and the column corresponding to *ei2* second,  
the first two rows are in lower triangular form. Now a straightforward induction yields the  
hypothesized lower triangular matrix with 1 's \(or -1 's\) on the diagonal. •  
We have seen, in the proof of Proposition 6.1, how a spanning tree on *f!lJ* yields an  
*\(m* - 1\) x *\(m* - 1\) nonsingular incidence matrix. But if *\(V, .14'\),* .14' s; .14, 1.14' 1 = *m* - 1 is  
not a spanning tree, then the underlying graph contains a cycle. Thus the incidence matrix 6\. A Primal Simplex Algorithm for Network Flow Problems 77  
of *\(V, .sIi'\)* contains a submatrix, which, after appropriate permutation of columns and  
multiplication of some columns by -1, is of the form  
# o  
# o  
# o  
-1  
-\~\) o .  
# o  
-1  
1  
Hence the incidence matrix of *\(V, .sIi'\)* is singular, and we have shown the following\:  
**Proposition** 6.2. *There is a one-to-one correspondence between spanning trees on qj\) and*  
*\(m* - *1\)* x *\(m* - *1\) nonsingular submatrices of A.*  
Thus each spanning tree on *qj\)* yields a basis matrix for the conservation equations \(4.l\),  
and if there are no upper-bound constraints \(4.2\), the tree corresponds to a primal feasible  
basis if the corresponding solution to \(4.l\) is nonnegative. Moreover, it is simple to  
compute the unique solution of \(4.1\) given that *xi\}* == 0 for all \(i, *j\)* E.sIi that are not tree  
arcs. We arbitrarily designate some node to be the root of the tree, say node *r.* Then we  
compute the solution of \(4.1\) recursively along each path from a leaf to the root, beginning  
with the arcs adjacent to the leaves.  
An example of this computation is shown in Figure 6.1. Suppose we are given  
*T* == *\(V, .sIi'\),* a spanning tree of *qj\).* We first compute the flows for the arcs incident to the  
leaves, that is, *X2r* == *b2, X61* == *-bI, X36* == *b3, X47* == *b4,* and *X75* == *-b5•* Then *X67* == *-\(b7*  
*+ X47* - *X75\)* is determined, and finally *X r6* == *-\(b6* - *X61* + *X36* - *xd.* Note that flows bal-  
ance at node *r* since *b2* + *br* = *xr6* = - *\(b l* + *b3* + *b4* + *bs* + *b6* + *b7\),* and we have assumed  
that '2.;=1 *bi* + *br* = O.  
Our computational scheme is nothing more than the obvious way of solving the lower  
triangular system beginning with the first variable, and so on. It illustrates that ifthe *b i* are  
integral, then the solution will be integral, which is, of course, a consequence of the  
diagonal elements of the lower triangular basis matrix having a magnitude of 1.  
The primal feasibility of a basis depends only on the vector *b.* For example, if  
*b* == *\(bI, b2, ••• , b7\)* == \(-3 2 3 4 -5 -10\), the induced spanning tree of Figure 6.l yields  
the basic feasible primal solution *X2r* == 2, *X61* == 3, *X36* == 3, *X47* == 4, *X75* == 5, *X67* == 1, *Xr6* == 2.  
Figure 6.1 78  
1.3\. Graphs and Networks  
• **••**  
**1----.-** -A  
Figure 6.2  
A Phase I procedure that uses artificial arcs may be necessary to determine an initial  
primal feasible basic solution.  
Now suppose we have a basic feasible primal solution that is not optimal. The criterion  
for optimality \(i.e., dual feasibility\) will be discussed subsequently. A primal simplex pivot  
corresponds to adding an arc to the tree and then deleting an arc from the cycle \(in the  
underlying undirected graph\). The arc to be deleted is chosen to maintain primal  
feasibility.  
The cycle of Figure 6.2 has been created by adding the arc \(i!, *ik \)* Ed. Now observe  
that if we set *Xilik* = *t\:,* \> 0 the conservation equations will be satisfied by increasing the  
flow by *t\:,* on all arcs of the cycle that have the same orientation as \(i!, *ik \)* and by decreasing  
the flow by *t\:,* on all arcs of the cycle that have the opposite orientation. Thus ifall arcs of  
the cycle have the same orientation as \(i!, *i k\),* the flow can be increased without bound.  
Otherwise, there is a unique largest value of *t\:,* ;;?; 0 \(\> 0 in the absence of degeneracy\) given  
by  
*t\:,* = min\{x *ij\:* \(i, *j\)* is an arc of the cycle whose orientation is  
opposite from \(i!, *ik \)\).*  
Suppose *t\:,* = *x ipip •I '* Then we obtain a new basis by deleting arc \(i *P'* i *p+l\)* from the cycle. The  
new solution *x* is given by  
and  
*X ij* + *t\:,* if \(i, *j\)* has the same orientation as \(i!, *i k\)* in the cycle  
*Xu* = *\{*  
*xij* - *t\:, if\(i,j\)* \~as the opposite orientation from \(i!, h\) in the cycle  
*x ij* otherwIse.  
In the absence of degeneracy, *t\:,* \> 0 and *x* '\* *x.*  
Suppose in the example of Figure 6.1 with *b* = \(-3 2 3 4 -5 -1 0\) we add the arc \(3,4\)  
\(see Figure 6.3\). Then *t\:,* = min\(x36, *X67\)* = min\(3, 1\) = 1 and arc \(6, 7\) is deleted. The  
resulting spanning tree is shown in Figure 6.4.  
We now consider the complementary dual solution and the computation of the reduced  
costs to establish optimality conditions and to find the arc to enter the tree when the  
optimality conditions do not hold. 6\. A Primal Simplex Algorithm for Network Flow Problems 79  
Figure 6.3  
Corresponding to the equations \(4.1\) and *x* E R\~ we obtain the dual constraints  
*Y* i - *Yj* ,;;; C *ij* for all *\(i,\}\)* E *.sti.* By complementary slackness, *Y* i - *Yj* = C *ij* for all tree arcs. We  
can arbitrarily set *Yr* = 0 and then use these *m* - I equations to compute the remainder of  
the dual variables. Then *ifYi* - *Yj* ,;;; *cij* is satisfied for all nontree arcs, the present solution  
is optimal. Otherwise, following the standard simplex criterion, we introduce an arc *\(i,\}\)*  
for which the reduced price *cij* = *cij* - *Yi* + *Yj* is minimum.  
The dual variables are computed by starting at the root of the tree with *Yr* = 0 and  
progressing toward the leaves \(see Figure 6.5\).  
As with the primal variables, after changing the basis it is not necessary to recalculate all  
of the dual variables. For example, if we add the arc \(3,4\) and delete the arc \(6,7\), then the  
dual variables change only at nodes 4, 7, and 5, and we obtain the new solution  
*Y;* = *Y;* for *i* = 1,2,3, *6'Y4* = *Y4* - *C 34'Y7* = *Y7 -C34, andys* = *Ys -C34·*  
As in the general primal simplex algorithm, the dual variables are needed only to  
calculate the reduced prices. But since the number of nontree arcs is generally much  
greater than the number of nodes, it makes sense to calculate and store the dual variables.  
Figure 6.4 80 1.3\. Graphs and Networks  
*Yl* = -crG - c61  
*Yr=O*  
Figure 6.5  
*Example 6.1.* calculations. The data are  
We continue with the example that has been used to demonstrate the  
c=  
8 5 9 2 4 r 0  
7 3 4 1 -3  
2 7 5 2 2  
0 7 7 3 b= 3  
6 4 2 4 4  
9 3 3 5 -5  
5 2 2 4 8 2 6 -1  
6 2 2 3 4 7 0  
r 2 3 4 5 6 7  
In Figure 6.6, the number adjacent to the nodes are the dual variables for the first  
primal basic feasible solution, solid lines are tree arcs, and the numbers adjacent to them  
-4 5 -2  
o  
Figure 6.6 6\. A Primal Simplex Algorithm for Network Flow Problems -4 5 4  
81  
-1  
o  
Figure 6.7  
are the flows. With dotted lines, we show the arcs with negative reduced price, that is, the  
ones that want to enter the basis; the adjacent numbers are the reduced costs.  
Arc \(3, 4\) enters the solution and arc \(6, 7\) leaves \(see Figure 6.7\). Now arc \(4, 5\) enters  
the solution. There is a tie for the leaving arc between \(4, 7\) and \(7, 5\). We choose \(7, 5\) and  
obtain the degenerate optimal solution shown in Figure 6.8. Now it can be checked that all  
reduced prices are nonnegative, so the solution shown in Figure 6.8 is optimal.  
It is a simple matter to include arc capacities in the network simplex algorithm by  
treating upper-bound constraints implicitly. Thus, in the absence of degeneracy, if a  
variable is at its upper bound, the corresponding arc is not in the tree. The optimality  
conditions and pivot rules need to be modified accordingly.  
Finally, it is important to observe that the effectiveness of the network simplex  
algorithm depends very substantially on the use of appropriate data structures for  
representing trees so that the calculations can be done efficiently.  
-4 5 4  
o  
o  
Figure 6.8 82 1.3\. Graphs and Networks  
7. NOTES  
Section 1.3.1  
Berge \(1973\), and Bondy and Murty \(1976\) are general books on graph theory.  
Data structures are extremely important to the implementation of efficient graph  
algorithms \(see Tarjan, 1983\). Although we have not dealt with this important aspect of  
graph and network algorithms, the notes for each of the following sections contains a  
reference to an article that gives some of the recent results on efficient algorithms.  
Section 1.3.2  
The shortest-path algorithm for nonnegative arc weights is due to Dijkstra \(1959\). The  
other algorithm appears in Ford and Fulkerson \(1962\). An earlier variant was given by  
Bellman \(1958\).  
Gallo and Pallotino \(1986\) presented a survey of shortest-path algorithms.  
Section 1.3.3  
The minimum-weight spanning tree algorithm is due to Kruskal \(1956\). Another classical  
algorithm is that of Prim \(1957\).  
Gabow et al. \(1986\) have presented results on efficient spanning tree algorithms.  
Section 1.3.4  
The classical reference for network flows is Ford and Fulkerson \(1962\). More recent texts  
are Bazarra and Jarvis \(1977\), Christo fides \(1975a\), Jensen and Barnes \(1980\), Kennington  
and Helgason \(1980\), and Lawler \(1976\).  
The maximum-flow algorithm presented is that ofFord and Fulkerson \(1956\).  
Edmonds and Karp \(1972\) showed the efficiency of augmenting along shortest-length  
paths.  
Tarjan \(1986\) gave a survey of efficient maximum-flow algorithms.  
Section 1.3.5  
The primal-dual algorithm is due to Ford and Fulkerson \(1962\). Scaling was introduced  
by Edmonds and Karp \(1972\).  
Bertsekas \(1985\) gave a unified framework of primal-dual network flow algorithms.  
Section 1.3.6  
Kennington and Helgason \(1980\) gave a detailed presentation of primal simplex network  
flow algorithms, including a computer code for solving large-scale problems. Also see  
Glover, Karney, and Klingman \(1974\), Bradley et al. \(1977\) and Bland and Jensen \(1987\).  
Ikura and Nemhauser \(1986\) gave a polynomial time dual simplex algorithm for the  
transportation problem and also investigated the use of scaling. A strongly polynomial  
network flow algorithm was described by Tardos \(1985\). **1.4**  
# Polyhedral Theory  
1. INTRODUCTION AND ELEMENTARY LINEAR ALGEBRA  
A considerable portion of this book involves the description of a set of points in *R n* by a set  
oflinear inequalities. In linear programming, we are given a description of the feasible set  
of points by a set of linear inequalities *P* = *\{x* E R\~\: *Ax* \~ *b\}.* When we solve a linear  
program by the simplex method, issues such as the dimension of *P* and which inequalities  
are necessary for the description of *P* do not need to be addressed.  
Integer programming is different. Typically, we are given a set S s\:; Z\~ of feasible points  
described implicitly, for example, the set of integer solutions to a linear inequality system  
S = *\{x* E Z\~\: *Ax* \~ *b\},* the set of binary vectors corresponding to tours in a graph, and so  
on. One of our objectives is to find a linear inequality description of the set.  
*Definition* 1.1\. Given a set S \~ *R n,* a point *x ERn* is a *convex combination* of points of S  
if there exists a finite set of points \{xi\}i=l in S and a *A* E R\~ with L;'=l *Ai* = 1 and *x* = L\}=l *AiXi.*  
The *convex hull* of S, denoted by conv\(S\), is the set of all points that are convex  
combinations of points in S.  
Figure 1.1 shows the convex hull ofa set of integral points in *R2.* We see that conv\(S\) can  
be described by a finite set of linear inequalities and that max\{cx\: *xES\} =*  
max\{cx\: *x* E conv\(S\)\}. Moreover, the latter problem is a linear program. The validity of  
these observations for general integer programs is shown in Section 6.  
Finding an inequality description of conv\(S\) is not easy, and questions such as the  
dimension of conv\(S\), the necessity of a certain inequality for the description of conv\(S\),  
and so on, are very important. Most of Chapter 11.1 is devoted to finding such a  
description. To facilitate the later developments, we collect together in this chapter some  
basic results on polyhedra.  
In this section we give, without proof, some standard results from linear algebra.  
*Definition* 1.2\. A set of points *Xl,* ... *,Xk* E *Rn* is *linearly independent* if the unique  
solution OfL1=1 *AiXi* = 0 is *Ai* = 0, *i* = 1, ... , *k.*  
Note that the maximum number oflinearIy independent points in *Rn* is *n.*  
83 84 1.4\. Polyhedral Theory  
2 conv\(S\) = *\{x* E *R2\:*  
\~------------\~\~\~==\~---------Xl  
2 3  
Figure 1.1. The black dots represent points in S; conv\(S\) is shaded.  
Proposition 1.1. *If A is an m* x *n matrix, the maximum number of linearly independent*  
*rows a/A, viewed as vectors* d *ERn, equals the maximum nwnber of linearly independent*  
*columns of A, viewed as vectors aj ERin.*  
*Definition* 1.3. the *rank* of *A* and is denoted by *rank\(A\).*  
The maximum number of linearly independent rows \(columns\) of *A* is  
Now we give a basic result for systems oflinear equalities.  
Proposition 1.2. *Thefollowing statements are equivalent\:*  
a. *\{x ERn\: Ax* = *b\}* \*' 0.  
b. rank\(A\) = rank\(A, *b\).*  
When dealing with linear equalities and inequalities it is often more appropriate to use  
the concept of affine independence.  
*Definition* 1.4\. A set of points *Xl,* ... , *Xk* E *Rn* is *affinely independent* if the unique  
solution of *L1=1 \(XiXi* = 0, *L1=1 \(Xi* = 0 is *\(Xi* = 0 for *i* = 1, ... , *k.*  
Linear independence implies affine independence, but the converse is not true.  
Proposition 1.3. *Thefollowing statements are equivalent\:*  
a. *Xl,* ' •. , *Xk* E *R* 11 *are a/finely independent.*  
b. *x 2*  
- *X* I, ... , *Xk* - *X* I *are linearly independent.*  
c. \(Xl, - 1\), ... , *\(xk*  
*,* - 1\) E *Rn+l are linearly independent.*  
Note that the maximum number of affinely independent points in *Rn* is *n* + 1 \(e.g., *n*  
linearly independent points and the zero vector\).  
The following proposition will be used frequently in proving results concerning  
polyhedra. 1. Introduction and Elementary Linear Algebra 85  
Proposition 1.4. *If\{x ERn\: Ax* = *b\}* "\* 0, *the maximum number of affinely independent*  
*solutions of Ax* = *b is n* + 1 - rank\(A\).  
*Example* 1.1\. Suppose  
*\(A, b\)* = -2 8 6'  
\( 1 -4 -3\)  
Then rank\(A\) = rank\(A, *b\)* = 1. By Proposition 1.4, the maximum number of affinely  
independent solutions of *Ax* = *b* is 3 - 1 = 2. Two such solutions are *Xl* = \(5 2\) and  
*x2*  
= \(1 1\).  
*Definition* 1.5. *H* \~ *R n* is a *subspace* if *x* E *H* implies *Ax* E *H* for all *A* E *RI* and if  
*x,y* E *Himpliesx* + *y* E *H.*  
Proposition 1.5. *The following are equivalent\:*  
a. *H* \~ *Rn is a subspace.*  
b. *There is an m* x *n matrix A such that H* = *\{x ERn\: Ax* = O\}.  
c. *There is a k* x *n matrix B such that H* = *\{x ERn\: x* = *uB, u* E *Rk\}.*  
Proposition 1.6. *If H* \~ *R n is a subspace, then \{x* E *Rn\: xy* = 0 *for y* E *H\} is a subspace.*  
This subspace is called the *orthogonal subspace* of *H* and is denoted by *H.i.*  
Proposition 1.7. *If H* = *\{x ERn\: Ax* = a\}, *with A being an m* x *n matrix, then*  
H.i = *\{x* E *R n\: x* = *AT U, u* E *Rm\}.*  
*Example* 1.2. *H* = *\{x* E *R2\: Xl* = *2X2\}* is a subspace. Here *A* = \(1 -2\) and *B* = \(2 1\).  
*= \{x* E *R2\: 2Xl* + *X2* = O\} \(see Figure 1.2\).  
*Definition* 1.6. If *p ERn* and *H* is a subspace, the *projection of p on H* is the vector *q* E *H*  
such that *p* - *q* E *H.i.* The *projection of* S *on H* is denoted by *projH\(S\)* = *\{q\: q* is the  
projection of *p* on *H* for some *pES\}.*  
2. DEFINITIONS OF POLYHEDRA AND DIMENSION  
*Definition* 2.1\. A *polyhedron P* \~ *Rn* is the set of points that satisfy a finite number of  
linear inequalities; that is, *P* = *\{x ERn\: Ax* \:\:\:\:; *b\},* where *\(A, b\)* is an *m* x *\(n* + 1\) matrix. A  
polyhedron is said to be *rational* if there exists an *m'* x *\(n* + 1\) matrix *\(A* " *b* '\) with rational  
coefficients such that *P* = *\{x ERn\: A IX* \:\:\:\:; *b'\}.* 86 1.4\. Polyhedral Theory  
*H.l.*  
Figure 1.2  
Throughout the text we consider only rational polyhedra and assume that if *P* is stated  
as *\{x ERn\: Ax* \~ *b\},* then *\(A, b\)* has rational coefficients.  
*Definition* 2.2\. A polyhedron *P* \~ *Rn* is *bounded* if there exists an *w* E Rl such that  
*P* \~ *\{x ERn\:* - *w* \~ *Xj* \~ *w* for *j* = 1, ... *,n\}.* A bounded polyhedron is called a *polytope.*  
*Definition* 2.3. *T* \~ *Rn* is a *convex* set if Xl, *x 2* E *T* implies that Axl + \(l - *A\)X2* E *T* for  
allO\~A\~ 1.  
Proposition 2.1. *A polyhedron is a convex set.*  
*Definition* 2.4. C \~ *Rn* is a *cone* if *x* E C implies *Ax* E C for all *A* E *Rl.*  
Proposition 2.2. *The polyhedron \{x* E *Rn\: Ax* \~ O\} *is a cone.*  
*Definition* 2.5\. A polyhedron *P* is of *dimension k,* denoted by dim\(P\) = *k,* if the  
maximum number of affinely independent points in *P* is *k* + 1.  
*Definition* 2.6\. A polyhedron *P* \~ *R n isfull-dimensional* if dim\(P\) = *n.*  
Below we will show that if *P* is not full-dimensional, then at least one of the inequalities  
*aix* \~ *bi* is satisfied at equality by all points of *P.*  
Let *M* = \{l, 2, ... *,m\}, M=* = \{i *EM\: aix* = *bi* for all *x* E *P\}* and let M\~ *=*  
\{i *EM\: aix* \< *bi* for some *x* E *P\}* = *M* \\ *M=.* Let *\(A=, b=\),* \(A\~, b\~\) be the corresponding  
rows of *\(A* , *b\).* We refer to the *equality* and *inequality* sets of the representation *\(A, b\)* of *P,*  
that is, *P* = *\{x ERn\: A=x* = *b=,* A\~x \~ b\~\}. Note that if *i* E M\~, then *\(ai,* bJ cannot be  
written as a linear combination of the rows of *\(A* =, *b=\).*  
*Definition* 2.7. *x* E *P* is called an *inner point* of *P* if *aix* \< *bi* for all *i* E M\~.  
*Definition* 2.8. *x* E *P* is called an *interior point* of *P* if *aix* \< *bi* for all *i EM.* 2. Definitions of Polyhedra and Dimension 87  
Proposition 2.3. *Every nonempty polyhedron P has an inner point.*  
*Proof* If M\~ = 0, every point of *P* is inner. Otherwise, for each *i* E M\~ there exists a  
point *Xi* E *P* with *aixi* \< *bi'* Now *x* = \(1/ IM\~ I\) *LiEM" Xi* E *P* since *P* is convex. Since  
*aix* \< *bi* for all *i* E M\~, *x* is an inner point. •  
Now we relate the dimension of *P* to the rank of its equality matrix *\(A* =, *b=\).* Below we  
will always assume that *P =1=* 0. However, the next result is still valid with the convention  
that if *P* = 0, then *dim\(P\)* = - 1.  
Proposition 2.4. *IfP* \~ *Rn, then* dim\(P\) + rank\(A=, *b=\)* = *n.*  
*Proof* Suppose rank\(A=\) = *rank\(A=, b=\)* = *n* - *k,* where 0 \~ *k* \~ *n.* Then by Proposi-  
tion 1.4 there are *k* + 1 affinely independent solutions of *A =X* = O. Let *yl,* ... , *yk+l* denote  
any such solutions, and let *x* be an inner point of *P.* Now for E sufficiently small, *x* + *Eyi*  
for *i* = 1, ... *,k* + 1 are affinely independent points in *P.* Thus *dim\(P\)* \~ *k* and we have  
that *dim\(P\)* + *rank\(A=, b=\)* \~ *n.*  
Now suppose that *dim\(P\)* = *k* and that Xl, ... *,Xk+l* are affinely independent points of  
*P.* Since *A=xj* = *b=* for *j* = 1, ... *,k* + 1, by Proposition 1.4 we have  
*rank\(A=, b=\)* \~ *\(n* + 1\) - *\(k* + 1\) = *n* - *k.* Hence *dim\(P\)* + *rank\(A=, b=\)* \~ *n. •*  
Corollary 2.5. *A polyhedron P is full-dimensional if and only if it has an interior point.*  
Note that we have shown that rank\(A=, *b=\)* is independent of the particular inequality  
description of P.  
*Example* 2.1\. Suppose *PeR* 3 is given by  
Xl + *X2 + X3* \~  
- Xl - *X2 - X3* \~ -1  
Xl + X3\~ 1  
-Xl \~ 0  
*- X2* \~ 0  
*X3* \~ 2  
Xl + *X2* + *2X3* \~ 2  
\(see Figure 2.1\).  
The three points \(1 0 0\), \(0 1 0\), \(0 0 1\) lie in *P* and are affinely independent.  
Hence dim\(P\) \~ 2. Because all points of *P* satisfy the equality Xl + *X2* + *X3* = 1, we have  
*rank\(A* =, *b=\)* \~ 1; hence, by Proposition 2.4, dim\(P\) \~ 2. Therefore *dim\(P\)* = 2. 88 1.4\. Polyhedral Theory  
\(0,0, 1\)  
Figure 2.1  
3\. DESCRIBING POLYHEDRA BY FACETS  
Given a polyhedron *P* = *\{x ERn\: Ax* \:\:\:s; *b\},* the question we address below is to find out  
which of the inequalities *aix* \:\:\:s; *bi* are necessary in the description of *P* and which can be  
dropped. In fact we will show that those necessary to describe *P* are the same, whatever the  
initial inequality description of *P.*  
*Definition* 3.1\. satisfied by all points in *P.*  
The inequality *nx* \:\:\:s; *no* \[or *\(n, no\)\]* is called a *valid inequality* for *P* if it is  
Note that *\(n, no\)* is a valid inequality if and only if *P* lies in the half-space  
*\{x ERn\: nx* \:\:\:s; *no\},* or equivalently if and only if max\{nx\: *x* E *P\}* \:\:\:s; *no* \(see Figure 3.1\).  
*Definition* 3.2. *F=t-P.*  
If\(n, *no\)* is a valid inequality for *P,* and *F* = *\{x* E *P\: nx* = *no\}, F* is called  
*aface* of *P,* and we say that *\(n, no\) represents F.* A face *F* is said to be *proper* if *F =1=* 0 and  
The face *F* represented by *\(n, no\)* is nonempty if and only if max\{nx\: *x* E *P\}* = *no.*  
When *F* is nonempty, we say that *\(n, no\) supports P.*  
As a first step in discarding superfluous inequalities, note that we can discard inequali-  
ties *aix* \:\:\:s; *bi* that are not supports of *P.* Hence from now on we suppose that all the  
inequalities *aix* \:\:\:s; *bi* for *i* EM support *P* and therefore represent nonempty faces.  
Proposition 3.1. *If P* = *\{x* E *Rn\: Ax* \:\:\:s; *b\} with equality set M=* \~ *M, and F is a nonempty*  
*face of P, then F is a polyhedron and F* = *\{x* E *Rn\: dx* = *bifor i* E *ME, dx* \:\:\:s; *bifor i EM;\}*  
*where M;.* \~ *M= and M;* = *M* \\ *M;'. The number of distinct faces of P is finite.* 3\. Describing Polyhedra by Facets 89  
*p*  
Xl  
*Valid i 2*  
*represents thneqUality* \(7\("2 2 *x*  
e *face* \[\~EP 7\("0 \) *that sup*  
*\: x=\:\:* A\~l *Ports P and*  
+ \(1 *-A\)x2* 0  
I *.\$A.\$lj*  
Figure 3.1  
*Proof* Suppose *F* is the set of optimal solutions to the linear program  
*no* = max\{nx\: *Ax* \~ *b\).* Let *u\** be an optimal solution to the dual linear program  
*min\{ub\: uA* = *n, u* E *R'\:\},* and let *1\** = \{i *EM\: u7\>* a\}. Now consider the polyhedron  
*F\** = *\{x ERn\: aix* = *bi* for *i* E *1\*, aix* \~ *bi* for *i EM* \\ *I\*\}.* We claim that *F* = *F\*.*  
Note first that if *x* E *F\*,* then  
\* \~ \* i \~ \*1-  
*nx* = *u Ax* = L... *Uia x* = L... *UWi* = *no.*  
iEI\* iEI\*  
But if *x* E *P* \\ *F\*,* then *akx* \< *bk* for some *k* E *1\*,* so *Uk* \> 0 and *nx* = *LiEI\* u7a ix \<*  
*LiEI\* u7bi* = *no.* Hence *F* = *F\** and *F* is a polyhedron. Since *F* s; *P,* the equality set *\(A;, b;\)*  
of *F* must have the required property.  
Finally, since *M* is finite, the possible equality subsets *M;* \[corresponding to the rows of  
*\(A;, b;\)\]* are finite in number, so the number of distinct faces is finite. •  
Note that by Proposition 2.4, if *F* is a proper face of *P,* then dim *\(F\)* \< dim\(P\). In  
particular, the dimension *ofF* is *k* if the maximum number of affinely independent points  
that lie in *F* is *k* + 1.  
*Definition* 3.3\. A face *F* of *Pis afacet* of *P* if dim\(F\) = dim\(P\) - 1.  
Proposition 3.2. *If F is a facet of P, there exists some inequality akx* \~ *bk for k* E M\~  
*representing E*  
*Proof* Since dim *\(F\)* = dim\(P\) - 1, it follows from Proposition 2.4 that rank\(A'F, *b'F\) =*  
*rank\(A* =, *b=\)* + 1. The result follows. •  
*Example* 2.1 *\(continued\). \(n, no\)* = \(-1 -1 1, 1\) is a valid inequality for *P* because  
max\{-xi - *X2* + *X3\: x* E *P\}* = 1 = *no.* Also *FI* = *\{x* E *P\:* -Xl - *X2* + *X3* = 1\} = \{\(O 0 I\)\} is  
a face of *P.* Note that the face *F\\* is not generated by any of the inequalities *aix* \~ *bi* in the  
description of *P.* 90 1.4\. Polyhedral Theory  
Now consider the face *F2* generated by the valid inequality 2Xl - *7X2* + *2X3* \~ 2, that is,  
*F2* = *\{x* E *P\:* 2Xl - *7X2* + *2X3* = 2\}. The two points \(l 0 0\) and \(0 0 1\) lie in *F2* and  
are affinely independent. In addition, the point \(0 1 0\) E *P* does not lie on *F 2,* so  
*F2* C *P.* Since *dim\(P\)* = 2 and *dim\(F2 \)* \~ 1, we have dim\(F2\) = 1. Thus *F2* is a facet of *P.*  
Now from Proposition 3.2, one of the initial inequalities must represent *F 2•* In fact,  
both Xl + *X* 3 \~ 1 and *-X* 2 \~ 0 represent the facet *F 2.*  
Finally consider *\(n, no\)* = \(0 0 1, 2\). Now max\{x3\: *x* E *P\}* = 1 \< *no,* so *X3* \~ 2 is a  
valid inequality but not a support of *P.* Hence *x* 3 \~ 2 can be discarded from the  
description of *P.*  
Proposition 3.3. *the description of P.*  
*For each facet F ofP, one of the inequalities representing F is necessary in*  
*Proof* Let *P F* be the polyhedron obtained from *P* by removing all the inequalities  
representing *F.* We will show that *P F* \\ *P* \*- 0 so that at least one of the inequalities is  
necessary. Let *x* be an inner point of the facet *F* and let *arx* \~ *br* be an inequality  
representing *F.* Since *ar* is linearly independent of the rows of *A* =, it follows from the  
Farkas lemma that there exists ayE *Rn* such that *A =y* = 0 and *ary* \> O. Because *x* is an  
inner point of *F, aix* \< *bi* for all inequalities *i* E M\~ that do not represent *F.* But now  
*x* + *eyE P F* \\ *P* for sufficiently small *e* \> O. •  
Besides being necessary, the facets are sufficient for the description of *P.*  
Proposition 3.4. *Every inequality arx* \~ *br for r* E M\~ *that represents a face of P of*  
*dimension less than dim\(P\)* - 1 *is irrelevant to the description of P.*  
*Proof* Suppose *ar x* \~ *b r* represents a face *F* of *P* of dimension dim\(P\) - *k* with *k* \> 1,  
and the inequality is not irrelevant. In other words, there exists *x\* ERn* such that  
*A=x\** = *b=, aix\** \~ *bifor i* E M\~ \\ *\{r\}, andarx\** \> *b,.* Let *x* be an inner point *ofP.* Then on  
the line between *x\** and *x* there exists a point *z* in *F* satisfying *A =z* = *b=, aiz* \< *bi*  
for *i* E M\~ \\ *\{r\},* and *arz* = *b,.* Hence the equality set of *F* is *\(A=, b=\)* and *\(a r, br\),* which is  
of rank *n* - dim\(P\) + 1. Therefore the dimension of *F* is dim\(P\) - 1, which is a contradic-  
tion. •  
*Example* 2.1 *\(continued\).* facet of *P.* The equality set of *F2* is  
We verify that the face *F2* = *\{x* E *R3\:* Xl + *X3* = 1, *x* E *P\}* is a  
-1  
*\(AJ;" bJ;,\)* = *i* -1 -1  
1 1  
o 1  
-\: \) 1 '  
\(  
-1 0  
# o  
which is a matrix of rank 2. Hence, by Proposition 2.4, *F2* is of dimension 1. Thus *F2* is a  
facet represented either by Xl + *X3* \~ 1 or *-X2* \~ O. In fact, since  
*F2* is also represented by 2Xl - *7X2* + *2X3* \~ 2, which is the representation we gave earlier. 3\. Describing Polyhedra by Facets 91  
Similarly it can be shown that *-Xl* \~ 0 defines a facet. Now consider *Xl* + *X2* + *2X3* \~ 2,  
which is a support of *P.* Let  
*F* 3 = *\{X* E *P\: Xl* + *X* 2 + *2x* 3 = 2\}  
*= \{X* E *P\: Xl* + *X2* + *X3* = 1, *Xl* + *X2* + *2X3* = 2, *Xl* + *X3* = 1, *-Xl* = 0, *-X2* = O\}.  
Hence *\(A p3, bp\)* is of rank 3. Thus the face *F3* is of dimension O. In fact *F3* = *Fl =*  
I\)\}, and hence *X* I + *X2* + *2X3* \~ 2 is redundant. Therefore a minimal description of  
\{\(O 0 *P* is given by  
\~ 0  
\~ 0  
The example raises the question as to when two inequalities \(e.g., Xl + *X3* \~ 1, *-X2* \~ 0\)  
are "equivalent". The answer is straightforward. The set *\{x\: A =X* = *b=, nx* \~ *no\} =*  
*\{x\: A =X* = *b=, \(An* + *uA =\)x* \~ *Ano* + *ub=\}* for all *A* \> 0 and all *u* E RIM\~I. Hence we say that  
\(nt, *n6\)* and *\(n2*  
*, n5\)* are *equivalent, or identical inequalities* with respect to *P* when  
*\(n2*  
*,* 7r5\) = *A\(nl, n6\)* + *u\(A* =, *b=\)* for some *A* \> 0 and *u* E RIM\~I. Now we can summarize the  
main result given so far.  
Theorem 3.5  
a. *A full-dimensional polyhedron P has a unique \(to within scalar multiplication\)*  
*minimal representation by a finite set of linear inequalities. In particular, for each*  
*facet Fi ofP there is an inequality aix* \~ *bi \(unique to within scalar multiplication\)*  
*representing Fi and P* = *ex ERn\: aix* \~ *bi* for *i* = 1, ... , *n.*  
b. *Ifdim\(P\)* = *n* - *k with k* \> 0, *then P* = *ex ERn\: aix* = *bifor i* = 1, ... *,k, aix* \~ *bi*  
*for i* = *k* + 1, ... , *k* + *t\}. For i* = 1, ... , *k, \(ai, bi\) are a maximal set of linearly*  
*independent rows of\(A=, b=\), andfor i* = *k* + 1, ... *,k* + *t, \(ai, bi\) is any inequality*  
*from the equivalence class of inequalities representing the facet Fi•*  
We now give a theorem that characterizes facets and that is useful in establishing when  
a valid inequality is a facet.  
Theorem 3.6. *Let \(A=, b=\) be the equality set of P* \~ *Rn and let F* = *\{x* E *P\: nx* = *no\} be a*  
*proper face of P The following two statements are equivalent\:*  
i. *F is afacet ofP.*  
ii. *If Ax* = *Aofor all X* E *F then*  
*\(3.1\) \(A,* Ao\) = *\(an* + *uA=, ano* + *ub=\)for some* a E Rl *and some u* E RIM\~I. 92 1.4\. Polyhedral Theory  
*Proof* ii =\> i. Let *L* = *\{\(A,* Ao\) E *R n*  
*l*  
*+*  
*\{\(A,* Ao\) E *Rn+l\:* Ax = Ao for all *x* E *F\}. L* s; *L',* since  
*\: \(A,* Ao\) IS of the form \(3.1\)\} and *L' =*  
*anx* + *uA=x* = *ano* + *ub=* for all *x* E *F.*  
By the hypothesis, *L'* s; *L.* Hence *L* = *L'.*  
Suppose that *dim\(P\)* = *n* - *k* so that rank\(A=, *b=\)* = *k.* Since *F* is a proper face, *\(n, no\)* is  
not a linear combination of the rows of *\(A* =, *b=\).* Thus *L* is a *\(k* + I\)-dimensional subspace.  
N ow let *x* I, ... *,x'* be a maximal set of affinely independent points in *F* and let  
*D* = \(\~' -\~\)  
*xr -1*  
be an *r* x *\(n* + 1\) matrix. Clearly *r* \~ *n* - *k.*  
By Proposition 1.4, the maximum number of affinely independent solutions of  
*\(A, Ao\)D T* = 0 is *\(n* + 1\) + 1 - rank\(D\) = *n* + 2 - *r.* Thus *L'* is an *\(n* + 1 - r\)-dimensional  
subspace. Since *L* = *L', r* = *n* - *k.* Hence *F* is a facet of *P.*  
i =\> ii. As above, *L* \~ *L'.* Here we need to show that *L* = *L'.* Suppose *dim\(P\)* = *n* - *k.*  
Since *F* is a facet of *P, F* contains *n* - *k* affinely independent points. Thus, as in the proof  
ofii =\> i, *dim\(L'\)* = *k* + 1. Since *dim\(L\)* = *k* + 1 and *L* \~ *L', L* = *L'. •*  
4. DESCRIBING POLYHEDRA BY EXTREME POINTS AND EXTREME RAYS  
Here we consider a representation of polyhedra in terms of lowest-dimensional faces.  
Proposition 4.1. *If P* = *\{x* E *Rn\: Ax* \~ *b\} =1=* 0 *and* rank\(A\) = *n* - *k, P has a face of*  
*dimension k and has no proper face of lower dimension.*  
*Proof* For any face *F* "\* 0 of *P,* rank\(A\:F, *b\:F\)* \~ *n* - *k.* Hence, by Proposition 2.4, the  
dimension of *F* is greater than or equal to *k.* Now let *F* be a face of *P* of minimum  
dimension. If dim *\(F\)* = *k* = 0, there is nothing to prove. So suppose *dim\(F\)* \> O.  
Let *x* be an inner point of *F.* Since *dim\(F\)* \> 0, there exists some other point *y* of *F.*  
Consider the line joining *x* and *y,* that is, *Z\(A\)* = *x* + *A\(Y* - *x\)* where *A* E *R* I. Suppose that  
the line intersects *aix* = *bi* for some *i* E *Mi\:.* Let *A\** = min\{ IAi I\: *i* E *Mi\:, Z\(Ai\)* lies in  
*aix* = *bi\}'* and *A\** = IAi"l. Then *A\* =1=* 0 because *x* is an inner point. Thus  
*Fi'* = *\{x* E *P\: A\:Fx* = *b\:F, ai\*x* = *bi'\} =1=* 0 is a face of *P* of smaller dimension than *F,* which is  
a contradiction.  
Therefore the line does not intersect *aix* = *b i* for any *i* E *Mi\:.* But this means that  
*Ax* + *A2\(y* - *x\)* \~ *b* for all 2 E *R* I. Since *Ax* \~ *b,* this implies that *A\(y* - *x\)* = ° for all  
*y* E *F.* Thus *F* = *\{y\: Ay* = *Ax\}.* Since rank\(A\) = *n* - *k,* Proposition 2.4 implies that  
dim *\(F\)* = *k. •*  
*Example* 4.1. *P* = *\{x* E *R2\:* Xl + *X2* \~ 1\}. See Figure 4.1. We have rank\(A\) = 1. A face of  
minimum dimension is the one-dimensional face *F* = *\{x* E *R2\: XI* + *X2* = n. 4\. Describing Polyhedra by Extreme Points and Extreme Rays 93  
\(1,0\)  
Figure 4.1  
In practice, we frequently deal with polyhedra lying within the nonnegative orthant R\~.  
For such polyhedra, *rank\(A\)* = *n;* and by Proposition 4.1, these polyhedra have zero-  
dimensional faces. For this reason and for simplicity, we assume for the next two sections  
that rank\(A\) = *n.* Note also that if *P* = *\{x ERn\: Ax* \~ *b\}* is a polytope, then rank\(A\) = *n.*  
*Definition* 4.1. such that *x* = !Xl + \~X2.  
*x* E *P* is an *extreme point* of *P* if there do not exist *Xl, x 2* E *P, Xl \:1= X2,*  
For *x* E *P,* let *\(A;, b;\)* be the equality set of *x,* i.e. *\(A;, b;\)* = *\(A F , bF \)* where *F* is the face  
of minimum dimension containing *x,* and *x* is an inner point of *F.*  
Proposition 4.2. *x is an extreme point of P if and only if x is a zero-dimensional face of P.*  
*Proof* Suppose *x* is a zero-dimensional face of *P.* By Proposition 2.4, rank\(A;\) = *n.*  
Let *\(A, b\)* be a submatrix of \(A\~, b\~\) with *A n* x *nand* nonsingular, so *x* = *A-lb.* If  
*x* = \~Xl + \~X2, *Xl, x 2* E *P,* then since *Axi* \~ *b* for *i* = 1, 2, *Axl* = *Ax2* = *E.* Hence  
*x* I = *x 2* = *x,* so *x* is an extreme point.  
rank\(A\~\) If *x* E *P* is not a zero-dimensional face of *P,* then by Proposition 2.4 we have  
\< *n.* But now there exists *y \:1=* ° satisfying *A;y* = 0, and for sufficiently small €,  
*Xl* = *X* + *sy* E *P* and *x 2* = *x* - *sy* E *P.* Now *x* = *1Xl* + *1x2*  
*,* so *x* is not an extreme point. •  
*Definition* 4.2\. Let *pO* = *\{r ERn\: Ar* \~ O\}. If *P* = *\{x ERn\: Ax* \~ *b\} =1=* 0, then  
*r* E *pO* \\ CO\} is called a *ray* of *P.*  
A point *r ERn* is a ray of *P* if and only if for any point *x* E *P,* the set  
*\{y ERn\: y* = *X* + *Ar, A* E *R!\}* r;;. *P.* 94 I.4. Polyhedral Theory  
*Definition* 4.3\. for any *A* E *Ri,* such that *r* = !rl + *!r2.*  
A ray *r* of *P* is an *extreme ray* if there do not exist rays rl, *r2* E *pO,* rl *=1= Ar2*  
Proposition 4.3. *If P =1=* 0, *r is an extreme ray of P if and only if \{Ar\: A* E *RD is a one-*  
*dimensionalface of pO.*  
*Proof* Let *A;* = *\{a i\: i EM, air* = a\}. If *\{Ar\: A* E *RD* is a one-dimensional face of *pO,*  
rank\(A;\) = *n* - 1. Hence all solutions of *A;y* = 0 are of the form *y* = *Ar, A* E *R* I. If  
*r* = \~rl + \~r2, we obtain a contradiction as in the previous proposition.  
Ifr E *pO* and rank *\(A;\)* \< *n* - 1, there exists f *=1= Ar, A* E *RI,* such thatA;f = O. The rays  
*r1* = *r* + sf, *r2* = *r* - sf show that *r* is not an extreme ray. •  
Corollary 4.4. *A polyhedron has a finite number of extreme points and extreme rays.*  
*Example* 2.1 *\(continued\).* Since the inequalities describing *P* include  
it is clear that rank\(A\) = 3.  
The face *FI* = \{\(O 0 I\)\} has the equality set  
XI + *X2* + *X3* = 1 \(also the negative of this row which is omitted\)  
XI =0  
*X2* = 0  
and since \(\: b b\) is of rank 3, \(0 o I 0  
Note that rl = \(l 0 -1\) satisfies  
0 1\) is a zero-dimensional face, or extreme point.  
*XI+X2+ X 3=0*  
XI *+X3=0*  
*X2* = 0  
\(I I I\)  
° 1 0  
1 -1\) is another extreme ray. The polyhedron *pO*  
and *a1rl* \< 0 for all other constraints. Since I ° I is of rank 2, rl is an extreme ray. A . . similar argument shows that *r2* = \(0 for Example 2.1 is shown in Figure 4.2.  
A polyhedron can be represented in terms of its extreme points and extreme rays. Some  
preliminaries are needed to obtain this fundamental result.  
Theorem 4.5. *If P =1=* 0, rank\(A\) = *n, and* max\{cx\: *X* E *P\} is finite, then there is an*  
*optimal solution that is an extreme point.* 4\. Describing Polyhedra by Extreme Points and Extreme Rays 95  
\(0,0,0\)  
Figure 4.2  
*Proof* The set of optimal solutions is a nonempty face *F* = *\{x* E *P\: cx* = *co\}.* By  
Proposition 4.1, *F* contains an *\(n* - rank\(A»-dimensional face. Since *n* - rank\(A\) = 0, by  
Proposition 4.2, *F* contains an extreme point. •  
Theorem 4.6. *For every extreme point \{xkhEK ofP, there exists acE zn such that Xk is the*  
*unique optimal solution of* max\{cx\: *x* E *P\}.*  
*Proof* Let *M;k* be the equality set of *Xk.* Let c\* = LiEM\~k *ai*  
*•* Since the *ai* are rational  
vectors, there exists a *A* \> ° such that c = *AC\** E *zn.* Since *Xk* is a zero-dimensional face of *P,*  
for all *x* E *P* \\ *\{Xk\}* there exists an *i* E *M;k* such that *aix* \< *bi.* Hence for *x* E *P* \\ *\{xk\},*  
*ex* = *L Aaix* \< *L Abi* = *L Aaixk* = *exk*  
*.*  
iEM\~k iEM\~k iEM\~k •  
Theorem 4.7. *If P =1=* 0, rank\(A\) = *n, and max\{ex\: x* E *P\} is unbounded, P has an*  
*extreme ray r\* with er\** \> 0.  
*Proof* Since *max\{ex\: Ax* \~ *b\}* is unbounded, by linear programming duality, the set  
*\{u* E *R\:;!\: uA* = c\} = 0. By Farkas' lemma, this implies there exists an *r ERn* such that  
*Ar* \~ ° and *er* \> O. Now consider the linear program *max\{cr\: Ar* \~ 0, *er* \~ 1\} = 1. By  
Theorem 4.5, this linear program has an optimal extreme point solution. An optimal  
extreme point is a point *r\** E *po* such that the equality set A\~. is of rank *n* - 1, and *cr\** \> O.  
Now by Proposition 4.3, *r\** is an extreme ray of *P. •*  
We now prove one of the fundamental results on the representation of polyhedra. 96 1.4\. Polyhedral Theory  
Theorem 4.8 \(Minkowski's Theorem\). *If P =1=* 0 *and* rank\(A\) = *n, then*  
*where \{x khEK is the set of extreme points of P and \{rj\}jEJ is the set of extreme rays of P.*  
*Proof* Let  
Since *Xk* E *P* for *k* E *K,* and *P* is convex, *x'* = *LkEK AkXk* E *P* for any *A* satisfying  
*LkEK Ak* = 1, *Ak* \~ 0 for *k* E *K.* Also since *r j* for *j* E *J* are rays, *Xl* + *LjEJ f.1jrj* E *P* for any  
*f.1j* \~ 0 for *j* E *J.* Hence *Q* \~ *P.*  
Now suppose that *Q* \* *P,* so there exists *yEP* \\ *Q.* In other words there do not exist *A,*  
*f.1* satisfying  
*2 AkXk* + 2 *f.1jrj* = *Y*  
*kEK jEJ*  
= -1  
*Ak* \~ 0 for *k* E *K,*  
*f.1j* \~ 0 for *j* E *J.*  
Then by Farkas' lemma, there exists *\(n, no\)* E *Rn+l* such that *nxk*  
- *no* \~ 0 for *k* E *K,*  
*nrj*  
\~ 0 for *j* E *J* and *ny* - *no* \> O. Now consider the linear program max\{nx\: *x* E *P\}.* Ifit  
has a finite optimal value, by Theorem 4.5 the optimum value is attained at an extreme  
point. However, *yEP* and *ny* \> *nxk* for all extreme points *\{Xk\}kEK,* which is a contradic-  
tion. On the other hand, if the linear program has an unbounded optimum, by Theorem  
4.7 there exists an extreme ray *r j* with *nrj* \> o. Again there is a contradiction. Hence  
*Q=P. •*  
*Example* 2.1 *\(continued\).* Since *P* has one extreme point Xl = \(0 0 1\) and two  
extreme rays rl = \(1 0 -1\) and *r2* = \(0 1 -1\), we have an alternative description of *P*  
given by  
Combining Minkowski's theorem and linear programming duality leads to a character-  
ization of certain projections of polyhedra; it also leads to an important converse to  
Minkowski's theorem, which says that every set obtained as a convex combination of a  
finite set of vectors in *R n* plus a nonnegative combination of some other finite set of  
vectors in *R n* is a polyhedron. 4\. Describing Polyhedra by Extreme Points and Extreme Rays First we restate the basic results for the dual pair of linear programs  
*z* = max\{cx\: *x* E *P\}* with *P* = *\{x* E R\~\: *Ax* \~ *b\}*  
97  
and  
*w* = *min\{ub\: u* E *Q\}* with *Q* = *\{u* E *R'\:\: uA* \~ c\}  
in terms of extreme points and extreme rays. Note that this is partially a repeat of  
Theorems 4.5 and 4.7. Let *\{Xk\}kEK* and *\{Ui\}iEI* be the sets of extreme points of *P* and *Q,*  
respectively, and let *\{rj\}jEJ* and *\{Vt\}tET* be the sets of extreme rays of *pO* and *QO,* respectively.  
Theorem 4.9  
i. *Thefollowing are equivalent\:*  
a\) *The primal problem is feasible, that is, P =1= 0;*  
b\) *vtb* \~ *Ofor all t* E *T.*  
ii. *The following are equivalent when the primal problem is feasible\:*  
a\) *z is unboundedfrom above;*  
b\) *there exists an extreme ray rj of P with cri* \> 0;  
c\) *the dual problem is infeasible, that is, Q* = 0.  
iii. *If the primal problem is feasible and z is bounded, then*  
*z* = max *cxk*  
= *w* = min *ui b.*  
*kEK iEI*  
*Proof*  
i. By the Farkas lemma, *P =1=* 0 if and only if *vb* \~ 0 for all *v* E *R'\:* with *vA* \~ O. By  
Minkowski's theorem,  
*QO*  
= *\{v* E *R'\:\: vA* \~ O\} = *\{v* E *R'\:\: v* = I */1tvt, /1t* \~ 0 for *t* E *T\}.*  
*lET*  
Hence *vb* \~ 0 for all v E *QO* if and only if *vt b* \~ 0 for all *t* E *T.*  
ii. Again by Minkowski's theorem,  
*Ak* ;;. 0 for *k* E *K, /l-j* ;;. 0 for *j* E *J* \} '\*' 0.  
Thus z is bounded if and only if *crj*  
\~ 0 for all\} E *J.* The equivalence of statements  
ii.b and ii.c is obtained by applying statement i to the dual problem.  
iii. This also follows from strong duality and Minkowski's theorem applied to *P* and *Q .*  
# •  
Now we consider the projection of a polyhedron. Note first that the projection of a  
point *\(x, y\) ERn* x *RP* onto the subspace *H* = *\{\(x, y\)\: y* = O\} is the point *\(x,* 0\). Therefore 98 1.4\. Polyhedral Theory  
it is natural to consider a projection of a polyhedron *P* \~ *Rn* x *RP* onto *y* = 0 as a  
projection from the *\(x,* y\)-space to the x-space, denoted by *projxCP\).*  
Theorem 4.10. *Let P* = *\{\(x, y\) ERn* x *RP\: Ax* + *Gy* \~ *b\}, then*  
*projxCP\)* = *\{x ERn; v t\(b* - *Ax\)* \~ 0 *for all t* E *T\},*  
*where \{Vt\}tET are the extreme rays of Q* = *\{v* E *R';!\: vG* = O\}.  
*Proof* If *H* = *\{\(x, y\) ERn* x *RP\: y* = O\}, then projH\(P\) = *\{\(x,* 0\) *ERn* x *RP\: \(x, y\)*  
E *P\}.* Applying statement i.b of Theorem 4.9 to *\{y* E *RP\: Gy* \~ *b* - *Ax\}* gives  
# •  
Corollary 4.11. *The projection of a polyhedron is a polyhedron.*  
Given two polyhedra *PC Rn* x *RP* and *Q eRn,* the question will arise of showing  
whether *Q* = *projxCP\)* or not.  
Corollary 4.12. *If P* = *\{\(x, y\) ERn* x *RP\: Ax* + *Gy* \~ *b\} and Q* = *\{x ERn\: Dx* \~ *d\},*  
*where D is q* x *n, then Q* = *projxCP\) if and only if\:*  
i. *For i* = 1, ... *,q, dix* \~ *db is a valid inequality for P.*  
ii. *For each x\** E *Q, there exists a y\* such that \(x\*, y\*\)* E *P.*  
*Proof*  
i. Equivalent to *Q* ;2 *projxCP\).*  
ii. Equivalent to *Q* \~ *projx\(P\).* •  
Another immediate consequence of Theorem 4.10 is the converse of Minkowski's  
theorem.  
Theorem 4.13 \(Weyl's theorem\). *matrix, and*  
*If A is a rational* ml x *n matrix, B is a rational m2* x *n*  
*then* Q *is a rational polyhedron.*  
*Proof Q* = *projxCP\),* where  
*P* = *\{\(X, y,* z\) *ERn* x *R';!!* x *R';!2\: x* - *yA* - *zB* = 0, I *Yk* =1\}.  
k=l •  
5\. POLARITY  
Here we consider a polyhedron n \~ *Rn+l* whose feasible points are the valid inequalities of  
*P.* We will characterize the facets of *P* in terms of the extreme rays of n and establish a  
duality between *P* and n. 5\. Polarity 99  
*Definition* 5.1\. polyhedron *P* = *\{x ERn\: Ax* \:\:s\:; *b\}.*  
IT = *\{en, no\)* E *R n + l\: nx* - *no\:\:S\:;* 0 for all *x* E *P\}* is called the *polar* of the  
Note that *\(n, no\)* E IT if and only if *\(n, no\)* is a valid inequality for *P.* For simplicity,  
assume that *rank\(A\)* = *n.*  
Proposition 5.1. *Given a nonempty polyhedron P* \~ *Rn with rank\(A\)* = *n,* IT *is a*  
*polyhedral cone described by*  
*nxk*  
- *no* \:\:s\:; 0 for *k* E *K*  
*nrj* \:\:s\:; 0 for *j* E *J*  
*where \{xkhEK, \{rj\}jEJ are the extreme points and extreme rays ofP.*  
*Proof* Let IT' = *\{en, no\)* E *Rn+l\: nxk*  
- *no* \:\:s\:; 0 for *k* E *K, nrj* \:\:s\:; 0 for *j* E *J\}.* Suppose  
*\(n, no\)* E IT. Since *Xk* + *f.1rj* E *P* for any *Xk,* any *r1,* and all *f.1* \~ 0, we have *n\(xk* + *f.1rj\)* \:\:s\:; *no*  
for all *f.1* \~ O. But this implies *nxk* \:\:s\:; *no* and *nrj* \:\:s\:; O. Hence *\(n, no\)* E IT', so IT \~ IT'.  
Conversely if\(n, *no\)* E IT' and *x* E *P,* then, by Theorem 4.8, *x* = *L-kEK AkXk* + *L-jEJ f.1jrj*  
for some *A, f.1* satisfying *L-kEK Ak* = 1, *Ak* \~ 0 for *k* E *K,* and *f.1j* \~ 0 for *j* E *J.* Hence  
Therefore *\(n, no\)* E IT, so IT' \~ IT. •  
*Example* 2.1 *\(continued\).* A polyhedral description of IT \~ *R4* is as follows\:  
*n3* - *no\:\:s\:;* 0  
nl - *n3* \:\:s\:; 0  
*n2* - *n3* \:\:s\:; o.  
Now we are ready to prove the main result on polarity.  
Theorem 5.2. *If* dim\(P\) = *n, rank\(A\)* = *n, and n\** \* 0, *then \(n\*,* n\~\) *is an extreme ray of* IT  
*if and only if \(n\*, no\) defines a facet of P.*  
*Proof* By Proposition 4.3, *\(n\*, no\) =1=* 0 is an extreme ray of IT ifand only ifits equality  
set is of rank *\(n* + 1\) - 1 = *n.* Using the description of IT from Proposition 5.1, this means  
there exist *\(Xl,* ... *,xt*  
*, rl+l,* ... , *rn\)* such that *n\*xi* - *no* = 0 for i = 1, ... , *t* and *n\*rj* = 0 for  
*j* = *t* + 1, ... , nand 100 1.4\. Polyhedral Theory  
is of rank *n.* \(Note that *t* \~ 1, since *re\*rj* = 0 for\) = 1, ... *,n* would imply *re\** = 0.\) But this  
implies that the vectors *\(Xl,* -1\), ... *,\(Xl,* -1\), *\(Xl* + *rl+l,* -1\), ... *,\(Xl* + *rn*  
*,* -1\) are linearly  
independent. Hence by Proposition 1.3, *Xl,* ... *,Xl, Xl* + *rl+l,* ... *,Xl* + *rn* are affinely  
independent. Therefore *\(re\*,* re\~\) defines a facet of *P.*  
Conversely if *\(n\*, reo\)* defines a facet of *P,* there exist *n* affinely independent points  
*\{Xi\}7=1* of *P,* with *re\* Xi* = *reo* for *i* = 1, ... , *n.* But now considering the polyhedral cone TI, the  
equality set of *\(re\*, reo\)* includes *\(Xl,* -1\), ... , *\(xn*  
*,* -1\) and hence is of rank at least *n.* If the  
equality set is of rank *n* + 1, then *\(re\*, reo\)* = \(0, 0\). Hence its rank is *n,* so  
*\{ere, reo\)* E *Rn+l\: \(re, reo\)* = *A\(re\*, reo\), A* E *Rl\}* is a one-dimensional face ofTI. It follows from  
Proposition 4.3 that *\(n\*, reo\)* is an extreme ray ofTI. ..  
We have also implicitly proved a dual result to Theorem 5.2.  
Theorem 5.3. *If dim\(P\)* = *nand* rank\(A\) = *n, rex\** - *reo* \~ 0 *defines afacet ofTI if and only*  
*if x\* is an extreme point of P, and rer\** \~ 0 *defines a facet ofTI if and only if r\* is an extreme*  
*rayofP.*  
*Proof.* By Proposition 5.1, every facet ofTI is either of the required form *rexk*  
- *reo* \~ 0  
for *k* E *K* or *rerj* \~ 0 for\} E *J.* To show that each of these inequalities defines a facet,  
remember that *x\** is an extreme point only if its equality set \(A\~., b\~.\) is of rank *n.* Hence  
there exist *\(rei, -n6\),* ... , *\(nn, -re3\)* such that *rei,* ... *,ren* are linearly independent, and  
*retx\** - *reb* = 0 for *t* = 1, ... , *n.* Now these *n* vectors plus \(0, 0\) are affinely independent,  
and hence *rex\** - *reo* \~ 0 defines a facet. A similar argument shows that *rer\** \~ 0 defines a  
facet. ..  
Now we specialize further and assume that *P* is a full-dimensional polytope. By  
translation we can take the origin 0 to be an interior point, so if *aix* \~ *bi* is an equality  
describing *P,* then *bi* \> O. Hence we can rewrite *P* as *P* = *\{x ERn\: Ax* \~ 1\}, where 1 =  
\(1 ... 1\). Now every valid inequality *\(n, reo\)* must also have *reo* \> 0, so we can normalize the  
polar and consider the so-called *l-polar* of *P\:* TIl = *\{re ERn\: \(re,* 1\) E TI\}. Furthermore,  
since *P* is a polytope, by Theorem 4.7 we have  
and by Proposition 5.1 we have TIl = *\{re ERn\: rexk*  
\~ 1 for *k* E *K\}.*  
Proposition 5.4 *IfP* = *\{x ERn\: Ax* \~ 1\} *is afull-dimensional polytope, then* TIl *is afull-*  
*dimensional polytope and P is the l-polar of* TIl.  
*Proof.* Since 0 is an interior point of TIl, by Corollary 2.5, TIl is full-dimensional.  
Suppose TIl has a ray *y,* so that *yxk*  
\~ 0 for *k* E *K.* This implies that *\(y,* 0\) is a valid  
inequality for *P,* which is a contradiction. Hence TIl is bounded.  
Now consider *P* = *\{y\: rey* \~ 1 for all *re* E TIl\}. If *x* E *P,* then *rex* \~ 1 for all *re* E TIl and  
hence *P* \~ *P.* Suppose *yEP* \\ *P.* Then there exists no solution to  
*Ak* \~ 0 for *k* E *K.* 5\. Polarity 101  
So there exists a *\(n, no\)* such that *nxk*  
- *no\:;;\:\:;;* 0 for all *k* E *K* and *ny* - *no* \> O. Since 0 E *P,*  
we can \~gain normalize so that *no* = 1. Then *n* E III but *ny* \> 1, which is a contradiction.  
*HenceP=P. •*  
Now we observe that there is complete symmetry between *P* and III.  
Theorem 5.5 *If P is full-dimensional and bounded, and* 0 *is an interior point of P, then*  
a. *P* = *\{x\: ntx* \:;;\:\:;; 1 *for t* E *T, where \{nt\}tET are the extreme points of* III\}, *and*  
b. III = *\{n\: nxk* \:;;\:\:;; 1 *for k* E *K, where \{xkhEK are the extreme points of P\}.*  
*Moreover, each of the inequalities in descriptions* a *and* b *defin e facets.*  
*Proof* We have already proven b, and a follows from Proposition 5.4.  
# •  
Corollary 5.6. *IfP is as described in Theorem* 5.5, *then*  
*x\** E *P if and only if* max\{nx\*\: *n* E III\} \:;;\:\:;; 1  
*and*  
*n\** E III *if and only if* max\{n\*x\: *x* E *P\}* \:;;\:\:;; 1.  
*Proof x\** E *P* if and only if *n x\** \:;;\:\:;; 1 for all *n* E III, which holds if and only if  
max\{nx\*\: *n* E III\} \:;;\:\:;; 1. The second equivalence is merely a dual statement. •  
Corollary 5.6 is important in establishing the equivalence between separation and  
optimization \(see Section 1.6.3\).  
*Example* 5.1 \(See Figure 5.1.\)  
\(P\) \:;;\:\:;; 1  
There are two other polars that are of special interest in combinatorial optimization.  
For the remainder of this section we assume that *A* is a nonnegative *m* x *n* matrix.  
Suppose that *P* = *\{x* E *R1\: Ax* \~ 1\}, where *A* has no zero rows. The *blocker p B* of *Pis*  
the polyhedron\:  
*p B*  
= *\{n* E *R1\: nx* \~ 1 for all *x* E *P\}.*  
Let *B* be a I *K* I x *n* matrix whose rows are the extreme points *\{xkhEK* of *P.*  
Proposition 5.7. *Let P* = *\{x* E *R1\: Ax* \~ 1\}, *where A is a nonnegative matrix with no zero*  
*rows. Then*  
i. *p B*  
ii. *\(PB\)B* = *P.*  
= *\{n* E *R1\: Bn* \~ 1\} *and* 102 1.4\. Polyhedral Theory  
\(-1, 1\) \(0,1\) \(3,1\)  
--\(---1-, 0 .... \)-----+--+---- Xl ---'\(-\_-l-,-O\)-r---\~\~-------\~l  
\(-1, -1\)  
\(1, - 2\)  
*p*  
Figure 5.1  
*Proof* Note that since *A* has no zero rows, *P* \* 0. Also, since *pO* = R\~, the extreme  
rays of *P* are the unit vectors *ej* for *j* = 1, ... , *n.* This means that *nj* \~ 0 for all *\(n, no\)* E II,  
so II = *\{en, no\)* E R\~ x *Rl\: nx* \~ *no* for *x* E *P\}* and so *n* E *pB* if and only if *\(-n,* -1\) E II.  
Now by Proposition 5.1, *pB* = *\{n* E R\~\: *nxk*  
\~ 1 for *k* E *K\},* where *\{Xk\}* are the extreme  
points of *P,* and statement i is verified.  
Since *P* is full-dimensional and rank \(1\) = *n,* we obtain from Theorem 5.2 that *\(-n\*, -1\)*  
is an extreme ray of n if and only if *n\*x* \~ 1 defines a facet of *P.* But since  
*pB* = *\{en, no\)\: \(-n, -no\)* E II, *no* = -l\}, it follows that *\(-n\*,* -1\) is an extreme ray of II if and  
only if *n\** is an extreme point of *pB.* Now we consider *\(PB\)B.* By statement i,  
*\(PB\)B* = *\{x* E R\~\: *Qx* \~ l\}, where the rows of *Q* are the extreme points of *pB,* or, as we have  
just shown, the facets of *P.* Hence statement ii holds. •  
*Example* 5.2. Let  
The reader can check that the extreme points of *P* = *\{x* E *Rl\: Ax* \~ l\} are \(l \(1 0 1\), \(0 1 1\), and \(1 1 1\). Hence  
1 0\),  
B\~ *\(1* 1  
# o  
1  
\:2  
We note that all the extreme points of *P* are minimal points because *pO* = R\~ and they  
are all necessary in the description of *pB.*  
Finally, we consider polytopes of the form *P* = *\{x* E R\~\: *Ax* \~ l\}, where *A* is a nonnega-  
tive matrix with no zero columns. The *antiblocker pC* of *P* is the polytope  
*pC* = *\{n* E R\~\: *nx* \~ 1 for all *x* E *P\}.* 5\. Polarity 103  
Let C be an *r* x *n* matrix whose rows are the extreme points of *P.*  
Proposition 5.8. *If P* = *\{x* E R\~\: *Ax* \~ 1\}, *where A is a nonnegative matrix with no zero*  
*columns, then*  
i. *pC* = *\{n* E R\~\: *Cn* \~ 1\} *and*  
ii. *\(pC\)C* = *P.*  
*Proof* Since *A* has only nonzero columns, *P* is a polytope. Statement i follows by  
replicating the proof of Proposition 5.1 with *n* \~ 0, *no* = 1, and *J* = 0. To establish  
statement ii, observe that if *x* E *P,* it follows that *nx* \~ 1 for all *n* E *pC* and hence  
*x* E *\(pC\)c.* Now suppose that *x* E *\(pC\)c.* Since *ai* E *pC,* it follows that *aix* \~ 1. Hence  
*p* = *\(pC\)c. •*  
*Example* 5.3\. Let  
The reader can check that the extreme points of *P* = *\{x* E *Rl\: Ax* \~ 1\} are \(l 0 0\),  
\(0 1 0\), \(0 0 1\), and \(1 1 D. Hence  
The extreme points of *pC* are the rows of *A* and the points \(1 0 0\), \(0 1 0\), \(0 0 1\),  
and \(0 0 0\).  
This example shows the difference between the blocking and antib10cking cases. We see  
that not all the extreme points of *P* are needed to describe its antiblocker. In fact, it is not  
difficult to show\:  
Proposition 5.9. *If P* = *\{x* E R\~\: *Ax* \~ 1\} *where A* \~ 0 *and has no zero columns, then*  
i. *The facet defining inequalities of pC are the inequalities xr n* \~ 1, *r* = 1, ... , *R,*  
*where* \{X'\}\~=l *are the extreme points of P that are maximal in P.*  
ii. *If X O is an extreme point of P that is not maximal in P, there exists a maximal*  
*extreme point x' for which xJ* = *xj for all j such that xJ* \> O.  
The main results of blocking and antiblocking can also be interpreted as problems  
involving the \(fractional\) packing and \(fractional\) covering by rows of *A.*  
*Definition* 5.2. If *A* and *B* are nonnegative matrices with the property that  
*\{n* E *R1\: Bn* \~ 1\} is the blocker of *\{x* E *R1\: Ax* \~ 1\}, then *A, B* is called a *blocking pair.*  
*Antiblocking pairs* are defined similarly.  
*Definition 5.3*  
i. The *max-min inequality* holds for a pair *ofm* x nand *r* x *n* nonnegative matrices  
*A, B* if for all *w* E *R1*  
max\{ly\: *yA* \~ *W, Y* E *R'\:\}* = min *bjw.*  
lsjg 104 1.4\. Polyhedral Theory  
ii. The *min-max inequality* holds for a pair *ofm* x nand *r* x *n* nonnegative matrices  
*A,* C if for all *w* E R\~  
*min\{1y\: yA* \~ *w, Y* E R\~\}= m\~x *cjw.*  
*l\:S\:j\:s\:r*  
Theorem 5.10. *The max-min \(min-max\) inequality holdsfor a pair A, B if and only if A*  
*and B form a blocking \(antiblocking\) pair of matrices.*  
*Proof* We consider only the blocking case. If the max-min inequality holds, then  
*max\{1y\: yA* \~ *w, y* E R\~\} = min\{wx\: *Ax* \~ 1, *y* E R\~\}  
*= min\{wxk*  
*\: Xk* is an extreme point of *P\}.*  
It follows that the rows of *B* are precisely the extreme points of *P* and that any other row of  
*B* is equal to or greater than a convex combination of these extreme points. Hence *B* is a  
blocking matrix associated with *A.* The converse is an immediate consequence of linear  
programming duality. •  
6\. POLYHEDRAL TIES BETWEEN LINEAR AND INTEGER PROGRAMS  
Now, as promised in the introduction to this chapter, we will show that an integer program  
can, in theory, be reduced to a linear program.  
Given *P* = *\{x* E R\~\: *Ax* \~ *b\},* where *\(A, b\)* is an integer *m* x *\(n* + 1\) matrix, and  
S = *P* n *zn,* we are going to show that conv\(S\) is a rational polyhedron. Whenever *P* is  
bounded, S is either empty or a finite set of points, so the result is a consequence of  
Theorem 4.13.  
To obtain the result when S contains an infinite number of points, we will show that  
conv\(S\) can be generated from a finite number of points in S and a finite number of  
integral-valued rays. The idea of the proof is shown in Figure 6.1. Geometrically, we see  
that conv\(S\) is the polyhedron generated by convex combinations of the points \{\(l, 2\),  
\(2, 1\), \(4, 0\)\) plus nonnegative linear combinations of the rays rl and *r2,* which are the  
extreme rays of *P.*  
The important step in the proof is to show that the set of integer points in a polyhedron  
can be finitely generated. We will give a finite set *Q* C S \(in Figure 6.1, the integral points  
in the shaded region of *P\)* and then show that S can be generated by taking a point in *Q*  
plus a nonnegative integer linear combination of the extreme rays of *P.*  
Theorem 6.1. *If P* = *\{x* E R\~\: *Ax* \~ *b\} =1=* 0 *and* S = *P* n *zn, where \(A, b\) is an integer*  
*m* x *\(n* + 1\) *matrix, then thefollowing statements are true\:*  
i. *There exist a finite set of points \{q I\} lEL of* S *and a finite set of rays \{ri\} iEJ of P such*  
*that* 6\. Polyhedral Ties Between Linear and Integer Programs  
105  
4  
3  
2  
...... -----\~\~ rl = \(1, 0\)  
2 3  
4  
Figure 6.1  
ii. *IfP is a cone \(b* = 0\), *there exists ajinite set of rays \{vhhEH ofP such that*  
*Proof*  
i. Let *\{xk* E R\~\: *k* E *K\}* be the finite set of extreme points of *P* and let *\{rJ* E R\~\: *j* E J\} be  
the finite set of extreme rays of *P.* Since *P* is a rational polyhedron, all of these extremal  
vectors have rational coordinates. We have  
Without loss of generality, we can assume that *\{rj \}* for\} E *J* are integer vectors.  
Let  
*Q* = *\{x* E Z\~\: *x* = *L AkXk* + *L fl.Jri, L Ak* = 1, *Ak* \~ 0 for *k* E *K,* 0 \~ *fl.J* \< 1 for\} E *J\}.*  
*kEK jEJ kEK*  
*Q* is a finite set, say *Q* = *\{ql* E *Z1\:* IE *L\},* and Q s S. Now observe that *Xi* E S if and only  
*ifxi* E Z\~ and  
\(6.1\) *Xi* = *\{L* A\~Xk + *L \(fl.\)* - *lfl.\)J\)rJ\}* + *\{L lfl.\)JrJ\}, L Ak* = 1, *Ak, fl.J* \~ 0  
*kEK jEJ jEJ kEK*  
for *k* EK and\} E *J.* 106 1.4\. Polyhedral Theory  
The first term of\(6.1\) is a point of *Q,* so there exists *I\(i\)* E *L* such that  
\(6.2\) *Xi* = *ql\(i\)* + *L pJri*  
*iEJ*  
*, pj* = *\[,ujJ* for all\} E *J.*  
The result follows.  
suffices to take  
ii. Observe that if *P* is a cone, then *ql* E S implies *yl* E S for all *Y* E Zl. Therefore it  
from part i. •  
Now we easily obtain the following theorem.  
Theorem 6.2. *IfP* = *\{x* E R\~\: *Ax* \~ *b\}, where \(A, b\) is an integer m* x *\(n* + 1\) *matrix, and*  
S = *P* n *zn, then* conv\(S\) *is a rational polyhedron.*  
*Proof* Since any point *Xi* E S can be written in the form \(6.2\), any convex combina-  
tion of points *\{Xi* E S, *i* E *I\}* can be written as  
*= L* \( *L Yi\)ql* + *L \(L YiPJ\)ri*  
*tEL* \(iEf\: *l\(i\)=l\) iEJ* iEf  
*= L alql* + *L Piri*  
*,*  
*'EL iEJ*  
where *a,* = *LUEf\: t\(i\)=l\) Yi* \~ 0 for *I* E *L, LIEL a,* = LiEf *Yi* = 1, and *Pi* = LiE! *YiP\}* \~ 0 for\} E *J.*  
Now it follows that  
conv\(S\) = *\{x* E R\~\: *x* = *L a,ql* + *L Piri, L a,* = 1, *a" Pi* \~ 0 for *I* ELand\} E *J\},*  
*'EL iEJ IEL*  
with *ql, ri* E Z\~ for *I ELand\}* E *J.* Hence by Theorem 4.13, conv\(S\) is a rational  
polyhedron. •  
The above proof extends straightforwardly to mixed-integer sets with rational data. As a  
consequence, all of the following results given in this section apply to mixed-integer sets  
and mixed-integer programs. The above proof also shows that if *P* n *zn* '\* 0, then the  
extreme rays of *P* = *\{x* E R\~\: *Ax* \~ *b\}* and *conv\(P* n *zn\)* coincide.  
Theorem 6.2 suggests that we can solve the integer program  
\(IP\) max\{cx\: xES\} where S = *P* n *zn*  
by solving the linear program  
\(eIP\) max\{cx\: *x* E conv\(S\)\}.  
This important, but elementary, result is formalized in the following theorem. 6\. Polyhedral Ties Between Linear and Integer Programs 107  
Theorem 6.3. *Given* S = *P* n *zn =1=* 0, *P* = *\{x* E R\~\: *Ax* \~ *b\}, and any cERn, it follows*  
*that\:*  
a. *The objective value of* IP *is unboundedfrom above if and only if the objective value of*  
CIP *is unbounded from above.*  
b. *If* CIP *has a bounded optimal value, then it has an optimal solution \(namely, an*  
*extreme point of* conv\(S» *that is an optimal solution to* IP.  
c. *If Xo is an optimal solution to* Ip, *then Xo is an optimal solution to* CIP.  
*Proof* Let *Zo* and *z\** be the optimal values of IP and Clp, respectively, with the  
convention that *Zo* or *z\** = 00 if the objective value is unbounded from above. Note that  
conv\(S\) ;2 S implies that  
\(6.3\)  
a. Inequality \(6.3\) implies that if *Zo* = 00, then *z\** = 00. On the other hand, if *z\** = 00,  
then there is an integral extreme point *Xo* E conv\(S\) and a ray *r* E Z\~ such that  
*cr* \> 0 and *Xo* + *\(Jr* E conv\(S\) for all 8 \~ O. But then *Xo* + *\(Jr* E S for all *\(J* E Zl,  
which implies that *Zo* = 00.  
b. Since conv\(S\) is a polyhedron, if CIP has an optimal solution, then it has an  
extreme point optimal solution, say *xo.* Thus *Xo* E S, so *ZO* \~ *cxo*  
= *z\*.* By \(6.3\),  
*Zo* = *z\*.*  
c. This follows from parts a and b along with *X O* E conv\(S\). •  
Corollary 6.4. IP *is either infeasible or unbounded or has an optimal solution.*  
Theorem 6.3 states that we can solve the integer program IP by solving the linear  
program CIP. In fact, if we knew a polyhedral representation of conv\(S\) in terms of linear  
inequalities, this would be a nice way to describe our integer program. But generally we do  
not know a set of linear inequalities that define conv\(S\). Thus we formulate our integer  
program using some polyhedron *P* = *\{x* E R\~\: *Ax* \~ *b\}* such that S = *P* n *zn.* Viewed in  
this framework, reducing an integer program to a linear program amounts to deducing a  
linear inequality representation of conv\(S\), or at least the relevant inequalities with  
respect to an objective function c, from the linear inequality representation of *P* and the  
integrality requirement. This is the principal topic of Chapter ILL  
Until now we have only considered valid inequalities for polyhedra. We say that *\(n, no\)*  
is a *valid inequality for a set* S if *nx* \~ *no* for all *xES.*  
Proposition 6.5 *If nx* \~ *no is valid for* S, *it is also valid for* conv\(S\).  
*Proof* Consider an *x* E conv\(S\). Then *x* = *LiE\] Jvxi*  
*,* where *Xi* E S for *j* E *J,* and  
*LjE\]* \)/ = 1 and *Aj* \~ 0 for *j* E *J.* Hence  
*nx* = I *Ainxi\)* \~ I *AinO* = *no·*  
*jE\] jE\]*  
# •  
To establish the dimensionality of a face of conv\(S\), it suffices to consider points of S. 108 1.4\. Polyhedral Theory  
Proposition 6.6. *If nx* \~ *no dejines a face of dimension k* - 1 *of* conv\(S\), *there are k*  
*afjinely independent points* xl, ... *,Xk* E S *such that nxi* = *nofor i* = 1, ... , *k.*  
*Proof.* By definition, there are *k* affinely independent points XI, ... *,Xk* E conv\(S\)  
such that *nxi* = *no* for *i* = 1, ... , *k.* If *Xi* E S for *i* = 1, ... *,k,* there is nothing more to  
prove, that is, take *Xi* = *Xi* for *i* = 1, ... , *k.* So suppose *Xl* \$. S. Then *Xl* = I.iEJ *Aixi,* where  
*xi* E S and *Ai* \> 0 for all\} E *J,* and \~EJ *Ai* = 1. Now *nx2* = *no* and *nxi* \~ *no* for\} E *J* imply  
that *nxi* = *no* for all\} E *J.* Since *Xl,* ... , *Xk* are affinely independent, there exists\}\* E *J*  
such that *xi·, X2,* ... *,Xk* are affinely independent. The proof is completed by repeating  
this process until the resulting set contains only elements of S. •  
Consider the problem  
\(LP\) *max\{cx\: Ax* \~ *b, x* E R\~\}.  
Previously we have related IP to CIP. Now we relate IP to LP.  
Let  
*zed\)* = *max\{cx\: Ax* \~ *d, x* E z\~\}  
and  
*zLP\(d\)* = *max\{cx\: Ax* \~ *d, x* E R\~\},  
so that *z\(b\)* = *max\{cx\: x* E *P* n *zn\}* and *zLP\(b\)* = *max\{cx\: x* E *P\}* with  
*P* = *\{x* E R\~\: *Ax* \~ *b\}.*  
Proposition 6.7  
a. ZLP\(O\) = *z\(O\).*  
b. *z\(O\)* = 0 *ifand only ifQ* = *\{u* E *R'\:\: uA* \~ c\} *=1= 0.*  
c. *z\(O\)* = 00 *if and only if Q* = 0.  
d. *IfQ =1=* 0, *then* S = *P* n *zn* = 0 *or z\(b\) isjinite.*  
e. *IfQ* = 0, *then* S = 0 *or z\(b\)* = 00.  
*Proof.* Clearly 0 \~ *z\(O\)* \~ ZLP\(O\). If *Q* "\* 0, then ZLP\(O\) = 0 by duality and hence  
*z\(O\)* = ZLP\(O\) = O. If *Q* = 0, then from Theorem 4.9 there exists an extreme ray *rJ* of *P* with  
*crJ* \> O. Since *rJ* can be taken to be integer, *z\(O\)* = 00 = ZLP\(O\). This proves statements a, b,  
andc.  
If *Q =1=* 0, then it follows, by duality, that *P* = 0 or *zLP\(b\)* is finite. Hence statement d  
follows. Similarly if *Q* = 0, then it follows, by duality, thatP = 0 or zLP\(b\) = ZLP\(O\) = 00. If  
S *=1=* 0, then from statement c it follows that *z\(O\)* = z\( *b\)* = 00. Hence statement e follows .•  
Corollary 6.8  
a. *If P* = 0, *then* S = 0.  
b. *IfzLp\(b\) isjinite, then* S = 0 *or z\(b\) isjinite.*  
c. *IfzLp\(b\)* = 00, *then* S = 0 *or z\(b\)* = 00. 8\. Exercises 109  
Note that by solving the linear program *max\{cx\: x* E *P\},* we establish which of the cases  
a, b, or c occurs. Corollary 6.8 says that, except for the fact that S may be empty when *Pis*  
not empty, IP and LP have the same status.  
7\. NOTES  
Sections 1.4.1-1.4.4  
Halmos \(1959\) and Strang \(1976\) are basic reference books on linear algebra. The  
fundamental works on general polyhedral theory and convexity are Grunbaum \(1967\),  
Rockafellar \(1970\), and Stoer and Witzgall \(1970\).  
Chapter I of Pulley blank's Ph.D. dissertation \(1973\) focuses on the aspects of polyhedral  
theory used in combinatorial optimization. Also see Bachem and Grotschel \(1982\) and  
Pulleyblank \(1983\).  
Section 1.4.5  
The basic reference on polarity is Rockafellar \(1970\). The study of blocking and antiblock-  
ing polyhedra is due to Fulkerson \(1968, 1970a, 1971, 1972\). Also see Tind \(1974, 1977,  
1979\).  
Section 1.4.6  
The proof of Theorem 6.1 is taken from Giles and Pulleyblank \(1979\). Also see Meyer  
\(1974, 1975\) and Meyer and Wage \(1978\).  
8\. EXERCISES  
1. Consider the polyhedron *P* described by  
*XI-X2 S 0*  
*-xl+x2 s1*  
*2X2;\:\:\: 5*  
*8Xl -X2* S 16  
i\) Find the dimension of *P.*  
ii\) Find an interior point \(if one exists\).  
iii\) Describe all the faces of *P.*  
iv\) Consider each of the faces *Fi* = *P* n *\{x* E *R2\: aix* = bJ for *i* = 1, ... , 5.  
What is the dimension of *pi?*  
Which inequalities define facets of *P?*  
v\) Give a "minimal" representation of *P.* 110 1.4\. Polyhedral Theory  
2\. Consider the assignment polytope  
*P* = *\{x* E R\~2\: ± *Xij* = 1, *i* = 1, ... , *n,* ± *xi\)* = *l,j* = 1, ... , *n\}.*  
\)=1 i=1  
i\) Determine its dimension and its facets.  
ii\) What happens if we replace the equality constraints by  
*n*  
\)=1  
L *xi\)* S 1, *i* = 1, ... *,n* and  
*n*  
i=1  
*LXi\)* 2 1, *j* = 1, ... , *n.*  
3\. A wheel *Wn=\(V,E\)* is a graph defined by *V=\{vo,vt, ... , vn\}* and  
*E* = *\{\(va,* vJ\: *i* = 1, ... , *n\}* U *\{\(Vi, Vi+I\)\: i* = 1, ... , *n* -l\} U *\{\(vm* VI\)\}'  
Let *P* = *\{x* E *RIEl\: 1\:\(xe\: e* contains node *v\)* = 2 for all *v* E *V,* 0 S *Xe* S 1 for all  
*eEE\}.*  
i\) Find the dimension of *P.*  
ii\) Show that the inequalities *xe* 2 0 are redundant.  
iii\) Show that the inequalities *Xe* S 1 are redundant for *e* = *\(vo, Vi\)* for *i* = 1, ... , *n.*  
iv\) Give a minimal representation of *P* by a system of linear inequalities and  
equalities.  
v\) Give a representation of *P* by means of its extreme points.  
4. Let *F* be the face of optimal solutions of the linear program max\{cx\: *x* E *P\},* where  
*P* = *\{x ERn\: Ax* S *b\}.* Let *Mp* be the equality set of *F* and let *u\** be any optimal  
solution of the dual linear program. Show that *Ui\** = 0 if *i* tE *Mp.*  
5\. Show that if Hand G are two faces of a polyhedron *P* of dimension rand *r* + *s,*  
respectively, and *H* is a face of G, there exists a sequence of faces *\{FJf=o* with\:  
i\) *Fo* = *H, Fs* = G;  
ii\) *Fi* is a facet of *Fi+1* for *i* = 0, ... , s - 1.  
6. Find all extreme points and extreme rays of\:  
i\) the polyhedron in Exercise 1;  
ii\) the polyhedron  
XI + *2X3* 2 2  
*-X2* + *X3* 2-4  
*XER3.*  
7. For each face *F* of *P* in Exercise 1, find the values of c such that max\{cx\: *x* E *P\}* has  
*F* as the set of optimal solutions. 8\. Exercises 111  
8. \(Fourier-Motzkin Elimination\). Given a polyhedron *P* \~ *Rn+l* described by the  
inequalities  
*a1x* + *y* \~ *ab* for *I* = 1, ... , *L*  
*bk X* - *Y* \~ b\~ for *k* = 1, ... , *K*  
*cix* \~ *cb* for *i* = 1, ... , *I,*  
where *x ERn, y* E *Rl\:*  
i\) Show that  
9. ii\) Find *projAP\),* where  
*P* = *\{\(x, y\)* E *R2\: x* - *y* ;;;\:\: -2, *x* + *y* \~ 3, *x* - *y* \~ -1, *y* ;;;\:\: O\}.  
i\) Given a polyhedron *P,* let *\{Fj\}iEI* and *\{Gj\}jEJ* be polyhedra with *F j, Gj* \~ *P.* Show  
that  
ii\) Show that the inequality is strict in the following example\:  
Let *Fi* = *P* n *\{x* E R\~\: Xl = i\} for *i* = 0, 1,2  
and *Gj* = *P* n *\{x* E R\~\: *X2* = i\} for *j* = 0, 1, 2  
with *P* as in Figure 8.1. Note that l\) *\(Fi* n *Gj \)* = *P* n Z\~.  
I,\)  
*X2*  
\(1,2\) \(2,2\)  
\(0,2\)  
2  
Figure 8.1 112 1.4\. Polyhedral Theory  
iii\) Show that equality holds in part i when the *\{FJ, \{Gj \}* are faces of *P.*  
iv\) Suppose *P* is contained in the unit hypercube in *R* 2  
• Take  
*Fa* = *P* n *\{x* E *R2\:* Xl = l5\} and G *a* = *P* n *\{x* E *R2\: Xi* E l5\} for \<\:5 E \{O, n. Interpret  
the equality in this case.  
10. Find the polar and its extreme rays for the polyhedron in Exercise 1.  
11. Let S = *\{x* E Z\~\: Xl - *X2\:\:\:;;* 1, *4XI* + *X2\:\:\:;;* 28, Xl + *4X2\:\:\:;;* 27\}.  
i\) Find an inequality description of conv\(S\).  
ii\) Find the extreme points of conv\(S\).  
iii\) Find the polar of conv\(S\).  
iv\) Find the extreme rays of the polar of conv\(S\).  
12. Find the I-polar of  
13. Find the blocker of  
*P* = *\{x* E R\~\: ixl + tx2 \~ 1, *1XI* + 1x2 ;;\:\:\: 1, ixl + \~X2 \~ n.  
14. Prove Proposition 5.9.  
15. Prove the min-max version of Theorem 5.10.  
16. Let *P* = *\{x ERn\: Ax* \:\:\:;; *b\},* where rank\(A\) = *k* \< *n.* Let *L* = *\{x ERn\: Ax* = O\},  
L.1 = *\{x ERn\: Bx* = O\} and *P\** = *P* n *L .i.*  
i\) Show that *P* = *P\** + *L.*  
ii\) Derive Minkowski's theorem when rank\(A\) \< *n.*  
iii\) Demonstrate it for  
17. Find a finite set of generators for the set S = *P* n Z2, where  
*P* = *\{x* E R\~\: *5XI* + *3X2* \~ 10, *5XI* - *5X2* \~ -1, *-Xl* + *2X2* \~ -2\}.  
18. Give examples of pairs,  
\(LP\)  
max\{cx\: *Ax* \:\:\:;; *b, X* E R\~\}  
\(IP\)  
*max\{cx\: Ax* \:\:\:;; *b, X* E Z\~\},  
where  
i\) LP and IP are unbounded,  
ii\) LP and IP have finite optimum value,  
iii\) LP is unbounded, and IP is infeasible,  
iv\) LP is bounded, and IP is infeasible. 8\. Exercises 113  
**19.** Using the polyhedron of Exercise 1 of Section 1.1.8, show that Theorem 6.2 does not  
hold for irrational polyhedra.  
**20.** Consider the graph G = *\(V, E\),* where *V* = \{l, 2, ... , *2k* + 1\}, *k* \~ 2, and  
*E* = \{\(l, 2\), \(2, 3\), ... , *\(2k* + 1, I\)\}. Let S = *P* n Z2k+l be the set of node packings on  
G where *P* = *\{x* E *Rik+l\: Xi* + *Xj* S 1 for *\(i,\}\)* E *E\}.* Show that conv\(S\) \~ *P.* Find  
another facet of conv\(S\). Now do you have conv\(S\)? Why? **1.5**  
# Computational Complexity  
1. INTRODUCTION  
The purpose of this chapter is to describe a theory of computational complexity that yields  
insights into how difficult a problem may be to solve.  
At the easy end of our spectrum, there are problems like the minimum-weight spanning  
tree problem. Recall that in Section 1.3.3 we gave an algorithm for the minimum-weight  
spanning tree problem with running time *O\(n* log *n\)* for a graph with *n* edges. One  
fundamental issue to be discussed here is when a problem can be solved in time *O\(lk\),*  
where *k* is a constant and *I* is an appropriate measure of the length of the input needed to  
describe the data.  
For most integer programming problems, no such algorithm is known. We will show  
that there are integer programming problems much more specific than the general pure-  
integer programming problem \(e.g., maximum cardinality node packing\) with the follow-  
ing property. If maximum cardinality node packing for a graph with *m* nodes can be  
solved in time *O\(mk\)* for some fixed *k,* then there exists a *k* such that the pure-integer  
programming problem can be solved in *O\(lk\),* where *I* measures the input needed to  
describe the data *A, b,* c.  
A very important concept introduced in this chapter is a "certificate of optimality."  
Given a certificate of optimality, one can prove in *O\(lk\)* time, for some fixed *k,* that a given  
solution is indeed optimal.  
After introducing some basic concepts in this section and Section 2, we will show in  
Section 3 that primal and dual basic optimal solutions provide a certificate of optimality  
for linear programming.  
Although no certificate of optimality is known for the general pure-integer program-  
ming problem, in Section 4 we will develop some results for pure-integer programs that  
will enable us to establish a weaker result, namely, a "certificate offeasibility." This means  
that given an appropriate feasible solution, we will be able to check feasibility quickly. The  
result is not trivial, since one can imagine feasible integer programs for which the only  
solutions have a large number of digits, so checking feasibility by substitution is a  
formidable task.  
In Section 5, we formalize the concept of a feasibility problem and the class offeasibility  
problems with a certificate of feasibility. In Section 6, we show that there are hardest  
feasibility problems in the above class and relate these results to optimization problems. In  
Section 7, we consider the complexity of problems associated with polyhedra such as  
whether *nx* \~ *no* is satisfied by all points in a given polyhedron.  
The presentation here represents a compromise between the rigor found in computer  
science texts, which would require many new definitions and concepts, and a very  
informal presentation that can lead to fundamental misconceptions. Thus it is necessary  
114 1\. Introduction 115  
for us to define rather precisely the meanings of terms such as problem, instance of a  
problem, polynomial solvability, and so on. But we will avoid using terms such as Turing  
machine, language, and so forth.  
*Mixed-integer programming* is the problem written generically as  
*max\{cx* + *hy\: Ax* + *Gy* \~ *b, x* E Z\~, *Y* E *Rf\},*  
where *m* is any positive integer, *p* and *n* are any nonnegative integers with  
*p* + *n* \~ 1, and *c, h, A,* G, *b* are matrices with integral coefficients; the dimensions of  
these matrices are as follows\: cis 1 x *n, h* is 1 x *p, A* is *m* x *n,* Gis *m* x *p,* and *b* is *m* x 1.  
We could just as well have assumed that the matrices have rational coefficients, but the  
assumption of integer coefficients is no less general and is more convenient.  
A problem consists of an infinite number of instances. An instance is specified by  
assigning numerical values called *data* to the problem parameters. In the case of mixed-  
integer programming, the data that specify an instance are integers *m, n,* and *p* as well as  
integral matrices *c, h, A,* G, and *b* of appropriate dimension.  
It is desirable to delineate special cases of the mixed-integer programming problem.  
This is done by restricting the parameters in natural ways. *Pure-integer programming* is  
the special case of mixed-integer programming in which *p* = 0, and hence the matrices *h*  
and G do not appear. *Linear programming* is the special case in which *n* = 0, and hence  
the matrices *c* and *A* do not appear.  
Every instance of a linear or pure-integer program is also an instance of a mixed-integer  
program. Thus an algorithm that can solve all instances of mixed-integer programming  
can, by definition, solve all instances of the special cases of pure-integer and linear  
programming. An obvious conclusion is that mixed-integer programming is at least as  
hard as pure-integer and linear programming.  
Figure 1.1 is a directed graph that shows relationships among some of the problems that  
have been formulated in Chapter 1.1. The problem at the head of an arc is a special case of  
the problem at the tail. The relationship extends transitively to directed paths. Thus, if  
there is a directed path from problem *Xl* to problem *X* 2, then every instance of *X* 2 is also an  
instance of *Xl.*  
Most of the arcs in Figure 1.1 are easily justified. For example, 0-1 *integer programming*  
contains those instances of pure-integer programming in which  
*A* = \( \~'\) and *b* = \( \~'\),  
where *I* is an *n* x *n* identity matrix and 1 is an *n* x 1 matrix of I's. *Set packing* contains  
those instances of 0-1 integer programming in which each coefficient of matrix *A* is ° or 1  
and *b* is a column of I's. *Node packing* contains those instances of set packing in which  
each row of *A* has exactly two 1 'so  
In attempting to classify these problems, an extreme view is to ignore the special cases.  
All of our problems are just mixed-integer programs to be solved by the same algorithm.  
While there are good reasons for having a robust algorithm, by carrying it to this extreme  
we would fail to take advantage of the structure and simplicity of important special cases.  
On the other hand, there are so many interesting special cases of mixed-integer program-  
ming that it would be foolish, if not hopeless, to consider each separately. The fundamen-  
tal issue is to find natural divisions. One possible way of achieving this is to attempt to add  
arcs to the graph of Figure 1.1 to create directed cycles. Then if problems *Xl* and *X* 2 are  
contained in a directed cycle, they are equivalent in the sense that each is a special case of  
the other. .....  
.....  
0\\  
# f  
Shortest  
path  
# \~  
\~  
l \~  
Linear  
programming  
# t  
Fixed-charge  
network  
flow  
# l  
M ixed-integer  
programming  
Mixed 0-1 Pure-integer  
programming programming  
I t l l l  
U ncapacitated 0-1 facility programming location  
I t  
# t  
I I t I I  
Integer  
knapsack  
\~  
Linear  
network  
flow  
Set packing 0-1  
Set coveri ng I  
knapsack I  
# I  
# l  
# I  
# \~  
l' t  
Transportation  
Matching Node packing Traveling  
salesman  
# I  
Assignment ...  
**Figure 1.1** 2. Measuring Algorithm Efficiency and Problem Complexity 117  
By the end of this chapter we will have shown that apart from the problems that are  
well-solved, all the other problems in Figure 1.1 lie on common cycles and are theoretically  
"equally difficult". However, in parts II and III we will see that there remain many reasons  
to distinguish between problem classes by using structure in developing algorithms.  
*Example* 1.1 *\(Set Packing is a special case a/Node Packing\). LetA* be an *m* x *n* matrix,  
all of whose coefficients are 0 or 1, and let 1 be a column vector of 1 'so We will construct a  
*p* x *n* 0-1 matrixA' withp \~ *n\(n* - 1\)/2 and *'L\}=l aij* = 2 for *i* = 1, ... *,p* such that  
Since a 0-1 matrix with exactly two l's in each row and no duplicate rows is the edge-node  
incidence matrix of a graph, it suffices to specify matrix *A'* as the edge-node incidence  
matrix of some graph. Let G *A* = *\(V, E\)* be the graph with *V* = \{l, 2, ... , *n\}* and  
*E* = *\{\(j, k\)\: a ij* = *a ik* = 1 for some i\}. G *A* is called the *intersection graph* of matrix *A.* The  
construction is illustrated in Figure 1.2.  
Now it is easy to see that *XO* E *B n* satisfies *Axo* \~ 1 if and only if the node set  
*VO* = \{j E *V\: xJ* = 1\} is such that\}, *k* E *VO* implies \(j, *k\)* \$. *E.* Hence there is one-to-one  
correspondence between feasible solutions to the set packing problem with matrix *A* and  
feasible solutions to the node packing problem on graph *GA'* Finally, we note that *VO* is a  
feasible solution to the node packing problem if and only if *A' XO* \~ 1, where *A'* is the edge-  
node incidence matrix of *GA.*  
Later in this chapter we will be more precise in what we mean by a special case. We want  
to have a definition that insofar as possible conveys the idea that if *X 2* is a special case of  
*Xl,* then an algorithm that can solve instances of Xl efficiently can also efficiently solve  
those instances of Xl that belong to *X 2•* For example, we could say that integer program-  
ming is a special case of linear programming by replacing the constraint set of an integer  
program by a linear inequality description of its convex hull. While this is true, it is  
misleading because the efficiency of the algorithm for linear programming may be a  
function of the linear inequality description of the convex hull, and in addition it may be  
extremely difficult to find these linear inequalities.  
2. MEASURING ALGORITHM EFFICIENCY AND PROBLEM COMPLEXITY  
It is common practice to relate computation time to problem size. Traditionally, the size  
of an instance of an optimization problem has been described by its number of variables  
and number of constraints. These two parameters, however, may not be adequate. There  
are algorithms whose number of steps depends explicitly on the magnitude of the  
numerical data. For example, there is an algorithm for the integer knapsack problem  
\~-----2  
# 3--------44  
*G A*  
Figure 1.2 118 1.5\. Computational Complexity  
whose number of computations is proportional to the number of variables times the right-  
hand side of the constraint. In the ellipsoid method for linear programming, the number of  
computations depends on the volume of the initial ellipsoid, which in turn depends on the  
magnitude of the numerical data. The size of numbers involved in elementary calcula-  
tions, such as additions and multiplications, may also be of concern. It is frequently  
reasonable to assume that these operations are done in constant or unit time. For example,  
if *a* and *b* are integers that are part of the data, then a reasonable assumption is that *a* is  
read in unit time, *b* is read in unit time and *a* + *b* is calculated in unit time. However, it  
may not be reasonable to assume that huge integers such as factorials can be added in unit  
time.  
We say that the *size of a problem instance* is the amount of information required to  
represent the instance. The data needed are generally obvious; for example, an instance of  
integer programming is specified by integers *m* and *n* and matrices *A, b,* and *c.* How  
should we represent the information? A model that is robust with respect to representing  
the essence of real computation is to use a two-symbol or *binary* \(0, 1\) alphabet for the  
representation of numerical and logical data. In this model, a positive integer *x,*  
*2n*  
\~ *x* \< *2n*  
*1*  
*+*  
*,* is represented by the vector *\(tSo,* tSI, ... , *tSn\),* where  
*n*  
*X* = 2 *tSi2i* and *tSi* E \{O, 1\} for *i* = 1, ... , *n.*  
i=O  
Note that *n* \~ *log2x* \< *n* + 1. An additional digit is necessary to represent the sign of *x,* and  
rational numbers are represented by pairs of integers. We always assume that the initial  
numerical data are integral or rational. Only rarely do we have to be concerned with  
irrational numbers in intermediate calculations. In such situations \(e.g., in the ellipsoid  
algorithm, which requires square roots\), we have to take care to specify the precision of the  
arithmetic calculations. However, for the most part, integer arithmetic suffices.  
Subsets of a set can be represented by *incidence* or *characteristic* vectors. Thus if  
*Q* = \{l, 2, ... , *n\},* the subset *Qj* is given by the vector *\(qI, q2,* ... , *qn\),* where *qi* = 1 if  
*i* E *Qj* and *q i* = 0 otherwise. This, of course, is a way of representing graphs, since an edge  
of a graph is just a subset of nodes of cardinality 2. Thus a graph G = *\(V, E\)* with *m* nodes  
and *n* edges can be represented by an *m* x *n* node-edge incidence matrix. Alternatively, it  
can be represented by the *m* x *m* symmetric *adjacency matrix A,* where *a ij* = 1 if nodes *i*  
*andj* are joined by an edge and *aij* = 0 otherwise. Another data structure for subsets and  
graphs is to represent a subset by a list of its elements.  
While choosing a good data structure can be very important in implementing an  
algorithm efficiently, it is fortunate that our primary classification scheme of algorithm  
efficiency is very insensitive to the choice of data representation. There are, however,  
some restrictions.  
The alphabet used to represent data must contain at least two symbols. In particular, for  
reasons to be explained later, a one-symbol representation of integers is not permitted. The  
second restriction deals with the amount of information that we agree to call data. We will  
explain this point by considering the symmetric traveling salesman problem on a complete  
graph G = *\(V, E\),* where *c e* for *e* E *E* is the cost of edge *e.* The natural representation of the  
data is a list of edges, named by their endpoints, and their costs. A representation of the  
data that is not permitted is a list of all *\(n* - 1\)!/2 tours and their costs. The number of tours  
grows exponentially with the size of the graph, and if an algorithm required this informa-  
tion, we would regard its generation to be part of the algorithm, not part of the data  
description. Similarly, the integer programming formulation of the symmetric traveling 2. Measuring Algorithm Efficiency and Problem Complexity 119  
salesman problem given in Chapter I.l, which requires an inequality for each *U* \~ *V* with  
2 \~ I *U* I \~ I *V* I - 2, is not permitted as a description of the data. Here the number of rows  
in the constraint coefficient matrix *A* grows exponentially with the size of the graph. If an  
algorithm required this formulation, we would regard its generation to be part of the  
algorithm.  
Having set up a model for describing a problem and the data of its instances, we now  
consider computation time. We want our measure of time to be independent of the  
characteristics of particular computers, so we basically count the number of elementary  
operations such as additions, multiplications, comparisons, and so on; that is, we assume  
that each elementary operation is done in unit time. This is a reasonable assumption as  
long as the size of the numbers does not grow too rapidly as the calculations progress. We  
will see later that one may need to be very careful in checking that this is the case.  
Consider an optimization problem *X* consisting of an infinite number of instances  
*\(db d2,* ••• \), where the data for the instance *d j* is given by a binary string of length  
*Ii* = *I\(dj \).* Let *A* be an algorithm that can solve every instance of *X* in finite time. We  
assume that the running time of *A* is specified by a function *gA\: X* -+ *Rl.* We would like to  
express running time as a function of *I.* Since it is not necessarily the case that if two  
instances have the same length, they have the same running time, we must use some  
statistic to aggregate the running times for all instances of the same length. Our approach is  
to use a *worst-case* analysis. In other words, the running time that we associate with all  
instances of size *k* is  
This highly conservative measure of running time, which only considers the worst-  
possible outcome for each size, has three advantages\:  
1. It gives an absolute guarantee on running time.  
2. It is independent of a probability distribution of the instances.  
3. It appears to be the easiest measure to analyze.  
However, it also has disadvantages. Foremost among these is its failure to give a true  
picture when a large percentage of instances of a given size can be solved rapidly and only  
a very small percentage require considerably more time. In these situations, measures such  
as expected running time may be preferable. But measures that require a probability  
distribution of the instances appear to be more difficult to analyze and require assump-  
tions about an underlying probability distribution.  
Rather than attempting to get a precise expression for the *functionfA\(k\),* it will suffice  
here to approximate it from above. Recall that we say *f\(k\)* is *O\(g\(k\)\)* whenever there exists  
a positive constant c and a positive integer *k'* such *thatf\(k\)* \~ *cg\(k\)* for all integers *k* \~ *k'.*  
With this convention, a polynomial l\:f=o *cjki* is *O\(kP\).* In other words, we ignore all of the  
terms of degree less than *p* and all of the constants. This means that only the asymptotic  
behavior of the function as *k* -+ 00 is being considered, since for "small" values of *k,*  
depending on the constants, the lower-degree terms may dominate.  
Algorithm A is said to be a *polynomial time algorithm* for problem X *iffA\(k\)* is *O\(kP \)* for  
some fixed *p.* Let *PJ\>* be *the class of problems that can be solved in polynomial time.*  
Problem X is in *PP* if and only if there is a polynomial time algorithm for solving *X.* A main  
theme of computational complexity is the inherent difference between problems known  
to be in *PJ\>* and others for which no polynomial time algorithm is known. 120 1.5\. Computational Complexity  
The functionJis said to be *exponential* if for some constants Cb C2 \> 0 and *db d2 \>* 1  
and a positive integer *k'* we have  
A typical example of exponential time calculation is the enumeration of the *2k* 0-1 *k-*  
dimensional vectors. The functionJ\(k\) = *2k* is not bounded by any polynomial in *k,* and it  
does not require very large values of *k* for *2k* to exceed polynomial functions of reasonably  
small degree. For example, with *k* = 60, an algorithm that required *2k* calculations, each of  
which took a microsecond, could not be completed in 300 centuries, whereas one that  
required *k S* calculations would be done in less than 15 minutes.  
Although most of the algorithms that we consider can be shown to either run in  
polynomial or exponential time, there are other possibilities. There are functions whose  
rate of growth is faster than polynomial but slower than exponential-for example,  
*J\(k\)* = *k1ogk•* There are also functions whose rate of growth is faster than exponentially-for  
example,J\(k\) = *kkk.*  
Exponential time can also occur when the number of computations is a function of the  
size of the numbers in the input. Let 8 be the largest integer in a given instance. Since the  
binary encoding of 8 only requires a string of length O\(log 8\), an algorithm that requires 8  
steps is at least exponential. This is one reason for our having stressed the encoding of  
numbers earlier in this section. We ruled out a one-symbol alphabet because it would  
permit 8 steps to be carried out in polynomial time.  
Also, if an algorithm computed very large numbers, such as *20*  
*,* that are not bounded by  
a polynomial function in 8, their encoding would require strings of length not polynomi-  
ally bounded in log 8. However, as long as the numbers remain polynomially bounded in  
8, the assumption of unit time calculations has no bearing on whether the algorithm runs  
in polynomial time. Besides being convenient, this assumption is made because comput-  
ers work with "words" in unit time, and quite large integers are represented by a single  
word. Thus when we say that an algorithm runs in *O\(kP \)* time, we generally have ignored a  
factor in log 8 that would be required if we had assumed that the time for an elementary  
calculation was proportional to the logarithm of the numbers involved. However, we will  
not ignore the possibility of exponential growth in the size of numbers.  
In this regard, we must consider the representation of rationals that are not integers. We  
assume that a rational *a Ibis* encoded by the pair of integers *a* and *b.* Thus *2-0* represented  
as \(1, 28\) is a very large number. A more subtle point is that 2 represented as *20*  
*+*  
*1/20* is a very  
large number. We avoid the latter problem by assuming that a rational *alb* is represented  
by two relatively prime integers *p* and *q* \(i.e., *alb* = *plq* and the greatest common divisor  
of *p* and *q* equals 1\). In fact, in Section 1.7.2 we will give a version of the euclidean  
algorithm, which, given *a* and *b,* finds *p* and *q* in polynomial time. So the assumption of  
representing rationals by two relatively prime integers is theoretically justified.  
While the distinction between polynomial time algorithms and the rest is important  
theoretically, it is not a satisfactory practical division. We will begin to see in the next  
section, and then in Part II, that some polynomial time algorithms are inefficient and that  
some algorithms known to be exponential in the worst case are very reliable algorithms for  
solving practical problems. Of course, polynomial time algorithms that run in, say, linear  
time are fast. The problems with the division occur with polynomial time algorithms in  
which the degree of the polynomial is not small and with exponential time algorithms that  
are fast in most cases. 3\. Some Problems Solvable in Polynomial Time 121  
We have chosen here to emphasize computation time, but the space or memory needed  
to solve a problem is also important. Observe that if *X* E *PP* there must be an algorithm for  
*X* whose space requirements are a polynomial function of the length of the input.  
However, the converse is false. In other words, there are exponential time algorithms  
whose space requirements are polynomially bounded.  
So far, we have ignored the question of whether integer programming problems can be  
solved finitely. Obviously they can be when the variables are bounded, since the enumera-  
tion of all points within the hypercube defined by the bounds is a finite process. In Section  
4, we will show that upper bounds on the variables can be found as a function of  
*\(A, b,* c, *m, n\)* for pure-integer programming problems with the property that if  
*max\{cx\: Ax* \~ *b, x* E z\~\} has an optimal solution, then it has an optimal solution within  
the specified bounds. This result, along with schemes for resolving infeasibility and  
unboundedness, shows that every pure-integer programming problem can be solved  
finitely. It also can be proved that mixed-integer programs can be solved finitely. Thus it is  
interesting to observe that some nonlinear problems with integer variables are impossible  
to solve. For example, it is impossible to describe an algorithm that decides whether  
*\{x* E *zm\:f\(x\)* = O\} is nonempty or not, wherefcan be any polynomial function.  
3. SOME PROBLEMS SOLVABLE IN POLYNOMIAL TIME  
In this section, we briefly discuss the complexity of some of the problems in Figure 1.1 that  
are known to be in *PP.*  
To point out some distinctions between the complexity of problems in *PP,* we consider  
five problems.  
*1. The minimum-weight path problem with nonnegative data* \(see Section 1.3.2\). An  
instance is specified by any *m* node graph and integral nonnegative edge weights.  
Dijkstra's algorithm requires *O\(m2\)* elementary calculations. Note that the  
number of calculations is independent of the numerical values of the edge weights.  
Moreover, each of the numerical operations is an addition or comparison, so the  
numbers involved only grow slowly. The performance of this algorithm is very  
satisfactory, since a complete graph on *m* nodes contains *m\(m* - 1\)/2 edges, and thus  
*O\(m2\)* integers are needed to describe some of the *m* node instances.  
*2. The minimum-weight path problem* \(see Section 1.3.2\). An instance is specified by  
any *m* node graph and integral edge weights.  
The Bellman-Ford algorithm either finds a minimum-weight path or detects a  
negative weight cycle with *O\(m3\)* elementary calculations. It is not known whether  
more theoretically efficient algorithms \[e.g., an algorithm with running time *O\(m2\)*  
or *O\(m2* log *m\)\]* are possible. In general, establishing lower bounds on the complex-  
ity of a problem is extremely difficult.  
*3. Solving linear equations.* Given an *n* x *n* system of equations *Ax* = *b,* where *A* is  
nonsingular, *x* = *A-I b* can be found by Gaussian elimination. The basic elimination  
method requires *n* pivots, each of which requires *O\(n2\)* calculations.  
The size of the numbers that occur is bounded from above by the largest  
magnitude of the determinant of any square submatrix of *\(A, b\).* Now since det *A*  
involves *n!* \< *nn* terms, the largest number is less than *\(nfJY,* where *fJA* = maxi,\) I *aij* I,  
*fJb* = maXi I *bi* I, and \(\) = *max«\(\)A,fJb\).* Hence Gaussian elimination is polynomial in *n.*  
By considering *\(A, I\),* where *A* is *m* x *n,* Gaussian elimination also yields polyno-  
mial time algorithms for calculating *rank\(A\)* and *det\(A\)* and for solving *m* x *n* linear  
systems. 122 1.5\. Computational Complexity  
*4\. The transportation problem* \(see Section 1.3.5\). An instance is specified by an  
ml x *m2 \(m!* + *m2* = *m\)* integral matrix C, where *cij* is the unit shipping cost from  
supply point *i* to demand pointj, an ml-vector of integral supplies *\(ab ... , am\),*  
and an mrvector of integral demands *\(b l ,* ••• *,bmJ,* where 1\:7,\:\~ *ai* = 1\:\}\:j *bj* = *a.*  
The primal-dual algorithm \(without scaling\) requires no more than *a* steps and  
*O\(m2\)* computations in each step. This is not a polynomial time algorithm, since the  
number of steps is exponential in log *a.* However, when scaling is included, the  
number of steps is reduced to *m* rlog *01,* where 8 = max\(maxiai, maxjbj \). Thus we  
obtain a polynomial time algorithm whose running time is O\(m3 10g 8\).  
Very recently, polynomial time algorithms with running time bounds that are  
independent of the numerical data have been found. The practical efficiency of  
these algorithms is not yet known. Furthermore, the practical significance of scaling  
is unresolved. Presently, it is generally believed that the most practical algorithm is a  
primal simplex algorithm, which is known to be exponential. So here we have an  
indication that the polynomial/exponential dichotomy is a dubious way to measure  
the practical efficiency of algorithms.  
*5\. The linear programming problem* \(see Chapter 1.2\). An instance is given by  
max\{cx\: *Ax\:\:\:\:\:; b, x* E R\~\}, where *\(A, b\)* is an integral *m* x *\(n+l\)* matrix and c is an  
integral n-vector.  
The simplex method, which is used in all commercial linear programming codes,  
is *not* a polynomial time algorithm. There are classes of linear programs for which  
the simplex method takes exponential time. This fact is the outstanding evidence for  
the argument against worst-case analysis of algorithms, since the simplex method  
has been enormously successful in the solution of real-world instances. Recently, the  
efficiency of the simplex method has been supported even further by probabilistic  
analysis, which shows that under rather general assumptions on the underlying  
distribution of instances, the *expected* running time of the simplex method is  
bounded by a polynomial in *m* and *n.*  
In Chapter 1.6 we will give two polynomial time algorithms for linear programming.  
The older of these two methods is the ellipsoid algorithm. It certainly seems to be inferior  
to the simplex algorithm as a computational tool. However, it provides an important proof  
technique for showing that some combinatorial optimization problems are in *1P.* The  
more recent method, Karmarkar's algorithm, appears to be a promising technique for  
practical computation. But as of this writing, not enough empirical evidence is available.  
Prior to the development and analysis of the ellipsoid algorithm, many researchers  
believed that linear programming was in *1P,* but no proof was known. The reason for this  
conjecture assumes a central role in the development and analysis of algorithms for integer  
optimization problems. Here we give an informal explanation of the reason. In Section 5,  
we reexamine it in the language of computational complexity.  
Suppose we owned a supercomputer that ran as fast as an exponential number of  
standard computers running in parallel. We could then solve a bounded instance of linear  
programming by using Gaussian elimination to enumerate all basic solutions. Each basic  
solution could be checked for nonnegativity, and from the feasible ones we could pick one  
that maximizes the objective function.  
Having determined an optimal solution in this way, how could we, in polynomial time,  
convince someone else, who did not have access to the supercomputer, that we really had  
found an optimal solution? One answer, of course, is to apply a polynomial-time ellipsoid  
algorithm. But there is a much simpler answer that was known long before the ellipsoid  
algorithm. 3\. Some Problems Solvable in Polynomial Time 123  
Suppose we ask our computer to produce an optimal dual solution *UO* as well as an  
optimal primal solution *xO.* Then given *\(XO, un\),* with *O\(mn\)* calculations, we could  
convince anyone that *XO* and *UO* were optimal. We would show the feasibility of *XO* and *UO*  
\(i.e., *\{Axo* \~ *b, XO* E R\~\} and *\{unA* \~ c, *UO* E *R'\:\}\)* and then show that *cxo* = *uOb.* Thus the  
duality theorem of linear programming gives the proof. One subtle point remains. We  
must show that the coefficients of *XO* and *UO* are polynomial in the length of the encoding of  
the data. Fortunately this is true for basic solutions and extreme rays. The argument is  
essentially a repeat of that used above to observe that the intermediate numbers in  
Gaussian elimination are polynomial in the input length.  
The following notation will be used throughout the text.  
*B A* = \~\~x I *a ij* I, *Bb* = m\~x I *b* ii,  
*I,\} I*  
*B* = *BA,b* = *max\(BA,Bb\).*  
Proposition 3.1. *Let xO, rO be an extreme point and extreme ray ofP* = *\{x* E R\~\: *Ax* \~ *b\},*  
*where \(A, b\) is an integral m* x *\(n* + 1\) *matrix. Then for j* = 1, ... , *n\:*  
i. *xJ* = *pj/q, where Pf,q are integers such that* 0 \~ *Pj* \< *nBb\(nBAt-1 and* 1 \~ *q* \< *\(nBAt.*  
ii. *rJ* = *pj/q, where Pj and q are integers such that* 0 \~ *Pj* \< *\(\(n* - *I\)BA\)n-l and*  
1 \~ *q* \< *\(\(n* - *I\)BA\)n-l.*  
*Proof* i. By the characterization of extreme points of *P, XO* is a solution to *A IX* = *b I,*  
where *A I* is *n* x nand nonsingular and each row of *A I* is either of the form *aix* = *bi* or  
*Xj* = O. Then, byCramer'srule,xJ = *pj/q, whereq* \~ 1 is the magnitude of the determinant  
of *A I* and *Pi* is the magnitude of the determinant of the matrix obtained by replacing the  
*jth* column of *A I* by *b'.* Each of these determinants contains *n!* \< *nn* terms. Hence  
1 \~ *q* \< *\(nBAt* and 0 \~ *Pi* \< *nBb\(nBAt-l*  
*•*  
ii. Similarly *rO* is determined by *n* - 1 equations, either of the form *aix* = 0 or  
*Xj=O. •*  
The bound of Proposition 3.1 states that the number of binary digits needed to represent  
*XO* is less than *2n 10g\(nBt* = *2n210g\(nB\),* which is polynomial in *n* and log *B.* Intuitively,  
Proposition 3.1 states that if a polyhedron has extreme points with both large and small  
integral coordinates, then it has very sharp angles \(see Figure 3.1\). But in order to obtain  
very sharp angles, the defining hyperplanes must have some very large coefficients.  
*\(0,* 1\)I\~\~\~\~\~\~\~\~\~\~\~\~\~Xl\~+\~2\~\~\~2\~2\~a\~\~\~\~\~ Xl +2\~2 0  
*\(a,* 1/ 2 \)  
*\(0,0\) xl*  
Figure 3.1 124 1.5\. Computational Complexity  
A theoretical consequence of this bound is that if *P* = *\{x* E *R1\: Ax* \~ *b\}* is an  
unbounded polyhedron and *max\{cx\: x* E *P\}* is finite, it suffices to solve *max\{cx\: x* E *P'\},*  
where *P'* = *\{x* E *R1\: Ax* \~ *b, x* \~ *\(nO\)n\)* is a polytope whose length of description  
*I'* = *I* + *O\(n2\(log nO»* is not significantly longer than the description length *I ofP.* This will  
be used in the ellipsoid and projection algorithms in the next chapter.  
Information that can be used to check optimality in polynomial time is called a  
*certificate of optimality* or a *good characterization.* A binary string is said to be *short* if its  
length is a polynomial function of the length of the input.  
For linear programming, a certificate consists of basic optimal primal and dual  
solutions. To use it we simply verify primal and dual feasibility and the equality of the  
objective function values. Of course, if a problem is in *'!P* it has a good characterization.  
Although it is not known whether a good characterization implies that a problem is in *'!P,*  
for nearly all optimization problems for which a good characterization is known, a  
polynomial-time algorithm is also known. Until 1979, linear programming was one of the  
rare exceptions. Some other exceptions in combinatorial optimization were also resolved  
through the use of the ellipsoid algorithm.  
We now consider a problem that may properly be designated an integer optimization  
problem and is a generalization of the assignment problem.  
*6\. The weighted matching problem.* An instance is specified by a graph G = *\(V, E\)* with  
*m* nodes, *n* edges, and integral weights *Ce* for *e* E *E.*  
We have previously given the integer programming formulation  
max *L CeXe*  
*eEE*  
\(3.1\)  
I *Xe* \~ I for *i* E *V*  
eEJ\(i\)  
*xEZ1,*  
where J\(i\) is the set of edges incident to node *i.* Here the linear program obtained by  
replacingx E Z1 by *x* E *RZ* does not necessarily have an integral solution. However,  
there is an algorithm for weighted matching that only requires *O\(m3\)* calculations.  
All of the known polynomial-time algorithms for weighted matching implicitly use a  
linear inequality description of the convex hull of matchings. We will show later that  
*x* E *R1* is a matching ifand only ifit is an extreme point of the polytope given by *x* E *R1,*  
\(3.1\), and the *odd set constraints*  
'" IUI-l  
*L Xe* \~ --'----  
*eEE\(U\) 2*  
for all *U* \~ *V* such that I *U* I = *2k* + 1, *k* = 1, 2, . . . ,  
where *E\( U\)* is the set of edges with both ends in *U.* An odd set constraint states the obvious  
fact that when I *U* I is odd, no matching can have more than \(I *U* I - 1\)/2 edges internal to  
*U.*  
One should note that this formulation, together with the fact that linear programming  
is in pjJ, does not immediately yield a polynomial-time algorithm for weighted matching.  
The reason is that the linear programming formulation has a number of constraints that  
are exponential in the size of the data needed to describe the weighted matching problem.  
Nevertheless, this formulation readily produces a good characterization. Again, duality  
provides the certificate of optimality. Although there is a very large number of dual  
variables, a basic dual solution contains no more than *n* positive variables. Moreover, it  
can be shown that in a basic dual solution, the values of the dual variables are not "too 4\. Remarks on 0-1 and Pure-Integer Programming 125  
large". A certificate then consists of an optimal matching, an optimal dual solution, and  
those odd sets with positive dual variables. Note that it is not necessary to check the odd set  
constraints to verify the feasibility of a matching.  
4. REMARKS ON 0-1 AND PURE-INTEGER PROGRAMMING  
In the previous section we mentioned that linear programming and matching, and hence  
all special cases of them in Figure 1.1, are in *PP.* The status of all of the other problems  
shown in Figure 1.1 is much less settled. It is not known whether any are in *PP,* but there is a  
theory that leads us to believe that none are in *PP.* This theory is the subject of Sections 5  
and 6. In this section we will make a few remarks on worst-case running times for some of  
the problems and, in particular, give bounds on the magnitude of the values of variables in  
an optimal solution to the pure-integer programming problem.  
*1. The 0-1 integer programming problem.* An instance of the general problem  
*max\{cx\: Ax* \~ *b, x* E *Bn\}* is specified by an integral *m* x *\(n* + 1\) matrix *\(A, b\)* and  
an integral n-vector c.  
It can be solved by a brute-force enumerative algorithm in *O\(2nmn\)* time. Even  
for such special cases as the node packing problem, no significantly better worst-case  
result is known. However, there are many special cases \(e.g., matching, node packing  
on appropriately restricted classes of graphs, and some matroid optimization  
problems\) that are in @J. These problems are the subject of Part III of this book.  
*2\. The integer knapsack problem.* An instance of the general problem  
*max\{cx\: ax* \~ *b, x* E z\~\} is specified by integral n-vectors c and *a,* and an integer *b.*  
There is an *O\(nb\)* algorithm, but it is exponential unless we restrict *b* to be a  
polynomial function of *n.* Although in some applications the magnitude of *b* can be  
restricted, large values cannot be dismissed. One reason is that rather general integer  
programs can be easily transformed into an equality-constrained version of the  
knapsack problem with constraints *ax* = *b* and upper bounds *Xj* \~ *dj* for *j* = 1, ... ,  
*n.* What makes this transformation uninteresting is that the magnitudes of the  
resulting constraint coefficients are generally exponential in the length of the  
encoding of the data.  
*3\. The pure-integer programming problem.* An instance of the general problem  
*max\{cx\: Ax* \~ *b, x* E Z\~\} is specified by an integral *m* x *\(n* + 1\) matrix *\(A, b\)* and  
an integral n-vector c.  
Let *P* = *\{x* E *R1\: Ax* \~ *b\}.* If *P* is bounded, by Proposition 3.1, we know that  
*Xj* \~ *\(nB\)n* for *j* = 1, ... *,n.* Hence it is possible to solve the problem  
*max\{cx\: x* E *P* n z\~\} by enumerating the finite number of points in z\~ satisfying  
xj\~\(nBtforj= 1, ... *,no*  
We now show that even if *P* is unbounded, the integer programming problem can  
be solved by enumeration. By Theorem 6.3 of Section 1.4.6 we know that if the  
problem has a finite optimum value, there is an optimal solution at an extreme point  
of conv\(S\), where S = *P* n *zn.* We will obtain a bound on the magnitude of any such  
extreme point.  
Theorem 4.1. *Let P* = *\{x* E R\~\: *Ax* \~ *b\} and* S = *P* n *zn. If XO is an extreme point of*  
conv\(S\), *then*  
*xJ* \~ *«m* + *n\)nBY for j* = 1, ... , *n.* 126 1.5\. Computational Complexity  
*Proof* In the proofs of Theorem 6.1 and 6.2 of Section I.4.6 we have shown that  
conv\(S\) = *\{x* E R\~\: *x* = I *a,q'* + I *Jijrj*  
*,* I *a,* = 1, *a* E *RJL* I, *Ji* E *RjJ* I\},  
*'EL JEJ tEL*  
where *qt, rj* E Z\~ for *I* ELand\} E *J.* Any extreme point of conv\(S\) must be one of the  
points *\{q'\}IEL,* that is, *Xo* E *Q,* where  
where *\{xkhEK* are the extreme points of *P* and *\{rj\}jEJ* are the extreme rays.  
Since *IJ* I \~ \(\~\~\~\), *Ix71* \~ *\(ne\)n,* and 1 *r11* \~ *\(ne\)n,* it follows that  
*Ix?* I \~ *\(net* \(1 + IJ I\) \< *«m* + *n\)ne\)n.* •  
Note that *«m* + *n\)net* \~ *\(2fl2et\\* where *fl* = max\(m, *n\).* We will use *WA,b* = *\(2ffe\)fi* as  
notation for this value from now on.  
Theorem 4.1 combined with Theorem 6.3 of Section I.4.6 implies that we can add the  
constraints *IXj* I \~ *WA,b* to any integer program, and because no extreme points are  
removed we can test for feasibility \(unboundedness\) and optimality by enumerating the  
integer points in S n *\{x* E Z\~\: *x* \~ *WA,b\}'* We can now show that any instance of a pure-  
integer program can be transformed in polynomial time to an instance of a 0-1 integer  
program.  
For\} = 1, ... , *n* let *Xj* = Lf=o *2kXjk,* where *\(XjO,* ... *,Xjd\)* E *Bd+l* and *d* = ffllog \(2fl28\)1.  
With this substitution, we obtain the 0-1 integer program *max\{c'x'\:*  
*A 'x'* \~ *b, x'* E *Bn\(d+l\)\},* where c' is 1 x *ned* + 1\) and *A'* is *m* x *ned* + 1\). Note that the  
largest coefficient of *A'* has magnitude less than *2de* = *\(\)\(2fi2\(\)fi* and that the largest  
coefficient of c' has magnitude less than *\(2fl28\)fi* x \(maXj=l, ... , *nCj\).* Thus the length of the  
data needed to describe the transformed 0-1 integer program is a polynomially bounded  
function of the length of the data needed to describe the original integer program. Hence  
we have the following proposition.  
Proposition 4.2. *An instance of a pure-integer programming problem can be transformed*  
*in polynomial time to an instance of a 0-1 integer programming problem.*  
We have observed that Theorem 4.1 gives a finite algorithm-namely, enumeration-  
for integer programming. Now consider the class of integer programs with *n* fixed. For 0-1  
integer programming, enumeration is polynomial. However, for pure-integer program-  
ming, enumeration is not polynomial, since the upper bound *WA,b* depends polynomially  
on *e.* Furthermore, the transformation of pure-integer programming to 0-1 integer  
programming given above yields *d* + 1 variables and *2d* is polynomial in *\(J,* so enumeration  
on the transformed problem is not polynomial for *n* fixed. In fact, it is a theorem that  
integer programming with a fixed number of variables is in P;, but the proof requires much  
deeper results than Proposition 4.2 \(see Section II.6.S\).  
Analogous to the results we have given on the size of numbers in feasible and optimal  
solutions to integer programs, there is a result on the size of numbers that can arise in a  
description of the convex hull of feasible solutions by linear inequalities. The following  
theorem can be obtained from Theorem 4.1 and polarity. 5\. Nondeterministic Polynomial-Time Algorithms and.N'1\} Problems 127  
Theorem 4.3. *Suppose* S = *\{x* E 2'\:-\: *Ax* \~ *b\}, where \(A, b\) is an integral \(m* + 1\) x *n*  
*matrix. If \(n, no\) defines a facet of* conv\(S\), *then the length of the description of the*  
*coefficients of\(n, no\) is bounded by a polynomialfunction ofm, n, and* log G.  
5. NONDETERMINISTIC POLYNOMIAL-TIME ALGORITHMS AND\}\(\~  
PROBLEMS  
The theoretical model that we study in both this section and the next one addresses the  
question of whether integer programming and many special cases are solvable in polyno-  
mial time. The model does not provide a definite answer, but one of the main conclusions  
is that it is just as unlikely that there are polynomial-time algorithms for most special cases  
of integer programming \(e.g., integer knapsack, node packing\) as there are for the general  
integer programming problem. We will prove that if, for example, integer knapsack or  
node packing is solvable in polynomial time, then general integer programming is solvable  
in polynomial time.  
Although we can use the model to draw conclusions about optimization problems, it  
has been developed for so-called *decision, recognition,* or *feasibility problems.* We will use  
the term *feasibility problem* because of the close connection with feasibility testing in an  
optimization problem.  
A *feasibility problem X* is a pair *\(D, F\)* with *F* \~ *D,* where the elements of *D* are finite  
binary strings. *D* is called the *set of instances* of *X,* and *F* is called the *set of feasible*  
*instances.* Given an instance *d* E *D,* we want to determine whether *dE F.* Given *d* E *D,*  
the answer is either yes or no.  
In the remainder of this chapter we will follow the notation commonly used in  
complexity theory and we will define \~ to be the class of feasibility problems that are  
solvable in polynomial time.  
Associated with an optimization problem we define a feasibility problem in which an  
instance corresponds to a description of a constraint set. *F* is the set of instances for which  
the constraint set is nonempty.  
*Example* 5.1 *\(0-1 integer programming feasibility\). D* is the set of all integral matrices  
*\(A, b\),* where *b* contains one column and the same number of rows as *A.* An instance is  
specified by integers *m* and *n,* the dimensions of *A,* and numerical values for the  
coefficients of *A* and *b.*  
This is the feasibility problem for S = *\{x* E *B n*  
*\: Ax* \~ *b\}.* Hence *F* = *\{\(A, b\)\:*  
*\{x* E *En\: Ax* \~ *b\} =1=* 0\}. Here a yes answer is commonly established by exhibiting a feasible  
*x.*  
A second feasibility problem concerns a lower bound on the objective function. Here  
we augment each instance by an objective function c and an integer z. The *lower-bound*  
*feasibility problem* is the feasibility problem with the additional constraint *ex* \~ z.  
*Example* 5.2 *\(0-1 integer programming lower-bound feasibility\). D* = *\{\(A, b,* c, *z\)\}*  
is the set of all integral matrices *A, b,* c and integers z, where *b* \(respectively, c\)  
contains one column \(row\) and the same number of rows \(columns\) as *A.*  
*F* = *\{\(A, b,* c, *z\)\: \{x* E *En\: Ax* \~ *b, cx* \~ z\} *=1=* 0\}.  
Note that if *b* E *Z\:\:,* the feasibility problem for 0-1 integer programming is trivial, but  
the lower-bound feasibility problem is not. This is frequently the situation as, for example,  
in node packing. 128 1.5\. Computational Complexity  
The lower-bound feasibility problem is closely connected to the optimization problem.  
*If\(A, b,* c, *ZO\)* E *F* and *\(A, b,* c, *ZO* + 1\) \$\:. *F,* then max\{cx\: *Ax* \~ *b, x* E *Bn\}* = *zoo* Thus if  
it is known that *z L* \~ *ZO* \~ *Z u,* we can find *ZO* by solving, at most, *z u* - *Z L* + 1 lower-bound  
feasibility problems. Note that *Zu* - *ZL* is not polynomial in the input length.  
There is, however, a more efficient method for finding *zO,* called *binary search.* Suppose  
we are given a function *h\:* ZI \~ *Bl* of the form  
*hex\)* = \{O for *x* \~ *Xo*  
1 for *x* \> *Xo,*  
where *Xo* is unknown. We are also given integers *XL* and *Xu* with *h\(xL\)* = 0 and *h\(xu\)* = 1.  
The problem is to find *xo.* By putting *h\(z\)* = 0 if *\(A, b,* c, *z\)* E *F* and *h\(z\)* = 1 otherwise,  
we see that the problem of finding *ZO* is of this form.  
The following binary search algorithm finds *Xo* with, at most, flog\(x *u* - *x L\)l* evaluations  
of the function *h.*  
*Step* 1\: If *Xu* - *XL* \~ 1, stop. *Xo* = *XL.* Otherwise go to Step 2.  
*Step* 2\: Let *x* = *l\(XL* + *xu\)/2J.* If *hex\)* = 0, set *XL* = *X;* otherwise set *Xu* = *x.* Go to Step 1.  
Each function evaluation halves the length of the interval that contains *Xo.* Hence the  
number of evaluations is bounded by *flog\(xu* - *xL\)l.* An example is shown in Figure 5.1.  
Thus with binary search, we can find *ZO* by solving *flog\(zu* - *ZL* + 1\)1 lower-bound  
feasibility problems. Since *flog\(zu* - *ZL* + 1\)1 is polynomial in the length of the input of the  
0-1 lower-bound feasibility problem, we obtain the following proposition.  
Proposition 5.1. *If the 0-1 integer programming lower-bound feasibility problem can be*  
*solved in polynomial time, the 0-1 integer programming problem can be solved in*  
*polynomial time.*  
This proposition has an obvious generalization to other optimization problems. In  
particular, it applies to the integer programming problem, where Theorem 4.1 is used to  
give bounds *Zu* and *ZL* such that *flog\(zu* - *ZL* + 1\)1 is polynomial in the length of the input.  
Certificates of Feasibility, the Class .Hpj\), and Nondeterministic Algorithms  
Analogous to certificates of optimality, information that can be used to check feasibility in  
polynomial time is called a *certificate offeasibility.* Given *X* = *\(D, F\),* for each instance  
*d* E *F* we let *Qd* denote such a certificate. We know that if *Qd* exists it must be short. Here  
we are interested in the class of feasibility problems having a certificate of feasibility.  
One might imagine an algorithm that makes a large number of guesses in the hope of  
eventually guessing *Qd.* This leads to the concept of a nondeterministic algorithm for a  
feasibility problem *X* = *\(D, F\).* The reader should take note that such algorithms cannot  
be realized in practical computation.  
5 4 3 *xu=34*  
**• • • • •**  
9 10 12 17  
*XL=O* 2  
**• •**  
8  
Figure 5.1 5\. Nondeterministic Polynomial-Time Algorithms and *KPJ* Problems 129  
A nondeterministic algorithm consists of two stages. The input to the algorithm is a  
*d ED.* The first stage is a guessing stage. Here we guess a binary string *Q* which is then  
passed on to the second stage. The second stage, called the *checking stage,* is an algorithm  
that works with the pair *\(d, Q\)* and may provide the output that *d* E *F.* For example, the  
checking stage may verify that *xES* and thus output that *dE F.* Two properties are  
required\:  
1. If *d* E *F,* there is a certificate *Qd* such that when the pair *\(d, Qd\)* is given to the  
checking stage, the algorithm gives the answer that *d* E *F.*  
2. If *d* \$. *F,* there is no output. Hence whenever there is output, *d* E *F.*  
We measure the work done by a nondeterministic algorithm only in the checking stage  
and only when the checking stage is given a *d* E *F* and a certificate of feasibility. We say  
that the *nondeterministic algorithm is polynomial* if, for each *d* E *F,* its running time in  
the checking stage is a polynomial function of the length of the encoding of *d* for some *Qd*  
for which it replies that *d* E *F.* This means that when *d* E *F,* there is a *short \(polynomial-*  
*time\) proof of feasibility.*  
We define.Kg\> to be *the class of feasibility problems such that for each instance with dE*  
*F, the answer d* E *F is obtained in polynomial time by some nondeterministic algorithm.*  
Nothing is said when *d* \$. *F.*  
We will also encounter feasibility problems that are not in *.Kg\>.* Many of these are in a  
related set called *Cfio.Ng\>,* which will be defined and discussed later in this section.  
*Example* 5.3 *\(Nondeterministic polynomial-time algorithm for 0-1 integer feasibility\).*  
Guessing stage\: Guess an *x* E *Bn.*  
Checking stage\: If *Ax* \~ *b,* output *\(A, b\)* E *F;* otherwise return.  
The algorithm for 0-1 integer programming lower-bound feasibility is similar. Now  
consider general integer programming feasibility. The same algorithm works with the  
guesses being *x* E Z\~ because Theorem 4.1 stipulates that if *\{x* E Z\~\: *Ax* \~ *b\} =1=* 0, then  
there is a feasible *x* such that the logarithm of its largest coefficient is bounded by a  
polynomial in the length of the encoding of *\(A, b\).* This is one of the few nontrivial *.Kg\>*  
results that we need.  
Proposition 5.2. *General integer programmingjeasibility is in .Kg\>.*  
The Hamiltonian cycle problem is to determine whether a graph G = *\(V, E\)* contains a  
Hamiltonian cycle. A *Hamiltonian cycle* is a cycle that contains all of the nodes of G.  
Proposition 5.3. *Hamiltonian cycle is in .Kg\>.*  
*Proof* We give a nondeterministic polynomial-time algorithm for Hamiltonian cycle.  
Input\:  
Guessing stage\:  
Checking stage\:  
A graph G = *\(V, E\).*  
Guess an *E'* \~ *E.*  
*Step a.* If the degree of each node ofG' = *\(V, E'\)* is two, go to Step  
b; otherwise return.  
*Step b.* If G' = *\(V, E'\)* is connected, output G E *F;* otherwise  
return. • 130 1.5\. Computational Complexity  
We have simply used the facts that \(a\) a graph G' is a Hamiltonian cycle if each node is  
of degree 2 and the graph is connected and \(b\) each of these properties is easily checked in  
polynomial time. The upper-bound feasibility problem associated with the minimum-cost  
traveling salesman problem is also in *.Kr!J.* This is shown by slightly generalizing the  
algorithm given in Proposition 5.3.  
A nondeterministic algorithm does not completely solve the feasibility problem, since it  
ignores *d* \$. *F.* However, by being just a little bit intelligent about our guesses, we can  
simulate a nondeterministic polynomial-time algorithm by a deterministic exponential-  
time algorithm. For each *d* E *F* there is a structure *Qd* whose length *I\( Qd\)* is polynomial in  
the length of *d,* say *I\(Qd\)* = *c\(l\(d\)\)p.* Therefore, for a given *dE D* we can limit our guesses  
to binary strings of length equal to or less than *L* = *c\(l\(d\)\)P.* Hence we need to consider, at  
most, *2L +!* structures. But there is a polynomial *functionf\(l\(d\)\)* that gives an upper bound  
on the running time of the checking stage for *d* E *F* when *Qd* is guessed. Hence for each of  
the *2L +!* structures, we run the checking stage for *f\(l\(d\)\)* time and then go on to the next  
structure if the checking stage has not verified *d* E *F.* Thus if a feasibility problem is in  
*.K\[!fJ,* it can be completely solved in exponential time.  
The Class *CfioJ\(r!J*  
§.ach feasibility problem *X* = *\(D, F\)* has a related feasibility problem *X* = *\(D, F\),* where  
*F* = *D* \\ *F,* called the *complement* of *X.* In the complement of 0-1 integer programming  
feasibility we have *F* = *\{\(A, b\)\: \{x* E *Bn\: Ax* \~ *b\}* = 0\}. It is not known whether the  
complement of 0-1 integer programming feasibility is in *.K\[!fJ.* In fact, it is not known  
whether the complements of any of the feasibility problems mentioned so far in this  
section are in *.Kr!J.*  
For the complement of the 0-1 integer programming lower-bound feasibility problem  
*F* = *\{\(A, b,* c, *z\)\: \{x* E *Bn\: Ax* \~ *b, cx* \~ *z\}* = 0\}, which is equivalent to showing that  
*cx* \< *z* is a valid inequality for *\{x* E *B n*  
*\: Ax* \~ *b\}.* Thus if the lower-bound feasibility  
problem and its complement are in *.Kr!J* we would have a good characterization for the  
optimization problem.  
To establish terminology for complements of *.Kr!J* problems, let *CfioJ\(r!J* = *\{X\: X* is a  
feasibility problem, *X* E *.K\[!fJ\}.*  
Proposition 5.4. *ffX is a feasibility problem and X* E *r!J, then X* E *J\{r!J* n *CfioJ\(r!J.*  
*Proof* Every polynomial-time algorithm is also a nondeterministic polynomial-time  
algorithm. We simply ignore the guessing stage and apply the polynomial-time algorithm  
in the checking stage. Hence *X* E *r!J* =\> *X* E *.K\[fJ\>.* But if *X* E *r!J* so isX E *\[fJ\>,* since if *d* \$. *F,* it  
follows that our polynomial-time algorithm, which needs no guesses, will also tell us this in  
polynomial time. Hence *X* E *\[fJ\>* implies *X* E.K\[fJ\> or, equivalently, *X* E *CfioJ\(r!J. •*  
Figure 5.2 6\. The Most Difficult *JVrP* Problems\: The Class *JVrP\<f!,* 131  
The linear programming feasibility problem for sets of the form *\{x* E R\~\: *Ax* \~ *b\}* is in  
\~ by virtue of the ellipsoid method. Hence we can use Proposition 5.4 to establish its  
membership in j\(\~ n *\<\{6oJfP\}.* But this fact was known long before the ellipsoid method,  
since membership in \<\{6oJf\~ is a consequence of linear programming duality. We leave it to  
the reader to show membership in j\(\~. Here we show membership in \<\{6oJf\~. The reader  
should refer back to the good characterization of linear programming given in Section 3  
which essentially does the same thing.  
*Example* 5.4 *\(Nondeterministic algorithmfor linear programming infeasibility\).*  
Input\: An integral *m* x *n* matrix *\(A, b\).*  
Guessing stage\: Guess a *u ERr;.*  
Checking stage\: If *uA* \~ ° and *ub* \< 0, output *\(A, b\)* E *F;* otherwise return.  
We have used the Farkas lemma-that is, if there exists a *u ERr;* such that *uA* \~ ° and  
*ub* \< 0, then *\{x* E R\~\: *Ax* \~ *b\}* = 0-and Proposition 3.1, which guarantees the existence  
of suitably small rational *u* so that the checking can be done in polynomial time.  
The sets *P\},* j\(\~, and \<\{6oJf\~ for feasibility problems are shown in Figure 5.2.  
The answers to the following questions are unknown.  
1. Does \~ = \<\{6oJf\~ n j\(\~?  
2\. Does *\<\{6oJf\(f\}* = *\}\(\(f\}?*  
3. Does *\(f\}* = *j\(P\}?*  
An affirmative answer to question 3 implies affirmative answers to question 1 and 2,  
since, by Proposition 5.4, we have *\(f\}* \~ *\}\(P\}* n *\<\{6oJfP\}.* Similarly, affirmative answers to  
questions 1 and 2 imply an affirmative answer to question 3.  
In the next section, we will study the class\}\(\~ further; at the end of that section, we will  
make some remarks about the impact on integer programming of answers to the above  
questions.  
6. THE MOST DIFFICULT *j\(P\}* PROBLEMS\: THE CLASS *\}\(P\}\<\{6*  
The main result of this section is that\}\(\~ contains hardest problems. By this we mean that  
there is a subset of j\(\~, called \}\(\~\<\{6, such that if there exists *X* E *\}\(P\}\<\{6* n \~, then every  
problem in j\(\~ is in \~, that is, \~ = \}\(\~. Problems in \}\(\~cg are called \}\(\~-complete.  
Moreover, we will show that amongst these hardest problems are feasibility problems  
associated with integer optimization and many special cases.  
The technique used here is that of polynomially transforming one problem into  
another. Suppose *Xi* = *\(Di' F i\), i* = 1, 2, are two' feasibility problems and there exists a  
function *g\: DJ* \~ Dz such that for every *dE DJ* we have *g\(d\)* E Fz if and only if *dE Fl.* If  
the function *g* is computable in time that is polynomial in the length of the encoding of *d,*  
then *Xl* is said to be *polynomially transformable* to *X 2•* The consequence of this definition  
is clear. 132 1.5\. Computational Complexity  
Proposition 6.1. *If Xl is polynomially transformable to X 2 and X 2* E *P, then Xl* E *P.*  
*Proof* The polynomial-time algorithm for Xl is to compute the function *g* and then  
apply the polynomial-time algorithm for *X 2• •*  
The transformation idea is surely familiar. When confronted with a new problem, a  
traditional approach to solving it is to restate it as a problem we already know how to solve.  
The only thing we have added is the requirement that the transformation be done in  
polynomial time.  
We say that Xl is a "special case" of *X 2* if DI C *D2* and *FI* = DI n *F 2•* Here *g\(d\)* = *dis*  
the *identity transformation.* Many of the arcs in the graph of Figure 1.1 were determined by  
identity transformations. But we have also done nontrivial transformations. In particular,  
we have shown that integer programming feasibility is polynomially transformable to 0-1  
integer programming feasibility \(see Proposition 4.2\). Also, in Example 1.1 we have shown  
that set-packing lower-bound feasibility is polynomially transformable to node-packing  
lower-bound feasibility. This means that the problem "Given a 0-1 *m* x *n* matrix *A,* an  
integral *n-vector* c, and an integer z, determine whether *\{x* E *Bn\: Ax* ;\:;\:\:;; 1, *cx* \~ z\} "\* 0" is  
polynomially transformable to the problem "Given a 0-1 *m'* x *n* matrix *A* ' with, at most,  
two I's per row, an integral *n-vector* c, and an integer z, determine whether  
*\{x* E *B n*  
*\: A' x* ;\:;\:\:;; 1, *cx* \~ z\} "\* 0".  
There is a technique, called *polynomial reduction,* that appears to be a more general  
approach than polynomial transformation for establishing that one problem can be solved  
in polynomial time given that another can. We say that *X* I is *polynomially reducible* to *X 2*  
if there is an algorithm for *X* I that uses an algorithm for *X* 2 as a subroutine and runs in  
polynomial time under the assumption that each call of the subroutine takes unit time.  
Note that transformation is the special case of reduction in which the subroutine is used  
only once; that is, it is applied directly to the transformed data *g\( d\).*  
A generalization of Proposition 6.1 is the following.  
Proposition 6.2. *If Xl is polynomially reducible to X 2* and *X 2* E gp, then *Xl* E *C!P.*  
Although polynomial reducibility appears to be more general than polynomial  
transformability, it is not known whether it really is. In any case, all of the polynomial  
reductions needed in this section can be accomplished through the simpler technique of  
polynomial transformation.  
We now address the question of the existence of hardest problems in *\}\(gp. X* E\}\(gp is  
said to be *\}\(r!/\>-complete* if all problems in.Hgp can be polynomially reduced to *X.* The set of  
\}\(gp-complete problems, which we will soon claim to be nonempty, is denoted by *\}\(r!/\>C\(i.*  
The implication of the existence of an \}\(r!/\>-complete problem is given by the following  
proposition.  
Proposition 6.3. *If X is \}\(gp-complete, then* gp = *\}\(r!/\> if and only if X* E gp.  
*Proof X* E *\}\(r!/\>* and *r!/\>* = *\}\(r!/\>* implies *X* E gp. On the other hand, if *X* is \}\(gp-complete  
and in *r!/\>,* then by Proposition 6.2 there is a polynomial algorithm for any problem in *.Hr!/\> .*  
# •  
Once we have an \}\(gp-complete problem, we may be able to find others by polynomial  
reduction.  
Proposition 6.4. *If Xl is \}\(C!P-complete and Xl is polynomially reducible to X 2* E *\}\(C!P, then*  
*X2 is \}\(gp-complete.* 6\. The Most Difficult *J\{1/J* Problems\: The Class *J\{1/J«!'* 133  
The proofis obvious, but it is important to note the direction of the statement to avoid  
making the mistake of concluding that *X 2* is K9J-complete by reducing *X 2* to *Xl.*  
The *satisfiability problem,* which is a classical problem in logic, is of historical interest  
because it was the first problem in *K9J* shown to be K9J-complete. It is described by a finite  
set *N* = \{l, ... , *n\}* and *m* pairs of subsets of *N,* denoted by Ci = *\(Cj,* Cn for *i* = 1, ... , *m.*  
An instance is feasible if the set  
\(6.1\) *\{ X* E *Bn\: L. Xj* + *L.* \(1 - *Xj\)* ;\:?; 1 for *i* = 1, ... , *m\}*  
JECr JECr  
is nonempty.  
The satisfiability problem is in *K9J.* We use subsets Jfl of *N* as guesses, set *Xj* = 1 if  
j E *N°* and *Xj* = 0 otherwise, and then simply check for feasibility in \(6.1\).  
Theorem 6.5 *\(Cook\). The satisfiability problem is K9J-complete.*  
We will not prove this famous theorem. The proof is technical but is not very difficult  
mathematically. To comprehend it, one must understand the formal model of a nondeter-  
ministic Turing machine that can solve any problem in *K9J* in polynomial time. The proof  
then amounts to a polynomial transformation of the nondeterministic Turing machine  
into the satisfiability problem.  
Since we have described the satisfiability problem as a 0-1 integer feasibility problem,  
we obtain the following proposition.  
Proposition 6.6. *The 0-1 integer programming feasibility problem is K9J-complete.*  
Soon after the appearance of Cook's theorem, the list of K9J-complete problems was  
substantially enriched. This list includes lower-bound feasibility versions of all of the  
problems in Figure 1.1 that we have not already stated are in *9J.* It is important to  
understand that showing that a problem is in *K9Jce* is a negative result about the likelihood  
of finding a polynomial time algorithm for it.  
To illustrate the use of polynomial transformations, we now show that some problems  
are Kgp-complete. In choosing candidates, it is important to try to get as close to the  
boundary \(if it exists\) between *gp* and *Kgpce.* By this we mean the following\: Given a  
problem in *gp,* what are the most simple generalizations of it that make it Xgp-complete?  
For example, lower-bound feasibility testing for matching can be solved in polynomial  
time. In terms of linear inequalities, this problem is to determine if  
*\{x* E *Bn\: Ax* \~ 1, *cx* ;\:?; *z\} =1=* 0, where *A* is an *m* x *n* 0-1 matrix with two l's in each column  
\(the node-edge incidence matrix of a graph\), c is an integral n-vector, and *z* is an integer.  
However, if we allow matrix *A* to contain three 1's in each column, the problem becomes  
Kgp-complete, even if we restrict c to be a vector of 1 'so  
A similar situation occurs when we limit the number of I's in each row of *A.* When the  
0-1 matrix *A* contains one 1 in each row, the feasibility problem for the set  
*\{x* E *Bn\: Ax* \~ 1, *cx* ;\:?; *z\}* is trivial. However, if we allow matrix *A* to contain two l's in  
each row, the problem becomesK9J-complete, even if we restrict c to be a vector ofl's. We  
now prove this result by a polynomial transformation from the satisfiability problem.  
An instance of the *unweighted node-packing problem* is\: Given a graph G = *\(V, E\)* and  
an integer *k,* is there a *U* \~ *V* such that I *U* I ;\:?; *k* and *U* is a node packing? Alternatively, is  
*\{x* E *B* I *v* I\: *Ax* \~ 1, *LjEV Xj* ;\:?; *k\} =1=* 0, where *A* is the edge-node incidence matrix ofG \(i.e.,  
where *A* is a 0-1 matrix with two 1 's in each row\)?  
Proposition 6.7. *Kgp-complete.*  
*The lower-bound feasibility problem for unweighted node packing is* 134 1.5\. Computational Complexity  
\(1, 1\)  
\(2, 1\)  
*cr Ci*  
1 \{l,2\} \{3\} \(6,3\)  
2 \{2,3\} \{4\}  
3 \{4\} \{l,2\}  
4 \{3\} 0  
\(8,2\) \(2,2\)  
Figure 6.1.  
*Proof* The problem is a special case of 0-1 integer programming feasibility, so it is in  
*\)\(PJ.* Membership in *\)\(PJcg* is established by polynomial transformation from the satisfia-  
bility problem.  
Given an instance of the satisfiability problem specified by *N* = \{I, ... *,n\}* and  
Ci = *\(Ct,Cn* for *i* = 1, ... *,m,* we set *k* = *m* and construct G = *\(V, E\)* as follows. Let  
*Vi* = *\{\(j, i\)\:j* E *cn, Vi* = *\{en* + *j, i\)\:j* E *cn,*  
*Vi* = *Vi* U *Vi* for *i* = 1, ... *,m* and *V* = U *Vi.*  
*m*  
i=l  
Each pair of nodes in *Vi* is joined by an edge; and for *j* = 1, ... *,n* and *I =1= i,* nodes *\(j, i\)*  
and *\(n* + *j, I\)* are joined by an edge.  
An example of the construction of G is shown in Figure 6.1. A feasible solution to the  
satisfiability problem is *N°* = \{l, 3\} or *Xl* = *X3* = 1 and *X2* = *X4* = O. A node packing of size 4  
is \{\(I, 1\), \(8, 2\), \(6, 3\), \(3, 4\)\}.  
In general, any node packing of size *m* is of the form \{Cab 1\), *\(a2,* 2\), ... , *\(am, m\)\}* and  
such a packing exists if and only if *N°* = \{ai\: ai \~ *n\}* is a solution to the satisfiability  
\~\~m. •  
An instance of the *set partitioning feasibility problem* is\: Given an *m* x *n* 0-1 matrix *A,*  
is *\{x* E *Bn\: Ax* = 1\} *=1=* 0?  
Proposition 6.8. *The set partitioning feasibility problem is \)\(PJ-complete.*  
*Proof* The problem is a special case of 0-1 integer programming feasibility, so it is in  
*\)\(PJ.* We prove membership in *\)\(PJcg* by transformation from the unweighted node-packing  
lower-bound feasibility problem. Given a graph G = *\(V, E\)* and an integer *k,* let *IE* be an  
I *E* I x I *E* I identity matrix, let *A* G be the edge-node incidence matrix of G, and let 1 be a  
row vector of I *V* II's. Construct the \( I *E* I + *k\)* x \( I *E* I + *k* I *V* I \) matrix  
I I I I  
*.iE\_t.-*A\~\~ A\~r\:\~J..\~£...  
I 1 I 0 I I 0  
A=I I I I  
o I 0 I I'" I 0  
I 6 I 6 I ! 6\. The Most Difficult \}\(@'J Problems\: The Class *,N'@'J\<\:g* 135  
Suppose *Ax* = 1, where *x* E B\~ft-kIVJ. This can be the case if and only if the columns of *A* for  
which *Xj* = 1 have the following structure. There is exactly one column from *B* i for *i* = 1,  
... *,k.* If *bip* is the column chosen from *B i,* and *b1q* is the column chosen from *B l,* then the  
nodes corresponding to these columns are not joined by an edge. Hence the *k* columns  
chosen from *\(Bb* ... *,Bk\)* define a node packing of size *k.* The partition is completed by  
choosing appropriate columns from *B* 0; these correspond to edges of G that are not met by  
any nodes in the packing. An example is shown in Figure 6.2. Nodes 1 and 3 yield a  
packing of size 2, and a partition is indicated by the checked columns. •  
Proposition 6.9. *three* 1 *s per column is .H1P-complete.*  
*The set partitioning!easibility problem in which matrix A has, at most,*  
*Proof* We polynomially transform the general set partitioning feasibility problem  
with an arbitrary 0-1 *m* x *n* matrix *A* into a 0-1 *m I* x *n I* matrix *A I* such that matrix *A I* has  
no more than three 1's per column and there is a one-to-one correspondence between  
solutions of *\{x* E *Bn\: Ax* = 1\} and *\{y* E *B n '\: A 'y* = 1\}, where *n'* \~ *n\(2m* - 1\) and  
*m'* \~ *m* + *2n\(m* - 1\). We assume that *A* has at least one column, say *a\[,* with *t* \~ 41's;  
otherwise there is nothing to prove. Let *A* = *\(a* r, *An-I\)* and  
*A'* = \(\~I *A n*  
*-*  
I *HI* 0  
*I*  
where \~I is an *m* x *t* matrix of unit columns such that *ei* is a column of \~I if and only if  
ail = 1, and 01 is an *m* x *\(t* - 1\) null matrix. *HI* and *KI* are 0-1 matrices that will be  
described subsequently. Consider the equations  
\~Iyl + *A n\_Iy2* = 1  
*Hlyl* + *K ly3* = 1  
*yl* E *Bt*  
*, y2* E *Bn-I, y3* E *B t*  
*-*  
*I*  
*.*  
Suppose *HI* and *KI* are such that the only two solutions to  
are *yl* = \(1, 1, ... , 1\), *y3* = \(0, 0, ... ,0\), and *yl* = \(0, 0, ... ,0\), *y3* = \(1, 1, ... , 1\). This  
condition can be achieved if *H* 1 and *K* 1 each have *2t* - 2 rows and the following structure\:  
Note that if *yl* = 1, then *yi* = 0 and *Y1* = 1. Similarly, if *YI* = 1, then yi = *yi* = 0 and y\~ = 1.  
We then proceed inductively to obtain the result. 136 1.5\. Computational Complexity  
2  
*A=* 4  
./ ./ ./  
1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 1 0 0  
1 0 0 1  
0 1 0  
0 1 0  
0 0  
0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0  
0 0 0 0  
Figure 6.2.  
The solution with *y3* = \(1, 1, ... ,1\) yields *A n\_Iy2* = 1, and we note that  
*\{x* E *B n*  
*\: Ax* = 1, *Xl* = O\} = \{CO, *y2\)* E *B n*  
*\: A n\_ly2* = 1\}. Also the solution with *yl* = \(1, 1, ... ,  
1\) yields Lllyl = *a* b so  
It is important to observe that *HI* and *KI* have been chosen so that each column of  
has no more than three 1 'so  
Now suppose that *A* = *\(Ab A n- k\),* where each column of *Ak* has more than three l's and  
each column *ofA n- k* has three or fewer l's. By applying the above procedure recursively, we  
eventually obtain the desired *m'* x *n'* matrix  
LlJ Ll2  
*HI* 0  
*A'* 0 *H2*  
\(  
6 0  
*Llk A n- k*  
o 0  
o 0  
o 0  
Since *k* \~ nand *t* \~ *m,* it follows that *m'* \~ *m* + *2n\(m* - 1\) and *n'* \~ *n\(2m* - 1\). •  
Next we consider the 0-1 integer programming feasibility problem with only one linear  
equation. Let *N* = \{l, ... , *n\}.* An instance of the *subset sum problem* is\: Given an integer  
*n,* an integral n-vector *\(at,* ... *,an\),* and an integer *b,* is *\{x* E *Bn\:LjEN ajxj* = *b\}* =1= 0? 6\. The Most Difficult .H\~ Problems\: The Class .H\~C\(5 137  
Proposition 6.10. *The subset sum problem is JV\[ljJ-complete.*  
*Proof* Membership in,K\[ljJ is shown by guessing subsets of *N.* We show membership in  
*,K\[ljJC\(i* by polynomially transforming the set-partitioning feasibility problem to the subset  
sum problem. Given a 0-1 *m* x *n* matrix *A,* define  
*m*  
*aj* = I *\(n* + *1\)i-1aij* for *j* = 1, ... , *n*  
*i=1*  
and  
*b* = I *\(n* + 1\)i-l = *\(n* + *l\)m* - 1.  
*i=l n*  
Since *LjEN ajxj* = *b* is a linear combination of *Ax* = 1 obtained by weighting the ith row by  
*\(n* + *1\)i-l,* we have *\{x* E *Bn\: Ax* = 1\} \~ *\{x* E *Bn\: LjEN ajxj* = *b\}.* Now to show that the two  
sets are identical, we note that the unique solution to *L7!l \(n* + *l\)i-1Ui* = *b, Ui* E *BI* is *Ui* = 1  
for *i* = 1, ... *,m.* Thus if LjES *aj* = *b* so that LjES *L7!1 \(n* + *1\)i-1aij* = *L7!1 \(n* + 1\)i-l, then  
LjES *a ij* = 1 for *i* = 1, ... , *m. •*  
An instance of the 0-1 *knapsack lower-boundfeasibility problem* is\: Given an integer *n,*  
integral n-vectors *\(at,* ... *,an\)* and \(ct, ... , *cn\),* and integers band *z,* is  
Corollary 6.11. *The 0-1 knapsack lower-boundfeasibility problem is ,K\[ljJ-complete.*  
*Proof* The problem is in *,K\[ljJ,* since it is a special case of the 0-1 feasibility problem.  
The subset sum problem can be reformulated as the feasibility problem for the set  
*\{x* E *Bn\: LjEN ajxj* \~ *b, LjEN ajxj* ;?; *b\}.* Hence it is a special case of the 0-1 knapsack lower-  
bound feasibility problem. •  
Membership in *JV\[ljJrl* for the 0-1 knapsack lower-bound feasibility problem does not  
immediately imply that the integer knapsack lower-bound feasibility problem is in *,K\[ljJC\(i*  
because upper-bound constraints are needed in the obvious transformation of a 0-1  
knapsack problem to an integer problem. Nevertheless, there is a polynomial transforma-  
tion of the 0-1 knapsack problem to the integer knapsack problem. This is left as an  
exercise.  
Figure 6.3 shows the class *JV\[ljJ* and the two subsets \[ljJ and *,K\[ljJC\(i,* which are disjoint unless  
\[ljJ = *,K\[ljJ.* If \[ljJ *=1= ,K\[ljJ,* it can be shown that \[ljJ U *,K\[ljJrl =1= \}\(\[ljJ.*  
Within the class *,K\[ljJrl,* it is useful to make some distinctions. The subset sum problem  
can be solved in *O\(nb\)* time. Although this is not polynomial, it is less formidable than  
*O\(2n\).* We say that an algorithm runs in *pseudopolynomial* time, if its running time is a  
polynomial function of the length of the data encoded in *unary* \(a one-symbol alphabet\).  
The principal significance of unary encoding is that an integer *k* is represented by a string  
of *k* symbols. The *O\(nb* \)-time algorithm for the subset sum problem is pseudopolynomial  
because the unary encoding of the integer *b* requires a string oflength *b.* It should be noted  
that a polynomial transformation of *Xl* E ,K\[ljJ to *X* 2, which is solvable in pseudopolyno-  
mial time \(e.g., set partition feasibility to subset sum\), does not imply a pseudopolynomial  
algorithm for *X* I. \(Why?\) 138 1.5\. Computational Complexity  
# o  
Figure 6.3  
At the other extreme, there are K9P-complete problems for which the existence of a  
pseudopolynomial algorithm would imply *rtP* = *K9P.* These problems are called *strongly*  
K9P-complete. Existence is obvious because there are K9P-complete problems for which  
the length of a unary encoding of the data is a polynomial function of the length of a binary  
encoding. An example is 0-1 integer feasibility in which all of the constraint coefficients  
are either 0 or ±1. Figure 6.4 shows the relationships among the subsets of *K9P* that we have  
discussed.  
Note thatK9Pcg n *cgo.N'9P* appears in Figure 6.4. The implication of *K9Pcg* n *cgo.N'9P* \* 0 is  
given in the following proposition.  
Proposition 6.12. *If K9Pcg* n *cgo.N'rtP* \* 0, *then K9P* = *cgo.N'9P.*  
A polynomial-time algorithm for determining Zo = *max\{ex\: xES\}* obviously implies a  
polynomial-time algorithm for the lower-bound feasibility problem *\{x* E S\: *ex* \~ z\} \* 0.  
Hence if the feasibility problem is K9P-complete, a polynomial-time algorithm for the  
optimization problem would imply g; = *Kg;.* But since the optimization problem is not in  
*K9P,* it is not K9P-complete.  
In speaking about these problems, we need to extend the notion of polynomial  
reducibility to problems other than feasibility problems. We call a problem *Kg;-hard* if  
there is an K9P-complete problem that can be polynomially reduced to it. Thus if a  
problem is KrtP-hard it is at least as difficult as any K9P-complete problem. It also follows  
that a polynomial algorithm for an K9P-hard problem implies *9P* = *KrtP.*  
There is also a converse for optimization problems such as the integer programming  
problem. We have already observed that a polynomial-time algorithm for the lower-bound  
feasibility problem and binary search yields a polynomial-time algorithm for the optimi-  
Strongly  
*,!V.o/"t6'*  
Pseudo- \_\_ -...........,  
polynomial  
See proposition  
6.12  
Figure 6.4 7. Complexity and Polyhedra 139  
zation problem \(see Proposition 5.1\). Hence if9J *=.N9J* the integer programming problem  
is solvable in polynomial time. The point is that when an optimization problem has this  
property, it serves the same role as an .N9J-complete problem with regard to the question of  
whether *9J* = *.N9J.*  
The classification scheme presented in this chapter can be very useful when we begin to  
study an integer optimization problem. Our knowledge of q'J and .N9J-hard problems  
makes it likely that most problems that we encounter will be classifiable. If *9J* \*- .Nq'J, our  
expectations of what can be accomplished algorithmically should be guided by the  
classification. If we know that the problem is .Nq'J-hard, we can expect that some large  
instances will be difficult for any algorithm. However, this definitely does not mean that  
we will be unable to solve many large instances in a reasonable amount of time. And even  
when we cannot find an optimal solution or prove that a known solution is optimal, it may  
very well be possible to obtain a good feasible solution and to show this feasible solution is  
within a specified tolerance of being optimal. If this were not the case, we would not have  
written this book.  
Part II of this book develops theory and algorithms largely for dealing with .N9J-hard  
integer optimization problems. Once one departs from the worst-case point of view, we  
will see that much can be accomplished.  
Above we observed that *9J* =.Nq'J would imply the existence of a polynomial-time  
algorithm for integer programming. However, it is difficult for us to imagine the impact of  
the existence of polynomial-time algorithms on the computational aspects of integer  
programming, since it is not clear what kind of algorithms would result. In the next  
chapter we will give two polynomial-time algorithms for linear programming. One of  
these certainly appears to be computationally inferior to exponential-time simplex  
algorithms. The other is more promising, but its practical implementations ignore some of  
the details required to prove polynomiality.  
7. COMPLEXITY AND POLYHEDRA  
We begin this section by considering the relationships among three feasibility problems  
associated with polyhedra.  
1. The *membership problem* for a family of polyhedra. An instance is given by an  
integer *n,* a polyhedron in the family *peR n,* and an *x ERn.* The instance is feasible  
if *x EP.*  
2. The *validity problem* for a family of polyhedra. An instance is given by an integer *n,*  
a polyhedron in the family *P eRn,* and a *\(n, no\)* E *Rn+l.* The instance is feasible if  
*\(n, no\)* is a valid inequality for *P.*  
3. The *lower-boundfeasibility problem* for a family of polyhedra. An instance is given  
by an integer *n,* a polyhedron in the family *PC Rn,* and a *\(n, no\)* E *Rn+l.* The  
instance is feasible if *P* n *\{x ERn\: nx* \~ *no\}* \*- 0.  
Proposition 7.1. *The following problems are equivalent\:*  
a. *the validity problem;*  
b. *the membership problem for the family of polars; and*  
c. *the complement of the lower-bound feasibility problem.* 140 1.5\. Computational Complexity  
*Proof.* The equivalence of problems a and b follows immediately from the definition  
of polarity, that is, *\(n, no\)* is a valid inequality for *P* ifand only if\(n, *no\)* belongs to the polar  
of *P.* Recall that the polar of *P* is a polyhedron in *Rn+l.*  
By definition of validity, *\(n, no\)* is valid for *P* if and only if *\{x* E *P\: nx* \> *no\}* = 0. But  
this is just the complement of the lower-bound feasibility problem with input *P* and  
*\(n, no* + e\) for some suitably small e \> O. •  
The complexity of these problems depends on the description of the polyhedra and the  
points *x* or *\(n, no\).* We will assume throughout this section that the allowable inputs for the  
points *x* and *\(n, no\)* are polynomial in the description of the polyhedra. Such vectors are  
called *short.* By restricting the input in this way, it is sufficient to consider the description  
of *P* alone. The results of Section 4 on the size of numbers that can arise in optimal  
solutions and coefficients of facet defining inequalities \(Theorems 4.1 and 4.3\) justify the  
assumption of short vectors in integer programming.  
Suppose *P* is described by a set of linear inequalities, that is, *P* = *\{x ERn\: Ax* \~ *b\}.*  
Then the membership problem for *P* is solved by substitution. The validity and lower-  
bound feasibility problems for *P* can be answered by solving the linear program  
z = *max\{nx\: Ax* \~ *b\},* since *\(n, no\)* is valid for *P* if and only if *no* \~ z, and  
*P* n *\{x ERn\: nx* \~ *no\} =1=* 0 if and only if *no* \~ z. Hence, given a linear inequality descrip-  
tion of *P,* there are polynomial-time algorithms for all three problems. We also obtain  
polynomial algorithms when *P* is described by a list of its extreme points and rays.  
However, in many integer and combinatorial optimization problems, *P* is the convex  
hull of a set of integral points. We may have an implicit description of the extreme points  
of *P* \(e.g, the node packings of a graph\), or we may have a set oflinear inequalities such that  
*P* is the convex hull of integral points that satisfy these inequalities, or we may even have a  
linear inequality description of *P,* but with the number of inequalities exponential in the  
natural description of *P.* With these descriptions, there is not an obvious polynomial-time  
algorithm for any of the three problems.  
*Example* 7.1 *\(The family of polytopes for 0-1 integer programming\).* An instance is  
specified by integers *m* and *n,* an *m* x *\(n* + 1\) matrix *\(A, b\),* and a short vector *x ERn* or  
*\(n, no\)* E *Rn+l.* The polytope for an instance is *P* = conv\{x E *Bn\: Ax* \~ *b\}.*  
a. *Lower-bound feasibility. P* n *\{x ERn\: nx* \~ *no\} =1=* 0 if and only if  
*\{x* E *Bn\: Ax* \~ *b, nx* \~ *no\} =1=* 0. We have already established that the latter lower-bound  
feasibility problem is Kg;-complete. Hence lower-bound feasibility for the family of 0-1  
integer programming polytopes is also Kg;-complete.  
b. *Validity.* By Proposition 7.1, the validity problem for 0-1 integer programming  
polytopes is in *cgo.Hg;.* However, if it was in *Kg;,* the lower-bound feasibility problem for  
0-1 integer programming polytopes would be in *Kg;* n *cgo.Hg;.*  
c. *Membership,* We claim that the membership is in *Kg;.* First, an instance is trivial if  
either *x* \$. *\{x* E R\~\: *Ax* \~ *b, Xj* \~ 1 for all\}\} or if *x* E *Bn.* So suppose  
*x* E *\{x* E R\~ \\ *Bn\: Ax* \~ *b, Xj* \~ 1 for all\}\}. Observe that ifdim\(P\) = *n,* any *x* E *P* can be  
written as a convex combination of a set of *n* + 1 binary vectors in *P.* Now the nondeter-  
ministic algorithm for membership is to guess vectors *Xi* E *B n* for *i* = 1, ... , *n* + 1. If each  
of these vectors are in *P,* we continue. Otherwise we guess a new set. Next we consider the  
linear system  
*n+l n+l*  
I *AjXi*  
= *x* and I Ai = 1.  
i=l i=! 7\. Complexity and Polyhedra 141  
If this system has a solution *A O* E R\~+I, guessing stage.  
we conclude that *x* E *P;* otherwise we return to the  
Proposition 7.2. *If lower-bound feasibility \(validity\) for a family of polyhedra is HPJ-*  
*complete and validity \(lower boundfeasibility\) is in HPJ, then HPJ* = *cgoJ\(PJ.*  
*Proof* Suppose validity is in *HrJ\>.* Then by Proposition 7.1, lower-bound feasibility is  
in *CfioJ\(PJ.* Hence lower-bound feasibility is in *HPJcg* n *CfioJ\(PJ.* Now by Proposition 6.12 we  
obtain *HPJ* = *cgoJ\(PJ. •*  
In other words, if one of the problems is HPJ-complete, it is very unlikely that the other  
is in *.HrJ\>.* We frequently encounter the case \(as in Example 7.1\) where the lower-bound  
feasibility problem is HPJ-complete, so it is unlikely that the validity problem is in *HrJ\>.* The  
following example, however, illustrates an *.HPJ* validity problem.  
*Example* 7.2 *\(Fractional node-packing polytopes\).* An instance is specified by a graph  
G = *\(V, E\)* and a *\(n,no\)* E *RJ v* I + I. Let *A* be the incidence matrix of maximal cliques by  
nodes of G and *P* = *\{x* E *RJ v* I\: *Ax* \~ n. Note that the number of rows of *A* is generally  
exponential in the size of G. Here *\(n, no\)* is valid if and only if, for some *k* \~ *n,*  
*k k*  
*n* \~ *L Uiai* and *no* \~ *LUi,*  
i=1 i=1  
where \{ai \}7=1 are rows of *A* and *Ui* \~ 0 for *i* = 1, ... *,k.* Since there is a polynomial-time  
algorithm for determining whether a 0-1 vector *a* i is the incidence vector of a maximal  
clique of G, there is an *.HPJ* algorithm for the validity problem.  
In Examples 7.1 and 7.2 we have implicitly considered the *extreme point membership*  
*problem* for a family of polytopes. The input is the same as in the membership problem,  
but it is feasible only if *x* is an extreme point of *P.* Note that in Example 7.2, extreme point  
membership was with respect to the polar; that is, a 0-1 vector *ai* is the incidence vector of  
a maximal clique of G only if it is an extreme point of the polar of the fractional node-  
packing polytope. In both examples, we have sketched proofs of the following proposition.  
Proposition 7.3. *If the extreme point membership problem for a family of polytopes is in*  
*HPJ, then the membership problem for thefamily is also in HPJ.*  
We now put together Propositions 7.2 and 7.3 by considering the *facet validity problem*  
for a family of polyhedra. The input is the same as in the validity problem, but it is feasible  
only if\(n, *no\)* defines a facet of *P.*  
Proposition 7.4. *If lower-bound feasibility is .HrJ\>-complete for a family of polyhedra and*  
*facet validity is in HPJ, then .HPJ* = *CfioJ\(PJ.*  
*Proof* Suppose facet validity is in *HPJ.* Then by Proposition 7.3, applied to the family  
of po lars, validity is in *.HPJ.* Now Proposition 7.2 implies that *HPJ* = *CfioJ\(PJ. •*  
Proposition 7.4 says that for an HPJ-complete lower-bound feasibility problem, a good  
characterization of all of the facets of the family of polyhedra is not possible unless  
*.H\(!P* = *CfioJ\(PJ.* In other words, there is some class of facets for the family of polyhedra for  
which there is no short proof that they are facets unless *HPJ* = *CfioJ\(\(!P.* 142 1.5\. Computational Complexity  
*Example* 7.3 *\(Node-packing polytopes\).* An instance is specified by a graph G = *\(V, E\)*  
and a *\(n, no\)* E *Rl v* I +1. Here *P* is the convex hull of node packings. If facet validity is in  
*,Hf5JJ,* then,Hf5JJ = *Cf£oJ\(g\},* since lower-bound feasibility is ,Hf5JJ-complete. The reader should  
note the subtle difference between Examples 7.2 and 7.3.  
8\. NOTES  
Sections 1.5.1 and 1.5.2  
Basic reference books on computational complexity are Aho et al. \(1974\), Garey and  
Johnson \(1979\), Knuth \(1979, 1981\), and Lewis and Papadimitriou \(1981\). Two surveys and  
an annotated bibliography prepared for the combinatorial optimization community are,  
respectively, Lenstra and Rinnooy Kan \(1979\), Johnson and Papadimitriou \(1985a\), and  
Papadimitriou \(1985\).  
Jeroslow \(1972\) discusses the unsolvability of quadratic integer programs.  
Section 1.5.3  
Polynomial-time algorithms for the minimum-weight path problem were presented in  
Section 1.3.2.  
Edmonds \(1967a\) pointed out that very large numbers could arise in Gaussian elimina-  
tion if rationals were not necessarily represented by a pair of relatively prime numbers. He  
also gave a modified elimination scheme and proved that with this scheme the size of  
integer numbers used to represent rationals was polynomially bounded.  
Tardos \(1985\) gave a polynomial-time algorithm for the transportation problem with  
the bound being independent of the numerical data. Her approach will be presented in  
Section 1.6.5 in the more general setting of linear programming.  
Klee and Minty \(1972\) have shown that the simplex algorithm with a standard pivoting  
rule does not have a polynomially bounded number of pivots. The expected behavior of  
the simplex algorithm has been analyzed by Borgwardt \(1982a, b\), Smale \(1983a, b\), and  
others. Shamir \(1987\) gives a survey of these results.  
Edmonds \(1965a, c\) proposed the concept of a good characterization. This was done in  
the context of the maximum-weight matching problem \(see Chapter 111.2\).  
Section 1.5.4  
Bell \(1977\) proved that the formulation ofa feasible n-variable integer program with linear  
inequalities and integrality restrictions requires no more than *2n*  
- 1 inequalities.  
The results on the size of numbers that arise in general integer programming problems  
have been obtained independently by several people, including Borosh and Treybig \(1976\),  
Von zur Gathen and Sieveking \(1978\), Kannan and Monma \(1978\), and Papadimitriou  
\(1981a\). The simple proof given in the text was suggested to us by Gerard Cornuejols.  
Sections 1.5.5 and 1.5.6  
The basic references for these sections are Garey and Johnson \(1979\) and the more recent  
survey by Johnson and Papadimitriou \(1985a\).  
The class,Hg\} was formally introduced by Cook \(1971\). A slightly different definition of  
*,Hf5JJ* was used by Karp \(1972, 1975\). Cook used polynomial reducibility in the definition of  
*,HPJ* and proved the fundamental result of the existence of complete problems in *,Hg\}.* Karp  
defined *,Hg\}* by polynomial transformability and showed that numerous combinatorial  
optimization problems are ,Hf5JJ-complete. The proofs of Propositions 6.9 and 6.10 are  
taken from Lenstra and Rinnooy Kan \(1979\). 8\. Notes 143  
Section 1.5.7  
Facet complexity problems have been studied by Karp and Papadimitriou \(1982\) and  
by Papadimitriou and Yannakakis \(1984\). A survey of these results is contained in  
Papadimitriou \(1984\).  
Also see the notes for Section I.6.3.  
9. EXERCISES  
1\. Verify the relations implied in Figure 1.1.  
2\. Give a tight bound for the magnitude of coefficients in the extreme points of  
*p* = *\{x* E *R1\:* 1\:\)=1 *ajxj* \:\:%; *b\},* where *aj, b* E Zl. Compare this bound with the bound  
of Proposition 3.1.  
3. 4. Can you find an example for which the bounds of Proposition 3.1 are tight?  
Give a certificate of optimality that *x* = \[W W\] is optimal in Example 3.1 of Chapter  
I.2.  
5\. Give a short proof that *M* = \{\(1, 2\), \(3, 5\)\} is a maximum-weight matching in the  
graph of Figure 9.1.  
6\. Show that if there is a polynomial algorithm to test feasibility of  
*p* = *\{x ERn\: Ax* \:\:%; *b\},* there is a polynomial algorithm to find a minimal face of *P.*  
7\. Give a tight bound for the magnitude of coefficients in extreme points of conv\(S\),  
where S = *P* n *zn* and *P* = *\{x* E *R1\:* 1\:\)=1 *ajxj\:\:%; b\}.* Compare it to the bound of  
Theorem 4.1.  
8. Can you find an example for which the bound of Theorem 4.1 is tight?  
9\. Prove Proposition 5.7 of Section I. 6.5.  
10. Given *A* and *b,* prove that if *x* E *R1* satisfies  
I ± *aijxj* - *bi* I \:\:%; E for *i* = 1, ... , *m*  
\)=1  
with E = *\(2mnlog\(\}mntl, \{x* E *R1\: Ax* = *b\}* \* 0. \(See Proposition 4.6 of Section I.6.4\).  
Figure 9.1 144 1.5\. Computational Complexity  
11\. Give algorithms to show that the following problems on graphs are in q\>.  
i\) Does G contain a cycle?  
ii\) Is G bipartite?  
In i and ii, how would you give a short proof when the answer is no?  
12\. A graph G = *\(V, E\)* is a *hole* if it contains a single cycle through all of the nodes and  
no other edges. Show that the problem "Does G or its complement contain a node-  
induced subgraph that is a hole of odd length?" is in *C\(ioJrq\>.*  
13\. Is the problem "Is *b* E zl \\ \{a\} a prime number?" in *.Nq\>, C\(ioJrq\>,* neither, or both?  
14. Given that the node-packing problem is .Nq\>-complete, show that the following  
problems are .Nq\>-complete\:  
i\) *Node cover.* Given a graph G = *\(V, E\)* and an integer *K,* is there a subset S \~ *V*  
with I S I \~ *K* such that every edge of *E* is incident to a node of S?  
ii\) *Un capacitated facility location.* Given sets *M* and *N* and integers *cij* for  
*i* E *M,j EN, jj* for *j* EN and *K,* is there a set S \~ *N* such that  
*LiEM* minjEs *c ij* + *LjES jj* \~ *K?*  
15\. Show that the following problems are .Nq\>-complete\:  
i\) *Set covering\:* Given an *m* x *n* 0-1 matrix *A* and an integer *K,* does there exist  
*x* E *Bn* such that *Ax* \~ 1 and L7\~1 *Xi* \~ *K?*  
ii\) *Directed Hamiltonian circuit.* Given a directed graph *\[!J\)* = *\(V, stl\),* is there a  
directed cycle passing through each vertex exactly once?  
iii\) *Matching with bonds.* Given a graph G = *\(V, E\),* pairwise disjoint subsets *B* i for  
*i* = 1, ... *,p* of *E,* and an integer *K,* does there exist a matching *Min* G such that  
*1M* I \~ *K* and, for *i* = 1, ... *,p,* either *Bi* n *M* = *Bi* or *Bi* n *M* = 0 \(i.e., either all  
the edges in *B* i are in the matching or none are\)? The subsets *B* i are called *bonds.*  
16. Asetfunctionf\(S\) = *LT\<;;S cTforS* \~ Nisdescribed by the data *\{T, CT\},* wherecT *=1=* O.  
A set function is *sub modular* on *N* if  
*f\(S\)* + *f\(T\)* \~ *f\(S* U *T\)* + *r\(S* n *T\)* for all S, *T* \~ *N.*  
Show that the problem\: "Isf not submodular?" is .Nq\>-complete.  
17\. Show that the traveling salesman problem is .Nq\>-hard.  
18\. Show that the minimum-weight path problem \(with positive and negative edge  
weights\) is .Nq\>-hard.  
19. Give a polynomial transformation of 0-1 knapsack to integer knapsack.  
20\. Show that the fixed-charge network flow problem is .Nq\>-hard.  
21\. Show that the maximum-cut problem "Given *\[!J\)* = *\(V, stl\)* and *c* E R\~, find  
*max\(i,\)EO+\(U\)C ij"* is .Nq\>-hard, where *l5+\( U\)* = *\{\(i, j\)* E *stl\: i* E *U, j* E *V* \\ *U\}.* 9. Exercise  
22\. Show that the problem  
145  
*n n*  
max *L L cijxij*  
i=l *j=l*  
*n*  
*j=l*  
*L x* ij = 1 for all *i*  
*n*  
i=l  
*L Xij* = 1 for all\)  
*n n*  
*L 'LtijXij* \~ *T*  
i=l *j=!*  
is Kg\}\}-hard.  
23\. Show that the single-machine scheduling problem with due dates is Kgp-hard.  
24. Let S = *\{x* E *Z1\: Ax* \~ *b\}.* Which of the following problems \(if any\) are known to be  
in *gp, Kg\}\}, cgoJfgp?* Which are unlikely to be in *Kgp?* Justify your answers.  
i\) Membership for conv\(S\).  
ii\) Extreme point membership for conv\(S\).  
iii\) Validity for conv\(S\).  
iv\) Facet validity for conv\(S\). **1.6**  
**Polynomial-Time Algorithms**  
**for Linear Programming**  
**1. INTRODUCfION**  
Simplex methods \(see Chapter 1.2\) are practical techniques for solving linear programs.  
But, according to the model of computational complexity presented in the previous  
chapter, they are unsatisfactory because their running time can grow exponentially with  
the size of the input. Here we give some polynomial-time algorithms for linear program-  
ming and discuss their consequences in combinatorial optimization.  
The ellipsoid algorithm, which will be presented in Section 2, was acclaimed on the  
front pages of newspapers throughout the world when it appeared in 1979. Although the  
algorithm turned out to be computationally impractical, it yielded important theoretical  
results. It was the first polynomial-time algorithm for linear programming. Also, as will be  
discussed in Section 3, it is a tool for proving that certain combinatorial optimization  
problems can be solved in polynomial time.  
In Section 4, we will present a version of a polynomial-time projective algorithm for  
linear programming. Remarkably good computational results have been claimed for  
projective algorithms, but only time will tell whether they are superior to, or a serious rival  
of, simplex methods.  
The running times of these polynomial-time algorithms typically depend on *m, n,* and  
log *8A ,b.c* where  
*8A ,b,c* = max\{max *laij* I, max *Ib;* I, max *ICj* I\}.  
In Section 5, it will be shown how the dependence on band c can be eliminated. Thus, for  
example, when *A* is a \(0, 1\) matrix, there are linear programming algorithms that are  
polynomial in *m* and *n.*  
To present polynomial-time versions of the ellipsoid and projective algorithms, some  
basic questions about linear programming must be addressed.  
1. Unlike the simplex methods, the ellipsoid and projective algorithms are naturally  
described as algorithms to find a feasible point in a polyhedron. Hence, we must convert a  
feasibility algorithm into an optimization algorithm. The standard approach is to formu-  
late a linear program as the feasibility problem\: Find *x* E R\~, *u* E *R'\:'* satisfying  
*Ax* \~ *b, uA* \~ c, *cx* \~ *ub.*  
**146** 2. The Ellipsoid Algorithm 147  
But this approach is computationally unsatisfactory, so we will need to consider other  
methods.  
2. Neither the ellipsoid nor the projective algorithm search extreme points. Whereas  
extreme points and extreme rays can be described in polynomial time, arbitrary points  
cannot. So care has to be taken that the points obtained have polynomial descriptions.  
3. The last step of the ellipsoid and projective algorithms requires the conversion of an  
"almost feasible/almost optimal" point to a basic feasible/optimal solution. We need to  
show that this operation can be executed in polynomial time. An intermediate step in this  
process is the perturbation of a constraint or of the objective function so that the resulting  
linear program has a unique primal or dual feasible solution.  
2. THE ELLIPSOID ALGORITHM  
To describe the ellipsoid algorithm, we need a few basic properties of ellipsoids.  
*Definition* 2.1. *x ERn* except *x* = O.  
An *n* x *n* symmetric matrix *D* is *positive definite* if *xTDx* \> 0 for all  
*Definition* 2.2\. An *ellipsoid* with center *y* is a set *E* = *\{x ERn\: \(x* - *yfD-1\(x* - *y\)* \~ 1\},  
written as *E\(D, y\),* where *D* is an *n* x *n* positive definite matrix and *y ERn.*  
*Definition* 2.3\. A *sphere* with center *y* and radius *r* is a set of the form  
S = *\{x ERn\: \(x* - *y\)T\(X* - *y\)* \~ *r2\},* written as *S\(y, r\).*  
Evidently a sphere is a special case of an ellipsoid with *D* = *r2 I n,* where *In* is the *n* x *n*  
identity matrix \(see Figure 2.1\). We let *sn* denote the unit sphere in *R n* with center 0, that is,  
*sn* = S\(O, 1\).  
*x2* Z2  
*\(x-y\}TD-l\(X-Y\)* s 1  
\(0, I\)  
----------\~--\~----\~\~-----xl --------+-----\~----\~-------Zl  
\(l,O\)  
Figure 2.1 148 1.6\. Polynomial-Time Algorithms for Linear Programming  
----------\~r-;-\~E++7\~\~---xl  
*E*  
Figure 2.2  
The following property is crucial to the ellipsoid algorithm.  
*The Ellipsoid Property.* Given an ellipsoid *E* = *E\(D, y\),* the half-ellipsoid  
*H* = *E\(D, y\)* n *\{x ERn\: dx* \~ dy\}obtained byintersectingEwithanyinequalitydx \~ *dy*  
through its center is contained in an ellipsoid *E'* with the property that vol\(E' \)/vol\(E\)  
\~ *e-1I2\(n+l\),* where vol denotes volume \(see Figure 2.2\). A constructive proof of this property  
will be given below.  
We begin by describing the ellipsoid algorithm for a membership problem.  
*Strict Membership Problem.* Given integers *m* and *n,* an integer *m* x *n* matrix *A,* and  
an integer m-vector *b,* find a point in *P\<* = *\{x ERn\: Ax* \< *b\}* or show that *P\<* = 0. \[The  
notation *Ax* \< *b* means that *aix* \< *bi* for *i* = 1, ... *,m* where *\(ai, bi\)* is the *ith* row of  
*\(A, b\).\]*  
Throughout this section we will assume that given a point *y ERn* we check whether  
*y* E *P\<* by testing whether *aiy* \< *bi* for *i* = 1, ... *,m.*  
We will also assume that *P\<* is bounded, so that there exists an *w* such that if  
*x* E *P\<,* then *IXj* I \< *w* for all *j EN.* This means that *P\<* s S\(O, *s\),* where s = *win,* and  
hence vol\(P\<\) \~ vol\(S\(O, *s»* = *sn* vol\(sn\).  
A second important observation concerns the volume of polyhedra.  
*The Strict Feasibility Property.* If *P\<* =1= 0, then vol\(P\<\) \> O. More precisely, given a point  
*y* E *P\<,* there is an *r* \> 0 such that *S\(y, r\)* S *P\<.* This implies that vol\(P\<\);a.  
vol\(S\(y, *r»* = *rnvol\(Sn\).*  
Now suppose we are given \(a\) a number *v* such that vol\(P\<\) \> *v* if *P\<* =1= 0 and \(b\) a  
number *V* such that vol\(Eo\) = *V,* where *Eo* = *E\(Do, xo\)* is an ellipsoid containing *P\<.* Let  
*t\** = *\[2\(n* + 1\) \(loge *V* -logev\)\]. 2. The Ellipsoid Algorithm  
149  
The Ellipsoid Algorithm for *P\<*  
*Initialization\: Eo* = *E\(Do, xo\).*  
Sett = 0.  
*Iteration t\:* If *Xt* E *P\<,* stop. A feasible solution has been found.  
If *t* \~ *t\*,* stop. *P\<* = 0.  
If *Xt* \$. *P\<,* suppose *ai\(t\)xt* \~ *bitt\).*  
\(Note that we have departed from our usual notation here in that *Xt ERn;* that is, *Xt* is  
not the *tth* component of *x.\)*  
Find an ellipsoid *Et+l* containing the half-ellipsoid *H t* = *E t* n *\{x ERn\: ai\(t\) x* \~ *ai\(t\)xt\}*  
as specified by the Ellipsoid Property.  
Let *Et+l* = *E\(Dt+b Xt+l\)* and *t* .... *t* + 1.  
Theorem 2.1. *Given* v, *V, and t\* as defined above, the ellipsoid algorithmfor P\< terminates*  
*correctly after no more than t\* iterations.*  
*Proof* Since *Xt* E *P\<* is readily verifiable, we only have to show that if *Xt* \$. *P\<* for *t* = 0,  
... , *t\*,* then *P\<* = 0.  
First we use induction to show that *P\<* s; *Etforallt* \~ *t\*.* We have constructed *Eo* so that  
*P\<* s; *Eo.* Now suppose that *P\<* s; *E k.* Then as *ai\(k\)xk* \~ *bi\(k\),* we have  
Hence  
Now consider the volume of *E t •.* Since *vol\(Et+1\)/vol\(Et\)* \~ *e-lf2\(n+1\),* it follows that  
*vol\(Et.\)/vol\(Eo \)* \~ *e-t'/2\(n+l\).* Hence  
*vol\(Et.\)* \~ *Ve-t'/2\(n+l\)* = *Ve-f2\(n+l\)\(logV-logv\)1!2\(n+l\)*  
\~ *Ve-1og\(v/v\)* = v.  
But now if *P\<* \*" 0, it would follow that *vol\(P\<\)* \> *v, vol\(Et·\)* \~ *v,* and *P\<* s; *Et"* which is  
impossible. Hence *P\<* = 0. •  
The actual details of how *Et+l* = *E\(Dt+b Xt+l\)* is constructed from *E t* = *E\(Dt\> Xt\)* are  
given by the following expressions. We assume *n* \> 1.  
Let *d* = *ai\(t\)* and *D* = *D t.*  
\(2.1\) 1 *Dd*  
*Xt+l* = *Xt* - *n+1 ,jdTDd*  
\(2.2\)  
We will now show that these transformations lead to a new ellipsoid satisfying the  
ellipsoid property. Without loss of continuity the reader can go directly to Example 2.1. 150 1.6\. Polynomial-Time Algorithms for Linear Programming  
Proposition 2.2. *Every symmetric positive-definite n* x *n matrix D has a decomposition*  
*D* = QfQl, *where* Ql *is an n* x *n nonsingular matrix.*  
*Definition* 2.4. If *A* is an *n* x *n* nonsingular matrix, *bERn,* and *TA \: Rn* .... *Rn* is defined  
by *TA\(x\)* = *Ax* + *b,* then *TA* is called an *affine transformation.*  
\{\~E Affine transformations have several important properties. We let *TA\(L\) =*  
*R n\:* \~ = *Ax* + *b, x* E *L\}.* Affine transformations preserve set inclusion.  
Proposition 2.3. *IfL* \~ *L'* \~ *R n*  
*, then TA\(L\)* \~ *TA\(L'\)* \~ *Rn.*  
Volumes are changed by a constant factor, so relative volumes are preserved.  
Proposition 2.4. *If L* \~ *Rn is full-dimensional and convex, then vol\(TA\(L» =*  
I *detA* I vol\(L\).  
Given an ellipsoid, there exists an affine transformation mapping it into a sphere  
centered at the origin.  
Proposition 2.5. *Let E* = *E\(D, y\) be an ellipsoid with D* = QfQl *and let T be the affine*  
*transformation given by T\(x\)* = \(Qf\)-lx - *\(Qf\)-ly. Then T\(E\)* = *sn.*  
*Proof*  
*T\(E\)* = \{\~\: \~ = \(Qf\)-l\(X - *y\)\: \(x* - *yfD-1\(x* - *y\)* \~ 1\}  
= \{\~\: «QD\~fD-l\(QD\~ \~ l\)  
= \{\~\: \~TQID-IQr\~ \~ l\}  
= \{\~\: \~T\~ \~ n. •  
In Figure 2.3 we see what happens to *E, E',* and the half-ellipsoid *H* when the above  
transformation is applied to *E* = *E\(D, y\).*  
*X2 Z2*  
*dx=dy*  
*dQiz=o*  
*E*  
*Xl* zl  
*E*  
*E'*  
*E'*  
*\(a\) \(b\)*  
Figure 2.3 2. The Ellipsoid Algorithm 151  
Affine transformations corresponding to rotations can be represented by transforma-  
tion matrices *Q2* with the property that *QfQ2* = *I.* Such matrices are called *orthonormal.*  
Proposition 2.6. \(1,0, ... , Of  
*Given an arbitrary nonzero vector d ERn. there exists an n* x *n orthonor-*  
*mal matrix Q2 such that Q2 d* = - *IIdllel, where* IIdll *is the length of d and where* el =  
Applying this proposition to the vector *Q1d,* we can rotate the sphere in Figure *2.3\(b\)* so  
that the shaded area is just the half-sphere with \~I \~ O. Setting *Q* = *Q* 1 *Q2,* the effect of the  
transformation \~ = *\(QTtl\(X* - *y\)* is to map *E, E',* and *H* as shown in Figure 2.4.  
Now we use the above transformations to show that the ellipsoid property holds for \(2.1\)  
and \(2.2\) with *E* = *Et, E'* = *Et+b* and *y* = *Xt.*  
We define  
1. QI to be any matrix such that *Qi* QI = *D,*  
*2. Q2* to be the rotation matrix such that *Q2Q 1 d* = - *IIQ1dlieb* and  
*3. Q* = *Q2Qb*  
and we use the transformation *x* = QT\~ + *Xt,* or *T\(x\)* = *\(QTtl \(x* - *Xt\).* Simple calcula-  
tions give the following proposition.  
Proposition 2.7  
i. *IIQdll* = \~dTDd.  
ii. *QD-1QT* = *I.*  
111. *\(QTtID* = *Q.*  
iv. *T\(Et \)* = \{\~\: \~T\~ \~ 1\}.  
v. *T\(\{x\: dx* \~ *dxt \}\)* = \{\~\: \~l \~ O\}.  
vi. \(a\) *T\(xt+l\)* = n\~1 *el.*  
\(b\) *T\(Et+l\)* = \{\~\: \(\~ - *n* \~ I\)T\(l\~ 1 *\(I* - *n* \~ 1 elem-I\(\~ - *n* \~ I\) \~ 1\}.  
Zz  
-+-I\~W--ZI ---+---+--++8@..@4¥1---- \~1  
Figure 2.4 152 1.6\. Polynomial-Time Algorithms for Linear Programming  
*Proof*  
i. IIQ2Ql *dll* = *../\(Q2Q1 df\(Q2Q 1 d\)* = *"/dTQfQ\[Q2Qld* = *../dTDd* because Q\[ = *Q21*  
since Q2 is orthonormal.  
ii. *QD-IQT* = *Q2Ql\(QfQlt1 \(Q2Qlf* = Q2QIQ1l\(Qf\)-IQfQ\[ = *Q2Q\[* = *I.*  
iii. By ii, *QD-IQT* = *I,* and hence *\(QTt1 D* = Q.  
IV. *T\(Et\)* = \{\~\: \(QT\~fD-l\(QT\~\) \~ 1\} = \{\~\: \~T \(QD-IQT\)\~ \~ 1\} = \{\~\: \~T\~ \~ 1\}.  
v. *T\(\{x\: dx* \~ *dxt \}\)* = \{\~\: dTQT\~ \~ O\} = \{\~\: \(Q2Qldf\~ \~ O\) = \{\~\: \~1 \~ a\}.  
VI. *T\(xt\)* = *\(QTt1 \(xt* - *Xt\)* = o.  
*T\(Et+l\)* = \{\~\: QT\~ = *X* - *Xt, \(x* - *xt+lfDiMx* - *Xt+l\)* \~ 1\}  
= \{\~\: \(QT\(\~ - \~l+l\)fDil1QT\(\~ - \~t+l\) \~ 1\}  
= \{\~\: \(\~- \~t+lfQDil1QT\(\~ - \~t+l\) \~ 1\}  
= \{\~\: \(\~ - \~t+lf«QTtlDt+l Q-ltl\(\~ - \~t+l\) \~ 1\}.  
We note that \(2.2\) yields  
*n2 \[I* 2 *\(Qd\)\(Qd\)T\]* . . ..  
*= n2*  
\_ 1 - *n+l dTDd* usmg 1ll  
*=* n2n\~ 1 *\[I* - *n\:l e1ef\]* using i and the definition of Q .  
# •  
Now we have what is needed to show that *Et+l* satisfies the ellipsoid property.  
Proposition 2.8  
i. *D 1+1 is positive definite.*  
ii. *vol\(Et+l\)/vol\(Et \)* \~ *e-1I2\(n+l\).*  
iii. *H t* = *E t* n *\{x\: dx* \~ *dxt\}* s; *E t+ 1 .*  
*Proof* i. From statement vi of Proposition 2.7, we see *thatDl+l* = *QT I'\:,.Q,* where I'\:,. is a  
diagonal matrix with positive diagonal entries *0;* for *i* = 1, ... *,n.* Let *1'\:,.112* denote the  
diagonal matrix with diagonal entries *0\)12* for *i* = 1, ... , *n.* It follows that 2\. The Ellipsoid Algorithm 153  
ii. Using Propositions 2.5 and 2.6, we have  
where the inequality is derived by two applications of the standard inequality \(l + *a\)* .-;;; *ea*  
for all *la I.*  
iii. Under the transformation, we have  
and  
It follows that *T\(Ht \)* !;; *T\(Et+l \)* since 0 .-;;; \~l .-;;; 1 for \~ E *T\(Ht \).* Applying Proposition 2.3  
to the inverse transformation of *T,* it follows that *H t* !;; *El+l. •*  
***Example*** 2.1. *P\<* = *\{x* E *R2\: Xl* + *X2* \< 2, - *2Xl* + *2X2* \< 1, - *X2* \< O\}.  
We suppose it is known that *Ix\}* I .-;;; 3 for *j* = 1,2 if *xE P\<* and that  
vol\(P\<\) \> 1\(\\0 if *P\<* \* 0.  
We take *Eo* = *\{x* E *R2\: rsxf* + rsx\~ .-;;; 1\} with *Xo* = \(0 0\) and *Do* = e\~ l\~\) so that  
vol\(Eo\) .-;;; vol\{x E *R2\: Ix\}* I .-;;; 3 for *j* = 1, 2\} = 81.  
We then calculate *t\** = *\[2\(n* + 1\) \(log 81 - log 160\)\] = \[6 loge81OO\] = 54.  
The numerical calculations of the iterations are given below. The shrinking of the el-  
lipses is shown in Figure 2.5 and the solutions are shown in Figure 2.6.  
*Iteration O. Xo* = \(0 0\) \$. *P\<* because *-2Xl* + *2X2* \< -1 is violated.  
Using the updating formulas \(2.1\) and \(2.2\) with *d* = \(-2 2\) and *D* = *eg* 1\~\) gives  
\( 16  
*Xl* = \(1 -1\) and *Dl* = 8 154 *Iteration* 1. Xl = \(l 1.6\. Polynomial-Time Algorithms for Linear Programming  
-1\) \$. *P\<* because *-X2* \< 0 is violated.  
*Iteration* 2. *X2* = \(i 1\) \$. *P\<* because Xl + *X2* \< 2 is violated.  
\( 11.06 -1.58\)  
*X3* = \(0.41 -0.30\) and *D3* = -1.58 6.32  
*Iteration* 3. *X3* = \(0.41 -0.30\) \$. *P\<* because *-X2* \< 0 is violated.  
*Iteration* 4. *X4* = \(0.20 0.54\) \$. *P\<* because -2XI + *2X2* \< -1 is violated.  
*x5=\(1.37* 0.27\) and D5=\(\~"\~\~  
1.60\) 3.16 .  
*Iteration* 5. Since *X5* E *P\<,* the algorithm terminates.  
Note that in contrast to the subgradient algorithm of Section I.2.4, the steps *XI* - XI+I  
taken in the ellipsoid algorithm are not normal to the violated inequality *ai\(t\)x* \< *bi\(t\)*  
except in special cases such as when the ellipsoid *EI* is a sphere.  
Now we describe how the ellipsoid algorithm can be modified to find nearly optimal  
solutions to a linear program. For convenience we will distinguish between the problem of  
maximizing *ex* over an arbitrary polytope  
Figure 2.5 2. The Ellipsoid Algorithm 155  
\(2.3\) ZLP = max\{cx\: *X* E *P\}*  
and the linear program where *P* is explicitly described by a set oflinear constraints  
\(2.4\) ZLP = max\{cx\: *x* E *P\},* where *P* = *\{x ERn\: Ax* \~ *b\}.*  
In both cases we assume that *P* is nonempty and bounded and that *P\<* \* 0. Then by first  
running the ellipsoid algorithm for *P\<* we can determine an initial point *ao* E *P\<.* We then  
set *Xo* = *ao* and consider the strict inequality system *P\<* n *\{x ERn\: -cx* \< *-cxo\}.*  
The idea behind the modification is simple. Every time a better feasible point *XI* E *P\<* is  
found, we take *P\<* n *\{x ERn\: -cx* \< *-CXt\}* as our new strict inequality system and reapply  
the ellipsoid algorithm. This approach, called the *sliding objective function method,* has  
the nice feature that the algorithm is always being applied to a feasible system unless *XI* is  
an optimal point.  
The Sliding Objective Function Approximate Ellipsoid Algorithm for \(2.1\)  
*Initial assumptions\:* A feasible point *ao* E *P\<* is given. There exists a sphere *S\(ao, r\)* C *P.*  
There exists a sphere *S\(ao, s\)* \:J *P.* A value for E is chosen.  
*N* = *2n\(n* + 1\) r log 2s;\~clIl  
*Initialization\: Xo* = *ao, Do* = *s2I,* \(0 = *cao, t* = O.  
*Iteration t\:* If *XI* \$. *P\<,* set *d* = *ai\(t\),* where *ai\(t\)xI* \~ *bi\(l\)* and \(1+1 = \(I' If *Xt* E *Pq* set  
*d* = *-c,* \(1+1 = max\{\(t, *cXI\),* and *x .... XI* if *CXI* \> \(I' Use formulas \(2.1\) and \(2.2\) to obtain  
*XI+I* and *D I+I. 1ft\> N,* stop *x* E *P\<* and *cx* \~ ZLP - E. Otherwise set *t* .... *t* + 1.  
To analyze this algorithm we need the following results on volumes.  
2  
*Xo*  
--\~--\~--------\~--------------Xl 2 3  
Figure 2.6 156 1.6\. Polynomial-Time Algorithms for Linear Programming  
*y*  
Figure 2.7  
Proposition 2.9  
a. vol\(sn+I\)/vol\(sn\) \< *2n/n.*  
b. *vol\(E\(D, y» =.J* Idet *D* Ivol\(sn\).  
c. *IfC is a cone with vertex y, and H is a hyperplane intersecting* C, *then the truncated*  
*cone TC* = *conv\(\{y\},* C n *H\) with base* C n *H and vertex y has volume given by*  
1  
*vol\(TC\)* = - vol\(C n *H\)d\(y, H\), where d\(y, H\)* = min\{lIy - *zll\: z* E *H\}*  
*n*  
\(see Figure 2.7\).  
Theorem 2.10. *When the approximate ellipsoid algorithm terminates, we have x* E *P and*  
*eN* = *eX* \~ ZLp - E.  
*Proof.* We use the volume argument given in the proof of Theorem 2.1. Suppose *x\** is  
an optimal solution to LP. Because *P\<\:\:\:\> S\(ao, r\),* the initial feasible region  
*P\<* n *\{x ERn\: cx* \> *cao\}* contains the truncated cone *TC* with base *S\(ao, r\)* n *\{x\: cx* = *cao\}*  
and vertex *x\*,* and after *N* iterations the final ellipsoid *EN* contains the truncated cone  
*TC'* = *TC* n *\{x ERn\: cx* \> *eN\}'* By Proposition 2.9, we have  
1 *c\(x\** - *ao\)*  
*vol\(TC\)* = *n vol\(S\(ao, r\)* n *\{x\: cx* = *cao\}\)* Ilcll '  
since the distance from *x\** to the hyperplane *cx* = *cao* is *c\(x\** - *ao\)/* IIcli. Also  
*vol\(S\(ao, r\)* n *\{x\: cx* = *cao\}\)* = *rn-*  
*I* vol\(sn-I\),  
since the intersection of a sphere with a hyperplane through its center is again a sphere with  
the same radius but of one dimension less. Finally  
*vol\(TC'\)* = \( ZLp - *eN\)n vol\(TC\),*  
ZLP - *cao*  
since the height of *TC'* is \(ZLP - *eN* \)/\(ZLP - *cao\)* times the height of *TC.* Hence 2. The Ellipsoid Algorithm 157  
*vol\(TC'\)* =!\( ZLp - *CN\)n rn-I* vol\(sn-I\) *cx·* - *cao.*  
*n* ZLP - *cao* IIcll  
Since *TC'* £; *EN,* it follows that  
*vol\(TC'\)* \:s\:\:; *e-NI2\(n+l\) vol\(Eo\)* = *e-NI2\(n+l\)* snvol\(sn\).  
Therefore  
\( \_ *r* \) *,,\:\:\:\:-NI2n\(n+l\) n* vo II Ili/n ZLP - *cao*  
\( l*\(sn»\) lin* \( \) *\(n-I\)ln*  
ZLp *'oN* "" *e* s vol\(sn-I\) *cr'*  
Also  
ZLP - *cao* = *c\(x\** - *ao\)\:S\:\:;* Ilcllllx\* - *aoll* \:s\:\:; s Ilcll,  
and hence  
*Z* \_ *r* \:s\:\:; *e-NI2n\(n+l\)* S2 Ilcll *\(n* VOI\(\~n»\)l/n \< *2e-N/2n\(n+l\)* S2 Ilcll.  
LP *'oN r* vol\(sn I\) *r*  
Since *N* = *2n\(n* + I\) \[log \(2 S2 Ilcll *Ire\)\],* it follows that ZLp - *CN* \< *e.* •  
Several results are needed to show that an ellipsoid algorithm solves the linear  
programming problem in polynomial time.  
We must deal with the precision of the arithmetic calculations. Square roots occur in  
the ellipsoid updating formula \(2.2\), and the assumption we have made so far is that these  
irrational numbers are found exactly. But, of course, this precludes the possibility of digital  
calculation, which requires finite representation of numbers. Thus we must specify a  
maximum number of digits permitted in the calculations. In particular, we now assume  
that the initial data and all intermediate numbers produced during the calculations are  
rational numbers represented by the ratio of two integers, each of which is specified with *p*  
binary digits of precision.  
With finite precision calculation it is still possible to obtain an approximate solution to  
linear programming problems. We need, however, to ensure that the feasible region, if any,  
remains inside the half-ellipsoid, given the numerical errors produced by the finite  
precision. This is done by using slightly larger ellipsoids. Then we compensate by using a  
larger number of iterations.  
The algorithm so modified is called the *finite precision approximate ellipsoid algo-*  
*rithm.* The following theorem, which we will not prove, gives the precision and number of  
iterations required provided there exists *ao* such that *S\(ao, r\)* C *P* C *S\(ao, s\).*  
Theorem 2.11. *When N* = *4n2\[log* \(2s211cll *Ire\)\], p* = *5N,formula* \(2.2\) *is replaced by*  
\(2.5\) *D* \_ *2n2* + *3\(D* 2 *\(Dd\) Ddf\)*  
1+1 - *2n2* - *n+1 dTDd '*  
*and D1 +* I *and Xl+* I *are calculated to p binary digits of precision, the finite precision ellipsoid*  
*algorithm applied to* \(2.3\) *terminates with a solution x* E *P such that eX* \~ ZLp - *e.* 158 1.6\. Polynomial-Time Algorithms for Linear Programming  
The precise values of Nand *p* are not important in this theorem. What is important is  
that *N* andp are polynomial functions *ofn,* log *s,* log *r,* log lIell, and 10g\(1/E\). In addition,  
the amount of calculation needed to update *X t* and *Dt* at each iteration is polynomial in *n*  
and *p.* Furthermore, the theorem applies to \(2.4\) provided a violated inequality  
*ai\(t\)x* \:\:\:;; *bi\(t\)* can be expressed with *p* digits of precision \(independently of the question of  
how it is found\).  
Now we consider how to relate the values of rand *s* to the initial description of the  
polytopeP.  
*Definition* 2.5. *T* is the largest numerator or denominator of any component of an  
extreme point of *P. T'* is the largest numerator or denominator of any component of a  
facet-defining inequality. \(The components of these vectors are rationals expressed as the  
ratio of integers.\)  
Propositi01l2.12. *For any full-dimensional polytope P, the following statements are true\:*  
1. *T'\:\:\:;; \(nTt2+n.*  
11. *T\:\:\:;; \(nT'\)n,*  
iii. *There exists ao* E *P sueh that S\(ao, r\)* C *P* C *S\(ao, s\) with r* = *\(nT\)-2n2 -2n and*  
*s* = *2nT.*  
*Proof* i. Suppose *ax* \:\:\:;; *b* is a facet-defining inequality of *P.* Without loss of generality,  
we assume that *b* = ±1 or O. Then there are extreme points *Xi* for *i* = 1, ... , *n* such that *a*  
is the unique solution to *axi* = *b* for *i* = 1, ... , *n.*  
Now we can write *xj=Pij/qij* with Ipijl, Iqijl integers not exceeding *T.* Taking  
*Oi* = ITJ=I *qij,* the system *\(aOi\)xi* = *Oib, i* = 1, ... *,n,* has integer coefficients bounded in  
magnitude by *Tn+l.* Then by Cramer's rule we have *aj* = *Pj/q,* where *Pj* and *q* are integers  
with *Ipjl,* Iql \:\:\:;; *n! \(Tn+lt* \< *\(nT\)n2+n.*  
ii. The proof is similar to i using polarity.  
iii. For all *x* E *P,* we have IXi I \:\:\:;; *T.* Hence *\(x* - *aoV\(x* - *ao\)* \:\:\:;; *n\(2T\)2* \< *\(2nT\)2,* and  
*Pc S\(ao, s\)* with s = *2nT.*  
Finally we show that there is an inscribed sphere of the given radius. Take *n* + 1 affine1y  
independent extreme points *\{xi \}7,\:l,* and let *ao* = *\[1/\(n* + 1\)\] '£7\:/ *Xi.* Clearly, *ao* E *P\<.* The  
distance from *ao* to any facet *ax* = *b* is *\(b* - *a Tao\)/* lIall. Our goal is to find a lower-bound *r*  
on this distance, since this will provide us with the imbedded sphere *S\(ao, r\).* Since  
*\(a, b\)* E *zn+l,* it follows that *b* - *aTao;\:\: 1/1f/,* where If/is the common denominator' of the  
components *aOj* of *ao.* Since *xj* = *p/q* with *p, q* E Zl, it follows that  
Ip I, Iq I \:\:\:;; *T, aOj* = *pj/q'* with *pi, q'* E zl, and Iq'l \:\:\:;; *\(n* + l\)rn+1• Thus *ao* = \(Pi\: ... ,  
p\~'\)/ql/ with *Iq"l* \:\:\:;; *\(n* + *l\)nrn2+n\:\:\:;; \(nT\)n2+n.* Hence *b* - *aTao;\:\: 1/1f/= 1/q";\:\: \(nT\)-n2 -n.*  
Now lIall \:\:\:;; *n\(nT\)n2 +n,* and for each *ai* we have *lai* I \< *\(nT\)n2+n.* Hence *\(b* - *aTao\)/llall*  
*\> \(nTt2n2-2n* = *r. •*  
We have established that log *s* and log *r* are polynomial in *n* and log *T,* or equivalently  
in *n* and log *T'.* Hence Nand *p* in Theorem 2.11 are polynomial in *n,* log *T,* log lIell, and  
log \(1/E\)  
To convert the E-approximate solution obtained in Theorem 2.11 to an optimal  
solution, we perturb the objective function of\(2.3\) so that the resulting linear program has  
a unique solution. 2. The Ellipsoid Algorithm 159  
**Proposition** 2.13. *Given P, let Q* = *2T2 n and* c' = *Qnc* + \(1, *Q,* ... , *Qn-l\). Then the linear*  
*program*  
\(2.6\) ZLP' = max\{c'x\: *x* E *P\}*  
*has a unique optimal solution x\*, and x\** is *an optimal solution of\(2.3\),*  
*Proof* Let Xl be some other vertex of *P.* Letting *x\** = *\(Pf/q7,* ... ,p\~/q\~\) Xl = *\(Pl!ql,* ... ,p\~/q\~\), we have  
and  
Hence we can write *x\** - Xl = *z/ex,* where *ex* \< *T 2n*  
*,* and Z is an integer vector with  
I *Zj* I \< *2T2n* for all\}.  
Since *x\** is optimal,  
But because I *Zj* I \< *Q* for all\}, it follows that  
Since *Qncz* \~ - LJ\~1 *Qj-Iz,* and cz is integer, we have cz \~ O. Hence we have shown that  
*cx\** \~ *exl* for any extreme point Xl, and *x\** is optimal to \(2.3\).  
Finally we observe that because Z \* ° and I *Zj* I \< *Q* for all\}, LJ\~1 *Qj-l Zj* \* 0\. This implies  
that *c'\(x\** - Xl\) \* 0, and hence *x\** is the unique optimum of\(2.6\). •  
The next step is to show that if we choose 8 appropriately in the finite precision  
approximate ellipsoid algorithm we can get very close to *x\*.*  
**Proposition 2.14.** *Ifthefinite precision approximate ellipsoid algorithm* is *applied to \(2.6\)*  
*and* E = *\(1/4n\)r-4n*  
*2*  
*-*  
*, the algorithm terminates with x* E *P satisfying c'x* \~ hp' - E *and*  
Ilx\* - xii \~ *1/2T2, where x\** is *an extreme point optimal solution of\(2.3\).*  
*Proof* Since *x* E *P, x* is a convex combination of extreme points, say\{x;\};\~l' that one of these extreme points is *x\*.* If not, let *cx* I = min;cxi. Then  
We claim  
*c'x\** - *c'x* \~ *e'x\** - *C'XI* =! *c'z* \~ ! \> \_l\_  
*ex ex T 2n'*  
contradicting *c'x\** - *c'x* \~ E. 160 1.6\. Polynomial-Time Algorithms for Linear Programming  
Therefore we can write *x* = l\:7=1 *AiXi* + *A..x\** with l\:7=1 *Ai* + A\* = 1, *Ai* \~ 0 for *i* = 1, ... , *n,*  
and A\* \> O. Now observe that since *c/\(x\** - *Xi\)* \> *1/T2 n* for all extreme points *Xi* '\* *x\*,* it  
follows that  
*n*  
*c'x* = I *C'AiXi* + *C'A..x\**  
*i=1*  
\< Z A\{ *c'x\** - *i2n* \)+ *C'A..x\**  
*= c'x\** - *i2n* \(1 - *A\*\).*  
But since *c'x* \~ *c'x\** - *e,* it follows that *e* \> *\(1/T2n\)* \(1 - *A\*\).*  
Now  
for some *yEP,* since l\:?=1 *Ai* = 1 - *A\*,* and *y* is thus a convex combination of *Xi* for *i =*  
1, ... *,n.* However, *Ily* - *x\*1I* \~ *2nT2 n,* and therefore  
*IIx* - *x\*1I* \~ *eT2 n 2nT2n* \< 2\~2'  
# •  
Proposition 2.15. *Let x and x\* be as described in Proposition* 2.14. *The vector obtained by*  
*rounding each coefficient Xj of x to the nearest rational multiple of\{* 1, *liz,* ... , liT\} *is x\*.*  
*Proof* Since *x\** is an extreme point of *P,* it follows that *xjis* some rational multiple of  
\{l, *liz,* ... , 1/T\} for all *j* = 1, ... *,n.* Suppose *x\** is not obtained as described. Then  
*xj- Xj* \> *1/2T2* for *somej,* and hence *IIx* - *x\*1I* \> *1/2T2,* contradicting Proposition 2.14 .•  
One approach to finding *x\** from *x* is by the method of continued fractions. In Section  
I. 7 .3, we will describe this method and show that it can be executed in time polynomial in  
*n* and log *T.*  
We have established how the finite precision approximate ellipsoid algorithm produces  
an optimal solution to \(2.3\) or \(2.4\) when *P* is a full-dimensional polytope. We have also  
shown that each step \(except that of finding a violated inequality\) requires a number of  
calculations that are polynomial in *n,* log *T,* and log IIcll \(see Theorem 2.11 and  
Propositions 2.13-2.15\). For the linear program \(2.3\), this number is polynomial in the  
length of the input description, since to find a violated inequality we simply check whether  
the p-digit number *Xl* satisfies each constraint. This requires *O\(mnp\)* calculations. Hence  
we obtain the following theorem.  
Theorem 2.16. *There is a polynomial algorithm to solve the linear programming problem*  
*over full-dimensional polytopes with description \(m, n, A, b, c\).* 3\. The Polynomial Equivalence of Separation and Optimization  
161  
3. THE POLYNOMIAL EQUIVALENCE OF SEPARATION AND  
OPTIMIZATION  
The importance of the ellipsoid algorithm in combinatorial optimization is as a tool to  
prove that certain problems can be solved in polynomial time.  
To illustrate this idea, consider the minimum-weight *s-t* cut problem \(see Section  
1.3.4\). Given a graph G = *\(V, E\)* with I *E* I = *n,* s, *t* E *V,* and a nonnegative weight vector  
c E R\~ on the edges of G, the problem is to find a minimum-weight set of edges that  
intersects every *s-t* path in G. Let *J\{* be the family of *s-t* paths in G.  
One way to formulate this problem is as the linear program  
min I *CeYe*  
*eEE*  
\(3.1\)  
I *y e* \~ 1 for *Kj* E *J\{*  
*eEKj*  
o \:\:\:;; *Ye* \:\:\:;; 1 for *e* E *E.*  
This is true because *y* E *Bn* is a feasible solution if and only if it is the incidence vector of an  
*s-t* cut and, as will be shown in Section III.1.6, all of the extreme points of the polytope  
defined by the constraint set of \(3.1\) are in *Bn.*  
There is one difficulty in solving \(3.1\) by the ellipsoid algorithm given in Section 2. This  
is that I *J\{* I is an exponential function of *n,* which means that the feasibility of a point *y\**  
determined from an iteration of the ellipsoid algorithm cannot be decided efficiently by  
the usual method of substitution. However, in this case there is an alternate way of  
checking feasibility.  
Suppose 0 \:\:\:;; *y\** \:\:\:;; 1, since this can be checked by substitution. Consider the problem of  
finding a minimum-weight *s-t* path with weight vector *y\*.* This can be done efficiently by  
the algorithm given in Section 1.3 .2\. Let \<! be the weight of a minimum-weight path. Then  
l\:eEKj *yj* \~ 1 for all *Kj* E *J\{* if and only if \<! \~ 1. Moreover, if \<! \< 1, any minimum-weight  
path yields an inequality that is most violated by *y\*.*  
So for this problem we have overcome the apparent difficulty of a large number of  
constraints by providing an efficient subroutine that implicitly checks feasibility and  
provides a violated inequality when the point in question is not feasible.  
This example motivates the separation problem, which combines both the membership  
and validity problems.  
*The Separation Problem/or a Family o/Polyhedra.* An instance is given by an integer *n,*  
a description of a polyhedron *P* s;; *R n* in the family, and an x\* E *Rn.*  
A solution is an answer to the membership problem and, if x\* ff\: *P,* a valid inequality  
*\(n, no\)* for *P* such that *nx\** \> *no.* \(Note that the separation problem is not a feasibility  
problem because it requires that we exhibit a valid inequality.\)  
Our objective here is to relate the complexity of the separation problem to the  
complexity of the linear programming problem over the family of polyhedra.  
We now formally describe the linear programming problem.  
*The Linear Programming Problem/or a Family o/Polyhedra.* An instance is given by  
an integer *n,* a description ofa polyhedron *PC R n* in the family, and acE *Rn.* Assuming  
that *P* =/= 0 and cx is bounded for all x E *P,* a solution is an X O E *P* such that  
*cxo* = max\{cx\: x E *P\}.* 162 1.6\. Polynomial-Time Algorithms for Linear Programming  
The principal result is that, under certain technical assumptions, the linear program-  
ming problem for a family of polyhedra is solvable in polynomial time if and only if the  
separation problem is solvable in polynomial time.  
As in the previous section, we confine the analysis to full-dimensional polytopes. But  
here we do not assume that the number of constraints is part of the description of *P.*  
Instead, we assume that for *P eRn* the length of the input *I* needed to encode *P* is bounded  
from below by a polynomial in *n* and log *T,* where *T* is given in Definition 2.5. This  
assumption enables us to work with the parameters nand *T* as well as to establish the  
polynomiality of an algorithm by showing that its running time is polynomial in *n* and log  
*T.* Note that by Proposition 2.l2 this assumption is equivalent to assuming that the length  
of the input needed to encode *P* is bounded from below by a polynomial in *n* and log *T'*  
*\(T'* is also given in Definition 2.5\).  
This assumption is reasonable for the families of polytopes of interest to us, because if  
log Twas superpolynomial in the true input length, we would have no hope of describing  
an optimal solution to the linear programming problem over the family in polynomial  
time.  
More specifically, consider the case where we are dealing with a family of full-  
dimensional polytopes where either *P* = *\{x ERn\: Ax* s *b\}* or *P* = conv\(S\) with  
S = *\{x* E *ZZ\: Ax* s *b\}* and where the standard input is the *m* x *\(n* + 1\) integer matrix  
*\(A, b\).* The input length needed to describe these problems is *1= O\(mn* log *e\),* where  
*e* = max\(maxij I *aij* I, max; I *b; I\).*  
When *P* = *\{x ERn\: Ax* s *b\},* by Proposition 3.l of Section I.5.3, the largest value that  
can be taken by the numerator or denominator of any extreme point is *T* = *\(net.* Hence,  
since log *T* = *n* log *e* + *n* log *n,* it follows that log *T* is certainly, at most, a polynomial  
function of *I.* A similar result holds when *P* = conv\(S\) \(see Theorem 4.l of Section I.5.4\).  
As illustrated by problem \(3.1\), it may not be efficient to solve the separation problem  
by substitution. Moreover, if *x* f\{\:. *P,* we need to establish that we can find a violated  
inequality whose encoding length is polynomial in *I.* But this follows, since if *x* f\{\:. *P,* then  
*x* does not satisfy some facet-defining inequality whose encoding length is *O\(log\(nT'\)\).*  
Moreover, a facet-defining inequality can be described exactly using the precision speci-  
fied in Theorem 2.l1.  
In the previous section we gave an ellipsoid algorithm in which each step, except that of  
finding a violated inequality, requires a number of calculations that are polynomial in *n,*  
log *T,* and log IIcli. We have just seen that a violated inequality can be described by a  
polynomial in *n* and log *T,* and it is easily checked that this meets the requirement of the  
precision required in the algorithm.  
The only step that remains is to solve the separation problem at each iteration.  
Immediately we can conclude that Theorem 2.l6 generalizes to\:  
Theorem 3.1. *Given a family of full-dimensional polytopes P\(n, T\) whose description*  
*length is at least a polynomial in n and* log *T, if the separation problem over the family is*  
*solvable in polynomial time, then the linear programming problem over the family is*  
*solvable in polynomial time.*  
We can also make use of polarity to give a polynomial algorithm for the separation  
problem based on a polynomial algorithm for the linear programming problem.  
Theorem 3.2. *The following statements are equivalent for a family of full-dimensional*  
*polytopes having the origin in their interior and whose input length is at least a polynomial*  
*in n and* log *T.* 3. The Polynomial Equivalence of Separation and Optimization 163  
a. *There is a polynomial-time algorithm for the separation problem.*  
b. *There is a polynomial-time algorithm for the linear programming problem.*  
c. *There is a polynomial-time algorithm for the separation problem over the family of*  
*l-polars.*  
d. *There is a polynomial-time algorithm for the linear programming problem over the*  
*family of l-polars.*  
*Proof a* \~ *b.* This is Theorem 3.1.  
*b* \~ c. We apply the results of Section 1.4.5 to relate the optimization problem for *P* to  
the separation problem for its I-polar III. By Proposition 5.4, III is also full-dimensional  
and bounded and contains 0 in its interior.  
Given *n\* ERn,* let x\* be an optimal solution to max\{n\*x\: x E *Pl.* By Corollary 5.6,  
*n\** E III if and only if *n\*x\** \:s 1. If *n\*x\** \> 1, then by Theorem 5.5 it follows that *\(x\*,* 1\) is a  
valid inequality for III that cuts off *n\*.* Finally, by Proposition 2.12, *T'* is a polynomial  
function of *n* and log *T,* so the length of the input needed to described III is polynomial in  
*n* and log *T.*  
c \~ *d.* Because III is a full-dimensional polytope, we can apply Theorem 3.1 to the  
family of I-polars.  
*d* \~ *a.* The proof is the same as *b* \~ c with the roles of *P* and III interchanged. •  
Now observe that any family offull-dimensional polytopes can be translated so that the  
origin becomes an interior point. The above argument then shows the equivalence of  
statements a and b for full-dimensional polytopes. This result extends to arbitrary rational  
polyhedra, though certain steps of the algorithms need modification, and for unbounded  
polyhedra we need to redefine *T* to include extreme rays.  
The following theorem justifies what we call the *equivalence of separation and optimi-*  
*zation* throughout this book.  
Theorem 3.3. *For a family of rational polyhedra P\(n, T\) whose input length is at least*  
*polynomial in n and* log *T, there is a polynomial-time reduction of the linear programming*  
*problem over the family to the separation problem over the family, and conversely there is a*  
*polynomial-time reduction of the separation problem to the linear programming problem.*  
Theorem 3.3 implies that the linear programming problem is solvable in polynomial  
time if and only if the separation problem is solvable in polynomial time. In Part III we  
will use this result to develop polynomial-time algorithms for some combinatorial  
optimization problems by giving polynomial-time algorithms for the separation problem.  
In contrast to the minimum-cut example, we will see an example where this provides the  
only known polynomial-time algorithm.  
Theorem 3.3 also implies that the linear programming problem is ,N"g\}l-hard if and only  
if the separation problem is ,N"9P-hard. However, as we shall see in Chapters II.5 and 11.6,  
separation is extremely important in the solution of ,N"g\}l-hard optimization problems  
where we know some classes of strong valid inequalities and are able to solve or  
approximate the solution of the separation problem efficiently.  
We close this section by using Theorem 3.3 to show that the linear optimization  
problem over the fractional node packing polytope is ,N"9P-hard.  
*Example* 3.1 *\(Example* 7.2 *of Section 1.5.7 continued\).* The I-polar of the fractional  
node-packing polytope is a polytope whose extreme points are the incidence vectors of the  
maximal cliques of G. Hence the linear programming problem over the I-polar is *,N"9P-*  
hard because it is equivalent to the maximum-weight clique problem on G or the 164 1.6\. Polynomial-Time Algorithms for Linear Programming  
maximum-weight node-packing problem on the complement of G. Now, by polarity, the  
separation problem over the fractional node-packing polytope is .Ngp-hard. Finally, by  
Theorem 3.3, we can reduce the linear program problem to the separation problem, so the  
linear programming problem over the fractional node-packing polytope is also .Ngp-hard.  
4\. A PROJECfIVE ALGORITHM FOR LINEAR PROGRAMMING  
Recently, remarkable claims have been made concerning the computational efficiency of  
a new algorithm based on projections. Here we describe a conceptually simple variant of  
that algorithm whose geometric rate of convergence is easily established. Our objective is  
just to show the significance of projections in solving linear programs. We neither claim  
that the version of the algorithm given here is efficient, nor do we prove polynomiality.  
We will need the formula for the projection of a vector onto a subspace.  
Proposition 4.1. *Let A be an m* x *n matrix, with* rank\(A\) = *m and H* = *\{x ERn\: Ax* = O\}.  
*The projection olp onto H is given by q* = *\[J* - *AT\(AATtIA\]p.*  
*Proof q* E *H* so *Aq* = O. Also, since *p* - *q* E Hi, by Proposition 1.6 of Section 1.4.1,  
there exists *u* E *R m* such that *A T u* = *P* - *q.* Therefore  
*AA T U* = *Ap* - *Aq* = *Ap* and *u* = *\(AA TtlAp*  
because *AA T* is nonsingular when rank\(A\) = *m.*  
Hence *q* = *p* - *ATu* = *\[J* - *AT\(AA TtIA\]p.* •  
We first apply the projective algorithm to the homogeneous feasibility problem.  
*Homogeneous Feasibility Problem.* Given an integer *m* x *n* matrix *A* with  
rank\(A\) = *m,* find a ray *rEP* \\ \{O\}, where *P* = *\{r* E R\~\: *Ar* = a\}, or show that *P* = \{O\}.  
The algorithm works with candidate rays *rk* \> 0 for *k* = 1, 2, ... and attempts to satisfy  
*Ar* = O. Suppose *rl* = 1 \$. *P* because *AI",* O. To obtain a point closer to being a solution  
than *rl,* we attempt to preserve nonnegativity by finding the closest point to 1 that satisfies  
*Aq* = O. This is the problem min\{III - qll\: *Aq* = O\}. Its solution *isq,* the projection of! onto  
*Aq =0.*  
Now if *q\:2\:\:* 0, then *q* E *P* and we are done. Otherwise \(unless we can deduce that  
*P* = 0\), we modify *q* to obtain a vector *q'* \> 0 that is "closer" to being a solution in *P* than  
the initial vector 1. We take a positive linear combination of *q* and 1, giving *q'* = 1 + *aq*  
with *a\>* O. *a* must be chosen so that *q'* \> 0, so *a* cannot be chosen arbitrarily large.  
Alternatively, the larger *a* is, the "closer" *q'* is to *q* and hence to satisfyingAq = O. We have  
described one iteration of the algorithm from the point *rk* = 1 to the new point  
*rk+1* = 1 + *aq.*  
We observe that even if *rk* '" 1, the same iterative step can be applied when *rk* \> 0 but  
*rk* \$. *P.* Let *Dk* be a diagonal matrix with *dt* = *r\}* \> 0 *forj* = 1, ... , *n,* letA *k* = *ADk,* and let  
*p k* = *\{r* E R\~\: *Akr* = O\} be a cone. Clearly *P* '" \{O\} if and only if *p k* '" \{O\}. Also, *Ark* = *Ak1.*  
Now we can describe iteration *k* in terms of the feasibility problem for *pk.* A candidate  
vector 1 is given. If *Akl* '" 0, so 1 \$. *pk,* we derive a new vector *q'* \> 0 that is "closer" to  
being a solution of *pk.* Let *q'* = 1 + *akqk,* where *ak* \> 0 and *qk* is the projection of 1 onto 4\. A Projective Algorithm for Linear Programming 165  
*Akq* = O. Restated with respect to *P,* the new point is *rk+1* = *Dk\(l* + *akqk\)* because  
*Ark+1 =Ak\(1* + *akqk\) =Akq'.*  
We need to make precise what it means for *rk+1* \> 0 to be "closer" to the cone *P* than  
*rk* \> O. Comparing the values of *Ark+1* and *Ark* is relatively meaningless. To obtain a useful  
measure of comparison we need to work with a homogeneous version of *r,* such as  
*r r*  
*\(l/n\) 1\:J=1 rj* or \(Ili=1 *rj \)l/n'*  
which is invariant under the transformation *r* .... *A.r* with *A.* \> O. Throughout this section we  
use the homogeneous version  
\~ *r*  
*r* = \(Il'! *.\)l/n·*  
*\:/=1 r,*  
Thus when comparing violations, if *IAi'-k+11* \< *lArk* I, it makes sense to say than *rk+1* is  
closer than *rk* to being a ray of *p,*  
Without specifying *ak,* we can already analyze the basic behavior of an iteration of the  
algorithm.  
Proposition 4.2  
*n n*  
I '. "" a,;;r,\~+1 = "" *a,;;r,k flor z'* - 1 *m*  
*L, L,* - ,00', .  
j=1 j=1  
*where*  
*Proof*  
1. Substituting *rj+1* = *rj\(l* + *akqj\)* yields  
*n*  
= I *aijrj* since *Akqk* = O.  
j=1  
*11. rj /rj+1* = 1/\(l + *akqj\).* Therefore  
*\[ n* \( *rk \)Jl1n \[n* \( 1 *\)Jl1n*  
n -\:k = n *k =fik*  
*j=1 rj* j=1 1 + *akqj\)* 166 and by i it follows that  
1.6\. Polynomial-Time Algorithms for Linear Programming  
= C\~ *aurJ* \)\[J\} \(,\]+1\) *rn*  
= C\~ *au'J* \)\[J\} *\(\:f1\) rn*  
# •  
Hence we see that from one iteration to the next, the values of all the terms of *A ,k*  
change by a constant factor *Pk.* By specifying a choice of *ak* for which *Pk* \< *P* \< 1, we will  
obtain an algorithm with the property that *A ,k* .... 0 geometrically.  
The Projective Algorithm to Find an e-Approximate Ray  
*Iteration k*  
*Step* 1\: If I LJ\~l *au,J* I \~ *e* for all *i,* stop. *,k* is an e-approximate ray.  
*Step* 2\: Find the projection *qk* of 1 onto the subspace *A kq* = O.  
*Step* 3\: If *qk* \~ 0, stop. *r* = *Dkqk* E *P* \\ \{O\}.  
*Step* 4\: *IfmaxjqJ* \< 1, stop. *P* = \{O\}.  
*Step* 5\: Find a point *ak* in the set  
*\{a* E *Rl\:* 1 + *aqJ* \> 0 for *j* = 1, ... , *n, Pk* = I1 1 *\[ n* \( 1 *\)\]lln*  
*k* \~ *P* \< n.  
\}\~1 + *aq\}*  
*Step* 6\: *rJ+1* = *rJ\(1* + *akqJ\)* for *j* = 1, ... *,n,*  
*A k+1* = *ADk+1.*  
Set *k* \<-- *k* + 1 and return to Step 1.  
The validity of Step 4 and the feasibility of Step 5 require verification. First we consider  
Step 4.  
Proposition 4.3. *I/max;qJ* \< 1, *there is no ray in P* / \{a\}.  
*Proof* Since *qk* is the projection of 1 onto *H* = *\{q\: Akq* = a\}, it follows that  
1 - *qk* E *H-\\* and hence there exists *u* E *R m* such that *uA k* = 1 - *qk* \> 0 or  
*uA* = \(1 - *qk\)\(Dkt1* \> O.  
Therefore by Farkas' lemma applied to the cone *P* = *\{r* E R\~\: *Ar* = a\}, we obtain  
*P* = \{O\}. • 4\. A Projective Algorithm for Linear Programming 167  
The following result, which is not difficult to prove, shows that for certain values of  
*P* \< 1, the set in Step 5 is nonempty and so the algorithm converges geometrically.  
Proposition 4.4. *Taking Cik* = 1/\(1 + *IIqkll\) as the step size in Step* 5 *of the projective*  
*algorithm, itfollows that rk+1* \> 0 *and*  
Theorem 4.5. *more than*  
*The Projective Algorithm to find an 8-approximate ray terminates after no*  
*iterations.*  
*Proof* After *k* iterations, we have  
\\ ± *aurj+l\\* = IT PI \\ ± *au \\* \:\:\:\:;, *pk\\* ± *au\\* for all *i.*  
J=I 1=1 J=I J=I  
With *k* as claimed, *pk* I I\:J=I *a* ij I \:\:\:\:;, 8 for all *i.*  
Now we consider the nonhomogeneous feasibility problem\:  
\(4.1\) Find *X* E *P,* where *P* = *\{x* E *RZ\: Ax* = *b\},* or show that *P* = 0  
Observe that if we take *r* = *\(x, rn+l\)* E *R n + 1* andA = *\(A, -b\),* we can apply the projective  
algorithm to the cone *P* = *\{r* E RZ+I\: *Ar* = O\}.  
We suppose that *P* is bounded, so there does not exist *\(rio'* .. *,rn,* O\) E RZ+I \\ \{O\} such  
*thatAr* = O. Hence if the cone *P* has a nonzero ray *r,* then necessarily *rn+1* \> 0, and then  
*x* = *\(rdrn+h"" rn/rn+l\)* is a solution of \(4.1\). This suggests that at each iteration of the  
projective algorithm, we should normalize candidate rays so that *r* n+1 = 1. In other words,  
we will choose a normalization factor *Pk* so that r\~+1 = 1 at each iteration.  
The Projective Algorithm for Problem \(4.1\)  
*Initialization.* rl = 1, *DI* = *In+h Al* = *A, k* = 1.  
*Iteration k*  
*Step* 1\: *\(xk,* 1\) = *rk.* If *lArk* I \:\:\:\:;, 8, then *Xk* = *\(rt,* ... , r\~\) is an 8-approximate solution to  
\(4.1\) with *IAxk* - *b* I \:\:\:\:;, 8 and *Xk* \> O.  
*Step* 2\: This is unchanged.  
*Step* 3\: If *qk* \> 0, stop. *Xk* E *P* where, *xj* = \(l/q\~+I\)rjqj forj = 1, ... *,n.* 168 1.6\. Polynomial-Time Algorithms for Linear Programming  
*Step* 4\: Ifmaxj\~I .. " *.n+lqj* \< 1, stop. *P* = 0.  
*Step* 5\: This is unchanged except that *n* .... *n* + 1.  
*Step* 6\: *rj+1* = *Pkrj\(1* + *akqj\)* for *j* = 1, ... , *n* + 1, where *Pk* = 1/\(1 + akq\~+I\)' step is as before.  
The rest of the  
Now using Proposition 4.2, we can analyze the behavior of the algorithm on the  
feasibility problem \(4.1\). We also use the following result, which can be proved using  
estimates of the size of solutions as in Section 1.4.5.  
Proposition 4.6 *\(The Perturbation Lemma\). that if x* E R\~ *satisfies*  
*Given A and b, there exists e\(A, b\)* \> 0 *such*  
II *aijXj* - *bil\:s; e\(A, b\)* for *i* = 1, ... *,m,*  
J\~I  
*then there exists x\** E R\~ *satisfying Ax* = *b.*  
Proposition 4.7. *If the projective algorithm is applied to Problem* \(4.1\) *and rk+1* = *\(xk+* I, 1\),  
*then\:*  
1.1 I *aijxj+1* - *bil* = *Pkl* I *aijxj* - *bil for all i;*  
J\~l J\~l  
*11. There exists w\(A, b\)* E Rl *such that*  
I I *aijxj+l* - *bil* \:S; *Pkw\(A, b\)1* I *aij* - *bil for all i.*  
J\~l J\~l  
*Proof* We observe first that because  
*\[n+l* 1 *\]l/\(n* + I\)  
*Pt* = JJ 1 + *atq5* and  
we have  
*\[ n \(xJ+1 \)\]l/\(n* + I\) =  
*Pt* JJ *X\] Pt·*  
Statement i follows immediately from statement i of Proposition 4.2. Now substituting for  
*PI* in statement i, we obtain 4\. A Projective Algorithm for Linear Programming 169  
since *PI* \:\:\:; *p,* and because the geometric mean does not exceed the arithmetic mean. Now  
dividing both sides by "£'1=1 Xj+1 and setting *Yj* = Xj+I/"£J\~1 *xj+I* \> 0, we obtain  
If "£'1=1 Xj+1 is unbounded as *k* .... 00 , then as P" .... 0, there exists a *k* for which I "£J\~laijyj I  
\:\:\:; *E\(A,* 0\), where *E\(A,* 0\) is as in Proposition 4.6. *HenceAy* = 0, *y* E R\~, has a solution with  
*y* =f\: 0, contradicting the boundedness assumption. Hence "£'1=1 Xj+1 remains bounded, and  
the claim follows. •  
Note that this algorithm has the property that the violation in each constraint decreases  
at exactly the same rate.  
*Example* 4.1. We apply the projective algorithm to the feasibility problem  
-XI + *2xz\:\:\:; 4*  
5xI + *Xz\:\:\:; 20*  
-2xI - *2X2* \:\:\:; -7  
*ex* = 7xI + *2X2* \~ *A.*  
# xER;  
with *A.* = 30.  
Converting into equality form; we have *A* = *\(A -b\)* with  
-1  
- 5  
\(  
*A=* -2  
7  
2 1  
1 0  
-2 0  
2 0  
o 0 0  
1 0 0  
-4\) -20  
o 1 0  
7 .  
o 0 -1  
-30  
We use the starting point Xl = \(l 1 1 1 1 1\) and *rl* = \(Xl, 1\).  
*Initialization. DI* = h *Al* = A.  
*Iteration 1*  
*Step 1\:*  
*Step* 2\: The projection of 1 on *Aq* = 0 is the vector  
ql = \(0.9907 0.6622 0.8254 -0.1806 1.337 0.08659 0.2698\)  
*Step* 5\: al = 4.433, *pz* = 0.425, PI = 0.455. 170 1.6\. Polynomial-Time Algorithms for Linear Programming  
*Step* 6\: *r2* = *\(x2*  
*,* 1\) = \(2.455 l. 711 2.l22 0.091 3.l55 0.630 1\)  
3.423 2.l22 0 0 0  
\( -2.455  
12 = 12.277 1.711 0 0.0909 0 0  
-4.911 -3.423 0 0 3.l55 0  
17.l88 3.423 0 0 0 -0.6302  
-4 \) -20  
7 .  
-30  
*Iteration 2*  
-0.911\)  
2 -2 -5.921 1  
*Ax* - *b* = *A* 1 = \(  
l.821 = *Pl\(Ax* - *b\).*  
-10.02  
*Step* 2\: *q2* = \(0.8328 1.205 0.1850 0.7620 1.232 -0.1514 0.6178\)  
*Step* 5\: 0\:2 = 5.431, *P2* = 0.326, *P2* = 0.230.  
*Step* 6\: *r3* = *\(x3*  
*,* 1\) = \(3.l14 2.964 0.9766 0.1072 5.574 0.02569 1\)  
*Iteration 3*  
-0.209 \)  
3 -1.36  
\(  
*Ax* = 0.418.  
-2.30  
*Step* 2\: *q3* = \(1.060 1.189 0.2750 0.5488 1.1878 0.9252 1.0044\).  
*Step* 3\: Because *q3* \> 0, it follows that  
*x3* = \(3.286 3.509 0.2674 0.05859 6.592 0.02366\).  
Stop. *x"* = \(3.286 3.509\) is feasible and *ex"* \> 30.  
Now we consider the linear programming problem\:  
\(4.2\) ZLP = *max\{ex\: Ax* = *b, x* E R\~\}.  
Viewed in terms offeasibility, we solve 4\. A Projective Algorithm for Linear Programming where *P\(O* = *\{x* E R\~\: *Ax* - *b* = 0, *cx* - \(= O\}. We *letA* = *\(A -b\)* -\( .  
171  
*-b\),* c, = \(c -O, and  
We only describe an algorithm for Phase 2, that is, we assume a feasible point *Xl* E R\~  
satisfying *Ax* I = *b* and *x* I \> ° is known.  
We apply the feasibility algorithm to *P\(* O. The algorithm is motivated by the following  
observations.  
i. Let *rk* = *\(x\\* 1\). By the choice of *Xl,* we have *Arl* = 0. Because *Ark+1* = *PkArk* for each  
*k,* we have *Ark* = ° or *AXk* = *b* for all *k.*  
ii. Since the only violated constraint is *cx* - \( = 0, the projective algorithm works to  
decrease \( - *cxk* geometrically at each iteration. To maintain geometric conver-  
gence, we need projection vectors with *maxjqJ* \~ 1. However, if \( is a strict upper  
bound on *ZLP,* then *P\(O* = 0 and hence the algorithm will stop with *maxjqJ* \< 1.  
We overcome this difficulty by viewing \(and calculating\) the projection vector *qk*  
as a function of \(. *qk* is of the form a + *P\(* with a, *p* E *Rn+l.* Therefore if  
*maxjqJ\(* ° \< 1, it is easy to calculate \(' \< \(such that *maxjqJ\(* n = 1. C is a new upper  
bound for *ZLP,* and the algorithm can now proceed to find a new iterate *rhl.*  
iii. If *\(k* is the value of \(at iteration *k* we can associate a dual feasible solution *Uk* to \(4.2\)  
with *ukb* S *\(k\>* so the algorithm simultaneously produces primal and dual feasible  
solutions.  
As before we assume that *\{x* E R\~\: *Ax* = *b\}* is a polytope.  
The Projective Algorithm for the Linear Program \(4.2\)  
*Initialization.* Given *Xl* feasible in \(4.2\) with *Xl* \> 0, set rl = *\(Xl,* 1\). If a specific upper  
bound on Z LP is known, set \(I to be this bound. If none is known, take  
\(I = max *jCj jnw\(A, b\),*  
*\}*  
*Al* = *\(A \_b\)\(rl ...* I \),  
*r* n+1  
*Iteration k*  
*Step* 1\: Let *Vk* = *\(k* - *cxk*  
*.* Ifvk S E, stop. *Xk* is an E-optimal solution of\(4.2\) *withAxk* = *b,*  
*Xk* E R\~, and ZLp - *cxk* S E.  
*Step* 2\: Find the projection *qk\(O* ofl onto the subspace *Akr* = 0, *ctr* = 0.  
*Step* 3\: If *qk\(\(k\)* \~ 0, stop. Let *r* = *Dkqk\(\(k\)'* Then *x* = *\[rI/rn+b* ... *,rn/rn+d* is optimal in  
\(4.2\).  
*Step* 4\: If *qk\( \(k\)* \< 1, find I;' \< *\(k* such that maxj\~l, , .. , n+1 *qJ\(* 1;'\) = 1, and set \(k+1 = C. Other-  
wise set \(hi = *\(k'* 172 1.6\. Polynomial-Time Algorithms for Linear Programming  
*Step* 5\: Take *qk* = *qk\(Ck+l\),* and find *ak* as before.  
*Step* 6\: This is unchanged.  
Proposition 4.8. *Suppose the projective algorithm is applied to problem* \(4.2\) *starting*  
*from afeasiblepointxl*  
*. At iteration k* + 1, *xk+l is afeasibiepoint and Ck+l is an upper bound*  
*on* ZLP *satisfying*  
1. *Ck+l* - *CXk+ l* S *Pk\(Ck* - *cxk\) and*  
ii. *Ck+l* - *CXk+ l* s */fw\(A, b\)\(Cl* - *cx l\).*  
*Proof* i. We just need to verify what happens during one iteration of the algorithm.  
Initially *Vk* = *Ck* - *cxk* = 1 *CCkrk* I. After Step 5,  
V *k+l* = *Ck+l* - *CXk+l* = 1 C *ch,rk+l* 1  
*= Pk* 1 *CCk.,rk* 1 = *Pk\( Ck+l* - *cxk\)* by statement i of Proposition 4.2  
s *Pk\(Ck* - *cxk\)* = *PkVk* since *Ck+l* S *Ck* from Step 4.  
ii. This follows from statement ii of Proposition 4.7 because *Xl* is feasible and hence  
bounded. •  
*Example* 4.2. The Projection Algorithm is applied to the linear program  
ZLp = max *7Xl* + *2X2*  
*-Xl* + 2xz + *X3*  
= 4  
*5Xl* + *X2 +*  
= 20  
*-2Xl* - 2xz +  
*Xs* = -7  
xER\~  
with E = 10-5,  
We initialize with *Xl* = \(1.01 2.5 0.01 12.45 0.02\), Cl = 200. The calculations are shown  
in Table 4.1.  
It can be shown that at each iteration the algorithm yields a dual feasible solution. Also,  
by combining the algorithms for problems \(4.1\) and \(4.2\), one can describe a single-phase  
primal-dual algorithm for linear programming. The remarkable convergence rate of the  
projective algorithm means that, in practice, never more than about 30 iterations seem to  
be necessary. The key practical question is how to carry out the projection step efficiently.  
5\. A STRONGLY POLYNOMIAL ALGORITHM FOR COMBINATORIAL  
LINEAR PROGRAMS  
We generally bound the running time of an optimization or feasibility algorithm by the  
number of variables *n,* the number of constraints *m,* and the size of the largest coefficient  
8 in the data. Polynomial-time algorithms require a bound that is a polynomial function of  
*m, n,* and log 8. An algorithm is *strongly polynomial* if the bound only depends on *m* and  
*n.* Thus for the family of instances in which log 8 is a polynomial function of *m* and *n,* we  
can trivially eliminate the dependence on 8 and say that the algorithm is strongly  
polynomial. Table 4.1.  
k *Xk \(k \(k* - *CXk* \(hi - *CXk ukb CXk Pk Pk*  
*r* A ""  
1 1.01000 2.50000 0.01000 12.45000 0.02000 200.00000 187.93000 187.93000 152.86228 201.47566 0.172 0.930  
2 2.89112 2.50302 1.88509 3.04137 3.78828 200.00000 174.75611 8.55008 32.48775 1.29514 0.703 0.577  
3 3.37029 2.63617 2.09795 0.51237 5.01292 33.79397 4.92958 1.63481 30.34192 2.29413 0.601 0.376  
4 3.33667 3.26400 0.80867 0.05263 6.20135 30.49920 0.61447 0.34921 30.21156 1.96888 0.565 0.371  
5 3.27225 3.59924 0.07377 0.03951 6.74298 30.23393 0.12970 0.07829 30.18217 3.71527 0.383 0.213  
6 3.27690 3.61377 0.04936 0.00175 6.78133 30.18251 0.01670 0.01620 30.18193 4.97273 0.307 0.168  
7 3.27261 3.63549 0.00162 0.00144 6.81622 30.18201 0.00273 0.00253 30.18182 5.02502 0.305 0.166  
8 3.27284 3.63577 0.00129 0.00005 6.81722 30.18182 0.00042 0.00042 30.18182 5.37459 0.291 0.157  
9 3.27272 3.63634 0.00004 0.00004 6.81813 30.18182 0.00007 0.00007 30.18182 5.35710 0.292 0.157  
10 3.27273 3.63635 0.00003 0.00000 6.81816 30.18182 0.00001 0.00001 30.18182 5.36843 0.291 0.157  
# ...  
\(;! 174 1.6\. Polynomial-Time Algorithms for Linear Programming  
Here we show that for the linear programming problem  
max\{cx\: *Ax\:\:; b, x* \~ O\}  
it is possible to eliminate the dependence of the running time on *\(\)b* and *\(\)e* for any  
algorithm that finds primal and dual feasible and complementary solutions. This is done  
by replacing the original problem by a sequence of problems in which the coefficients *b*  
and c are a polynomial function of *m, n,* and log *\(\)A.* In particular, the bounds in the  
projective and ellipsoid algorithm will depend only on *m, n,* and log *\(\)A.*  
Also, if *A* is a \(0, I\)-matrix, as in a network flow problem or the linear programming  
relaxation of a set covering problem, a polynomial-time linear programming algorithm  
can be refined to a strongly polynomial algorithm. For example, the primal-dual  
algorithm for the transportation problem given in Section 1.3.5 enjoys this property.  
First we will show how the dependimce on c is eliminated. The problem is reduced to  
solving a sequence of no more than *n* linear programs in each of which the coefficients of  
the cost vector are bounded by n2\~, where\~isan upper bound on I\~\(A\) I, the maximum  
absolute value of any subdeterminant of *A.* We will assume throughout this section that  
either \~\(A\) is known and we set\~ = \~\(A\) orthat\~ = *\(n\(\)A\)n* which, given *m* \:\:; *n,* is known  
to be an upper bound on \~\(A\). Now log *\(n 2M* is a polynomial function *ofm, n,* and log *\(\)A,*  
and so the dependence on c disappears.  
We consider linear programs of the form  
\(5.1\) max\{cx\: *x* E *P\),* where *P* = *\{x* E R\~\: *Ax* = *b\}.*  
The dual problem is  
\(5.2\) *min\{ub\: u* E *U\},* where *U* = *\{u* E *Rm\: uA* \~ *c\}.*  
The first result we need concerns primal and dual solutions *\(x, u\)* that are close to  
satisfying the optimality conditions. We let *U\(e\)* = *\{u* E *Rm\: uA* \~ c - en.  
*Definition* 5.1. *\(x, u\)* are a pair of *e-approximate solutions* for problem \(5.1\) if  
i. *xEP*  
ii. *u* E *U\(e\);*  
iii. if *uaj* \> Ch then *Xj* = O.  
Proposition 5.1. *Given an e-approximate pair \(x, u\) jor problem* \(5.1\), *let*  
*J* = \{j *EN\: uaj* \~ *Cj* + en\~\}. *Then ijx\* is any optimal solution to* \(5.1\), *xj= Ojor allj* E *J.*  
*Proof* Suppose the contrary, so there exists a *k* such that *Uak* \~ *Ck* + en\~ and an  
optimal solution *x\** to \(5.1\) with *xZ* \> O.  
The vector *Z* = *x\** - *x* satisfies *Az* = 0 because *Ax\** = *Ax* = *b, Zk* \> 0 since *uak\> Ck*  
implies *Xk* = 0; also, *Zj* \~ 0 whenever *Xj* = 0 because *xj* \~ O. Hence z is a feasible solution  
to  
*Az=O*  
\(5.3\) *Zk \>0*  
*Zj* \~ 0 for allj with *Xj* = O. 5\. A Strongly Polynomial Algorithm for Combinatorial Linear Programs 175  
Moreover, *cz* \~ 0 because *x\** is optimal to \(5.1\).  
Now there exists an integer basic feasible solution *Z* to \(5.3\) with cZ \~ O. Furthermore,  
we know from Cramer's rule that maxj I *Zj* I \:s; *Ll\(A\) s* Ll. In addition, ifZj \< 0 we know that  
*Xj* \> 0 and hence, by condition iii of Definition 5.1, that *uaj* \:s; *Cj.*  
Now  
*cz* = \(c - *uA\)z*  
\:s; *\(Ck* - *uak\)Zk* + EL IZj I  
*/"k*  
*sinceAz* = 0  
since *u* E *U\(E\)* implies Cj - *uaj* \:s; E  
\:s; - *EnLl* + *E\(n* - l\)Ll  
= - ELl  
However, *cz* \~ 0, so there is a contradiction. •  
A simple way to obtain I-approximate solutions for \(5.1\) is to let c' = IcJ and to solve the  
linear program  
\(5.4\) *max\{c'x\: x* E *Pl.*  
Proposition 5.2. *Let \(x, u\) be an optimal solution pair for the linear program* \(5.4\), *then*  
*\(x, u\) is a i-approximate pair for the linear program \(5.1\).*  
*Proof* Since *x* is optimal in \(5.4\), *x* E *P.* Since *u* is dual feasible,  
*uA* \~ Ie\] = C - \(c - leD \~ C - 1.  
Finally, *uaj\>* Cj implies *uaj\>* lej\]; and hence, by complementary slackness for \(5.4\),  
\~=Q •  
Combining Propositions 5.2 and 5.1, we obtain  
Corollary 5.3. *Let u be a dual optimal solution of\(5.4\) and let J* = \{j E *N\: uaj* ;;\:.. *Cj* + *nLl\}.*  
*If x\* is any optimal solution to \(5.i\), then xl=* 0 *for all\}* E *J.*  
*Proof* Let *\(x, u\)* be an optimal solution pair for \(5.4\). Then by Proposition 5.2, *\(x, u\)*  
is a I-approximate pair for \(5.1\). Hence by Proposition 5.1, if *J* = \{j *EN\: uaj* \~ *Cj* + *nLl\}*  
and *x\** is any optimal solution to \(5.1\), then *xj=* 0 for all\} E *J. •*  
*Example* 5.1\. We consider the transportation problem of Example 5.1 of Section 1.3.5  
with weights *wij* = *4wu* + *CPu* for all *i* and\}, where 0 \:s; *CPu* \< 1. Since the constraint matrix *A*  
of a transportation problem is totally unimodular \(see Section m.l.2\), Ll = 1.  
To find a I-approximate pair *\(x\*, u\*\),* we solve problem \(5.4\), which has weights  
*cij* = *Iwij\]* = *4wij.* From Section 1.3.5 we know a dual optimal solution of \(5.4\)\:  
*u\** = \(24 36 8 36\)  
*v\** = \(-16 -12 4 -16 -8 -4\) 176 with  
1.6\. Polynomial-Time Algorithms for Linear Programming  
12  
- 0  
\(  
C = 16  
24  
o 0 4 16  
o 8 0 0  
36 0 24 32  
o 0 0 12  
1\~ \) 4 .  
4  
Now applying Corollary 5.3, we see that *n!J.* = 24\. Hence *J* = *\{\(i,\}\)\: cij* \~ 24\}, and  
*X32* = *X34* = *X35* = *X41* = 0 in any optimal solution.  
In Example 5.1, 4 of the 24 variables were eliminated on the basis of Corollary 5.3. The  
next proposition shows that if the weight vector c is appropriately normalized and scaled,  
it is always possible to set at least one variable to zero. Hence after applying the procedure  
no more than *n* times, the original linear program is solved.  
*Definition* 5.2. For any !J. satisfying *!J.\(A\)* s!J. s *\(ney,* we say that c is *polynomially*  
*normalized* for \(5.1\) if c = 0 or *Ac* = 0 and *maxj* I *Cj* I = *n2!J..*  
Proposition 5.4. *Given the linear program* \(5.1\) *with n\>* 1 *and a polynomially normal-*  
*ized weight vector* C \* 0, *let u be a dual optimal solution to the linear program* \(5.4\). *Then*  
*J* = \{j\: *uaj\> Cj* + n!J.\} \* 0.  
*Proof* Let *Cj* = *uaj* - *Cj.* We need to show that maxA \~ *n!J..*  
Since *u* is dual feasible in \(5.4\), *uaj* \~ c; = *ICjJ* \~ *Cj* - 1, and hence *Cj* \~ -1 for all\}.  
The projection of C onto *H* = *\{x\: Ax* = O\} is the vector -c since *uA* E *Hl.* and -c E *H.*  
Therefore, Ilcll \~ IIcli.  
Now observe that  
*n*  
IIcW = I c\] s *n* \(max I *Cj* 1\)2 \< *\(n* max I *Cj* 1\)2.  
\]\~1 \] \]  
Therefore  
max I C\] I \~ .!.. IIcll \~ .!.. Ilcll \~ .!.. \(max I C\] I\) = .!.. *n2!J.* = *n!J..*  
*j n n n j n*  
Since *Cj* \~ -1 for all\}, we must have maxA \~ *n!J.* as required. •  
The final step is to show that the objective function vector of any linear program \(5.1\)  
can be put in polynomially normalized form without affecting the set of optimal solutions.  
An Algorithm to Polynomially Normalize C  
*Step* 1\: Find the projection C of C onto *H* = *\{x\: Ax* = O\}. Ifc = 0, set *d* = 0 and stop.  
*Step* 2\: Set *a* = *n2!J./maxjlcj* I and *d* = *ac,* where *!J.\(A\)* s !J. s *\(neA\)n. d* is the required  
objective function vector.  
Proposition 5.5. *and let d be as derived above\:*  
*Let F\(c\) be theface ofP of optimal solutions to* \(5.1\) *with weight vector* c, 5\. A Strongly Polynomial Algorithm for Combinatorial Linear Programs 177  
i. *F\(c\)* = *F\(d\)*  
ii. *If d* = 0, *then F\(c\)* = *P.*  
*Proof* i. The scaling in Step 2 does not affect the set of optimal solutions, so  
*F\(e\)* = *F\(d\).* Since e is the projection of c onto *H,* and *H* = *\{z\: z* = *yA, Y* E *R m\},* it follows  
that e = e - *yA* for some *y* E *Rm.* But for all *x* E *P,* we have *ex* = *cx* - *yAx* = *ex -yb.*  
Since *yb* is a constant, *x* is optimal for e if and only if it is optimal for c. Hence  
*F\(c\)* = *F\(e\)* = *F\(d\).*  
ii. *F\(O\)* = *P,* and hence *F\(c\)* = *P. •*  
Now we can describe the algorithm.  
An ''Objective Rounding" Algorithm for the Linear Program \(5.1\) with *P* \*' 0  
*Step* 1 *\(Initialization\)\:* N 1 = *N, t* = 1, *n* 1 = 1 N 1 I.  
*Step* 2 *\(Iteration t\)\:* Consider the linear program  
*max\{cx\:* I *ajXj* = *b, Xj* E R\~ *forj* E *NI\}.*  
*jEN'*  
a. Put c into polynomiaHy normalized form with *d\(A\)\:\:\:;* d\:\:\:; *\(n\(JA\)n.*  
b. If e = 0, stop. The feasible solutions of \(5.1 I\) are the set of optimal solutions to \(5.1\).  
c. Otherwise solve the linear program  
d. If \(5.41\) is unbounded, stop. \(5.1\) is unbounded \(since if *uaj* \~ lCjJ for *j* E *N I* is  
infeasible, then *uA* \~ c is infeasible\).  
e. Otherwise, let *ul* be an optimal dual solution and let  
*Sett\<-t+1*  
Theorem 5.6. *If the linear program* \(5.1\) *is feasible, the objective rounding algorithm*  
*either shows that* \(5.1\) *is unbounded or it terminates with the set of optimal feasible*  
*solutions to* \(5.1\) *after no more than n iterations.*  
*Example* 5.2\. ming problem \(5.1\)  
We apply the objective rounding algorithm to the feasible linear program-  
max *931xl* + *724x2* + *296x3*  
*8Xl* + *5X2* + *3X3* = 527  
*xER!* 178 1.6\. Polynomial-Time Algorithms for Linear Programming  
We initialize with \~ = \~\(A\) = 8, *N 1* = \{l, 2, 3\}, *n1* = 3, *t* = 1.  
*Iteration* 1. Consider *\(S.11\)* = *\(S.1\).*  
a. Projecting C onto *8X1+SX2+3x3=0,* we obtain *C=\(-4S* 114 -70\). Also,  
*maxjlcjl* = 114. Therefore a=n2\~/maxlcjl =72/114. Now *d=ac=* \(-28.42 72  
-44.21\).  
b. *d\*O.*  
c. We solve the linear program *\(S.41\)* with objective function *\[dj,*  
max - *29x1* + *72X2* - *4SX3*  
*8X1+ SX2+ 3X3=S27*  
*X* \:2\: 0,  
giving an optimal dual solution *u* = *72/S.*  
d. Now  
*ua1* - *d 1* = S\~6 + 28.42 \> n\~ = 24  
*ua2* - *d2* = °  
*- d 216*  
*ua3* - 3 = 5 + 44.21 \> 24.  
Hence *J 1* = \{l, 3\}, *N 2* = \{2\}, *n2* = 1, *t* \<- 2.  
*Iteration* 2. Solve *\(S.12\)*  
a. Projecting C = \(724\) onto *SX2* = 0, we obtain c = \(0\).  
b. The set of optimal solutions to *\(S.1\)* is given by  
Given a feasible linear program *\(S.1\),* we have seen how the dependence of the running  
time on C can be eliminated. Now we consider the dependence on *b* . Note that the running  
time of the linear program *\(S.4t\)* still depends on *b.* Furthermore, although the objective  
rounding algorithm terminates with the face *P\** = *\{x* E R\~\: *Ax* = *b, Xj* = *O,j EN* \\ *N\*\}* of  
optimal solutions, finding an optimal solution still involves finding a feasible solution to a  
linear program when the face *P\** is not just a point.  
So now we consider the elimination of the dependence on *b.* For convenience we  
consider inequalities here, that is *P* = *\{x ERn\: Ax* \:s; *b\}.* Also, we first consider the  
*feasibility problem,*  
Is *P* = 0? If not, find *x\** E *P.*  
We will need the following result, which makes precise how to perturb the objective  
function so that all dual feasible basic solutions are nondegenerate. \(Applied to *b,* this is 5\. A Strongly Polynomial Algorithm for Combinatorial Linear Programs 179  
precisely the perturbation approach to avoiding cycling in the simplex algorithm; see  
Section 1.2.3\).  
Proposition 5.7. *Consider the linear program* max\{cx\: *x* E *P\} with P* = *\{x ERn\: Ax* s *b\}*  
*and* c = L;\:;l\(\~ + 1Yd. *Suppose its dual* min\{ub\: *uA* = c, *u* E *R'\:'\} has a finite optimal*  
*solution u\*. Let* 1\* = \{i EM = \{1, ... , *m\}\: ur\>* O\} *and let x\* be any solution of dx* = *bJor*  
i E 1\*. *Then x\* is an optimal solution to the linear program.*  
Feasibility Algorithm for *P* = *\{x ERn\: Ax* s *b\}.*  
1. Take c as in Proposition 5.7. Note that c is polynomial in the size of *A .*  
2. The dual problem  
\(5.5\) min\{ub\: *uA* = c, *u* E *R'\:'\}*  
has a feasible solution *u* = \(\~ + 1,\(\~ + 1\)2, ... , \(\~ + *1\)m\).* Hence we can apply the  
objective rounding algorithm to \(5.5\).  
3. If the dual problem is unbounded, then *P* = 0.  
4. Otherwise the algorithm terminates with the face of optimal solutions *\{u* E *R'\:'\:*  
*uA* = C, *Ui* = 0 for *i* EM \\ *I'\}.*  
5. Find a point *u* in this face using a linear programming algorithm. Let *I\** = *\{i\: Ui* \> O\}.  
6. Use Gaussian elimination to find a solution *x\** to the system of linear equations  
*aix* = *bi* for *i* E *I\*.*  
7. By Proposition 5.7, *x\** E *P.*  
Now observe that in Step 2 when we apply the objective rounding algorithm, the  
subproblems \(5.4t\) have a modified objective *b* and the right-hand side c that are  
polynomial in *m, n,* and log *eA'* Finally, in Step 6 the running time of Gaussian elimina-  
tion does not depend on *b.* Hence, if a polynomial-time algorithm is used to solve each of  
the linear programs, Steps 1-7 can be executed in polynomial time.  
Proposition 5.8. log *eA.*  
*The feasibility problem can be solved in time polynomial in m, n, and*  
Putting together the objective rounding and feasibility algorithms it is now possible to  
solve any linear program in time polynomial in *m, n,* and log *eA'*  
The only question that needs to be dealt with is the resolution of the feasible linear  
programs \(5.4t\) in Step c of the objective rounding algorithm, which are of the form  
max\{ctx\: *Ax* = *b, x* E R\~\},  
where ct is polynomial in the size of *A.* The steps are\:  
1. Check whether *\{u* E *Rm\: uA* \~ *c t \}* is feasible with a polynomial linear programming  
algorithm. If not, the primal is unbounded.  
2\. Use the objective rounding algorithm to solve the dual of \(5.4t\)\: min\{ub\: *uA* \~ *ct \}.*  
Here the subproblems are linear programs with both the modified objective *band*  
the right-hand side ct polynomial in the size of *A.* The basic algorithm terminates  
with the face of optimal solutions. 180 1.6\. Polynomial-Time Algorithms for Linear Programming  
3. Use a polynomial linear programming algorithm to find an optimal solution 1/ lying  
on the face.  
4\. An optimal dual solution *lit* for problem *\(5.4t\)* is precisely what is required in Step e  
of the objective rounding algorithm.  
Again it is easy to check that the data for each problem solved are polynomial in the size  
of *A.* Hence we have shown the following\:  
Theorem 5.9. *and* log *eA.*  
*The linear programming problem can be solved in time polynomial in m, n,*  
6. NOTES  
Section 1.6.1  
Until the summer of 1979, it was not known whether linear programming was solvable in  
polynomial time. However, there were theoretical and empirical reasons for believing that  
there existed a polynomial-time algorithm for linear programming. It had been known for  
several years that linear programming belonged to the complexity class.NPJ' n *Cf5o.N'PJ',* and  
although the simplex method does not have a polynomial time bound \(see Klee and Minty,  
1972\), it is empirically very efficient.  
Section 1.6.2  
An ellipsoid algorithm for linear programming, as well as a proof of its polynomial-time  
bound, first appeared in a brief Russian article by Khachian \(1979\). It was brought to the  
attention of Western researchers at a meeting in Oberwolfach, West Germany in June  
1979. An English version of Khachian's results, including many missing details, was  
produced by Gacs and Lovasz \(1981\). Their article appeared as a technical report toward  
the end of the summer of1979, and the results were announced to the research community  
at the Xth Mathematical Programming Symposium in Montreal, Canada in August 1979.  
A flood of articles on ellipsoid algorithms subsequently appeared \[see Bland, Goldfarb,  
and Todd \(1981\) for a survey\]. However, the flood subsided nearly as quickly as it had  
appeared when it was realized that ellipsoid algorithms were not empirically efficient.  
Lawler \(1980\) discussed the reaction of the popular press.  
Section 1.6.3  
"The use of ellipsoid algorithms in combinatorial optimization - in particular, the polyno-  
mial equivalence of optimization and separation-is due to Grotschel, Lovasz, and  
Schrijver \(1981\) and was also studied by Karp and Papadimitriou \(1982\). Also see  
Grotschel, Lovasz, and Schrijver \(1984a,b,c\); and their monograph \(1987\), which gives all  
of the technical details.  
Section 1.6.4  
Projective algorithms for linear programming and their polynomiality were introduced by  
Karmarkar \(1984\). The variant of Karmarkar's approach given here is taken from de  
Ghellinck and Vial \(1986, 1987\).  
The choice of homogenization *r* = *r* /IIJ\~l *r/n* is one of the key ideas in Karmarkar's  
algorithm. It encourages the successive iterates to stay away from the boundaries and  
thereby avoid the combinatorial problems associated with extreme points. This idea had 6\. Notes **181**  
been used earlier by Huard \(1967\) in the method of centers and in Frisch's \(1955\) barrier  
method for nonlinear programming.  
Also, de Ghellinck and Vial \(1987\) show that the projective algorithm to find an E-  
approximate ray produces the same sequence of points as Karmarker's algorithm applied  
to the phase I problem\:  
min\{y\: *Ax* - *\(A l\)y* = 0, *Ix* + *y* = *n* + 1, *x* E R\~, *y* E *Rl\).*  
The idea of a primal-dual approach to Karmarkar's algorithm is due to Todd and  
Burell \(1986\).  
At this time there is some indication that projective algorithms may compete with, or be  
superior to, simplex algorithms for some, or maybe even all, classes of linear programs.  
This issue is unresolved and is the subject of some controversy. Todd \(1987\) gives a survey  
of results on this topic.  
# Section 1.6.5  
The strongly polynomial-time algorithm for linear programming is due to Tardos \(1986\).  
This article was a sequel to her article on a strongly polynomial-time algorithm for  
network flow problems \[Tardos \(1985\); also see Orlin \(1984\)\]. Fujishige \(1986\) and Orlin  
\(1986\) discuss dual versions of Tar dos' algorithm. Frank and Tardos \(1987\) have extended  
these results to linear programs in which the number of constraints is not polynomially  
bounded in the number of variables. **1.7**  
# Integer Lattices  
1. INTRODUCTION  
In this chapter the basic problem is\:  
*The Linear Equation Integer Feasibility Problem.* Given *m, n,* and an integral *m* x *\(n* + 1\)  
matrix *\(A, b\),* find a point *x* E *zn* satisfying *Ax* = *b* or show that no such point exists.  
*Definition* 1.1 The set *L\(A\)* = *\{y* E *R m*  
*\: y* = *Ax, x* E *zn\},* where *A* is an *m* x *n* matrix, is  
called the *lattice* generated by the columns of *A .*  
In lattice terms, the linear equation integer feasibility problem becomes  
\(1.1\) Determine if bE *L\(A\)* and if so give a representation of *b*  
as an integral linear combination of the columns of *A.*  
A natural generalization of problem \(1.1\) is\:  
*The Closest Vector Problem.* Given *m, n,* and *A* as above, along with *bERm,* find  
\(1.2\) min\{llb - yll\: *y* E *L\(A\)\}.*  
*y*  
Taking *b* = 0 and excluding *y* = 0 yields\:  
*The Shortest Vector Problem\:* Given *m, n,* andA, find  
\(1.3\) min\{llyll\: *y* E *L\(A\), y* \* O\}.  
*y*  
*Example* 1.1. In Figure 1.1 we see the lattice generated by *a* \~\). to *b* = \(\:i\) is *y* @' and the shortest vector is *v* = \(D.  
The closest lattice point  
An important difference between problem \(1.1\) and problems \(1.2\) and \(1.3\) is their  
respective complexities. There is a polynomial-time algorithm for solving \(1.1\), whereas  
\(1.2\) is .N'9J\>-hard, and \(1.3\) is suspected to be .N'9J\>-hard. Also, the problems obtained by  
replacing the euclidean norm by the maximum norm in \(1.2\) and \(1.3\) are .N'9J\>-hard.  
However, there are algorithms for problems \(1.2\) and \(1.3\) that run in polynomial time for  
a fixed value of *n.* Since these algorithms and the algorithm for \(1.1\) depend upon finding  
an appropriate representation of the lattice *L\(A\),* this is the main theme of the chapter.  
182 1\. Introduction 183  
5  
• 4  
3  
2  
2 3 4 5 6 7  
Y1  
-2 -1 0 -1  
• -2 • •  
Figure 1.1. *al* = \(\~\), *a2* = \(\~\)  
*Definition* 1.2. The *greatest common divisor* of the integers *a* 1 and *a2,* not both zero,  
denoted by *gcd\(a\}, a2\),* is the largest positive integer *r* such that *r* divides *a* 1 and *a2* exactly;  
that is, there exist integers *Z* i such that *rz* i = *a i* for *i* = 1, 2. If gcd\( *a* b *a* 2\) = 1, then *a* 1 and *a2*  
are *relatively prime.*  
When *n* = 1, problems \(1.1\)-\(1.3\) essentially reduce to the problem of finding  
gcd\(a\}, *a2\).* In the next section, we will describe the euclidean algorithm to find *gcd\(a* h *a2\).*  
This will be interpreted as an algorithm that either solves the system  
\(1.4\)  
or shows it to be infeasible. It also provides the basic step in finding alternative descriptions  
of the lattice *L\(A\).*  
In Section 3, we will establish the connection between the euclidean algorithm and the  
continued fraction expansion of a rational number *at/a2'* This allows us to solve, in  
polynomial time, the following problem.  
*The Rational Approximation Problem.* Given positive integers at, *a2,* and *K,* determine  
integers *p* and *q* that solve 184 1.7\. Integer Lattices  
A solution to this problem is required in the ellipsoid algorithm \(see Section 1.6.2\).  
An important generalization of this problem is\:  
*The Simultaneous Diophantine Approximation Problem.* Given *n* positive rationals *\{ai\}f\:=1*  
and an integer *K,* determine positive integers *q* h ••• , *q* nand *p* that solve  
\(1.5\) min\{ m,a+\~; -\~;I\: *p* E Z\~, *q* E Z\~, *P* '" *K\}.*  
This problem is X9J\>-hard.  
In Section 4, we will introduce some basic properties of the lattice *L\(A\).* We will then  
develop a canonical representation of *L\(A\),* called the *Hermite normalform,* and sketch a  
polynomial-time algorithm for finding the Hermite normal form. This also provides a  
polynomial-time algorithm for problem \(1.1\).  
In Section 5, we will introduce an alternative representation of *L\(A\),* called a *reduced*  
*basis,* which can also be obtained in polynomial time. Such bases have various interesting  
properties. We use them to give an algorithm for problem \(1.3\) that is polynomial for fixed  
*n* and a polynomial algorithm for an approximate version of\(1.5\). Other applications-in  
particular, an outline of a polynomial-time algorithm for the linear inequality integer  
feasibility problem with a fixed number of variables-will be given in Section 11.6.5.  
2. THE EUCLIDEAN ALGORITHM  
In this section we present a polynomial algorithm to find gcd\( *a, b\),* where *a* and bare  
integers satisfying *a* ;;?; *b* \> 0. We use the notation *u* I *v* to mean that *u* divides *v.* The  
algorithm will terminate with integers *p* and *q* such that *p* and *q* are relatively prime and  
*pa* - *qb* = gcd\(a, *b\).*  
Note that for any positive integers *a* and *b,* with *b* \~ *a,* we have *a* = *la/bJb* + c, where  
° \~ c \< *b.* The basic idea of the euclidean algorithm is embodied in the following  
proposition.  
Proposition 2.1. *Suppose a and b, a* ;;?; *b, are positive integers and* c = *a* - *la/bJb.*  
i. *If* c \* 0, *then* gcd\(a, *b\)* = gcd\(b, *c\).*  
ii. *If* c = 0, *then* gcd\(a, *b\)* = *b.*  
*Proof* i. Let *r* = gcd\(a, *b\)* and *s* = *gcd\(b, c\).* Since c = *a* - *db* and *d* is an integer, *ria*  
and *rib* imply that *ric.* Hence *r* Is. Similarly *sir.*  
ii. This is obvious. •  
Notethatbecausec \< *b* \~ *a,* we can apply the proposition first to the pair *\(a, b\),* then to  
the pair *\(b, c\),* and so on.  
In the description of the euclidean algorithm given below, c *t* is the remainder at  
iteration *t.* We also carry along integers *\(Pt, qt\),* which will be used later.  
The Euclidean Algorithm To Find gcd\(a, *b\)*  
*Initialization\:* Order so that *a* ;;?; *b.*  
\(C-h *co\)* = *\(a, b\),* \(P-I, *Po\)* = \(1, 0\), *\(q-b qo\)* = \(0, 1\).  
Set *t* = 1. 2. The Euclidean Algorithm  
*Iteration t\: dt* = lCt-2J  
C'-l  
*C t* = *C t-2* - *dtC t-l*  
*Pt* = *Pt-2* + *dtpt-I*  
*qt* = *qt-2* + *dtqt-I*  
If *C t* = 0, stop. Set *T* = *t. gcd\(a, b\)* = *CT-l.*  
Otherwise set *t* +- *t* + 1.  
185  
Proposition 2.2. *The euclidean algorithm is correct and*  
*C t* = *\(-I\)t+I\[pta* - *qtb\] for t* = -1, 0, ... , *T.*  
*Proof* We use Proposition 2.1. By using statement i repeatedly, we have *gcd\(a, b\) =*  
gCd\(C-b *co\)* = gcd\(co, CI\) = ... = *gCd\(CT-2, CT-l\).* Since *CT-2* - *dTcT-l* = 0, we note that  
*gCd\(CT\_2, CT-l\)* = *CT-I* follows from statement ii.  
Note also that  
and  
Co = \(\_1\)1 *\(poa* - *qob\)* = *b.*  
Thus by induction,  
*= \(-ly-l\(Pt\_2a* - *qt-2b\)* - *\(-I\)tdt\(pt\_Ia* - *qt-lb\)*  
*= \(-I\)t+l\[\(Pt\_2* + *dtpt-l\)a* - *\(qt-2* + *dtqt-l\)b\]*  
*= \(-IY+l\[pta* - *qtb\].*  
# •  
*Example 2.1.* Find gcd\(51, 36\). Using the euclidean algorithm we get  
*dt Ct Pt qt*  
-1 51 1 °  
° 36 ° 1  
1 1 15 1 1  
2 2 6 2 3  
3 2 3 5 7  
4 2 0 12 17  
Hence  
*T* = 4, gcd\(51, 36\) = 3,  
\(-1\)4\(5·51 - 7·36\) = 3,  
and  
\(-1\)\\12·51 - 17·36\) = 0, etc. 186 1.7\. Integer Lattices  
To show that the euclidean algorithm runs in polynomial time, we need to show that the  
number of iterations and the size of the numbers produced are polynomial in *log\(a\).* For  
later use, we also consider the values taken by *P t, q t* as *t* increases.  
Proposition 2.3  
i. *\(Ptqt+1* - *Pt+lqt\)* = \(-ly+1 *for t* = -1, 0, ... , *T;*  
ii. *gcd\(Pb qt\)* = 1 *for t* = 1, ... , *T;*  
iii. *alb* = *qT/PT;*  
iv. *F T* \~ *q T* \~ *a, where Ft is the Fibonacci number given by F* -I = 0, *F* 0 = 1, *and*  
*Ft* = *Ft-2* + *Ft-1for t* = 1, ... , *T.*  
*Proof* i. For *t* = -1, we obtain *\(Ptqt+1* - *Pt+lqt\)* = 1 = *\(\_I\)t+I.* Using induction,  
*\(Ptqt+1* - *Pt+lqt\)* = *Pt\(qt-l* + *dtqt\)* - *\(Pt-I* + *dtpt\)qt*  
*= \(-I\)\(pt-lqt* - *Ptqt-l\)* = *\(-I\)\(-IY*  
*= \(\_I\)t+I.*  
ii. Since *Pt-Iqt* - *Ptqt-I* = *\(-IY andpt, qt* \> ° for *t* \~ 1, it follows that *gcd\(pt, qt\)* = 1.  
iii. *CT* = 0 = *PTa* - *qTb.* Hence *qT/PT* = *a/b.*  
iv. Since *dt* is a positive integer for all *t* \> 0, it follows that  
*qt* = *qt-2* + *dtqt-I* \~ *qt-2* + *qt-l.*  
Since *F-I* = *q\_1* = 0, *Fo* = *qo* = 1, and *Ft* = *Ft-2* + *Ft-h* we have *qt* \~ *Ft* for all *t.* Finally we  
observe from statements ii and iii that *qT* \~ *a. •*  
Proposition 2.4. *The euclidean algorithm runs in polynomial time.*  
*Proof* Since the Fibonnaci series grows exponentially fast and *F T* \~ *qT* \~ *a,* it follows  
that *T* is, at most, D\(log *a\).* Furthermore, the size of the *numbersPt* and *qt* never exceeds  
the size of *a. •*  
Now consider the equation  
\(2.1\)  
Proposition 2.5. *Let r* = *gcd\(ab a2\) with r* = *pal* - *qa2, where P and q are relatively prime.*  
*Equation* \(2.1\) *has a solution if and only if r* I *ao. If r* I *ao the set of solutions of* \(2.1\) *is*  
*described by*  
*Proof* For any *x* satisfying \(2.1\), we obtain *\(adr\)xI* + *\(a2/r\)x2* = *ao/r.* Since  
*\(adr\)xI* + *\(a2/r\)x2* E Zl, \(2.1\) is infeasible *if\(ao/r\)* \$\:. Zl. On the other hand, if *ao/r* E Zl,  
then *x\** = *\(ao/r\)\(!!q\)* is a solution, since *pal* - *qa2* = *r.* But any solution can be written as 3\. Continued Fractions 187  
*x* = *x\** + *y,* where *alYI* + *a2Y2* = 0, *Y* E Z2. Hence *Y* = *YI\(-a\:/a2\)* E Z2. Since *gcd\(ab a2\)* = *r,*  
it follows that *YI* must be a multiple of *a2lr,* and *x* is as claimed. •  
*Example* 2.1. *\(continued\).* We determine the set of solutions to  
By Proposition 2.5 with *p* = 5, *q* = 7, and *r* = 3, the solution set is non empty because  
3127. The complete set of solutions is  
3\. CONTINUED FRACTIONS  
The problem of finding the gcd of two positive integers *a* and *b* is equivalent to the  
problem  
min\{p\: \~ - \~ = 0, *p, q* \> ° and integer\}-  
By adding the constraint *p* \~ *K,* we obtain the *diophantine approximation problem*  
\(3.1\) min\{l\~ - \~I\: *p, q* \> ° and integer, *p* .,; *K\}-*  
This problem arose in Section 1.6.2, where we needed to find the rational point *i* = *\(qt!Pb*  
... *,qnIPn\)* nearest to *x\** with *Pi* and *qj* being integers for *i* = 1, ... , nand 1 \~ *Pi* \~ *K.*  
To solve this problem we need to represent rationals as continued fractions.  
*Definition* 3.1. ... , *dj \>,* is an expression of the form  
Given a rational /1, its *continued fraction expansion,* denoted by *\<db*  
*d l +--------------------*  
/1=  
*dj -*  
1  
*l* + *d·*  
\}  
where *db'* .. , *dj* are integers, all positive except possibly *d* I. 188  
1.7\. Integer Lattices  
We also use partial expansions of the form  
*d 1* + ----------  
*d2* + --------  
# p=  
+ 1  
*di - 1* +-  
*X*  
with *x* \~ 1, denoted by *\<db'* .. *,di-l; x\>.*  
*Example* 3.1. Let *p* = *alb* = \*'  
1  
*P* = 1 + *36/15* = \<1; \~\>  
= 1 + 1 = \<1 2·.!S.\> 1 " 6  
2 + *15/6*  
... = \<1, 2, 2; 1\>  
= 1 + 1 = \<1, 2, 2, 2\>.  
2+\~ +2  
The example indicates that the continued fraction expansion is unique, and a compari-  
son with Example 2.1 suggests that there is a close relationship between the continued  
fraction expansion of a rational *P* = *alb* and the euclidean algorithm applied to *\(a, b\).*  
We suppose, without loss of generality, that *P* = *a I b* \~ 1.  
Proposition 3.1. *Let dt, Ct, Pt, qt and T be as in the euclidean algorithm\:*  
i. *P* = *\<db d2,* ••• , *dt; ct-dct\> for t* = 0, ... , *T;*  
ii. *P* = *\<db d2,* ••• *,dT \>* = *qTlpT;*  
iii. *\<db d2 ,* ... *,dt\>* = *qt!ptfor t* = 1, ... , *T.*  
*Proof* i. We use induction. For *r* = 0, we have *P* = *alb* = *c-llco.* Now assume  
*P* = *\<d!, d2 ,* ... , *dr-I; cr-2/cr-I\>.* Expanding *l/x* with *x* = *cr-2/cr -1* we obtain  
*C r* \)-1 \( 1 \)-1  
*=d+- =d+--*  
\(  
*r Cr-l r cr-l/cr •* 4. Lattices and Hermite Normal Form 189  
Hence *p* = *\<dj,* ... , *d,;* C,\_I/C,\>,  
ii. We know from the euclidean algorithm that 0 = *CT* = *PTa* - *qTb* with *PT* and *qT*  
relatively prime.  
iii. This follows from applying the euclidean algorithm to the rational *P'* = *\<d* 1,  
... *,dt\>. •*  
To solve the diophantine approximation problem \(3.1\), note that the successive values  
*qt/Pt* approach *p.* Therefore suppose we truncate the continued fraction expansion just  
before the value of *Pt* exceeds *K.*  
Letting\} = max\{t\: *Pt* \~ *K\),* a candidate for the best approximation is *qjPj* = *\<d" d2,*  
. , . *,dj \>.* However, this does not always solve the problem.  
Proposition 3.2. *Let\}* = max\{t\: *Pt* \~ *K\} and k* = *t\(K* - *pj-d/pjJ. Then either q/Pj or*  
*\(qj\_1* + *kqj\)/\(Pj-l* + *kpj\) solves problem \(3.1\).*  
The idea of the proof is to show that any rational between these two values necessarily  
has a denominator exceeding *K.*  
*Example* 3.1 *\(continued\). P* \~ 10.  
Suppose we wish to find the best approximation *q* / *P* to \~ with  
For *t* = 1, 2, 3, 4, we have *qt/Pt* = 1, \~, \~,H, respectively. Since *P3* \~ 10 \< *P4,* we take  
\} = 3, and one estimate is *q3/P3* =\~. Since *k* = t\(10 - 2\)/5J = 1, the other estimate is  
\(3 + 1· 7\)/2 + 1· 5\) = f. Since I H - 51 = to, and If - HI = *ii,* the best approximation is f.  
4. LATTICES AND HERMITE NORMAL FORM  
Here we consider the lattice  
*L\(A\)* = *\{y* E *zm\: y =Ax, x* E *zn\),*  
where *A* is an *m* x *n* integer matrix. In this and the next sections we will consider different  
ways to represent *L\(A\),* The basic operations that can be carried out on the matrix *A* are  
column operations that do not change the lattice.  
*Definition* 4.1. An *n* x *n* matrix C is *unimodular* if it is integer and I det C I = 1.  
Proposition 4.1. *L\(AC\)* = *L\(A\).*  
*If A is an integer m* x *n matrix, and* C *is a unimodular n* x *n matrix, then*  
*Proof* By substituting *x* = *Cw,* we have that  
*L\(A\)* = *\{y* E *zm\: y =Ax, x* E *zn\)* = *\{y* E *zm\: y* = *ACw, Cw* E *zn\),*  
The result follows by showing that *\{w\: Cw* E *zn\)* = *\{w\: w* E *zn\).* Since C is an integer  
matrix, wE *zn* implies *Cw* E *zn.* Conversely, since C is unimodular, C-I is an integer  
matrix and hence *Cw* E *zn* implies C-1Cw = *w* E *zn. •* 190 1.7\. Integer Lattices  
*Definition* 4.2\. An *m* x *m* nonsingular integer matrix *H* is said to be in *Hermite normal*  
*form* if\:  
a. *H* is lower triangular and *h ij* = 0 for *i* \< *j;*  
b. *hu* \> 0 for *i* = 1, ... *,m;* and  
c. *h ij* \~ 0 and I *h ij* I \< *h ii* for *i* \> *j.*  
The main result of this section is\:  
Theorem 4.2. *unimodular matrix* C *such that\:*  
*If A is an m* x *n integer matrix with* rank\(A\) = *m, then there exists an n* x *n*  
a. *AC* = *\(H,* 0\) *and H is in Hermite normalform;*  
b. *H-IA is an integer matrix.*  
*\(H,O\)* is called the *Hermite normal form of A.* We will outline a polynomial-time  
algorithm for finding C and *H* which will serve as a constructive proof of Theorem 4.2. It  
also can be shown that *H* is unique.  
*Example* 4.1  
It is readily verified that the matrices  
3  
-1  
# o  
satisfy the conditions of Theorem 4.2.  
There are several immediate consequences of Theorem 4.2. The first, a canonical  
description of *L\(A\),* uses Proposition 4.1.  
Proposition 4.3. *L\(H\)* = *L\(A\).*  
*Definition* 4.3. *L\(A\).*  
If *L\(A\)* = *L\(B\)* and *B* is nonsingular, then *B* is a *basis* for the lattice  
Corollary 4.4. *Every lattice L\(A\) with* rank\(A\) = *m has a basis.*  
Given the above characterization of *L\(A\),* we can solve the Linear Equation Integer  
Feasibility Problem. Let S = *\{x* E *zn\: Ax* = *b\}* and let *Hand* C = \(CI , *C2\)* be as in  
Theorem 4.2, with Clan *n* x *m* matrix and C 2 an *n* x *\(n* - *m\)* matrix. Observe that S \*" 0  
if and only if *b* E *L\(A\).* 4\. Lattices and Hermite Normal Form  
Theorem 4.5  
i. S *=1=* 0 *if and only if H-I b* E *zm.*  
ii. *If* S *=1=* 0, *every solution ofS is oftheform*  
191  
*Proof*  
S = *\{x* E *zn\: Ax* = *b\)*  
*= \{x\: x* = *Cw, ACw* = *b, w* E *zn\)* \(since C is unimodular\)  
*= \{x\: x* = *Cw, \(H,* 0\) *w* = *b, w* E *zn\)*  
*= \{x\: x* = C1WI + *C2W2, HWI* = *b,* WI E *zm, W2* E *zn-m\)*  
*= \{x\: x* = *C 1H-1b* + *C2W2, H-1b* E *zm, W2* E *zn-m\).*  
# •  
*Example* 4.2. Find the set of integer solutions, if any, to  
*2x* I + *6x* 2 + 1 *x* 3 = 7  
*4Xl+7x2+ 7x3=4.*  
*Hand* C were given in Example 4.1. Now, by Theorem 4.5, the solution set is nonempty  
since  
w,\(\~\) = \~G n\(\~\) = G\) E Z2.  
The general solution is  
\(\~\:\) = c,G\) + *C,W2*  
= C\~ -DG\) + *CDw,*  
\~ un + *\(=Dw"* w,E Z'.  
We also obtain an integer version of Farkas' lemma.  
Corollary 4.6. *Either* S = *\{x\: Ax* = *h, x* E *zn\) =1=* 0 *or \(exclusively\) there exists u* E *R m*  
*such that uA* E *zm, ub* fl\:. Zl.  
*Proof* Both cannot hold, since this would imply *uAx* = *ub* with *uAx* E ZI and  
*ub* \$. Zl. If S = 0, then by Theorem 4.5 we have *H-1 b* \$. *zm.* Suppose the *ith* coefficient  
of *H-1b* \$. *ZI,* and then take *u* to be the *ith* row of *H-I*  
*• •* 192 I. 7\. Integer Lattices  
*Example* 4.2 *\(continued\).* If *b* = \(\~\), then  
and hence S = 0.  
must satisfy  
Taking *u* = a \~\), we we obtain *uA* = \(2 5 2\) and *ub* = -¥; in other words, any *xES*  
which is impossible.  
Now let *gcd\(a* b ... , *an\)* denote the greatest common divisor of *a* b ... , *an.* An obser-  
vation that is used later is\:  
Corollary 4.7. *Let* S = *\{x* E *zn\:* L\~I *ajXj* = *b\} with aj' b* E ZI. *If gcd\(al'* ... , *an\)* I *b, there*  
*exist n affinely independent points in* S.  
*Proof* Let *A* = *\(ai,* ... *,an\)* and let Hand C = \(CI , *C2\)* be as described in Theorem  
4.2\. C 1 is an *n* x 1 matrix, and C2 is *n* x *\(n* - 1\) withACI = *hll* = *gcd\(ab a2,* ... *,an\),* and  
*AC2* = \(0, ... ,0\). Hence using Theorem 4.5, we obtain  
s = *\{x\: x* = \:1 C1 + *C,w,* wE *zn-I\}*  
Since rank\( C\) = *n,* it follows that rank\( C 2\) = *n* - 1, and the claim is true. •  
We now turn to the proof of Theorem 4.2 and define the elementary column operations  
that correspond to right multiplication by a unimodular matrix C.  
The *elementary column operations* of interest are\:  
a. Interchange columns\} and *k.*  
b. Multiply column\} by-I.  
c. Add *A* E ZI times column *k* to column\}.  
The corresponding unimodular matrices C are easily constructed. Figure 4.1 shows an  
example with *m* = *n* = 6.  
# o  
-1  
C=  
C=  
C=  
# o  
\} = 2, *k* = 5  
\(a\)  
3  
1  
\}=2  
\(b\)  
Figure 4.1  
\} = 2, *k* = 5, *A* = 3  
\(c\) 4\. Lattices and Hermite Normal Form 193  
Now we give an algorithm that constructs the Hermite normal form of an *m* x *n* integer  
matrix *A* with rank\(A\) = *m.* The basic operation of the algorithm involves a row *i,* as well  
as columns sand *t* of the matrix *A* with *s* \< *t.* A sequence of elementary column  
operations are performed so that *ais* +- *gcd\(ais, ail\), ail* +- O.  
Proposition 4.8. *Let A* = *\(ai,* ... , *an\), gcd\(ais, ail\)* = *r, and pais* + *qait* = *r, where p and q*  
*are relatively prime. There exists an n* x *n unimodular integer matrix* C *such that AC* = *A',*  
*where*  
*a;* = *at* for *I =1= s, t*  
*a· a·*  
*a'* = *--.!!...a* + *-.!\:!..a.*  
*t r* s *r 1*  
*In particular, ais* = *rand ait* = o.  
*Proof* Take C to be an identity matrix in all but columns sand *t.* In column *s,* we  
have *C ss* = *p, cts* = *q,* and *Cis* = 0 otherwise. In column *t,* we have *Cst* = *-ail/r, C tl* = *ais/r,* and  
*Cit* = 0 otherwise. It is readily verified that *A* C = *A'* , and det C = *pa is/ r* + *qa it/ r* = 1. •  
The Hermite Normal Form Algorithm  
*Initialization\: i* = 1.  
*Step* 1\: Work on row *i.* Set\} +- *i* + 1.  
*Step* 2\: Work on row *i* and columns *i* and\} \> *i.* If *au* = 0, do nothing. Otherwise use the  
euclidean algorithm to find *r* = *gcd\(aii' au\)* and *p, q* relatively prime such that  
*paii* + *qau* = *r.* Set *A* +- *AC,* where C is the unimodular matrix described in  
Proposition 4.8, with *s* = *i, t* =\}. If\} \< *n,* set\} +- \} + 1 and return to Step 2. If\} = *n,*  
go to Step 3.  
*Step* 3\: Work on row *i* and column *i.* If *au* \< 0, *setA* +- *AC,* where C multiplies column *aj*  
by -1.  
*Step* 4\: Work on row *i* and column\} \< *i.* Set\} +- 1. Set *A* +- *A* C, where C replaces column  
*aj* by *aj* - *rau/aulai'* If\} = *i-I,* set *i* +- *i* + 1. If *i* \> *m,* stop. Otherwise return to  
Step 1. If\} \< *i-I,* set\} +- \} + 1 and return to Step 4.  
Proposition 4.9. *as described in Theorem 4.2.*  
*The Hermite normalform algorithm terminates with matrices Hand* C  
*Proof* All the operations performed are column operations corresponding to right  
multiplication by a unimodular matrix. Hence the product C of these matrices is  
unimodular. Let *H'* = *AC* be the final matrix. Note that after Step 2, *hij* = 0 for all\} \> *i;*  
after Step 3, *hij* \~ 0; and after Step 4, *hij* \< 0 and I *hij* I \< *hii* for\} \< *i* unless *hii* = O.  
Furthermore, these values are never changed in later steps. Hence we only need to show  
that after completing Step 2 for row *i,* we obtain I *hii* I \> O.  
Suppose *hJi* \> 0 for\} \< *i* but *hij* = O. *LetA* I be the matrix consisting of the first *i* rows of  
*A.* Then the algorithm has produced a unimodular matrix C\* such that *A* I C\* = *H\*,* where  
ht = 0 for all *k* \~ *i* and\} \~ *i.* Hence rank\(H\*\) = *i-I.* However, since C\* is unimodular,  
rank\(A I\) = rank\(A 1 *C\*\)* = *i-I,* which contradicts rank\(A\) = *m. •* 194 1.7\. Integer Lattices  
*Example* 4.3. We find the Hermite Normal Form of matrix *A* given below.  
2 6  
1 =\( 1 -3 n 4 7 *\(a* 11, *a* 12\) = \(2 6\). *A=\(*  
0 0 *\(p, q\)* = \(1 0\), *r* = 2\. 0 0  
D *i= l,j=2.*  
C  
0 1  
2 0  
0 0 -D 4 -5 *\(a* 11, *a* 13\) = \(2 1\). 0 1  
# A=\(  
0 0 *\(p, q\)* = \(1 0\), *r* = 1. 0  
D *i* = *l,j* = 3.  
*C'= \(*  
1 0 -\~\) 7 -5 *i=2,j=3.* 0 -1  
# A=\(  
0 ID *i* = *l,j* = 1. No change.  
*C3 = \(*  
0 *\(a22, a23\)* = \(-5 10\). 0 0 -1  
*\(p,q\)=\(-10\),r=5.*  
= \( *-i* 0 n 7 5 *i* = *2,j* = 1. 1  
*A=\(* 0 j\) *i* = *2,j* = 2. No change.  
4  
C  
0 0  
*A* = \(-\~ 0  
c' = \( I -J 5  
0  
-D *i=3,j=3.*  
*A* = \(-\~ 0 n *i* = *3,j* = 1. *C6 =* \( 1 J 5  
0 -1  
*A* = \(-\~ \~\} 5 5  
-1 0 -1 0  
0 n *i* = *3,j* = 2. No change.  
*H=* \(-\~ 0  
Finally,  
C = TI *Ci*  
6 \(1 3 -7\)  
= 0 -1 22.  
i=1 -1 0  
Although the number of iterations of the HNF algorithm is polynomially bounded, it is  
not known whether the size of the numbers is polynomially bounded. In practice, they get  
so large that the algorithm is difficult to execute on a computer. We now modify the  
algorithm to guarantee that the numbers remain sufficiently small. We assume first that *A*  
ism x *m.*  
Observe that if *b* E *L\(A\),* then *L\(A, b\)* = *L\(A\).* Hence the Hermite normal form of  
*\(A, b\)* is the same as that of *\(A, 0\).*  
The following proposition gives multiples of the unit vectors in *L\(A\).* Note that  
*D* = det *H* = II;!1 *hii•* 5\. A Reduced Basis of a Lattice 195  
Proposition 4.10. *Let di* = n\~i *hkkfor i* = 1, ... , *m. Then diek* E *L\(A\)for k* = *i,* ... , *m.*  
*Proof* The vector *Xk* = *\(DA-1\)ek* is integer because *DA-1* is integer and *AXk =*  
*ADA-1ek* = *Dek.* Hence *Dek* E *L\(A\).* Now observe that *hi,* ... *,hm* E *L\(A\)* and apply the  
above result to the lattice *L\(hi,* ... , *hm\)* with *D* replaced by *d i* = n\~i *hkk . •*  
The way to use these results is to calculate *D* first and then add the vectors *die k* for *k* = *i,*  
... *,m* and *i* = 1, ... , *m* to the lattice *L\(A\)* as soon as we have calculated *hll'* ... *,hi-I,i-I'*  
This allows us to reduce all components in rows *i,* ... , *m* modulo *\(dJ*  
Based on this observation, a modified Hermite normal form algorithm is easily  
described in which no intermediate numbers in the computation ever exceed *2D2* in  
absolute value. The resulting algorithm runs in polynomial time.  
Simple modifications of the algorithm give the Hermite normal form of a general  
*m* x *n* integer matrix *A* when *rank\(A\)* = *k* \< mine *m, n\).* Specifically we obtain  
where C is an *n* x *n* unimodular matrix and *H* is a *k* x *k* Hermite matrix.  
The construction of the Hermite normal form of a matrix *A* depends on column  
operations. By also doing similar transformations on the rows, we obtain another  
canonical representation.  
Theorem 4.11. *If A is an m* x *m nonsingular integer matrix, there exist unimodular*  
*matrices Rand* C *such that*  
i. *RAC=6,*  
ii. 6 *is a diagonal matrix with diagonal entries* Ji E Zl \\ CO\},  
iii. JtI J2 ••• I *Jm ,*  
iv. 6. *is unique.*  
The matrix 6 is called the *Smith normal form of A.* Using an algorithm based on the  
Hermite normal form algorithm, 6 can be constructed in polynomial time.  
5\. A REDUCED BASIS OF A LATTICE  
Suppose that *L\(A\)* s; *R n* is a full-dimensional lattice. In the previous section the Hermite  
basis for *L\(A\)* was used to solve the Linear Equation Integer Feasibility Problem. Here we  
construct another basis *B* for *L\(A\),* called a *reduced basis,* and indicate how it can be used  
to solve the shortest vector problem. In addition, two applications of reduced bases in  
integer programming will be given in Section 11.6.5.  
Before we can define and explain the use of a reduced basis, we need to introduce  
several facts about bases and how they relate to determinants. First we introduce the  
Gram -Schmidt orthogonalization of a basis. 196 1.7\. Integer Lattices  
*Definition* 5.1. ... , *n, i \*j.*  
A basis *B* = *\(bb* ... *,bn\)* is said to be *orthogonal* if *bibj* = ° for *i,j* = 1,  
The Gram-Schmidt Orthogonalization of a Basis *B*  
bf = bl  
*b!* = *b2* - *al2bT*  
\(5.1\)  
*k-l*  
*b'k* = *bk* - I *ajkbj* for *k* = 3, ... , *n,*  
j=l  
Proposition 5.1  
i. *The Gram-Schmidt procedure constructs an orthogonal basis B\** = *\(bf,* ... ,b\~\)  
*for Rn,*  
ii. *b'k is the component of bk orthogonal to the subspace generated by bf,* ... ,btl,  
iii. Idet *B* I = Idet *B\** I = TIJ=1 *IIbjll.*  
To examine how the lengths of the vectors of a basis are related to det *B,* we need to  
work with a more general definition of a determinant.  
*Definition* 5.2\. Idet\(b l, ... *,bk\)* I = TIj=1 *IIbjll* for all *k* \:\:\:\:; *n.*  
We see from the definition that *det\(b\},* ... , *bk \)* is the k-dimensional volume of the  
parallelepiped with vertices given by *Lj=l bjxj,* where *Xj* E \{a, 1\} for *j* = 1, ... *,k* \(see  
Figure 5.1\). Furthermore, since *bj* is the component of *bj* orthogonal to the subspace  
generated by *bf,* ... , *bj-l,* it follows that *IIbjll* \:\:\:\:; Ilbjll.  
Given a full-dimensional lattice *L,* we know by statement iii of Proposition 5.1 that  
I det *B* I has the same value for all bases *B* of the lattice. Let *d\(L\)* be this common value  
and let *a\(B\)* = TIJ=1 *IIbj ll.* From the above observations we obtain the following proposi-  
tion.  
Figure 5.1 5\. A Reduced Basis of a Lattice 197  
Proposition 5.2 *\(The Hadamard Inequality\). For all bases B ofL, we have a\(B\)* \~ *d\(L\).*  
*Example* 5.1. We consider a full-dimensional lattice *L* with basis  
*B=* 4 7 7 .  
\(2 6 1\)  
# 001  
We have *d\(L\)* = Idet *B* I = 10. Note that *a\(B\)* = \(20·85·51\)1/2 = *\(86,700\)'12* is much larger  
than I det *B* I. The Gram -Schmidt Orthogonalization Procedure applied to *B* gives  
Now we show how *a\(B\)* relates to the shortest vector problem \(1.3\).  
Proposition 5.3. *Given a full-dimensional lattice L* \~ *Rn and a basis B of L, let XO be a*  
*solution of the problem*  
. \{ . \~ *a\(B\) fi* 0 \}  
mIn *IIBxll· IXj* I "" Idet *B* I *or* j = 1, ... , *n, x* E *zn* \\ \{ \} .  
*Then v* = *Bxo is a shortest vector in the lattice L.*  
*Proof* Let *v* = *Bx* be a shortest nonzero vector with *x* E *zn* \\ \{O\}. Using Cramer's  
rule, we have Ix\}1 = IdetB\}/lldetBI, where *B\}=\(b" ... ,bj\_"v,bj+b ... ,bn\).* By  
Hadamard's inequality \(Proposition 5 .2\), we have  
However, since *b\}* E *L,* it follows that Ilvll \~ Ilb\}ll. Hence Idet *Bj* I \~ *la\(B\)* I, and so  
*Ix\}* I \~ *a\(B\)/* Idet *B* I. •  
As a consequence of Proposition 5.3, we are motivated to find a basis such that  
*a\(B\)1* I det *B* I is small; in particular, we would like to find one with rlog *\(a\(B\)/* I det *B* /\)1  
polynomial in *n.*  
We also have the following lower bound on the shortest vector in the lattice.  
Proposition 5.4. *If bEL, b* =1= 0, *and B is a basis of L, with B\* being its Gram-Schmidt*  
*orthogonalization, then* IIbll \~ *minj* IIb\~l. 198 1.7\. Integer Lattices  
*Proof* Since *bEL,* there exists a *k* \~ *n* such that *b* = \:r.j=1 *biz\}* with z E *Zk* and *Zk =1=* O.  
By substituting for *b\},* using \(5.1\), we obtain *b* = \:r.j=1 *bjzjwith Zk* = *Zk* E Zl. Since the *\{bj\}*  
are orthogonal, *IIbli* \~ *IZk* I *IIbZ"* \~ min\} *IIbjll· •*  
Unfortunately, *B\** is typically not a basis for *L* because *\{aij\}i\<J* are not all integer. This  
does suggest, however, the need to find a basis that is "ne\~r1y orthogonal".  
*Definition* 5.3. if\:  
Let *L* be a full-dimensional lattice, let *B* be a basis of *L,* and let *B\** be the  
basis obtained from the Gram-Schmidt orthogonalization procedure. *B* is a *reduced basis*  
a. *aij* \~ ! for all *i* \< j and  
b. *IIbj+1* + *a\},\}+lbjll2* \~ \~ IIb\~12 for j = 1, ... , *n* - 1.  
The interest in a reduced basis lies in the following results, giving an upper bound on  
*lib* III \(and hence on the shortest vector\) and an upper bound on *a\(B\).*  
Theorem 5.5. *Let B be a reduced basis for the full-dimensional lattice L. Then*  
i. *IIbjll2* \~ *211bj+III2.*  
*11. lib* III \~ 2\(n-l\)/4 *\(d\(L* \)\)l/n.  
111. *lib* III \~ 2\(n-l\)/2 min\{llbll\: *bEL, b =1=* a\}.  
iv. *a\(B\)* \~ *2n \(n-l\)/4 d\(L\).*  
*Proof* basis we have  
i. Since *bjand bj+l* are orthogonal, by statement b in the definition of a reduced  
By statement a of Definition 5.3, we have *aJ,\}+1* \~ \~ and hence *IIbj+11I2*  
\~ \~lIbjIl2.  
ii. By i, it follows that *IIbjll2* \~ 2-u-1\) *IIbrll2* and since b l = *br,* we obtain  
*n*  
*\(d\(L\)?* = 11 *IIbjl12* \~ 2-kJ\~1\(j-I\) \}=1  
*IIbIll2n* = *2-n \(n-I\)/2 IIbdfn*  
*.*  
iii. From the proof of ii, we have  
Hence, using Proposition 5.4, we obtain  
*Ilbll* \~ m\~nllbjll \~ 2-\(n-l\)/2 *IIbdl.*  
*1*  
iv. Since *bj* = L\{,=I *aijbT* and the vectors *bT* are orthogonal, it follows that  
\}  
*IIbj ll 2*  
= *L* a\~ *IIbrll2* \(with *ajj* = 1\)  
i=l  
*j-I*  
\~ *IIbjll2* + \~ I *IIbrll2* by statement a of Definition 5.3.  
*i=1* 5\. A Reduced Basis of a Lattice  
Now, using i, we obtain *Ilb;U2* \~ *2j-*  
*i IIbjll2* and hence  
199  
Finally,  
*Example* 5.1 *\(continued\). n n*  
*\(a\(B»2* = TI *IIbj ll 2*  
\~ *2n \(n-I\)/2* TI *IIbjll2* = *2n \(n-l\)/2\(d\(L »2*  
*j=l j=l*  
The following computations show that  
# •  
is a reduced basis for the lattice *L\:*  
M= *b,* = G\) so *a'2* = 0 and *al3 =1.*  
*b;=b2-a'2M= \(-I\)* so *a23=O.*  
bj = *b3* - al3M - *a23bi* = \( \~ \).  
Hence part a in the definition of a reduced basis holds. We will now check b of  
Definition 5.3 as follows\:  
*lib;* + *a12bfll2* = 5 \~ illbfl12  
= i· 4 = 3  
*IIbi* + *a23bill2* = 5 \~ \~lIbill2 = \~. 5 = \~.  
Hence *B* is a reduced basis.  
Checking the bounds in Theorem 5.5, we observe  
ii. *lib* III s 21/2 *\(d\(L»1/3* = 21/2 101/3 and  
iv. *a\(B\)* = TI\]=l *IIbjll* = \(120\)1/2  
\~ *23/2d\(L\)* = 20.21 /2.  
A Reduced Basis Algorithm for a Full-Dimensional Lattice *L*  
*Step 0\:* Let *B* be a basis of the lattice *L.*  
*Step* 1\: Let *\(bf,* ... , b\~\) be the Gram-Schmidt orthogonalization of *\(b l,* ... , *bn \)* with  
*aij* = *b7b\)lIbrIl2*  
*.*  
*Step* 2\: For *j* = 2, ... , *n* and for *i* = *j* - 1, ... , 1 replace *bj* by *bj* - *aijbi,* where *aij* is the  
integer closest to *aij.*  
*Step* 3\: If *IIbj+1* + aj,j+lb\~12 \< \~lIb\~12 for some *j,* interchange *bj* and *bj+ 1* and return to Step 1  
with the new basis *B.* 200 1.7\. Integer Lattices  
Theorem 5.6. *The above algorithm finds a reduced basis for L in polynomial time.*  
We will not prove this result. However, the reader may observe that even though the  
basis *B* changes in Step 2, the corresponding *B\** does not change. This implies that on  
termination of Step 2, I *ai\}* I \~ \~ for all *i* \< *j,* so condition a for a reduced basis is satisfied.  
Now we return to the shortest vector problem. We have seen that a reduced basis can be  
found in polynomial time and that for such a basis we obtain *a\(B\)/* Idet *B* I \~ *2n \(n-I\)/4.* By  
Proposition 5.3 it suffices to enumerate over *x* E *zn* \\ \{a\} with *Ix\)* I \~ *2n \(n-I\)/4.* Hence we  
have shown the following\:  
Theorem 5.7. *For fixed n, the shortest vector problem can be solved in polynomial time.*  
*Example* 5.1 *\(continued\).* Find a shortest vector in L. We already have a reduced basis  
-1  
2  
# o  
For *n* = 3, it suffices to enumerate the 53 - 1 vectors v = *Bx* with I *x\)* I \~ l23/2J = 2, and  
*x* E Z3 \\ \{a\}, giving  
# v=G\)  
as a shortest vector.  
The final item in this section is to show how a reduced basis can be used to solve  
"approximately" a feasibility version of \(1.5\), namely\:  
*The Simultaneous Diophantine Approximation Feasibility Problem \(SDAF\).* Given  
rationals *ab* ... *,am* e, and an integer *K* \> 0, decide if there exist integers *q"* ... , *qn* and  
*p* \> 0 such that *p* \~ *K* and I *paj* - *q i* \\ \~ e for *i* = 1, ... , *n.*  
The approach is to reformulate a "weak" version of SDAF as a problem of finding a  
short vector in a lattice.  
Theorem 5.8. *There is a polynomial-time algorithm which either*  
i. *determines that* SDAF *is infeasible or*  
ii. *finds integers qI,* ... , *qn and p* \> 0 *such that \\paj* - *qi* I \< *2n /2* e *\(n* + 1\)1/2 *for i =1,*  
... , *nand p* \< *2n /2 K \(n* + 1\)1/2.  
*Proof* Let *a* = *\(a"* ... *,an, elK\)* E *Rn+'* and let *ei* be the ith unit vector in *Rn+'.*  
Consider the lattice *L* generated by *\(e"* ... , *en,* - *a\).* For any *\(qI,* ... *,qn, p\)* E *zn+l,* we  
have *w* = 1\:7=1 *qiei* - *pa* E *L.* Furthermore, if *\(q"* ... *,qn, p\)* is a solution to SDAF, then  
*IWi* 1= *Iqi* - *aiP* I \~ efori = 1, ... *,n* and *IWn+l\\* = *IPe/KI* \~ e. Hence IIwil \~ *e\(n* + 1\)1/2  
•  
Now let *B* be a reduced basis for *L* with *b* 1 being its first column. By Theorem 5.6, *B* can  
be found in polynomial time. 6\. Notes 201  
By iii of Theorem 5.5, if lib 1 II \> *2\(n-l\)/2* E *\(n* + 1\)1/2  
, then lib II \~ *2-\(n-l\)/2* lib 1 II \> *E\(n* + 1\)1/2  
for all *bEL* \\ \{O\}. Hence SDAF has no solution. If Ilbtll \~ *2\(n-l\)/2* E *\(n* + 1\)1/2  
, choose  
*\(q', p'\)* E *zn+l* such that *b1* = *L7=1 qiej* - *p'a.* If *p' =1=* 0, then *\(q', p'\)* satisfies the conditions  
ofii because  
*Iqj* - *p'ai* I \~ *2\(n-l\)/2 E\(n* + 1\)1/2 for *i* = 1, ... , *n*  
and  
If *p'* = 0, then *b1* = *\(qi,* ... ,q\~, 0\) and IlbIlI \~ 1. Hence *2\(n-l\)/2 E\(n* + 1\)1/2 \~ 1, and  
*p* = 1, *q* i = laiJ satisfies ii. •  
6\. NOTES  
Section 1.7.1  
The problems considered in this chapter are related to classical topics in number theory  
\(see, e.g., Cassels, 1971\). Our study of these topics arises from the interest in polynomial-  
time algorithms and applications in integer programming. Also see Section 11.6.5.  
Section I.7.2  
The euclidean algorithm is surely the oldest algorithm in this book. Its complexity is  
analyzed in detail by Knuth \(1981\).  
Section 1.7.3  
In the preparation of this section and Section 5, we have made liberal use of both the  
article and the monograph by Grotschel, Lovasz, and Schrijver \(1984b, 1988\). Proposition  
3.2 is due to Khintchine \(1930\).  
Section 1.7.4  
Kannan and Bachem \(1979\) give a provably polynomial-time algorithm for computing  
Hermite normal form. The algorithm sketched here is due to Domich et al. \(1987\).  
The integral Farkas lemma, Corollary 4.6, appears in Edmonds and Giles \(1977\).  
Section 1.7.5  
.H\~-complete. Lagarias \(1985\) has shown that the simultaneous diophantine approximation problem is  
It is also the case that the nearest vector problem is .H\~-hard \(see Van Emde  
Boas, 1981\).  
The polynomial-time algorithm for finding a reduced basis and a short vector in a  
lattice is due to Lenstra, Lenstra and Lovasz \(1982\). These and related problems are also  
discussed by Bachem and Kannan \(1984\) and Kannan \(1987b\). 202 1.7\. Integer Lattices  
7. EXERCISES  
1. i\) Find the gcd of27,692 and 100,000.  
ii\) Find the continued fraction expansion of 100,000/27,692.  
iii\) Find the best rational approximation of 100,000/27,692 with denominator of  
100 or less.  
2\. Show that the continued fraction expansion of a rational number is unique.  
3. Prove Proposition 3.2.  
4. i\) Find the Hermite normal form of \(l\~ \~ J\).  
ii\) Find all solutions \(if any\) to the system of equations  
*12x* 1 + *6x* 2 + 7 *x* 3 = 8  
*2x* 1 + *9x* 2 + *4x* 3 = 7  
*xEZ3*  
*•*  
5\. Show that the Hermite normal form is unique.  
6\. Describe, in detail, a polynomial Hermite normal form algorithm based on Proposi-  
tion 4.10\. Apply the algorithm to the matrix  
*A* = *\(-I* -6  
-1  
-2  
7\. Describe a polynomial algorithm to find the Smith normal form of a matrix.  
8. Use the reduced basis algorithm to find a reduced basis of *L\(B\),* where  
6  
7  
# o  
is the basis of Example 5.1.  
9\. Solve the simultaneous diophantine approximation feasibility problem with  
\(at, *a2, a3\)* = \(\~ \~ \~\), € = t and *K* = 18. Part II  
GENERAL INTEGER  
PROGRAMMING **11.1**  
# The Theory  
of Valid Inequalities  
1. INTRODUCTION  
We consider the discrete optimization problem max\{cx\: *xES\},* where S \~ *Z1,* and we  
formulate it as an integer program by specifying a rational polyhedron  
*p* = *\{x* E *R1\: Ax* \~ *b\}* such that S = *zn* n *P.* Hence S = *\{x* E *Z1\: Ax* \~ *b\},* and the integer  
program can be written as  
max\{cx\: *Ax* \~ *b, x* E *Z1\).*  
Throughout this chapter, unless otherwise specified, *A* and bare *m* x nand *m* x 1  
matrices, respectively, with rational coefficients.  
The topics to be studied in this chapter and the next one concern the representation of  
an integer program by a linear program that has the same optimal solution. We have  
already established the existence of such a representation, namely,  
*max\{cx\: xES\}* = *max\{cx\: x* E conv\(S\)\}  
\(see Theorem 6.3 of Section 1.4.6\).  
Here we are interested in the constructive aspects of the representation. This will be  
done by using integrality and valid inequalities for *P* to construct suitable valid inequal-  
ities for the set S.  
*Example* 1.1. S = *\{x* E *Z1\: Ax* \~ *b\}.*  
-1  
*A* = 5  
\(  
-2 \~\), -2  
Figure 1.1 shows the polytope defined by the constraints *Ax* \~ *b* and *x* \~ 0 \(the outer  
polytope\), the feasible integral points \(black dots\), and conv\(S\) \(the inner polytope\).  
We have  
s = \{\(;\), G\), G\), \(;\), G\), \(\~\)\} = \{Xl, *x*  
*2*  
*6*  
*,* ••• , *x*  
*\}.*  
205 206 11.1. The Theory of Valid Inequalities  
4  
3  
2  
o '---------2----3-....a..-.-·  
----- Xl  
4  
Figure 1.1  
conv\(S\) is a polytope defined by the four extreme points  
In this small example, it is a simple matter to obtain a linear inequality representation of  
conv\(S\) from the four lines defined by the adjacent pairs of extreme points. In particular,  
conv\(S\) is defined by the constraints *A'x* \~ *b',* where  
-1  
*A'=* 0  
-1  
\(  
3 -;\} *b'=* \(\~f\}  
From the extreme points of conv\(S\) we can obtain its polar set, which is the set of valid  
inequalities for conv\(S\) \(see Proposition 5.1 of Section 1.4.5\). This is also the set of valid  
inequalities for S, since S \~ conv\(S\) and any valid inequality for S is also valid for  
conv\(S\) \(see Proposition 6.5 of Section 1.4.6\).  
Thus the set of valid inequalities for S is given by  
2nl + *2n2* - *no* \~o  
2nl + *3n2* - *no* \~o  
3nl + *3n2* - *no* \~o  
4nl - *no* \~o. 1\. Introduction 207  
The polyhedral cone defined by these four half-spaces is shown in Figure 1.2.  
The valid inequalities *1CX* \:\:\:\:\:; *no* and *yx* \:\:\:\:\:; *Yo* are said to be *equivalent* if *\(y, Yo\)* = *A\(n, no\)*  
for some A\> O. If they are not equivalent and there exists *Jl* \> 0 such that *y* \~ *Jln* and  
*Yo\:\:\:\:\:; Jl1Co,* then *\{x* E R\~\: *yx* \:\:\:\:\:; *Yo\}* c *\{x* E R\~\: *nx* \:\:\:\:\:; *no\}.* In this case we say that *yx* \:\:\:\:\:; *Yo*  
*dominates* or *is stronger than 1CX* \:\:\:\:\:; *no* or that *nx* \:\:\:\:\:; *1Co is dominated by* or *is weaker than*  
*yx* \:\:\:\:\:; *Yo.* A *maximal* valid inequality is one that is not dominated by any other valid  
inequality. Any maximal valid inequality for S defines a nonempty face of conv\(S\), and  
the set of maximal valid inequalities contains all of the facet-defining inequalities for  
conv\(S\).  
*Example* 1.1 *\(continued\).* The valid inequality 3XI + *4X2* \:\:\:\:\:; 24 is not maximal since it is  
dominated by the maximal valid inequality Xl + *X2* \:\:\:\:\:; 6. The valid inequality Xl \:\:\:\:\:; 4  
defines the zero-dimensional face \{\( 4 O\)\}, but it is not maximal since it is dominated by the  
facet-defining inequality *3x* I + *X2* \:\:\:\:\:; 12. \(see Figure 1.3\).  
Given *P* = *\{x* E R\~\: *Ax\:\:\:\:\:; b\}* and S = *P* n *zn,* facets of conv\(S\) can be constructed  
iteratively using integrality and the linear inequality description of *P.* This means that we  
start with the valid inequalities *Ax* \:\:\:\:\:; *band,* if they are not enough to define conv\(S\), we  
progressively construct stronger valid inequalities.  
We obtain valid inequalities for *P* by taking nonnegative linear combinations of rows of  
*Ax* \:\:\:\:\:; *b.* \(These can be weakened by adding in nonnegative linear combinations of  
*-x\:\:\:\:\:;* 0\). This gives the infinite family of valid inequalities  
\(1.1\) *\(uA* - *v\)x* \:\:\:\:\:; *ub* + *a* for all *u* E *R'\:,* v E R\~, and *a* \~ O.  
Moreover, under some technical assumptions stated below, all valid inequalities for *P*  
can be obtained in this way, and the linear combinations can be restricted to using, at  
most, min\(m, *n\)* rows of *A.*  
*\(0,4, 12\)*  
\(3, I, 12\)  
\(-6,0, -12\) \~\_----------\:\:0\(o,o,o\)  
\(-3,-3,-12\)  
Figure 1.2 208 11.1. The Theory of Valid Inequalities  
3  
*3Xl +x2* = 12  
Xl *+x2 =6*  
o 2 3  
Figure 1.3  
Proposition 1.1. *Let nx* \~ *no be any valid inequality for P* = *\{x* E R\~\: *Ax* \~ *b\}. Then*  
*nx* \~ *no is either equivalent to or dominated by an inequality of the form uAx* \~ *ub,*  
*u* E *R'\: if any of the following conditions hold\:*  
a. *P =1=* 0 *\(in this case no more than* min\(m, *n\) components of u need be positive\).*  
b. *\{u* E *R'\:\: uA* \~ *n\} =1= 0.*  
c. *A* = \(J '\), *where I is an n* x *n identity matrix.*  
*Proof* a. Since *\(n, no\)* is valid and *P =1=* 0, the linear program max\{nx\: *Ax* \~ *b,*  
*x* E R\~\} has a feasible solution and its value is bounded by *no.* Hence the dual linear  
program has a basic feasible solution *Uo* E *R'\:* with *uOA* \~ nand *uOb* \~ *no.* The vector *UO*  
has no more than min\(m, *n\)* positive components and *nx* \~ *\(uOA\)x* \~ *uOb* \~ *no.*  
b. If *P =1=* 0, see part a. So assume *P* = 0 and *u* E *R'\:* satisfies *uA* \~ *n.* If *ub* \~ *no,* we are  
done. Otherwise, since *P* = 0 there exists *li* E *R'\:* such that *liA* \~ 0 and *lib* \< O. Hence for  
some *P* \> 0 we obtain *\(u* + *pli\)A* \~ nand *\(u* + *pli\)b* \~ *no.*  
c. It is a simple exercise in linear programming duality to show that for any *n* there exists  
a *u* E *R'\:* with *uA* \~ *n. •*  
When *P* = 0, every inequality is valid for *P.* However, if conditions a and b fail, which  
is equivalent to the primal and dual linear programs being infeasible, then we cannot  
generate the valid inequality by nonnegative linear combinations.  
*Example* 1.2  
*Ox!* + *X2* \~ 1  
*Ox!* - *X2* \~ -2  
*xERi.* 1\. Introduction 209  
Here *P* = 0. Consider the valid inequality *x* I \~ 1. The dual feasibility region given by  
*OUI* + *OU2* \~ 1  
*UI* - *U2* \~ 0  
*U* ER\~  
is also empty. Thus conditions a and b fail. Moreover, it is obvious that *x* I \~ 1 is not  
equivalent to or dominated by an inequality of the form *\(Ou* 1 + *OU2\)X* 1  
*+ \(UI* - *U2\)X2* \~ *UI* - *2U2.*  
To avoid the trouble that can arise when *P* = 0 we frequently assume that the linear  
inequality description of *P* contains explicit bounding constraints, that is, *A* = \(1 '\).  
Since S \~ *P,* the inequalities \(1.1\) are also valid for S. However, unless conv\(S\) = *P,*  
there are valid inequalities for S that are not valid for *P* and hence cannot be obtained just  
from nonnegative linear combinations.  
Integrality must be used to obtain the inequalities for S that are not valid for *P.* We now  
consider techniques that use integrality to obtain valid inequalities.  
**Integer Rounding**  
This approach is based on the simple principle that if *a* is an integer and *a* \~ *b,* then  
*a* \~ lbJ, where lbJ is the largest integer less than or equal to *b.*  
Consider the matching problem on the graph G = *\(V, E\).* For *V* \~ *V,* let *E\( V\)* be the set  
of edges with both ends in *V* and let 6\( *V\)* be the set of edges with one end in *V.* A subset of  
edges is a matching if  
\(1.2\)  
*L x e* \~ 1 for all *i* E *V*  
\(1.3\)  
eE6\(\{i\}\)  
*xEZJE*  
*"*  
where *Xe* = 1 if *e* is in the matching and *Xe* = 0 otherwise.  
For any set *V* \~ *V,* the number of edges in a matching with both ends in *V* is at most  
HI VIJ. Thus if I VI = *2k* + 1, then  
\(1.4\)  
*L* xe\~k  
*eEE\(U\)*  
is a valid inequality for all *k* \~ 1.  
The inequalities 0.4\) cannot be obtained just by taking nonnegative linear combina-  
tions of the constraints \(1.2\). However, they can be justified algebraically by the following  
three-step argument.  
i. Take a linear combination of the constraints \(1.2\) with weights *Ui* = 1 for all *i* E *V*  
and *Ui* = 0 for all *i* E *V* \\ *V.* This yields the valid inequality  
\(1.5\) 1 1  
I *Xe* + -2 *L Xe* \~ -2 I *V I·*  
*eEE\( U\)* eEJ\( *U\)* 210  
11.1. The Theory of Valid Inequalities  
11. Since *Xe* \~ 0 for all *e* E *E,* it follows that  
\(1.6\)  
1  
-- *L Xe* \~ 0  
*2eEt5\(u\)*  
is a valid inequality. Adding \(1.5\) and \(1.6\) yields  
1  
\(1. 7\)  
*L* xe\~-2IUI.  
*eEE\(U\)*  
iii. From \(1.3\), the left-hand side of\(1. 7\) is an integer. Therefore, the right-hand side can  
be replaced by the largest integer equal to or less than it; that is, if 1 *U* 1 = *2k* + 1,  
then l\~ 1 *u* 11 = *k* is a valid right-hand side. Thus \(1.4\) is a valid inequality for all  
*k* \~ 1.  
In Section 111.2.3 we will prove that the convex hull of matchings is given by \(1.2\) and  
\(1.4\) for all odd sets *U* with 1 *U* 1 \~ 3, and *Xe* \~ 0 for all *e* E *E.* But, in general, the three-  
step procedure must be applied recursively.  
For S = *\{x* E Z\~\: *Ax* \~ *b\},* where *A* = *\(a* I, *a2,* ... , *an\)* and *N* = \{l, ... *,n\},* the three-  
step procedure yields the following\:  
i. *LjEN \(uaj\)xj* \~ *ub* for all *u* \~ 0;  
ii. *LjEN \(luajj\)xj* \~ *ub,* since *x* \~ 0 implies *-LjEN \(uaj -luajJ\)xj* \~ 0; and  
iii. *LjEN \(luajj\)xj* \~ *lub1,* since *x* E *zn* implies *LjEN \(luajJ\)xj* is an integer.  
The crucial step is iii, where we invoke integrality to round down the right-hand side.  
The valid inequality  
\(1.8\) I *\(luajj\)xj* \~ *lubJ*  
*JEN*  
can be added to *Ax* \~ *b,* and then the procedure can be repeated by combining generated  
inequalities and/or original ones. As noted in Proposition 1.1, in one application of the  
procedure it is sufficient to combine, at most, *n* inequalities. The procedure is called the  
*Chvatal-Gomory \(C-G\) rounding method,* and the inequalities it produces are called  
*C-G inequalities.* In Section 2, we will prove that by repeating the *C-G* procedure a finite  
number of times, all of the valid inequalities for S can be generated.  
In Example 1.1, *u* = \(fr *fz* 0\) in step i yields  
Then  
100  
X2\~- 22 \(step ii\)  
and  
\(step iii\).  
A geometric explanation of the procedure is shown in Figure 1.4. 1\. Introduction 211  
4 •  
3  
2  
o 2 3 4  
Figure 1.4  
In Example 1.1, *u* = Crr -rr 0\) yields *Xl* + *X* 2 \~ W. But since there are no points of S such that  
Xl + *X2* = W, it is permissible to push the hyperplane inward until it meets a point in S.  
However, step iii does not, in general, allow the hyperplane to be pushed in this far. It can  
be pushed in no further than an intersection with some point of Z2, and perhaps not even  
this far if the integral coefficients of *X* I and *X2* are not relatively prime. It is fortuitous in the  
example that the line *Xl* + *X* 2 = 6 happens to contain a point in S. Note, however, that if we  
started with the equivalent inequality *2x* 1 + *2X2* = W, then step iii would have yielded  
*2x* I + *2X2* \~ 13. The line *2x* 1 + *2X2* = 13 contains no points of Z2.  
The above discussion is summarized by the following result. Let the greatest common  
divisor of the integers *a* and *b* be denoted by gcd\{a, *b\}.*  
Proposition 1.2. *Let* S = *\{x* E *zn\: LjEN ajxj* \~ *b\}, where aj* E ZI *for* j *EN, and let*  
*k* = gcd\{al, ... , *an\}. Then* conv\(S\) = *\{x ERn\: LjEN \(a/k\)xj* \~ *lb/kj\).*  
*Proof* Using steps i and iii of the C-G procedure with *u* = *l/k,* it follows that  
*LjEN \(aJlk\)aj* \~ l\~J is a valid inequality for S. Since *gcd\{at!k,* ... , *an/k\)* = 1, it follows  
from Corollary 4.7 of Section 1.7.4 that *LjEN \(aj/k\)xj* = *lb/kJ* contains an infinite number  
of points of S - in particular, *n* affinely independent ones. Therefore the inequality  
represents a facet of conv\(S\). But conv\(S\) contains no other facets, since if *nx* \~ *no* is a  
different facet, then *nx* \~ *no* for all *x ERn* such that *LjEN \(aj/k\)xj* = *lb/kJ,* which is  
impossible. •  
Proposition 1.2 shows the limitations of one application of the C-G rounding method.  
The step i inequality *LjEN \(uaj\)xj* \~ *ub* must be weakened in step ii if there exists aj such  
that *ua j* is not integral. Then the best we can hope for from step iii is to intersect  
*\{x* E R\~\: *Ax* \~ *b\}* with conv\{x E *zn\: LjEN luajjxj* \~ *lubj\}.* 212 II.l. The Theory of Valid Inequalities  
Many other arguments can be used to generate valid inequalities. Some are explained  
and illustrated below.  
Modular Arithmetic  
Here we derive a valid inequality for the set of solutions to one linear equation in  
nonnegative integers, that is, S = *\{x* E Z\~\: *LjEN ajXj* = *ao\},* where *aj* E *R'* for allj.  
Let *d* be a positive integer and  
*Sd* = *\{x* E Z\~\: I *ajXj* = *ao* + *kd* for some integer *k\}.*  
*JEN*  
We are going to derive a valid inequality for S *d.* Then, since S s; S *d,* the inequality is valid  
for S.  
Let *aj* = *bj* + *ajd* for *j* = 0, 1, ... , *n,* where 0 \~ *bj* \< *d* and *aj* is an integer, that is, *bj* is  
the remainder when *aj* is divided by *d.* Then  
*Sd* = *\{x* E Z\~\: I *bjxj* = *bo* + *kd* for some integer *k\}.*  
*JEN*  
Now *LjEN bjxj* \~ 0 and *bo* \< dimply *k* \~ 0; hence we obtain the valid inequality  
\(1.9\) *IbjXj* \~ *boo*  
*JEN*  
Inequality \(1.9\) is nontrivial only if *d* does not divide *ao,* that is, only if *bo* \> O.  
For the set S given by  
*XEZ!,*  
inequality \(1.9\) with *d* = 12 yields Xl + *4X2* + *6X3* + *X4* \~ 9.  
An important set of inequalities of this type arises when *d* = 1 and *ao* is not an integer.  
Then \(1.9\) yields the valid inequality  
\(1.10\) I *\(aj* - *lajJ\)xj* \~ *ao* - laoL  
*JEN*  
which is called a *Gomory cutting plane* \(see Section 3\).  
For example, suppose *Xo* = 3i - *!x,* + lx2 - *\*X3* is an equation obtained in the solution  
of a linear program and we also require the variables to be nonnegative integers. Then  
\(1.10\) yields *!x,* + tx2 + ix3 \~ i.  
Disjunctive Constraints  
Proposition 1.3. *If LjEN n\}xj* \~ nJ *is valid for* S, C R\~ *and LjEN nJxj* \~ *n6 is valid for*  
S2 C R\~, *then*  
\(1.11\) I min\(n\), *nJ\)xj* \~ max\(n6, n\~\)  
*JEN*  
*is validfor* S, U *S2'* 1\. Introduction 213  
In other words, if we must satisfy one set of constraints or another set, but not  
necessarily both, and we know valid inequalities for each set, then \(1.11\) is a valid  
inequality for the disjunction of the two sets.  
This yields another systematic way, called the *disjunctive procedure,* of generating valid  
inequalities for the region S = *\{x* E Z\~\: *Ax* \~ *b\}.* The two steps of the procedure are\:  
i. *LjEN \(uaj\)xj* \~ *ub* for all *u* \~ O.  
ii. Given 6 E Zl, if  
\(a\) I *njx\)* - *a\(Xk* - 6\) \~ *no*  
JEN  
is valid for S for some *a* \~ 0 and  
\(b\) I *n\)x\)* + *P\(x k* - 6 - 1\) \~ *no*  
*JEN*  
is valid for S for some *P* \~ 0, then  
\(c\)  
is valid for S.  
Note that \(a\) shows that \(c\) is valid for Sl = S n *\{x* E Z\~\: *Xk* \~ 6\} and \(b\) shows that \(c\)  
is valid for S2 = S n *\{x* E Z\~\: *Xk* \~ 6 + n. Since S = Sl U *S2,* Proposition 1.3 establishes  
that \(c\) is valid for S.  
Inequalities generated by repeated application of the disjunctive procedure are called  
*D-inequalities.*  
*Example* 1.3. An example of the D-inequalities is shown in Figure 1.5, where  
The first two inequalities can be rewritten as  
1 3 1  
*--Xl* + *X2* - *-Xl* \~ -  
4 4 2  
and  
Using the disjunction *Xl* \~ 0 or *Xl* \~ 1 leads to the valid inequality -txl + *X2* \~! for  
S *=P* n Z2. 214 11.1. The Theory of Valid Inequalities  
o 2  
Figure 1.5  
An interesting property of the disjunctive procedure is that if we just consider a  
disjunction on a single variable, then every valid inequality for the disjunction is a  
D-inequality.  
Proposition 1.4. *Let P* = *\{x* E R\~\: *Ax* \~ *b\} and suppose A* = \(1'\). *Then nx* \~ *no is a valid*  
*inequality for*  
*P f* = *\(P* n *\{x'\: x k* \~ J\}\) U *\(P* n *\{x* \: *x k* \~ *is* + I\}\)  
*only if there exist a, p* \~ ° *such that LjEN 1!jX\}* - *a\(xk* - *is\)* \~ *no and L\}EN njX\} +*  
*P\(Xk* - *is* - 1\) \~ *no are valid/or P.*  
*Proof* Suppose that *1!X* \~ *no* is valid for *P'.* Then *1!X* \~ 1!o is valid for *P* n *\{x\: Xk* \~ J\}  
and *P* n *\{x\: Xk* \~ *is* + n. Hence by Proposition 1.1, there exists *\(u l*  
*, a\)* \~ ° and *\(u 2*  
*, P\)* \~ 0  
such that  
and  
where *ek* is the *kth* unit vector. Since *u 1*  
\~ 0 and *u2*  
*i* = 1, 2, are valid for *P* and are equal to or dominate  
\~ 0, the inequalities *uiAx* \~ *uib* for  
# •  
In the next section we will show that for 0-1 problems it is possible to use this procedure  
variable by variable to produce all valid inequalities for S = *P* n *Bn.* 1. Introduction 215  
Another application of Proposition 1.3 is to derive a valid inequality for the system  
\(1.12\)  
where *aj* E *RI* for all\}. The feasible set S of\(1.12\) is contained in SI U *S2,* where  
and  
Note that when *ao* is not integral, S 1 U S2 is the standard disjunction used in enumeration,  
that is *Xo* \~ laoJ or *Xo* \~ laoJ + 1. Since  
and  
we obtain from \(1.11\) the valid inequality *LjEN 1CjXj* \~ - 1, where  
\(1.13\) . *\(aj a j \)* fi . *N*  
*1Cj* = mIn l J *'l* J 1 or *\}* E .  
*ao* - *ao ao* + - *ao*  
If *Xo* = 3\~ - \~Xl + ix2 - *¥X3,* where *Xo* E Zl and *x* E R\~, then \(1.13\) yields the valid  
inequality 1xI + *7X2* + \~X3 \~ 1. Note that this inequality is weaker than the one obtained  
previously where we required *x* E *zl.*  
# Boolean Implications  
Here we derive some valid inequalities that require the assumption  
S C *B n* = *\{x* E Z\~\: *Xj* \~ 1 for\} EN\}.  
Suppose  
\(1.14\)  
where the *aj's* and *b* are positive integers, that is, S is the feasible set for a 0-1 knapsack  
problem. Let C C *Nbe* such that *LjEC aj* \> *b.* Then a valid inequality for \(1.14\) is  
\(1.15\) *L Xj* \~ ICI -1.  
*JEC* 216 11.1. The Theory of Valid Inequalities  
Valid inequalities of this form are used in the solution of general 0-1 integer programs \(see  
Sections 11.2.2 and 11.6.2\).  
The system  
\(1.16\) *T= \{x* EBl, *Y ER1\: L Yj* \~ *nx, Yj* \~ 1 for\} *EN\}*  
*JEN*  
arises in many models. Here we see that *Yj* = 0 if *x* = 0 and *Yj* \~ 1 if *x* = 1. Hence  
\(1.17\) *Yj* \~ *x* for\} EN  
are valid inequalities for \(1.16\).  
These are just two examples of a variety of ad hoc tricks for obtaining valid inequalities  
from Boolean implications.  
Geometric or Combinatorial Implications  
The valid inequalities that we illustrate here are related to the logical implications  
considered above but are associated with combinatorial systems such as graphs and  
matroids. One example is the set of constraints \(1.4\) for the matching problem.  
To give another, consider the node-packing problem on the graph G = *\(V, E\).* A subset  
of nodes is a packing if  
\(1.18\) *Xi* + *Xj* \~ 1 for all \(i,\}\) E *E*  
and  
\(1.19\)  
where *Xi* = 1 ifnode *i* is in the packing and *Xi* = 0 otherwise. In the graph of Figure 1.6, no  
more than one of the pairwise adjacent nodes \{4, 5, 6, 7\} can be in a packing and no more  
than two of the nodes from the cycle \{l, 2, 3, 4, 5\} can be in a packing. Thus we obtain the  
valid inequalities  
\(1.20\)  
and  
\(1.21\)  
neither of which can be obtained from nonnegative linear combinations of \(1.18\) and  
non negativity.  
Figure 1.6 2\. Generating all Valid Inequalities 217  
Valid Inequalities and Model Formulation  
At this point, we suggest that the reader go back to Chapter 1.1 to observe that valid  
inequalities, particularly those derived from Boolean and combinatorial implications,  
have been used to formulate models. So where should we draw the line between using valid  
inequalities in formulating models and using them in the solution of the model? There is  
no right answer to this question. It is sufficient to formulate the model so that  
*\{x* E z\~\: *Ax* \~ *b\}* is precisely the set of feasible points. But the formulation is not unique  
and, in particular, any valid inequalities can be added.  
In some cases, such valid inequalities imply the ones from which they are derived, so the  
original ones can be deleted from the formulation. For example, by summing the  
constraints \(1.17\) we obtain the linear constraint of\(1.16\), which can then be deleted from  
the formulation. By using the constraint \(1.20\), we can delete the constraints \(1.18\) for the  
six edges joining nodes 4, 5, 6, and 7\. However, \(1.21\) does not imply any of the edge  
constraints \(1.18\).  
With hindsight, the valid inequalities that we would like to have in our formulation are  
precisely those that are active in an optimal solution. Thus, given a formulation, a guiding  
principle is to include any additional valid inequalities that we know, which we believe  
might be active in an optimal solution.  
The methods given above for generating valid inequalities do not exhaust all possibil-  
ities, and others will be introduced later. Usually, a valid inequality can be obtained by a  
variety of different arguments, although one method may produce it directly while  
another requires repeated applications of the procedure. For example, two applications of  
the C-G procedure are needed to derive \(1.20\) from \(1.18\) and \(1.19\).  
2. GENERATING ALL VALID INEQUALITIES  
Given S = *\{x* E z\~\: *Ax* \~ *b\},* where *A* = *\(aI,* ... *,an\),* in Section 1 we used the C-G  
procedure to develop the valid inequalities  
*L luajJxj* \~ *lubJ*  
*JEN*  
for any *u* \~ 0 and showed how integrality can be used to develop D-inequalities for S. We  
will show now, by using these procedures a finite number of times, that it is possible to  
generate all valid inequalities for S. To simplify exposition, we say that any valid  
inequality dominated by a C-G inequality \(D-inequality\) is also a C-G inequality  
\(D-inequality\).  
0-1 Problems  
We consider 0-1 problems with *P* = *\{x* E R\~\: *Ax* \~ *b, x* \~ 1\} and S = *P* n *zn.* First we will  
show that all valid inequalities for S are D-inequalities.  
Define *pn* = *P,* and for *t* = *n* - 1, ... ,0 define  
*pt* = *conv\[\(pt+1* n *\{x\: Xt+1* = O\}\) U *\(pt+1* n *\{x\: Xt+1* = l\}\)J.  
For *n* = 2, *p2* = *P, pI* and *po* are shown in Figure 2.1. Note that in this example,  
conv\(S\) = *po.* 218 11.1. The Theory of Valid Inequalities  
\(0,1\) *p2* \(1,1\)  
\(0,0\) \(1,0\)  
Figure 2.1. *p2* is the outer polytope. *pI* is the shaded polytope. po is the line joining \(0, 1\) and \(1, 0\).  
Since *pO* contains all of the integral points in *P,* in general we have conv\(S\) \~ *pO.* We  
prove that all valid inequalities for S are D-inequalities by showing that all valid  
inequalities for S are D-inequalities for *pO.* This also yields *pO* \~ conv\(S\), so *pO* = conv\(S\).  
Suppose *nx* \~ *no* is valid for S and *OJ* \> 0, then so are the inequalities  
where NJ n *N'* = 0, NJ U *N'* = \{l, ... *,t\},* and *t* = 0, ... *,n \(t* = ° means that  
\~ = *N'* = 0\). We will show that if *nx* \~ *no* is valid for S, then It\(\~, *N'\)* is aD-inequality  
for *pt.* In particular, when *t* = 0, we obtain that *nx* \~ *no* is a D-inequality for *pO.*  
The proof uses a backward induction that is convenient to represent by an enumeration  
tree in which the nodes at level *t* represent all partitions of \{l, ... , t\} \(see Figure 2.2\). The  
intention of the figure is to show that *ItCN\\ N'\)* is derived from It+'\(\~ U \(t + l\), *N'\)* and  
*It+,\(NJ, Nt* U *\{t* + l\).  
Proposition 2.1. Iflt+'\(\~ *pl+', then* It\(\~, U \{t + I\}, *N'\) and* Il+'\(\~' *N'\) is aD-inequality for pt.*  
*Proof* By hypothesis, we have  
*nx* - *OJ L Xj* - *OJ L* \(1 - *Xj\)* \~ *no*  
jENllU\{t+l\} *JENI*  
*N I* U *\{t* + 1\}\) *are D-inequalitiesfor*  
and  
*nx* - *OJ L Xj* - *OJ L* \(1 - *Xj\)* \~ *no*  
jENll jEN1U\{t+1\}  
are D-inequalities for *pt+l.* Since *Xt +,* E \{o, l\), step ii of the disjunctive procedure estab-  
lishes that It\(\~, *N'\)* is a D-inequality for *pt. •* 2. Generating all Valid Inequalities 219  
Therefore if we can establish that the inequalities *I* n\(\~' Nt\) are D-inequalities for  
*P* = *pn,* then Proposition 2.1 yields that It\(\~, Nt\) is a D-inequality for *pt* for all partitions  
\(\~, Nt\) of \{l, ... , *t\}* and all *t* = 0, 1, ... , *n* - 1. In fact, it suffices to show that all of the  
inequalities In\(\~' Nt\) are valid for *P,* since any valid inequality for *P* can, by Proposi-  
tion 1.1, be obtained by step 1 of the disjunctive procedure.  
Proposition 2.2. *If I.jEN njXj* \~ *no is a valid inequality for* S, *there exists an \(JJ* \~ 0 *such*  
*that all of the inequalities* In\(\~, Nt\) *are valid for P.*  
*Proof* If *P* = 0, the result follows immediately from Proposition 1.1. If *P* "\* 0, con-  
sider the extreme points *\{Xk\}kEK* of *P.* If *Xk* E S, then *Xk* satisfies the inequality In\(\~' Nt\)  
because *nxk*  
\~ *no* by hypothesis and *\(JJ* \~ O.  
If *Xk* \$. S, let  
and *a* = min *ak* \> O. Thus for *Xk* \$. S we obtain  
*\(kEK\:xk\$S\}*  
*-\(JJ* I *xj* - *\(JJ* I \(l - *xj\)* \~ - *\(JJa.*  
*jEJVO JENI*  
Now let *y* = *maxxEP\(nx* - *no\).* Hence *nxk*  
\~ *y* + *no* for all *k* E *K.* Thus for *Xk* \$. S we have  
*1CXk* - *\(JJ* I *xj* - *\(JJ* I \(1 - *xj\)* \~ *y* + *no- ma.*  
jENO JENI  
The result follows by taking *m* \~ *yja* and observing that an inequality valid for the  
extreme points of *P* is valid for *P. •*  
*NO=* 0, *Nl=* 0  
*NO* = 0, *N* 1 = \{I, 2\}  
Figure 2.2 220 11.1. The Theory of Valid Inequalities  
Theorem 2.3. *Every valid inequality for* S = *P* n *zn with P* = *\{x* E R\~\: *Ax* \~ *b, x* \~ I\} *is*  
*aD-inequality.*  
Theorem 2.4. *pO* = conv\(S\).  
Theorem 2.4 indicates a surprising property of 0-1 integer programs-namely, that it  
suffices to integralize one variable at a time to obtain the convex hull.  
We have actually shown a somewhat stronger result that will be used in Section 6.  
Let *Ql* = conv\{x E *P\: \(xt+\[,* ••• *,xn\)* E *zn-l\},* so *Ql* s; *pt* for *t* = 0, 1, ... *,n* and  
*QO* = conv\(S\). By assuming that *nx* \~ *no* is a valid inequality for *Qt,* the proof we have  
given shows\:  
Theorem 2.5. *Every valid inequality for Qt, t* = 0, ... , *n, is a D-inequality and pt* = *Qt,*  
Now we show that all valid inequalities for the 0-1 problem are also C-G inequalities.  
The structure of the proof is the same as for D-inequalities, with two differences. The  
induction step requires the inequalities  
*L 7CjXj* - *L Xj* - *L* \(1 - *Xj\)* \~ *7Co.*  
*JEN jENO JENl*  
Note that *I;\(NJ, N 1\)* is the inequality *It\(NJ, N 1\)* with *0\)* = 1. Also, we need to assume that  
*7CX* \~ *no* + 1 is a C-G inequality to prove that *TCX* \~ *7Co* is a C-G inequality.  
Proposition 2.6. *If \(n, no\)* E *zn+ 1*  
*, LjEN njXj* \~ *no* + 1 *is a C-G inequality, and*  
*LjEN njXj* \~ *no is validfor* S, *then*  
*L njXj* - *L Xj* - *L* \(l - *Xj\)* \~ *no*  
*JEN jENO JENI*  
*is a* C - G *inequality for* S *for all partitions* \~, N1 *of N.*  
*Proof* By Proposition 2.2, inequality *In\(NJ, N 1\)* is a C-G inequality. If *0\)* \< 1, we add  
\(1 - *\(j\)* times the inequality *Xj* \~ 1 for *j* E *N 1*  
*.* The resulting inequality is then dominated  
by I\~\(\~, *N 1\).* If *OJ* \> 1, then combining *I* n\(\~' N l \) with weight *1/0\)* and *LjEN njxj* \~ *7Co* + 1  
with weight *\(0\)* - *1\)/0\)* and rounding gives I\~\(\~, *N 1\). •*  
Theorem 2.7. *If \(7C, 7Co\)* E zn+ 1  
, *LjEN njXj* \~ *7Co* + 1 *is a C-G inequality, and LjEN*  
*7CjXj* \~ *no is validfor* S, *then I;\(NJ,* Nt\) *is a C-G inequality for all disjoint* subsets\~, N 1 *of*  
*N.*  
*Proof* Here again we argue by working up through the nodes of an enumeration tree.  
Suppose NJ U N 1 = \{l, ... , t\}. Then by the induction hypothesis, it follows that  
and  
are C-G inequalities. Combining these two inequalities with weights of! and rounding  
establishes that *I;\(NJ,* N 1\) is a C-G inequality. • 2. Generating all Valid Inequalities 221  
We now show that every valid inequality for S with integral coefficients is a C-G  
inequality.  
Theorem 2.8. *Let nx* \~ *no with \(n, no\)* E zn+ t *be a valid inequality for* S = *P* n *zn with*  
*P* = *\{x* E R\~\: *Ax* \~ *b, x* \~ 1\}. *Then nx* \~ *no is a C-G inequality for* S.  
*Proof* Let  
\(2.1\) n\~P = max\{nx\: *x* E *P\}.*  
If *P* = 0, the result is immediate from Case b of Proposition 1.1. Otherwise 1r\~P is finite  
because *P* is bounded. Let *\(VO, WO\)* be an optimal solution to the dual of\(2.1\). Consider the  
C-G inequality  
\(2.2\)  
By dual feasibility, it follows that *lvoa\}* + *wJJ* \~ 1r\} for *j* EN, so if ln\~PJ \~ *no* we are done.  
Otherwise we apply Theorem 2.7 to \(2.2\) l1r\~PJ - *no* times. •  
*Example* 2.1\. Given a set S = *P* n *Z2,* where *P* is given by  
xER\~,  
we show that the valid inequality 9Xl + *7X2* \~ 10 is a C-G inequality. To prove this it  
suffices to show that 9xI + *7X2* - XI \~ 10 and 9xI + *7X2* - \(l - *xd* \~ 10 are C-G inequal-  
ities.  
Solving max\{9xI + *7X2\: X* E *P\},* we obtain an optimal dual solution *u* = a \~ 0\) and the  
inequality 9xI + *7X2* \~\~. Rounding gives us the C-G inequality  
\(2.3\)  
Now we construct a tree as in Figure 2.2. For the leaves of the tree we determine  
inequalities that are equivalent to or dominate  
By examining all the leaves of the tree we establish a priori that it suffices to take ill = 9. We  
then consider the leaves with \~ U N I  
= \{l, 2\} and 1 E N I  
.  
For \~ = \{2\} and Nt = \{l\}, weights of \(0 18 0\) on the original inequalities give the valid  
inequality *18x* I \~ 18, or  
\(2.4\)  
which dominates 9xI + *7X2* - 9\(1 - XI\) - *9X2* \~ 10. 222 11.1. The Theory of Valid Inequalities  
Now as explained in the proof of Proposition 2.6, we weight \(2.4\) by \~ and \(2.3\) by,  
and round to obtain  
\(2.5\)  
For ftJ = 0 and Nt = \{l, 2\}, weights of\(2 8 4\) on the original inequalities give the valid  
inequality 18xI + *16x2* \~ 26 for *P,* which is the same as  
\(2.6\)  
Using the rounding procedure to combine \(2.3\) and \(2.6\) with respective weights of\~ and \~  
yields  
\(2.7\)  
Now we move up the tree as explained in Theorem 2.7. For ftJ = 0 and *N 1*  
and \(2.7\) combined with weights \(!!\) and rounded yield  
= \{l\}, \(2.5\)  
\(2.8\)  
Similarly, for ftJ = \{I\} and *N 1*  
= 0, we obtain the C-G inequality  
\(2.9\)  
The integer rounding procedure applied to \(2.8\) and \(2.9\) with weights \(!!\) gives the  
desired result.  
Bounded Integer Variables  
We now consider the case where S = *P* n *zn* and where *P* = *\{x* E R\~\: *Ax* \~ *b, x* \~ *d\}* is a  
polytope but is not contained in the unit cube. Again we will establish constructively that  
every valid inequality is a C-G inequality by showing that if *nx* \~ *no* + 1 is a C-G  
inequality and *nx* \~ *no* is valid for S, then *nx* \~ *no* is also a C-G inequality.  
The approach, however, is different and more complicated than our approach to the  
0-1 case because for bounded integer variables we cannot obtain conv\(S\) by imposing  
integrality one variable at a time.  
We are given that *nx* \~ *no* is valid for S and that *x* \~ *d* = *\(d, d,* ... , *d\)* is valid for *P-*  
the bounded variable assumption. Now let Si be any integer between 0 and *d* and let *k* be  
any integer between 0 and *n.* Then the following inequality is valid for S\:  
*k k k*  
f1 *\(d* + 1 - sJ *\(nx* - *no\)* + I TI *\(d* + 1 - *Sj\) \(Xi* - *d\)* \~ O.  
*i=1 i=1 j=i+l*  
We denote this inequality by *L\(s* h ... , S *k\).* If *k* = 0, the inequality is simply *nx* - *no* \~ 0  
and is denoted by *L\(0\). N\(s* h ... , S *k\)* denotes the same inequality with right-hand side  
of 1, and *Mk* denotes the inequality *Xk* - *d* \~ O. Thus we are given that *N\(0\)* is a C-G  
inequality, and we wish to establish that *L\(0\)* is a C-G inequality. 2. Generating all Valid Inequalities 223  
*Example 2.2.* With *d* = 2 and *n* = 2, the inequalities *L\(Sb* ... *,Sk\)* are\:  
*L\(2,* 2\) *\(nx* - *no\) + \(xl-2\)+ \(X2* - 2\) \~ 0  
*L\(2, 1\) 2\(nx* - *no\)* + *2\(x!* - 2\) + *\(X2* - 2\) \~ 0  
*L\(2,* 0\) *3\(nx* - *no\)* + *3\(x!* - 2\) + *\(X2* - 2\) \~ 0  
*L\(2\) \(nx* - *no\) + \(Xl* - 2\) \~O  
*L\(l,2\) 2\(nx* - *no\) + \(x!-2\)+ \(X2* - 2\) \~ 0  
*L\(l,* 1\) *4\(nx* - *no\)* + *2\(x!* - 2\) + *\(X2* - 2\) \~ °  
*L\(l,O\) 6\(nx* - *no\)* + *3\(Xl* - 2\) + *\(X2* - 2\) \~ °  
*L\(l\) 2\(nx* - *no\) + \(x!* - 2\) \~o  
*L\(O,* 2\) *3\(nx* - *no\) + \(x!-2\)+ \(X2* - 2\) \~ °  
*L\(O,* 1\) *6\(nx* - *no\)* + *2\(x!* - 2\) + *\(x!* - 2\) \~ 0  
*L\(O,* 0\) *9\(nx* - *no\)* + *3\(x!* - 2\) + *\(Xl* - 2\) \~ °  
*L\(O\) 3\(nx* - *no\) + \(Xl* - 2\) \~O  
*L\(0\) \(nx* - *no\)* \~ 0.  
An important component of the proof is the order in which we show that the  
inequalities *L\(s* b ••• , S *k\)* are C-G inequalities. We say that *t* = *\(t* h ••• , *t n\)* is *lexicograph-*  
*ically larger* than S = *\(Sb* ••. *,sn\), t* 5. *s,* if, for some *i,* 1 \~ *i* \~ *n, tj* = *Sj* for\} \< *i* and *ti* \> *Si.*  
We then show that the inequalities are C-G in lexicographically decreasing order using the  
convention that if *k* \< *n, \(SI,* ..• *,Sk\)* is regarded as the n-vector \(sJ, ... *,Sk,* -1, ... ,-1\).  
Thus *\(t* b ••• , *tt\)* 5. *\(Sb* ... , *Sk\)* either if, for some *i* \~ min\(l, *k\), tj* = *Sj* for\} \< *i* and *ti* \> *Si*  
or if *I* \> *k* and *tj* = *Sj* for\} = 1, ... , *k.* This order is shown in Example 2.2.  
The order can also be interpreted as a right-to-Ieft search through an enumeration tree  
\(see Figure 2.3\).  
We will be repeatedly adding together valid inequalities.  
\(0,2\) \(1,2\) \(2,2\)  
Figure 2.3 224 11.1. The Theory of Valid Inequalities  
Proposition 2.9  
i. *L\(sJ,* ... *,Sk* - 1\) = *L\(sJ,* ... *,Sk\)* + *L\(sJ,* ... *,Sk-I\)* for 1 \~ *Sk* \~ *d.*  
ii. *L\(sJ,* ... *,Sk-J, d\)* = *L\(sJ,* ... , *Sk-I\)* + *Mk*  
iii. *L\(sJ,* ... , *Sk-J, Sk\)* = *L\(sJ,* ... , *Sk-I\) \(d* + 1 - *Sk\)* + *Mk*  
By repeated application of equalities i and ii, we obtain the following proposition.  
Proposition 2.10  
i. *L\(SJ"",Sk\)=* I *L\(SJ, ... ,Si-J,Si+ 1\)+L\(0\)+* I *M i;*  
*\{i\:isk,s;\<d\}* \{i\: *isk,s;=d\}*  
ii. *N\(sJ,* ... *,Sk\)* = I *L\(sJ,* ... , *Si-J, Si* + 1\) + *N\(0\)* + I *Mi.*  
*\{i\:isk,s;\<d\} \{i\:isk,s;=d\}*  
The proofs of these two propositions are elementary exercises.  
Observe from statement ii of Proposition 2.10 that if *L\(tJ,* ... , *t\{\)* is a C-G inequality  
for all *\(tJ, ... , t\{\)* \$. *\(sJ,* ... , *Sk\),* it follows that *N\(sJ, ... ,sd* is a sum ofC-G inequal-  
ities and hence is a C-G inequality. Therefore the critical step is to deduce from this that  
*L\(sJ,* ... , *Sk\)* is also a C-G inequality.  
Proposition 2.11. *For any S* E *Zk with \(SI,* ... , *sd/;* 0, *if x ERn satisfies at equality the*  
*inequalities L\(SI,* ... , *Si* + 1\) *for i* \~ *k, Si* \< *d, Mi for i* \~ *k, Si* = *d,and N\(\(\), then Xi* = *Si*  
*for i* = 1, ... , *k.*  
*Proof* We argue by induction. If *k* = 1 and *Sk-I* \< *d,* we obtain  
*\(d* + 1 - *\(SI* + *l\)\(nx* - *no\)* + *\(XI* - *d\)* = 0,  
or *\(d* - *SI\)* + *XI* - *d* = 0, or *XI* = *s\],* since *nx* - *no* = 1. If *SI* = *d,* then *XI* = *SI* is immediate,  
since *Ml* holds at equality. Now suppose by induction that the claim holds for *k* - 1, that  
is, *Xi* = *S* i for *i* = 1, ... , *k* - 1. If *S k* = *d,* the result is immediate, since *M k* is satisfied at  
equality. If *Sk* \< *d,* we observe that since *N\(sJ,* ... *,Sk-l\)* is a sum of inequalities satisfied at  
equality, it also is satisfied at equality; it follows that *L\(sJ,* ... , *Sk-I\)* has a slack of 1. But by  
statement iii of Proposition 2.9,  
Since *L\(sJ,* ... , *Sk* + 1\) is satisfied at equality, it follows that *d* - *Sk* + *Xk* - *d* = 0\. •  
We now associate with *\(s\],* ... *,Sk\)* a polytope *P\(sJ,* ... *,Sk\)* given by  
*\{X* E R\~\: *Ax* \~ *b, X* \~ *d, nx* \~ *no* + 1, *X* satisfies *L\(t\],* ... , *t\{\)*  
for *\(tJ,* ... , *tl\)* \$. *\(sJ,* ... , *Sk\)\}.* 2\. Generating all Valid Inequalities 225  
Proposition 2.12. *L\(sl, ... , sd is a C-G inequality for pes!,* ... , *Sk\)* n *zn.*  
*Proof* If *k* \< *n,* then *L\(st,* ... *,Sk,* 0\) is an inequality defining *pest,* ... *,Sk\)'* By  
statement iii of Proposition 2.9,  
*L\(st,* ... , *Sk,* 0\) = *\(d* + 1\) *L\(st,* ... , *Sk\)* + *Mk+l'*  
Multiplying the inequality *L\(sb* ... , *Sk,* 0\) by *1/\(d* + 1\), followed by rounding, establishes  
that *L\(st,* ... *,Sk\)* is a C-G inequality for *pes!,* ... *,Sk\)* n *zn.*  
Now suppose *k* = nand *S* '\* *d.* By statement ii of Proposition 2.10, the inequality *N\(s!,*  
... , *S n\)* is valid for *pes* b ... , *S n\).* Suppose it is satisfied at equality. Then each of the  
inequalities appearing in the statement of Proposition 2.11 is satisfied at equality, and it  
follows that *Xi* = *Si* for *i* = 1, ... , *n.* Moreover, since *N\(0\)* is satisfied at equality, we obtain  
*nx* = *no* + 1. However, since S = *pes* b .•• , *S n\)* n *zn* and *nx* \~ *no* is valid, there is no  
feasible integer point with *nx* = *no* + 1. Hence *N\(s* h ••• , *S n\)* cannot be satisfied at equality  
for any point in *pes* b ••• , *S n\).* This means that *N\(s* b .•• , *S n\)* with its right-hand side  
reduced bYe\> 0 is a valid inequality for *P\(Sl,* ... *,sn\)'* Hence by rounding this inequality  
we obtain *L\(s* b ••• , *S n\).* Thus *L\(s* h .•. , *S n\)* is a C-G inequality for *pes* t, ••• , *S n\)* n *zn.*  
Finally, the proof that *L\(d\)* is a C-G inequality for *P\(d\)* n *zn* is similar to the proof of  
Proposition 2.2 and is not repeated here. •  
In particular, Proposition 2.12 states that *L\(0\)* is a C-G inequality for *P\(0\)* n *zn* = S.  
Thus  
Theorem 2.13. *Let P= \{x* E R\~\: *Ax* \~ *b, x* \~ *d\} and let* S = *P* n *zn. lfnx* \~ *no* + 1 *is a*  
*C-G inequality for Sand nx* \~ *no is validfor* S, *then nx* \~ *no is a C-G inequality for* S.  
Corollary 2.14. *IfS* = 0, *then Ox* \~ -1 *is a C-G inequality for* S.  
Finally, using the argument given in the proof of Theorem 2.8, we establish the  
generality of C-G inequalities.  
Theorem 2.15. *Let nx* \~ *no with \(n, no\)* E *zn+* 1 *be a valid inequality for* S = *P* n *zn with*  
*P* = *\{x* E R\~\: *Ax* \~ *b, x* \~ *d\}. Then nx* \~ *no is a C-G inequality for* s.  
Theorem 2.15 also holds for unbounded sets in *zn,* but the only known proof of the  
result uses a very different technique.  
Theorem 2.16 *Let nx* \~ *no with \(n, no\)* E *zn+l be a valid inequality for* S = *\{x* E 2'\:.\:  
*Ax* \~ *b\}* '\* 0. *Then nx* \~ *no is a C-G inequality for* s.  
We now consider how many applications of step iii of the C-G procedure are necessary  
to define the convex hull of S = *P* n *zn,* when *P* is a nonempty rational polytope. We saw  
earlier that ifmax\{nx\: *x* E *P\}* = n\~P, then the inequality *nx* \~ ln\~PJ can be obtained by one  
application of the C-G procedure.  
Define the *elementary closure of P* to be  
*e\(P\)* = *\{en, no\)\: nj* = *luajJ* for *j* EN, *no* = *lubj* for some *u* E *R'\:\}.*  
Then, by definition of the C-G procedure, *e\(P\)* contains all of the nondominated C-G  
inequalities that can be obtained by one application of the procedure. 226 11.1. The Theory of Valid Inequalities  
**Proposition** 2.17. *ll\(n, no\)* E *e\(P\), then no* \~ ln\~Pj.  
*Proof* Since *\(n, no\)* E *e\(P\),* there exists *u* E R\:;Z such that *luajJ* = *nj* for *j* EN and  
*lubJ* = *no.* Consider any such *u.* Since *uaj* \~ *luajJ* for *j EN,* it follows that *u* is a feasible  
solution to the dual *ofmax\{nx\: x* E *Pl.* Thus *ub* \~ *n&P* and *no* = *lubJ* \~ In&PJ. •  
For Example 2.1, the inequality *9xI* + *7X2* \~ 11 is of the form *LjEN luajJxj* \~ *tubJ* since it  
has been obtained from an optimal dual solution. Hence *\(n* = \(9,7\), *no* = 11\) E *e\(P\).*  
Proposition 2.17 implies that if *n* = \(9, 7\) and *no* \~ 10, then *\(n, no\)* \$. *e\(P\).* Thus, it would  
be interesting to know, for example, the minimum number of repetitions of the C-G  
procedure needed to derive *9xI* + *7X2* \~ 9 and, more generally, any valid inequality.  
We say that a valid inequality *nx* \~ *no* for S = *zn* n *P =1=* 0 is of *rank k* with respect to  
*P* \~ *R1* if *nx* \~ *no* is not equivalent to or dominated by any nonnegative linear combina-  
tion of C-G inequalities, each of which can be determined by no more than k-1  
applications of the C-G procedure, but is equivalent to or dominated by a nonnegative  
linear combination of some C-G inequalities that require no more than *k* applications of  
the procedure. Thus the rank 0 inequalities are those that are equivalent to or dominated  
by a nonnegative linear combination of the defining inequalities of *P,* and the rank 1  
inequalities are those that are not of rank 0 but are equivalent to or dominated by a  
nonnegative linear combination of the defining inequalities of *P* and those in *e\(P\).*  
Theorem 2.16 shows that every valid inequality for S = *zn* n *P* =1= 0 is of finite rank for  
any rational polyhedron *P.*  
In Example 2.1, 9Xl + *7X2* \~ 11 is of rank 1. The construction *of9xl* + *7X2* \~ 10 shows  
that its rank is, at most, 4. By constructing an inequality by the C-G procedure, we  
determine an upper bound on its rank, but determining the actual rank appears to be very  
difficult.  
We use the notation *r\(n, no\)* = *k* to represent the rank of a valid inequality *nx* \~ *no* for  
S = *zn* n *P.* The rank of *P* is defined to be  
*pep\)* = *max\{r\(n, no\)\: \(n, no\)* is valid for S = *P* n *zn\}.*  
Thus *pep\)* is the number of applications of the C-G procedure needed to determine some  
facet of conv\(S\) if we begin with S = *P* n *zn.* Note that *p* = 0 if and only if conv\(S\) = *P.* If  
S is the set of matchings of a graph and *P* = *\{x* E R\~\: x satisfies \(l.2\)\}, then  
conv\(S\) = *P* n *\{x\: x* satisfies \(1.4\)\}. Since inequalities \(1.4\) are rank 1, it follows that  
*pep\)* = 1. Matching is a rare example of a family of polyhedra of positive and bounded  
rank.  
For most integer programming problems, the rank of the polyhedron increases without  
bound as a function of the dimension of the polyhedron. For example, suppose  
*pn* = *\{x* E *R1\: Xi* + *Xj* \~ 1 for *i, j* E iV, *i* =1= *j\}* and *sn* = *pn* n *zn.*  
We note that *LjEN* Xj \~ 1 is a valid inequality for *sn,* and it is not hard to show that  
conv\(Sn\) = *\{x* E *R1\: LjEN Xj* \~ 1\}. But the rank of *LjEN Xj* \~ 1 is *O\(log\(n* ».  
Even when the dimension of *P* is fixed, there are families of polyhedra such that the  
rank increases without bound as a function of the magnitude of the coefficients in the 3\. Gomory's Fractional Cuts and Rounding 227  
linear inequality description of *P.* For example, suppose *pt* is defined by the inequalities  
*\[Xl* + *X2* .\:\:\:\:; 1 + \[  
*XJ,X2* \~ 0  
and *St* = *pt* n *Z2.* Here it can be shown that *p\(pt\)* = \[ - 1 for \[ = 1, 2, ....  
An infinite family of polyhedra *fF* is said to have *bounded rank* if there is an integer *k*  
such that *pep\)* .\:\:\:\:; *k* for all *P* E *3f.* Thus if *P* E *fF* and *nx* .\:\:\:\:; *no* is valid for S = *P* n *zn,* we  
have *r\(n, no\)* .\:\:\:\:; *k.* Hence to verify the validity of *nx* .\:\:\:\:; *no,* we need to produce no more  
than *n* inequalities that are obtained by rounding inequalities of rank no higher than *k* - 1  
and weights *\(uJ,* ... , *un\)* to combine them. Each of these lower-rank inequalities can be  
produced from *n* inequalities of still lower rank. Thus, altogether we need the original  
inequalities and 1 + *n* + ... + *nk*  
*l*  
*-*  
.\:\:\:\:; *nk* weight vectors to prove the validity of *nx* .\:\:\:\:; *no* for  
any *P* E *fF* of dimension *n.*  
This observation leads to an important implication concerning the computational  
complexity of integer programs. Let *@f* be an infinite family of polyhedra and consider the  
integer programming problem whose instances are given by max\{cx\: *xES\},* where  
S = Z1 n *p* for each *P* E *@f* of dimension *n.* The optimality of *X O* E S can be established  
by showing that *ex* .\:\:\:\:; ZO is a valid inequality, where *cxo*  
= *zo.* Hence if *@f* is of bounded  
rank, the optimality of a proposed solution can be checked by displaying no more than *n*  
of the original inequalities of *P* and *nk* weight vectors for some fixed integer *k.* Thus,  
provided that the weight vectors are polynomial in the description of *P,* we have an  
optimality proof whose length is a polynomial function of *n,* which suggests that it is highly  
unlikely that the problem is .H9P-hard. In other words, it may well be the case that if an  
integer programming problem is ,N9P-hard, then the family of polyhedra over which it is  
defined does not have bounded rank.  
3\. GOMORY'S FRACTIONAL CUTS AND ROUNDING  
Although the results on rounding in the previous section were developed by V. Chvatal, we  
have attributed the procedure to him and R. Gomory. The reason is that, from a rather  
different viewpoint, these results appear in Gomory's much earlier work on finite cutting-  
plane algorithms. In this section, we will show the relationship between the valid  
inequalities used by Gomory and the rounding procedure.  
Here we write the constraints S = *\{x* E *Z1\: Ax* .\:\:\:\:; *b\}* in equality form as  
*se* = *\{x* E z\~+m\: *\(A, I\)x* = *b\),* where the original variables are *\(Xl,* ••• *,xn\)* and the slack  
variables are *\(x n+J,* ... , *X n+m\)'* We assume that *\(A, b\)* is an integral *m* x *\(n* + 1\) matrix.  
Let A = \(AI, ... , *Am\)* be a weight vector and consider the linear combination of equa-  
tions given by *A\(A, I\)x* = *Ab.* Suppose *N* = \{l, ... *,n\},* \~ = \{l, ... *,m\}, A* = *\(ab* ... *,an\),*  
and *1= \(eJ,* ... *,em\)* and define *a\}* = *Aa\}* for *j EN* and *b* = *Ab.* Then *A\(A, I\)x* = *Ab* can be  
written as  
\(3.1\) 228 In Section 1 \[see \(1.10\)\], we used modular arithmetic and *x* E z\~+m 11.1. The Theory of Valid Inequalities  
to derive the valid  
inequality  
\(3.2\) I *fjXj* + I *giXn+i* \~ /0,  
*jEN iEM*  
wherefj = *aj -lajJ* for *j EN, gi* = *Ai* -lAd for *i* EM, andfo = *b* -lbJ.  
Inequality \(3.2\) is the *Gomory fractional cut.*  
*Example* 3.1\. We return to Example 1.1, where *se* = *\{x* E Z\~\: *\(A, /\)x* = *b\}* and  
*\(A,* /\) = \( -; ; \~ \~ \~ \), *b* = \( \_2\~7\).  
-2 -2 0 0 1  
Let *A* = \(11- rr 0\), which yields the equation  
\(3.3\)  
Applying \(3.2\) to \(3.3\) yields the valid inequality  
\(3.4\)  
Now we eliminate the slack variables from \(3.4\) to obtain  
10 2 3  
-\( 4 + *Xl* - *2x* 2\) + -\(20 - *5x* 1 - *x* 2\) \~ -  
11 11 11  
or  
\(3.5\)  
*2X2\:\:\:\:'\:\:; 7.*  
To obtain \(3.5\) by rounding, use the weight vector *u* = *\(W,* rr, 0\) on the original inequalities  
and round.  
Observe that in the example *Ui* = *Ai -lAil* for *i* EM. This, in fact, is the general  
relationship.  
Theorem 3.1. *Let* S = *\{x* E Zi-\: *Ax\:\:\:\:.\:\:; b\}, where \(A, b\) is an m* x *\(n* + 1\) *matrix with*  
*integral coefficients. Thefractional cut* \(3.2\) *derived from* \(3.1\) *is a C-G inequality for* S  
*obtained with weights Ui* = *Ai* - *lAilfor i* EM.  
*Proof* Let lAJ = \(lAd, ... , lAmD and *U* = *A -lAl* \~ O. Then  
\(3.6\) *uAx* = *AAx* - *lAJAx* \:\:\:\:.\:\:; *Ab* - lAJb = *ub*  
or  
\(3.7\) 4\. Superadditive Functions and Valid Inequalities Since the *au's* and *b/s* are integers, rounding \(3.7\) yields  
229  
or since *x n+i* = *b* i - *L'1EN a ijXj* we obtain  
\(3.8\)  
Subtracting \(3.8\) from \(3.1\) yields \(3.2\). •  
There is an obvious converse to Theorem 3.1, which states that every rank 1 C-G  
inequality can be obtained as a fractional cut. Thus, analogous to Theorem 2.16, we have  
that every valid inequality for S = *\{x* E z\~+m\: *\(A, I\)x* = *b\}* is equivalent to or dominated  
by an inequality obtained from the recursive generation offractional cuts of the form \(3.2\).  
Gomory's proof of this result was algorithmic. He showed that the integer program  
*max\{ex\: xES\}* could be solved by solving a finite sequence of linear programs, each of  
which was obtained from its predecessor by the addition of an inequality \(3.2\). Thus if  
ZO = *exo* = *max\{ex\: xES\},* the algorithm derived the valid inequality *ex* \~ *zoo* We will  
study this algorithm in Section 11.4.3.  
4\. SUPERADDITIVE FUNCfIONS AND VALID INEQUALITIES  
Suppose S = *zn* n *P,* where *P* = *\{x* E R\~\: *Ax* \~ *b\}.* Our first objective in this section is to  
give a functional description of valid inequalities for S. For example, the C-G rank 1  
inequality *'£.jEN luajJxj* \~ *lubJ,* where *aj* is the *jth* column of *A* and *u* E *R,\:!,* can be  
described functionally by  
\(4.1\) *L F\(aj\)xj* \~ *F\(b\),*  
*JEN*  
where *F\(d\)* = *ludJ* for all *d* E *Rm.*  
A function *F\: D* f; *R m* ... *R* I is called *superadditive* over *D* if  
\(4.2\)  
Notethatdl = OyieldsF\(O\) + *F\(d2\)* \~ *F\(d2\)* or *F\(O\)* \~ o. Throughout the book, whenFis  
superadditive, we assume *F\(O\)* = 0 and 0 *ED.*  
A function *F\: D* ... *RI* is called *nondeereasing over D* if *d l , d2* E *D* and *d 1* \:\:\:\:\:; *d2* implies  
*F\(dd* \~ *F\(d2\).*  
Functions with these two properties yield valid inequalities.  
Proposition 4.1. *If F\: Rm*  
*... Rl is superadditive and nondeereasing, then* \(4.1\) *is a valid*  
*inequality for* S = *zn* n *\{x* E R\~\: *Ax* \~ *b\} for any \(A, b\).*  
*Proof* There are three steps in showing that \(4.1\) holds for all *xES\:*  
i. *L.jEN F\(aj\)xj* \~ *L.jEN F\(ajxj\).*  
ii. *L.jEN F\(ajxJ* \~ *F\(Ax\).*  
iii. *F\(Ax\)* \~ *F\(b\).* 230 11.1. The Theory of Valid Inequalities  
Since *Ax* \~ *b* for all *xES* and *F* is nondecreasing, inequality iii holds. The first two steps  
use superadditivity, and the first also uses *F\(O\)* = 0.  
i. It suffices to show that *F\(aj\)xj* \~ *F\(ajxj\)* for all *j.* If *Xj* = 0, then *F\(aj\)xj =*  
° = *F\(O\)* = *F\(ajxj\).* If *Xj* = 1, then *F\(aj\)xj* = *F\(aj\)* = *F\(ajxj\).* Suppose it is true for  
*Xj* = *k* - 1. Then  
*kF\(aj\)* = *F\(aj\)* + *\(k* - *l\)F\(aj\)*  
\~ *F\(aj\)* + *F«k* - *l\)aj\)*  
\~ *F\(aj* + *\(k* - *l\)aj\)* \~ *F\(kaj\).*  
ii.  
*n n*  
L *F\(ajxj\)* = *\(F\(atxt\)* + *F\(a2x 2»* + L *F\(ajxj\)*  
j=l *j=3*  
*n*  
\~ *F\(alxl* + *a2x2\)* + I *F\(ajxj\)* \~ ... \~ *F\(Ax\).*  
*j=3* •  
When the linear constraints are equalities \(i.e., *Ax* = *b\),* then step iii of the proof is  
irrelevant. Thus we obtain the following corollary.  
Corollary 4.2. *Ij F\: Rm*  
-+ *R* 1 *is superadditive and F\(O\)* = 0, *then* \(4.1\) *is a valid inequality*  
*jor se* = *zn* n *\{x* E R\~\: *Ax* = *b\}.*  
Corollary 4.2 \(and Proposition 4.1\) suggest the following terminology. IfF is superaddi-  
tive \(and nondecreasing\) with *F\(O\)* = 0, we call \(4.1\) a *superadditive valid inequality jor*  
*seeS\).*  
Linear functions are obviously superadditive. Starting with this simple fact and  
applying some elementary operations that preserve superadditivity allows us to construct  
some useful superadditive functions.  
Proposition 4.3. *be superadditive jor i* = 1, ... , *k.*  
*Let H\: Rk* -+ *Rt be superadditive and nondecreasing and let Fi\: Rm* -+ *Rt*  
a. *The compositejunction H\(Fl'* ... , *Fk\) is superadditive.*  
b. *If, in addition, Fj,jor i* = 1, ... *,k, is nondecreasing, then H\(Ft,* ... *,Fk\) is nonde-*  
*creasing.*  
*Proof* a. We have  
*H\(Ft\(dl* + *d2 \),* ... , *Fk\(d1* + *d2 »* \~ *H\(Fl\(dl\)* + *F t\(d2 \),* ... , *Fk\(dt\)* + *F k\(d2 »*  
\~ *H\(Fl\(dd,* ... , *Fk\(d1 »* + *H\(Fl\(d2 \),* ... , *Fk\(d2 »,*  
where the first inequality holds since the *F/s* are superadditive and *H* is nondecreasing,  
and the second inequality holds since *H* is superadditive.  
b. Suppose *d* 2 \~ 0 in the proof of a. Since the *F/s* are nondecreasing, *Fi \( d* 2\) \~ ° for *i* = 1,  
... , *k.* Since *H* is nondecreasing, *H\(F1 \(d2\),* ••• , *Fk\(d2»* \~ 0\. Hence  
so *H\(Ft,* ... , *F k\)* is nondecreasing. • 4\. Superadditive Functions and Valid Inequalities 231  
**Corollary 4.4.** *Let F,* G\: *Rm* \~ *R* I *be superadditive. Then the following functions are*  
*superadditive.*  
1. *K* = *AF for all A* \~ O.  
*2. K=lFJ.*  
*3. K=F+G.*  
*4. K* = min\(F, *G\).*  
*Proof* We apply Proposition 4.3.  
1. *FJ* = *F* and *H\: Rl* \~ *Rl* is given by *H\(d\)* = *Ad.*  
*2. FI* = *F* and *H\: Rl* .... *RI* is given by *H\(d\)* = ldJ. Clearly, *H* is nondecreasing. *H* is  
superadditive since  
*H\( b\)* = \{laJ + lbJ + 1  
*a* + laJ + lbJ  
\~ laJ + tbJ.  
if *a* + *b* - \(la J + *lb* J\) \~ 1  
if *a* + *b* - Cla J + *lb* 1\) \< 1  
*3. FI* = *F, F2* = G, and *H\: R2* \~ *RI* is given by *H\(a, b\)* = *a* + *b,* which is linear and  
nondecreasing.  
*4. H\: R2* \~ *Rl* is given by *H\(a, b\)* = min\(a, *b\).* Clearly, *His* nondecreasing. Also, *H* is  
superadditive since  
*H\(a* \[, *b* 1\) + *H\(a2' b2\)* = min\(a 1, *b* 1\) + min\(a2' *b2\)*  
\~ min\{\(a I + *a2\), \(b* I + *b2 \)\}*  
*= H\(al* + *a2, b l* + *b2\).*  
# •  
*F\(d\)* = ldJ  
4  
3  
2  
----------------------\~--------------------------------d  
-2 -1 o 2 3 4  
-1  
-2  
-3  
-4  
Figure 4.1 232 11.1. The Theory of Valid Inequalities  
We now give several illustrations of superadditive valid inequalities. Some of them have  
been developed previously in the text.  
1. *Integer Rounding\: C-G Rank* 1 *Inequalities.* Let *F\: Rm* \~ *R* I be defined by *F\(d\) =*  
*ludj, u* E *R';!.* Here we apply statement 2 of Corollary 4.4 to a linear function to conclude  
that *F* is superadditive. Moreover, *F* is also nondecreasing since if *d l* \< *d2 ,* then *u* E *R';!*  
implies *ludd* \~ *lud2 J.* This function is illustrated in Figure 4.1 with *m* = 1 and *u* = 1.  
*2. Integer Rounding\: General Inequalities Constructed by the C-G Rounding Procedure.*  
This is illustrated by the example presented below.  
*Example* 4.1\. To construct the function that yields the inequality 9xI + *7X2* \~ 10 for  
Example 2.1, we use the earlier calculations. Note that *t* = I"NJ U Nil.  
*t* = 0\:  
*t* = 2\:  
1. 9xI+7x2\~1l  
*2.* 9XI+7xz-9xl-7xz\~0  
3. 9xI + *7X2* - 9xI - 9\(1 - *X2\)* \~ 7  
*4.* 9xI+7xz-9\(I-XI\)-7xz\~9  
*5. 9XI* + *7X2* - 9\(1 - *Xl\)* - 9\(1 - *Xz\)* \~ 9  
is given by  
is given by  
is given by  
is given by  
is given by  
*FI\(d\)* = *lidl* + \~d2J  
*F2\(d\)* = 0  
*F3\(d\)* = *16d3*  
*F4\(d\)* = *18d2*  
*F5\(d\)* = \~dl + *¥dz* + *lJd3*  
*t* = 1\: 11. 9XI + *txz* - \(1 - Xl\) \~ 10 is given by *Fll \(d\)* = Hl\~FI *t* = 0\: 12. 9xI + *txz* \~ 10 is given by  
+ \~F4J + H\~Fl + \~F5JJ  
*F12* = l\~ Hl\~Fl + \~F2j + \~ l\~FI + \~F3jJ + ! H l\~Fl + \~F4J + \~ l\~Fl + \~F5JJJ.  
We see that each of these functions is superadditive because it is obtained by taking  
nonnegative linear combinations of superadditive functions and then rounding. In  
general, we have the following result.  
Proposition 4.5. *If TCX* \~ *TCo is a valid inequality for* S = *zn* n *\{X* E R\~\: *Ax* \~ *b\} con-*  
*structed by the C-G rounding procedure, then there is a superadditive and nondecreasing*  
*F\: Rm*  
\~ *Rl such that 71\:j* = *F\(aj\)for j* EN *and no* = *F\(b\).*  
*Proof* Suppose *71\:X* \~ *no* is of rank *k.* Then the C-G construction procedure yields  
*nj* = *F\(aj\)* for *j* EN and *no* = *F\(b\),* where *F* is obtained by recursive application of  
nonnegative linear combinations and rounding and hence is nondecreasing and superad-  
\~\~. .  
Theorem 4.6. *Every valid inequality for a nonempty* S = *zn* n *\{X* E R\~\: *Ax* \~ *b\} is equiv-*  
*alent to or dominated by a superadditive valid inequality.*  
*Proof* By Theorem 2.16, every valid inequality for S is equivalent to or dominated by  
an inequality constructed by the C-G procedure. By Proposition 4.5, every inequality  
constructed by the C-G procedure can be obtained from a superadditive nondecreasing  
function. • 4\. Superadditive Functions and Valid Inequalities 233  
We have shown that all maximal valid inequalities for S are superadditive. Moreover,  
they can be obtained from the family of superadditive functions generated by the recursive  
application of linear combinations and rounding. But as we have seen in Example 4.1, to  
determine a particular valid inequality from one of these functions can require a very long  
expression. In other words, although the basic formula is simple, it must be applied  
recursively to obtain particular inequalities. Perhaps with more complex basic functions,  
the number of recursive applications can be decreased.  
*3. Strengthened Integer Rounding.* Consider the set  
Applying the function *F\(d\)* = ldJ gives the valid inequality  
We will show that this inequality is not maximal by producing a superadditive nondecreas-  
ing function that yields an inequality that dominates it.  
Consider the family of functions *Fa\: R* 1 -+ *R* 1 with 0 \~ *a* \~ 1 defined by  
\(4.3\)  
ldJ  
*Fa\(d\)* = 1\:  
# \{  
*ldj* + *Id* - *a*  
*-a forld\> a,*  
whereld = *d* - *ldj.*  
Let *\(at* = max\(O, *a\)* for any *a* E *RI.* Then  
\(4.4\) *Fa\(d\)* = ldJ + ct; = \~t for *a* \< 1.  
The function *F 1 \(d\)* = ldJ, but here we are interested in *a* \< 1. The function *Fl/3* is drawn  
in Figure 4.2.  
Proposition 4.7. *Fa is continuous, nondecreasing, and superadditive lor* 0 \~ *a* \< 1.  
*Proof Fa* is continuous and nondecreasing because it is piecewise linear with slope of  
either 0 or 1/\(1 - *a\)* and has no jumps. To prove superadditivity, let  
*j;* = *di* - ldd for *i* = 1, 2.  
*Case 1./1* + *h* \< 1.  
*F \(d* \) + *F \(d* \) = *ld* J + *\(11* - *at* + *ld* J + *\(12* - *at*  
*a* 1 *a* 2 I *I-a* 2 *1-a*  
*\(11* + *12* - *at*  
l*d d* J  
\~ I + 2 + 1 = *Fa\( d* I + *d 2\)'*  
*-a* 234 11.1. The Theory of Valid Inequalities  
*Case 2.11* + *12* \~ l,fi \~ *a.*  
\(\[1 - *at*  
*d d* l*d* J  
*Fa\(* 1\) + *Fa\<* 2\) = 1 + 1 \_ *a* + l *d* 2J  
\< ldd + ld2J + 1 = *ld1* + *d2j* \~ *Pa\(d1* + *d2\).*  
\(The same argument applies if/1 \~ *a.\)*  
*II-a fi-a*  
*Pa\( d* 1\) + *Pa\( d* 2\) = l *d* d + 1 \_ *a* + l *d* 2J + 1 \_ *a*  
*II+/2- 1 - a*  
*= ld* d + l *d* 2J + 1 + 1 \_ *a* \~ *Pa\( d* 1 + *d 2\).* •  
Consider S = *\{x* E Z\~\: *'1\:.jEN ajxj* \~ *b\}* with *b* E *RI.* If *10* = *b* -lbj \> 0, then when  
*10* \~ *a* \< 1 it follows that *\}2jEN F'a\(aj\)xj* \~ *Pa\(b\)* dominates *'1\:.jEN FI \(aj\)xj* \~ *Ft\(b\)* since  
*Pa\( b\)* = *Fl \(b\)* and *Pa\( a j\)* \~ *Fl \(a j\)* for all j. Moreover, the strongest of these cuts is obtained  
with *a* = *10* since forlo \~ *a* \< 1 we have *Ffo\(aj\)* \~ *Fa\(aj\)* for allj.  
In the above example with *a* = 1 we obtain the valid inequality  
3 1  
*3X1* - *6"4X2* + *22x3* + *X4* \~ 4.  
------------------------\~--\~---------------------d  
-2 - 5  
-1 - 2  
3  
3  
4  
3  
2  
-1  
-2  
Figure 4.2 4\. Superadditive Functions and Valid Inequalities 235  
The practical disadvantage of this inequality in comparison with the C-G inequality is  
that when *x* E Z\~, the slack variable *Fa\(b\)* - *LjEN Fa\(aj\)xj* is not necessarily integer.  
*4. A Two-Dimensional Function.* The only nonlinear superadditive functions considered  
so far have been one-dimensional. Here we introduce a two-dimensional function, based  
on *Fa.* Let  
\(4.5\)  
The contours of this function are exhibited in Figure 4.3.  
Proposition 4.8. *Thefunction Fa given by* \(4.5\) *is nondecreasing and superadditive.*  
*Proof* Since *Fa* is nondecreasing, *Fa* is nondecreasing in *d* 2. With respect to *d* I, the first  
term of \(4.5\) has slope 1/\(1 - *a\)* and the second term is piecewise linear with slope of  
-1/\(1 - *a\)* or O. Hence *Fa* is nondecreasing in *dl •*  
Since the first term in \(4.5\) is linear, to prove that *Fa* is superadditive it suffices to show  
that the second term is superadditive. The second term is *Fa\(G\(d\)\),* where *G\(d\)* = *d2* - *d l*  
is linear. Hence by Propositions 4.3 and 4.7, the second term is superadditive. •  
Combining Propositions 4.3 and 4.8 yields the following corollary.  
Corollary 4.9. *If FI and F2 are superadditive on Rm*  
*, the function F\(Fb Pz\) =*  
1/\(1 - *a \)FI* + *Fa\(Pz* - *FI\) is superadditive on Rm. If FI and F2 are also nondecreasing, then*  
*F\(Fb F2\) is also nondecreasing.* 236 11.1. The Theory of Valid Inequalities  
As an example of the use of the function *Fa* to construct a valid inequality, consider the  
set  
*CX* - *P\( nx* - *no\)* \~ Co  
*cx* + *P\( nx* - *no* - 1\) \~ Co  
xEZ\~  
with *\(n, no\)* E zn+l, *\(c, co\)* E *Rn+l* andp \> O.  
Let *F\(dI, d2\)* = *PFI/2\(dl /2P, d2 /2P\).* Then  
'" *\(co* - *pno* Co + *pno* + *P\)* \[\( Co - *pno\)* \( 1 \)\] \[C 0 \]  
*PFlf2 2P' 2P* = *P* 2 *2P* + *FI/2 no* + 2 = *P P* - *no* + *no* = *Co,*  
and we obtain the valid inequality *cx* \~ *Co.*  
This shows that *Fa* permits us to generate the disjunctive inequality in one step, while  
with *F\(d\)* = l\~dl + *!d2 J* and c, *Co,* and *p* restricted to be integral, we only obtain  
*cx* \~ Co + IP/2j. Thus, except for *p* = 1, this example suggests that it can be advantageous  
to use functions other than the rounding function *F\( d\)* = I *udJ.*  
*5. Modular Arithmetic and Gomory Fractional Cuts.* Let *F\: RI* -+ *RI* be defined by  
*F\(d\)* = *-d\(mod* t5\), where J is a positive integer, that is, *-F\(d\)* is the remainder when *d* is  
divided by J \(see Figure 4.4\).  
Since *d/o* = ld/oj - *F\(d\)/o,* we have *F\(d\)* = o\(l\~j - \~\). We claim that *F\(d\)* is superadditive.  
Note that *F\(d\)* = *J\(lG\(d\)j* - *G\(d»,* where *G\(d\)* is the linear function *d/o.* Since IGJ is  
superadditive \(by statement 2 of Corollary 4.4\) and -G is linear, IGJ - Gis superadditive  
by statement 3 of Corollary 4.4. Finally 0\> 0 and statement 1 of Corollary 4.4 yield that *F*  
is superadditive.  
When 0 = 1, we obtain *F\(d\)* = IdJ - *d* = *-Id'* The m-dimensional version of this func-  
tion, *F\: R m*  
-+ *R* 1, given by *F\(d\)* = *ludj* - *ud* for *u* E *R'\:* generates the Gomory fractional  
cut, *LjENjjXj* \~ *10,* wherejj = *uaj -luajJ forj EN* and/o = *ub* -lubJ.  
*F\(d\)* = - *d* \(mod 0\)  
*-30 -20* -0 o 25 35  
# ----  
# ------  
# ----  
\~----\~------  
# \_\_  
# \_\_  
# \_\_  
*\_\_* ------\~------\~--d  
-0  
Figure 4.4 5\. A Polyhedral Description of Superadditive Valid Inequalities for Independence Systems  
237  
*6. A Stronger Fractional Cut.* Let \~a\: *R* I \~ *R* I be defined by  
forO \~fd \~ *a*  
for *a \<fd* \< 1,  
where 0 \< *a* \~ 1. Note that \~I\(d\) = *-Id* for all *d.* The function is shown in Figure 4.5 for  
# a=i.  
Since \~a\(d\) fractional cut obtained from \~fo is  
= *Fa\(d\)* - *d,* the superadditivity of \~a is a corollary to Proposition 4.7. The  
\(4.6\) I *jjXj* + \~ I \(1 - *jj\)Xj* \~fo,  
*\(jEN\:!j-vo\)* 1 - *!o UEN\:!j\>fo\)*  
which dominates *I.jEN jjXj* \~ *10'* Valid inequalities of the form \(4.6\) are important for  
mixed-integer regions \(see Section 7\).  
5\. A POLYHEDRAL DESCRIPTION OF SUPERADDITIVE VALID  
INEQUALITIES FOR INDEPENDENCE SYSTEMS  
An S C Z\~ is called an *independence system* if  
i. 0 E Sand  
ii. *Xl* E S, *X2* E Z\~ and *X2* \~ *Xl* =\> *X2* E S.  
It is easy to see that *\{x* E Z\~\: *Ax* \~ *b\}* is an independence system if all of the coefficients of  
*\(A, b\)* are nonnegative integers. Here we consider independence systems generated in this  
way. We also assume that *bi* \~ *max\{aij\:* for allj\} so that the *n* vectors *ej* forj EN are in S.  
All valid inequalities have *no* \~ 0 since *nx* \~ -1 is not satisfied by *x* = O. The *n*  
constraints *x* \~ 0 are valid and define facets of conv\(S\) since *Xj* = 0 is satisfied by the *n*  
affinely independent points \(0, *e\),* ... , *ej\_I, ej+l,* ... , *en\)* forj = 1, ... , *n.* Since *ej* E S for  
allj, any other valid inequality of the form *nx* \~ 0 has *n* \~ 0 and therefore is not maximal.  
Thus, except for *x* \~ 0, all facets ofconv\(S\) are of the form *nx* \~ 1. Moreover, all of these  
facets have *n* \~ 0 because if *nx* \~ 1 is valid and *nj* \< 0, then *nl* with *n1* = *nb k =1=* j, and  
*nJ* = 0 is valid.  
Let *A* = *\(ab* ... *,an\),* where *aj* E *D\(b\)* = *\{d* E Z\~\: *d* \~ *b\}* for all j. Here we give a  
polyhedral description of the valid inequalities of the form *I.jEN F\(aj\)xj* \~ *F\(b\),* where *F* is  
superadditive and nondecreasing, that contains all of the maximal valid inequalities other  
than *X* \~ O.  
-2 -% -1 -%  
o \~ % 2 \~ 3  
\~--\~----\~ *\_\_* --\~----\~\~\~------\~--\~----\~ *\_\_* --\~----\~d  
Figure 4.5 238 11.1. The Theory of Valid Inequalities  
Since we are dealing with functions on the finite domain *D\(b\),* any such function *F* can  
be represented by a vector *\(F\(O\), F\(el\), F\(e2\),* ... , *F\(b»* with n\~l *\(bi* + 1\) components. In  
addition, we assume that *F\(O\)* = 0 for all *F,* and we normalize the functions so that  
*F\(b\)* = 1 for all *F.*  
Proposition 5.1. *F\: D\(b\)* \~ \[0, 1\] *is nondecreasing and superadditive if and only if its*  
*corresponding vector is in the polytope given by*  
*F\(dl\)* + *F\(d2 \)* - *F\(dl* + *d2 \)* \~ 0 for dt, *d2* E *D\(b\), d 1* + *d2* \~ *b*  
*\(5.1\) F\(d\)* ;\:\:\: 0 for *d* E *D\(b\)*  
*F\(b\)* = 1.  
Since *F* is defined for all *d* E *D\(b\),* it is natural to consider the constraint set where the  
matrix *A* has a column for each *d* E *D\(b\).* In other words, *S\(b\)* = Z *ID\(b\)* I n *PCb\),* where  
*PCb\)* = *\{x* E *RjD\(b\)* I\: *LdED\(b\) dx\(d\)* \~ *b\}.*  
We call *conv\(S\(b»* the *master polytope* for the independence system with right-hand  
side *b.* In this section we will first derive results for S\( *b\)* and then show how they carryover  
to our given constraint set S involving only a subset *\{aj\}jEN* of the vectors in *D\(b\).*  
*Example* 5.1. Consider S\(3\) = *\{x* E Z\~\: *x\(1\)* + *2x\(2\)* + *3x\(3\)* \~ 3\}. The system \(5.1\)  
yields  
*2F\(l\)* - *F\(2\)* \~ 0  
*F\(1\)* + *F\(2\)* - *F\(3\)* \~ 0  
*F\(l\), F\(2\)* ;\:\:\: 0  
*F\(3\)* = 1.  
The feasible solutions are shown in *\(F\(l\), F\(2»* space in Figure 5.1. The feasible region  
contains all of the maximal superadditive inequalities. We will see that the maximal  
extreme points \(0 1\), G j\) define the facets of conv\(S\(3» \(other than *x* ;\:\:\: 0\). Thus  
conv\(S\(3» is defined by the inequalities  
1 2  
*3x \(1\)* + *3x\(2\)* + *x\(3\)* \~ 1  
*x\(2\)* + *x\(3\)* \~ 1  
*x\(1\), x\(2\), x\(3\)* ;\:\:\: O.  
Since the maximal points of S\(3\) are \{\(3 0 0\), \(l 1 0\), \(0 0 1\)\}, the 1-polar  
restricted to *n* ;\:\:\: 0 and *no* = 1 yields  
\(see Figure 5.2\). 5\. A Polyhedral Description of Superadditive Valid Inequalities for Independence Systems  
239  
\(0, 1\)  
*F\(2\)*  
o  
*F\(1\)*  
Figure 5.1  
Thus we see that the superadditive inequalities are properly contained in the I-polar.  
Note that in the example, all of the maximal inequalities lie on the line  
*F\(1\)* + *F\(2\)* == *F\(3\)* == *F\(b\)* == 1. This is a necessary and sufficient condition for maximality,  
which is made precise in the following proposition.  
From Theorem 4.6, we have that all maximal valid inequalities for *S\(b\)* are superaddi-  
tive. Hence the statement that *F* is a maximal feasible solution to \(5.1\) is identical to the  
statement that the superadditive inequality *LdED\(b\) F\(d\)x\(d\)* \~ I is a maximal valid  
inequality for *S\(b\).*  
\(0,1\)  
1  
11"2  
\(0, 0\) \(1/3, 0\)  
o  
Figure 5.2 240 11.1. The Theory of Valid Inequalities  
Proposition 5.2. *F\(d\)* + *F\(b* - *d\)* = 1 *for all* dE *D\(b\).*  
*F is a maximal feasible solution to* \(5.1\) *if and only ifF isfeasible and*  
*Proof* If *F* satisfies \(5.1\) and *F\(d\)* + *F\(b* - *d\)* = 1 for all *d,* then no component of *F*  
can be increased while maintaining feasibility. Hence *F* is maximal.  
We now prove that if there exists a *dO* such that *F\( dO\)* + *F\( b* - *dO\)* \< 1, then  
*LdED\(b\) F\( d\)x\( d\)* \~ *F\( b\)* is not a maximal inequality for S\( *b\).* There are two cases, namely  
*dO* = *b/2* and *dO* \*' *b/2.*  
*Case* 1. *dO* = *b/2* and *F\(dO\)* \<!. We will show that *LdED\(b\) n\(d\)x\(d\)* \~ 1 is valid for *S\(b\),*  
where *ned\)* = *F\(d\)* for *d* \*' *dO* and *n\(dO\)* =!. We have *x\(dO\)* E \{a, 1, 2\} for all *x* E *S\(b\).* If  
*x\(dO\)* = 0, then *LdED\(b\) n\(d\)x\(d\)* = *LdED\(b\) F\(d\)x\(d\)* \~ 1 is valid for *S\(b\).* If *x\(dO\)* = 2, then  
*xed\)* = ° for *d* \*' *dO* and *LdED\(b\) n\(d\)x\(d\)* = *2n\(dO\)* = 1. If *x\(dO\)* = 1, then *Ld\*dO dx\(d\)* \~ *dO.*  
Hence  
where the first inequality follows from superadditivity and the second one follows from  
monotonicity. Thus  
1 1  
I *n\(d\)x\(d\)* = I *F\(d\)x\(d\)* + *n\(dO\)* \< -2 + -2 = 1.  
*dED\(b\) d\*do*  
*Case* 2. *dO =1= b* /2 and *F\(dO\)* + *F\(b* - *dO\)* \< 1. Without loss of generality, we can assume  
that for some *i* we have *d?* \> *bd2.* Hence *x\(dO\)* E \{a, 1\} for all *x* E *S\(b\).* We will show  
that *LdED\(b\) n\(d\)x\(d\)* \~ 1 is valid for *S\(b\),* where *ned\)* = *F\(d\)* for *d* \*' *dO* and  
*n\(do\)* = 1 - *F\(b* - *dO\). Ifx\(dO\)* = 0, then *LdED\(b\) n\(d\)x\(d\)* = *LdED\(b\) F\(d\)x\(d\)* \~ 1 is valid for  
*S\(b\).* If *x\(dO\)* = 1, then *Ld\*dO dx\(d\)* \~ *b* - *dO.* Hence  
Thus  
I *n\( d\)x\( d\)* = I *F\( d\)x\( d\)* + *n\( dO\)*  
*dED\(b\) d\*do*  
# •  
Proposition 5.2 allows us to tighten the system \(5.1\) by ruling out functions that fail to  
satisfy *F\(d\}\)* + *F\(d2\)* = 1 when *d\}* + *d2* = *b.* Thus we obtain the polytope defined by  
*F\(d\}\)* + *F\(d2\)* - *F\(d\}* + *d2\)* \~ ° for all db *d2* E *D\(b\), d\}* + *d2* \< *b*  
*F\(d\)* + *F\(b* - *d\)* = 1 for all *d* E *D\(b\)*  
*\(5.2\) F\(d\)* \~ ° for all *d* E *D\(b\)*  
*F\(b\)* = 1. 5\. A Polyhedral Description of Superadditive Valid Inequalities for Independence Systems  
241  
In Example 5.1, the system \(5.2\) in *\(F\(1\), F\(2»* space is given by  
*2F\(l\)* - *F\(2\)* \~ 0  
*F\(l\)* + *F\(2\)* = 1  
*F\(I\), F\(2\)* \~ O.  
The feasible region is the line joining the points \(i \~\) and \(0 1\) in Figure 5.1, that is,  
precisely the set of maximal points.  
Besides characterizing the maximal valid inequalities for *conv\(S\(b»* \(other than  
nonnegativity\), the system \(5.2\) gives a useful description of the facets of *conv\(S\(b* ». We  
thus obtain the main result of this section.  
Theorem 5.3. *LdED\(b\) F\(d\)x\(d\)* \~ 1 *is afacet ofconv\(S\(b» \(other than nonnegativity\) if*  
*and only ifF is an extreme point solution of\(5.2\).*  
*Proof* All feasible solutions to \(5.2\) generate valid inequalities for *S\(b\).* Suppose that  
*F* generates a facet of *conv\(S\(b»* but that *F* = *1Fl* + *1F2*  
*,* where *Fl* and *F2* satisfy \(5.2\).  
Then *Fl* and *F2* generate valid inequalities, and *LdED\(b\) F\(d\)x\(d\)* \~ 1 is a convex combina-  
tion of *LdED\(b\) Fk\(d\)x\(d\)* \~ 1 for *k* = 1, 2, which is a contradiction.  
On the other hand, suppose *F* is an extreme point solution of\(5.2\) but does not generate  
a facet of *conv\(S\(b* ». By Proposition 5.2, *LdED\(b\) F\(d\)x\(d\)* \~ 1 is maximal and, by the  
hypothesis, is a convex combination of valid inequalities. In other words, it is dominated  
by a convex combination of maximal valid inequalities. Thus there exist *Fi* "\* *F* for *i* = 1,  
... *,p* such that for all *d* E *D\(b\)* we have *F\(d\)* \~ Lf=l *AiFi\(d\),* where Lf=l *Ai* = 1 and *Ai* \~ 0  
for *i* = 1, ... *,p.* However, maximality of the inequality generated by *F* implies that  
*F\(d\)* = *Lf=1 AiFi\(d\)* for all *d* E *D\(b\),* which is a contradiction. •  
Now we use projection and Theorem 5.3 to go from the master polytope to the general  
polytope.  
Theorem 5.4. *Let* S = *zn* n *P, where P* = *\{x* E R\~\: *Ax* \~ *b\} and all coefficients of \(A, b\)*  
*are nonnegative and integral. Then*  
conv\(S\) = *conv\(S\(b»* n *\{x\: xed\)* = 0 if *d =1= aj* for somej EN\}.  
*Proof* Since *xed\)* \~ 0 is a facet of *conv\(S\(b»* for all *d* E *D\(b\),* it follows that conv\(S\)  
is the face of *conv\(S\(b»* obtained by setting *xed\)* = 0 if *d =1= aj* for somej EN. •  
*Example 5.2.* Consider S\(5\). The extreme points of \(5.2\) with *F\(5\)* = 1 are  
*\(F\(l\), F\(2\), F\(3\), F\(4»* = G 2 3 \~\)  
5 5  
*\(F\(l\), F\(2\), F\(3\), F\(4»* = \(0 0 1\)  
*\(F\(l\), F\(2\), F\(3\), F\(4»* = \(0 1  
2 21 \). 242 11.1. The Theory of Valid Inequalities  
Hence conv\(S\(S» is given by the inequalities  
1 2 3 4  
*S-x\(l\)* + *S-x\(2\)* + *S-x\(3\)* + *S-x\(4\)* + *x\(S\)* \~ 1  
*x\(3\)* + *x\(4\)* + *x\(S\)* \~ 1  
1 1  
*2"x\(2\)* + *2"x\(3\)* + *x\(4\)* + *x\(S\)* \~ 1  
*x\(d\)* \~ 0 for *d* = 1, ... , *S.*  
Now given S = *\{x* E *Z!\: x\(l\)* + *2x\(2\)* + *4x\(4\)* \~ *S\}* it follows from Theorem *S.4* that  
conv\(S\) is given by the inequalities  
1 2 4  
*S-x\(l\)* + *S-x\(2\)* + *S-x\(4\)* \~ 1  
*x\(4\)* \~ 1  
1 *2"x\(2\)* + *x\(4\)* \~ 1  
*x\(l\), x\(2\), x\(4\)* \~ o.  
Note here that the first and third inequalities are facets of conv\(S\) but the second one is  
redundant.  
6\. VALID INEQUALITIES FOR MIXED-INTEGER SETS  
Suppose we are given the mixed-integer region  
*T* = *\{x* E *Z1, y* E R\~\: *Ax* + *Gy* \~ *b\},*  
where *\(A,* G, *b\)* is an *m* x *\(n* + *p* + 1\) rational matrix. Our objective in this section is to  
develop a procedure for generating valid inequalities for *T.* Note that the C-G procedure  
does not work when there are continuous variables. In particular, we cannot round down  
the right-hand side of an inequality to its integer part when all of the coefficients on the  
left-hand side are integers. However, we will be able to obtain a procedure, related to the  
disjunctive procedure, that generalizes the C-G procedure.  
To motivate the approach, consider the example with *T* defined by  
*x* E Z\~, *Y* ER\~.  
In the absence of the *y* variables, we obtained the valid inequality 3Xl - *7X2* + *2X3* \~ 4.  
Can one find a valid inequality for *T* of the form  
\(6.1\) 6\. Valid Inequalities for Mixed-Integer Sets 243  
i. *A Bound on j1+.* Suppose there is a feasible solution with 3xI - *7X2* + *2X3* = 4,  
*Y2* = 0, and YI \> 0\. The inequality \(6.1\) can only be valid if 4 + *j1+YI* - ° \~ 4 or *j1+* \~ 0.  
ii. *A Bound on j1-.* Suppose there is a feasible solution with 3Xl - *7X2* + *2X3* = 5, Yl =  
0, and *Y2* = \~. Validity of\(6.1\) implies that 5 - *2j1-/3* \~ 4 or *j1-*\~ 3/2.  
Letting *b* = lbj + *fo,* the example indicates that *j1+* \~ 0, *j1-*\~ 1 / \(1 - *fo\)* and motivates  
the following proposition.  
Proposition 6.1. *Let T* = *\{x* E *ZZ, Y* E R\~\: *LjEN ajXj* + *LjEJ gjYj* \~ *b\}, where N* = \{I, ... ,  
*n\}, J* = \{I, ... *,p\}, and aj' gj, b* E *Rl for all\}. The inequality*  
\(6.2\)  
*where J-* = \{\} E *J\: gj* \< O\} *and fo* = *b* -lbj, *is validfor T*  
*Proof* Suppose *LjEJ gjYj* \> *fo* - 1. Then  
*L lajjxj* \~ *L ajxj* \~ *b* - *L gjYj* \< *b* - \(fo - 1\) = lbJ + 1.  
*JEN JEN jEJ*  
Since *LjEN lajJxj* is an integer, we have *LjEN lajJ Xj* \~ lbj. Adding this inequality to  
1 \~fo *LjEJ- gjYj* \~ ° yields \(6.2\).  
Now suppose that *LjEJ gjYj* \~ *fo* - 1 so that *LjEJ- gjYj* \~ *fo* - 1. Hence  
*= b* + 1 *fo* \{' \( I *gjYj\)* \~ *b* - *fo* = lbJ.  
*-\)0 jEJ-* •  
*Example* 6.1. *T* = \{Xl E Zl, Yl E *Rl\:* Xl + Yl \~ 1\}. From \(6.2\), we obtain the valid  
inequality Xl \~ 2. The geometry is shown in Figure 6.1. Note that  
*\{\(X" y,\)* E *R2\: x,* + *y,* ,,; \~, *x,* ,,; 2, *y,* ;;. o\} = conv\{ *x,* E *ZI, y,* E R\~\: *x,* + *y,* ,,; \~l  
Now let *T* = \{Xl E Zl, *Yl* E *Rl\:* Xl - YI \~ !\}. From \(6.2\), we obtain the valid inequality  
Xl - 2YI \~ 2 \(see Figure 6.2\). Note that  
Example 6.1 illustrates the following proposition.  
Proposition 6.2. *Let T* = *\{x* E *zn, Y* E R\~\: *LjEN ajXj* + *LjEJ gjYj* \~ *b\}, where aj* E ZI *for*  
\} *EN, gcd\{aJ,* ... , *an\}* = 1, *and b* \~ Z1. *Then* \(6.2\) *is afacet ofconv\(T\).*  
*Proof* We have already shown that \(6.2\) is valid for *T.* To show that it is a facet of  
*conv\(T\),* we first take *n* affinely independent points, *x\\* ... *,xn* E *zn,* that satisfy 244 11.1. The Theory of Valid Inequalities  
*LjEN ajXj* = *\[bJ.* Let PI = IJ \\ *J-I.* We represent points in *T* as triples *\(u,* v, *w\),* where  
*u* E *zn,* v E R\~', and *W* E R\~-p,. We get *n* affinely independent points in *T* that satisfy  
\(6.2\) at equality by taking *ui* = *Xi,* Vi = 0, and *Wi* = ° for *i* = 1, ... , *n,* and we get another  
PI points by taking *ui* = *xr,* Vi = *6ei,* and *Wi* = 0, where *ei* is the ith unit vector in R\~' and  
6 \> ° is suitably small. Now let *x* E *zn* be a solution to *LjEN a jXj* = *\[b* J + 1. The final set of  
*P* - PI points are obtained by taking *ui*  
= *x,* Vi = ° and *Wi* = *Yiei,* where *ei* is the ith unit  
vector in R\~-P' and *Yi* = \(fo - 1\) / *gi.* These last points satisfy \(6.2\) since  
"A 1 *\(fo-1\)*  
*L ajxj* + -- -- *gi* = *\[bJ* + 1 - 1 = *\[bJ*  
*JEN* 1 - *fo gi*  
and are in *T* since  
# •  
As we saw in the derivation of \(6.2\), it was necessary to use the non negativity of the  
continuous variables. In particular, we must use the non negativity of slack variables to  
generate other valid inequalities. We now give a procedure based on \(6.2\) for generating  
valid inequalities for the set *T* = *\{x* E *Z1, Y* E R\~\: *Ax* + *Gy* \~ *b\}.*  
Mixed-Integer Rounding \(MIR\) Procedure  
*Step* 1\: The inequalities  
*L \(uaj\)xj* + *L \(ugj\)Yj* \~ *ub* are valid for all *u* E *R';!.*  
*JEN JEJ*  
*Step* 2\: Given two valid inequalities  
\(6.3\) *L n\}xj* + *L f.1\}Yj* \~ *n6* for *i* = 1, 2,  
*JEN JEJ*  
construct the third valid inequality  
*\(6.4\) L \[n\]* - *nJJxj* + -1 1 \{' *\(L nJxj* + *L* min\(u\), *f.1\]\)Yj* - *nb\)* \~ *\[n6* - *nbJ,*  
*JEN* -\)0 *JEN JEJ*  
where *n6* - *nb* = *\[n6* - *nbJ* + *fo.*  
Yl  
2  
Cutoff region  
\~-----------------Xl  
o 2  
Figure 6.1 6\. Valid Inequalities for Mixed-Integer Sets 245  
*Yl*  
\~ \_\_ K---- Cutoff region  
------\~------O+-----\~----\~2-=\~--------------------Xl  
Figure 6.2  
Proposition 6.3. *Given the two valid inequalities* \(6.3\) *for T, it follows that* \(6.4\) *is also*  
*validfor T.*  
*Proof* Since \(6.3\) is valid for *T* and *Y* \~ 0, it follows that  
\(6.5\) I *n\}xj* + I minCu\}, *J1\]\)Yj* \~ *nb* for *i* = 1, 2  
*JEN iEJ*  
is valid for *T.* Rewrite \(6.5\) for *i* = 2 as  
I *\(n\]* - *n\}\)xj* - *\(nb* - I *n\}xj* - I min\(J1\), *J1\]\) Yi\)* \~ *n5* - *nb.*  
*JEN JEN JEJ*  
Now \(6.5\) with *i* = 1 implies  
s = *nb* - *L n\}xj* - I *min\(J1J, J1\]\) Yj* \~ 0.  
*JEN JEJ*  
I *\(n\]* - *n\}\)xj* - *S* \~ *n5* - *n6*  
Thus we can apply Proposition 6.1 to  
*JEN*  
to obtain \(6.4\). •  
We say that any valid inequality equivalent to or dominated by an inequality con-  
structed by the MIR procedure is an *MIR inequality.*  
*Example* 6.2. obtain the two valid inequalities  
*T* = *\{x* E *B2, Y* E R\~\: *YI* + *Y2* \~ 7, *Yi* \~ *5xi* for *i* = 1, 2\}. Using Step 1, we  
1 7  
*3" \(YI* + *Y2\)* \~ 3"  
5 1  
- 3" *\(Xl* + *X2\)* +"3 *\(YI* + *Y2\)* \~ 0. 246 11.1. The Theory of Valid Inequalities  
Taking the first of these as the *i* = 1 inequality, and the second as the *i* = 2 inequality, we  
obtain from \(6.4\) the valid inequality given by\:  
or  
Proposition 6.4. *Let T* = *\{x* E Z\~, *Y* E R\~\: - *\(Uk* + *CX* + *hy* \~ *Co, PXk* + *CX* + *hy* \~ *Co* + *P\}*  
*with a, P* \> O. *Then cx* + *hy* \~ Co *is an MIR inequality for T.*  
*Proof* Scale each of the inequalities in the definition of *T* by 1/\( *a* + *P\)* to obtain  
*-a* 1 Co  
*a* + *\[?k* + *a* + *P \(cx* + *hy\)* \~ *a* + *P* \[i = 1 in \(6.3\)\]  
*P --nXk* + \_1\_ *\(cx* + *hy\)* \~ *Co* + *P \[i* = 2 in \(6.3\)\].  
*a+p a+p a+p*  
We obtain from \(6.4\) the valid inequality\:  
*1/\(a* + *P\) Xk* + *a/\(a* + *P\)* \(- *\(Uk* + *CX* + *hy* - *Co\)* \~ l *P* J  
*a* + *P* = 0  
or  
1 *Co*  
*Xk* - *Xk* + *-\(CX* + *hy\)* \~ -  
*a a*  
or  
*CX* + *hy* \~ *Co.* •  
Proposition 6.4 shows that the MIR procedure accomplishes what is done in the  
disjunctive procedure. Now if *x* E *Bn,* we can invoke Theorem 2.5 to conclude that every  
valid inequality for conv\(T\) is a D-inequality. Thus we obtain the following theorem.  
Theorem 6.5. *Suppose T* = *\{x* E *Bn*  
*, y* E R\~\: *Ax* + *Gy* \~ *b, x* \~ I\} =I\: 0 *where \(A,* G, *b\) is*  
*an m* x *\(n* + *p* + 1\) *matrix with rational coefficients. Any valid inequality*  
*nx* + *I1Y* \~ *nofor T is an MIR inequality.*  
Theorem 6.5 is false for bounded integer variables. A counterexample is discussed in  
Exercise 22.  
7\. SUPERADDITIVITY FOR MIXED-INTEGER SETS  
In this section, we extend the development of superadditive valid inequalities to mixed-  
integer constraint sets. Suppose *T* = *\{x* E *Z1, y* E R\~\: *Ax* + *Gy* \~ *b\}* and *F* and Hare  
functions from *R m* to *R* 1. We first consider conditions for which 7\. Superadditivity for Mixed-Integer Sets 247  
\(7.1\) I *F\(aj\)xj* + I *H\(gj\)Yj* \~ *F\(b\)*  
*JEN jEi*  
is valid for *T* for all *A,* G, and *b.* Since we want to generalize the results for the pure-integer  
constraint set, we assume throughout this section that *F* is nondecreasing and superaddi-  
tive and that *F\(O\)* = O. The problem is to determine the appropriate conditions to be  
imposed on *H.*  
We first develop two necessary conditions.  
*Positive Homogeneity.* Since the substitution *y;* = *AYj* for A \> 0 is permissible and makes  
no essential change to *T,* we must have  
\(a\) *H\(Ad\)* = *AH\(d\)* for all A \~ 0 and *d* E *Rm.*  
*Dominance.* If some valid inequality is satisfied at equality by a solution *\(x, y\)* with  
*Ax* = *d,* and some continuous activity is a multiple of *d* \(i.e., *gj* = *Ad\),* then we must have  
*H\(Ad\)/A* \~ *F\(d\).* Positive homogeneity then implies  
\(b\) *H\(d\)* \~ *F\(d\)* for all *d* E *Rm.*  
Conditions \(a\) and \(b\) also are sufficient for the generation of valid inequalities.  
Theorem 7.1. *If F is superadditive and nondecreasing and F\(O\)* = 0, *and H satisfies*  
*conditions \(a\) and \(b\), then* \(7.1\) *is a valid inequality for T for all A,* G, *and b.*  
*Proof* We have *LjEN Fiaj\)xj* \~ *F\(Ax\)* by superadditivity and  
where we use, respectively, property \(a\), property \(b\), and the superadditivity of *F.* Finally,  
since *F* is superadditive and nondecreasing and *Ax* + *Gy* \~ *b,* it follows that  
*F\(Ax\)* + *F\(Gy\)* \~ *F\(Ax* + *Gy\)* \~ *F\(b\). •*  
Note that superadditivity of *H* is not required here. However, we shall see below why it  
is natural also to impose the conditions that *H* be superadditive and nondecreasing.  
From conditions \(a\) and \(b\), it follows that  
\(7.2\) *H\(Ad\) F\(Ad\)*  
*H\(d\)* = *-A-*\~ *-A-* for all *d* E *Rm* and A\> O.  
The condition \(7.2\) restricts the class of superadditive nondecreasing functions that can be  
used in \(7.1\) for *F.* For example, suppose *F* is the C-G function *F\(d\)* = *ldj.* Then \(7.2\)  
implies, with *d* = -1,  
*H\(-I\)* \~ *F\(-A\)* = -1 for all 0 \< A \~ 1  
# A A  
or *H\(-l\)* = -00.  
We define limA, \\0 0+ *g\(x,* A\) = *g\(x\)* to mean that for each *x* and any E\> 0 there exists a  
*J\(x,* E\) \> 0 such that *Ig\(x,* A\) - *g\(x\)* I \< *e* whenever 0 \< A \~ *J\(x, e\).* The C-G function is  
not useful for *T* since limA, \\0 0+ *F\(Ad\)/A* \~ -00 for *d* = - 1. 248 11.1. The Theory of Valid Inequalities  
However, suppose *F* is superadditive and nondecreasing and  
\(7.3\)  
exists and is finite for all *d.* Then from \(7.2\) we see that  
\(7.4\) *H\(d\)* \~ *F\(d\)*  
is necessary in \(7.1\).  
Since for a given *F* we would like to have the function *H* that gives the strongest possible  
valid inequality, we would like to choose *H* as large as possible, subject to the conditions  
\(a\), \(b\), and \(7.4\). Thusi\(F satisfies conditions \(a\) and \(b\), the desired function is *H* = *F.*  
Fortunately, whenever *F* exists and is finite for all *d* E *Rm,* it satisfies conditions \(a\) and  
\(b\).  
Proposition 7.2. *Given a nondecreasing superadditive function F, for which F given by*  
*\(7.3\) is defined andfiniteJor all d, itJollows that* \(i\) *F is positively homogeneous and* \(ii\) *F*  
*is dominated by F*  
*Proof* i. For any given.u \> 0 and any dE *Rm,* we have  
ii. For any *k* \> 0, let *t* = lkJ and *r* = *k* - lkJ. Then  
\~ F\(t\(\~\)\) + F\(r\(\~\)\) by superadditivity  
\~ tF\(\~\) + F\(r\(\~\)\) by superadditivity  
Now let *A* = *11k* so that  
Taking the limit as *A* '\\. 0+ gives *F\(d\)* \~ *F\(d\).*  
# •  
We get a bonus by taking *H* = *F* since *F* shares the properties of *F.*  
Proposition 7.3. *IJ the F given by* \(7.3\) *is defined and finite Jor all d, then it is*  
*superadditive and nondecreasing.* 7\. Superadditivity for Mixed-Integer Sets 249  
*Proof* Since *F* exists, given any *e* \> 0 there exists *Ai* for *i* = 1, 2 such that  
I *F\(di\)* - *F\(Adj \)* / *A* I \~ *e* for all 0 \< *A* \~ *Ai,* and there exists a *A3* such that I *F\(d!* + *d2 \) -*  
*F\(A\(d!* + *d2 »* / *A* I \~ *e* for all 0 \< *A* \~ *A3.* Taking *A* \~ min\{AJ, *A2, A3\},* we have that  
Hence *F\(d!\)* + *F\(d2 \)* \~ *F\(d!* + *d2 \),* and *F* is superadditive. Since *F* is nondecreasing,  
*F\(Ad2 \)* / *A* - *F\(Ad!\)* / *A* \~ 0 for all *A\>* 0 and *d2* \~ *d!.* Hence, taking the limit as *A* \\. 0+  
yields *F\(d2 \)* - *F\(d!\)* \~ O. Thus *F* is nondecreasing. •  
Thus we have justified the use of *F* in place of *H* in \(7.1\).  
Theorem 7.4. *IfF is superadditive and nondecreasing, F\(O\)* = 0, *and F exists, then*  
\(7.5\) *2\: Fj\(aj\)xj* + *L F\(gj\) Yj* \~ *F\(b\)*  
*JEN JEI*  
*is a superadditive valid inequality for T for all A,* G, *and b.*  
*The Function Fa and the Gomory Mixed-Integer Cuts.* Consider the function  
given by \(4.4\). We obtain  
o for *d* \~ 0  
*Fa\( d\)* = \_1\_ *d* for *d* \< 0  
# \{  
# I-a  
\(see Figure 7.1\). Thus, with *H* = *Fa,* we obtain a generalization of Proposition 6.1.  
----------------------\~-----------------------d  
-3 -2 2 3  
Figure 7.1 250 **Proposition** 7.5. *Let T* = *\{x* E\~, *for all j. The inequality*  
11.1. The Theory of Valid Inequalities  
*Y* E R\~\: *LjEN ajXj* + *LjE\] gjYj* \~ *b\}, whereaj, gj, b* E *Rl*  
*is valid/or T, where J-*= *\{j* E *J\: gj* \< *O\}.*  
N ow consider the system  
\(7.6\)  
*x* E z\~, *Y* ER\~  
with *b* = *lb* J + /0, 0 \< *fo* \< 1. Replacing the equality in \(7.6\) by an inequality and then  
applying Proposition 7.5 with ex = *fo,* we obtain the valid inequality  
for the system \(7.6\).  
Combining the equality of \(7.6\) with the above inequality yields  
or  
where *J+* = *U* E *J\: gj* \> O\} andjj = *aj -lajJ* for *j EN.* This is the *Gomory mixed-integer*  
*cut.*  
We can derive this cut for the original mixed-integer set *T* = *\{x* E Z\~, *Y* E R\~\:  
*Ax* + *Gy* \~ *b\}* and can also express it in terms of the original variables. The procedure to  
do this involves the introduction of slack variables *s* E *R'J!,* the use of row multipliers  
*u* E *Rm* to produce the system  
*\{x* E Z\~, *Y* E R\~, *s* E *R'J!\: uAx* + *uGy* + *uIs* = *ub\}*  
of the form \(7.6\), generation of the cut, and then elimination of the slack variables by  
substitution. We leave as an exercise the task of showing that the resulting inequality is  
where 7\. Superadditivity for Mixed-Integer Sets 251  
and *Fa\(d\}, d2 \)* = \[1/\(1- *a\)\]dl* + *Fa.\(d2* - *d l \)* is the two-dimensional function given by \(4.5\).  
We now derive a property of *Fa.*  
Proposition 7.6. *IfF* = \[1/\(1 - *a\)\]FI* + *Fa.\(F 2* - *Fd, where FI and F2 are superadditive and*  
*!londecreasing and* \_ *w!lere* PI *and P 2 exist and are finite with a* \> 0, *then*  
*F* = \[1/\(1 - *a\)\]* min\(Fb *F2\)'*  
*Proof* First we show that *F\(Ad\)/A* \~ *min\(FI\(d\), F2\(d».* For *A* \~ 0, we obtain  
*F\(Ad\)* = \_1\_ *FI\(Ad\)* +! *F\(F2 \(Ad\)* \_ *FI\(Ad»*  
*A I-a A A* a  
1 *FI\(Ad\)* 1-  
\~ 1 - *a -A* - + X *Fa\(FiAd\)* - *FI\(Ad»*  
1 *FI\(Ad\)* 1 1 .  
= 1 - *a -A* - + 1 \_ *a* ;\:mIn\(F2\(Ad\) - *FI\(Ad\), 0\)*  
\_\_ 1\_ . *\(FI\(Ad\) F2 \(Ad»\)*  
- 1 - *a* mIn *A* ' *A*  
Now we must show the inequality in the opposite direction for sufficiently small  
positive A.\_Since FI and *F2* exist, given *d* and e \> 0, there exists *A\** such that  
*Fi\(Ad\)/A* \~ *Fi\(d\)* + e for *i* = 1, 2 and for all 0\< *A* \< *A\*.* Hence  
*F\(Ad\)* = \_1\_ *FI\(Ad\)* ! *F\(F\(Ad\)* \_ *F\(Ad»*  
*A I-a A* + *A* a 2 I  
since *Fa.* is nondecreasing, and *FI\(Ad\)* \~ *FI\(Ad\)* = *AFI\(d\).* Now for *A* sufficiently small, we  
see from Figure 4.2 that *Fa\(Ax\)* = -1 \_1\_ mineO, *Ax\).* So  
# -a  
*F\(Ad\)* \~ \_1\_ *FI\(Ad\)* + \_1\_ min\(O, *F*  
*2\(d\)* \_ *FI\(d\)* + e\)  
*A* I-a *A I-a*  
\_ 1 . *\(FI\(Ad\) F-\(d\) Fl\(Ad\) F-\(d\) \)*  
- 1 - *a* mIn *-A-'* 2 + *-A-*- I + e  
and  
*F\(Ad\)* 1 --  
*-A-* \~ 1 \_ *a \[min\(FI\(d\), F2 \(d»* + 2e\].  
Hence *F* = \[1/\(1 - *a\)\]* min\(FJ, *F2\)'* •  
Thus *Fa\(dJ, d2\)* = \[1/\(1 - *a\)\]* mined!, *d2\).* UsingF = *FaandH* = lin \(7.1\) we obtain the  
following proposition. 252 11.1. The Theory of Valid Inequalities  
Proposition 7.7. *If T* = *\{x* E 2';., *y* E R\~\: \(c - *pn\)x* + *hy* \~ Co - *Pno,* \(c + *pn\)x* + *hy*  
\~ Co + *P7Co* + *P\), where \(c, h,* co\) E *Rn+p+1 and \(n, 7Co\)* E *zn+ l*  
*, then cx* + *hy* \~ Co *is a*  
*superadditive valid inequality for T.*  
*Proof* Taking the function *F\(dl , d2\)* = *PFl/idd2P, d2/2P\),* we obtain  
*F\(c\}* - *fin\}, c\}* + *fin\}\)* = *fi \[2\(Cj ;/n\)* + *F1/2\( Cj ;/n\}* \_ *Cj ;/nj \)\]*  
*= Cj* - *P7Cj* + *pFI/2\(nj\)* = *Cj* - *P\(7Cj* - *7Cj\)* = *cf,*  
*F\(* \_ *P P P\)* - *P* \[2 *\(co* - *P7Co\) F* \(Co + *pno* + *P* \_ Co - *pno\)\]*  
Co *no,* Co + *7Co* + - *2P* + *1/2 2P 2P*  
*= Co* - *fino* + *fiF1/2\( no* + 4\) = *co·*  
This yields the superadditive valid inequality *cx* + *hy* \~ *Co.* •  
As a result of Proposition 7.7 and Theorem 6.5 we can establish the generality of  
superadditive inequalities.  
Theorem 7.8. *Given T* = *\{x* E *zn, Y* E R\~\: *Ax* + *Gy* \~ *b, x* \~ I\} *=1=* 0, *every valid inequal-*  
*ity nx* + *IlY* \~ *7Co is equal to or dominated by some superadditive valid inequality*  
*L F\(aj\)xj* + *L F\(gj\)yj* \~ *F\(b\).*  
*JEN JEJ*  
Theorem 7.8 holds for mixed-integer regions of the form *T* = *\{x* E *zn, y* E *RP\}* n *P,*  
where *P* is any rational polyhedron. However, the proofs are not constructive.  
Note that the function *F* in Theorem 7.8 can be constructed iteratively using nonnega-  
tive linear functions and *Fl/2* a tinite number of times. Furthermore, since the procedure  
starts with linear functions and *FI/2* is the minimum of linear functions, the corresponding  
function *F* is the minimum of a finite number of linear functions and is therefore  
piecewise linear and concave.  
*Example* 7.1. *T* = *\{x* E *B2, Y* E R\~\: *YI* + *Y2* \~ 7, *Yi* \~ *5Xi, i* = 1, 2\). We construct the  
functions representing the valid inequality *YI* + *Y2* - *2x* I - *2X2* \~ 3. Consider the enumer-  
ation tree shown in Figure 7.2. Let the linear constraints be given in matrix form by  
0 0 1 1 7  
-5 0 1 0 *XI* 0  
0 -5 0 1 *X2*  
\~ 0  
1 0 0 0 *YI*  
0 0 0 *Y2*  
x,y\~O. 7\. Superadditivity for Mixed-Integer Sets 253  
7  
*Xl =0*  
5  
2 4  
Figure 7.2  
At each node \(NJ, Nt\) with NJ U Nt = *N,* we use a linear function to construct an  
inequality dominating the inequality  
\(7.7\) - 3 *L Xj* - 3 *L* \(1 - *Xj\)* - *2x* t - *2x* 2 + *Y* t + *Y2* \~ 3.  
*jE\}/O JEN!*  
1. NJ = \{l, 2\}, Nt = 0. *Ft\(d\)* = \(0 1 1 0 *O\)d* gives  
which is stronger than \(7.7\),  
2. NJ = \{l\}, Nt = \{2\}. *F2\(d\)* = \(0 1 0 *6\)d* gives  
- 3xt - 3\(1 - *X2\)* + \(- 2xt - *2X2* + *Yt* + *Y2\)* \~ 3.  
3. NJ = \{2\}, Nt = \{l\}. *F3\(d\)* = \(0 1 1 6 *O\)d* gives  
4. NJ = 0, Nt = \{l, 2\}. *F4\(d\)* = \(1 0 0 1 *l\)d* gives  
Now to obtain the inequalities that dominate \(7.7\) for the sets \(NJ, *N l \)* with  
NJ U Nt = \{l\}, we combine the superadditive functions generating the above inequalities  
as in the proof of Proposition 7.7.  
Combining the function *Ft* generating the NJ = \{l, 2\}, Nt = 0 inequality and the  
function *F2* generating the NJ = \{l\}, Nt = \{2\} inequality yields the following\:  
5. JIO = \{l\}, Nt = 0. *Fs* = *3FI/2\(Ft/6, F2/6\)* gives  
Combining *F3* and *F4* yields the following\:  
6. JIO = 0, Nt = \{l\}. *F6* = *3Ft/2\(F3/6, F4/6\)* gives 254 11.1. The Theory of Valid Inequalities  
To obtain the inequality at the root, we combine *Fs* and *F6\:*  
7. NJ = 0, N 1  
= 0. *F7* = *3F1 /2\(Fs/6, F6/6\)* gives  
*- 2x* I - *2x* 2 + *Y* I + *Y2* .\:\:\:\:;; 3.  
8\. NOTES  
Section 11.1.1  
Valid inequalities that are implied by integrality constraints were introduced by Dantzig,  
Fulkerson, and 10hnson \(1954, 1959\) in a study of the traveling salesman problem. Their  
pioneering work demonstrates the derivation of logical or combinatorial inequalities that  
can be obtained from problem structure and 0-1 variables. Another early study of this type  
is Markowitz and Manne \(1957\).  
The initial study of valid inequalities for general integer programs was carried out  
almost single-handedly by Gomory in the late 1950s and early 1960s. His work emphasized  
the generation of a finite number of valid inequalities to solve the general integer  
programming problem. The integer rounding procedure appears implicitly in Gomory  
\(1958, 1960a, 1963a,b\) and explicitly in Chvatal \(1973a\). Its derivation uses a modular  
argument which was exploited to a greater extent by Gomory \(1965\) in his derivation of all  
valid inequalities for the group relaxation of an integer program \(see Section II.3.5\).  
The valid inequalities of exercise 11 of Section II.4.5 were introduced by Dantzig \(1959\)  
and refined by Charnes and Cooper \(1961\) and Bowman and Nemhauser \(1970\).  
Surveys on algebraic methods for obtaining valid inequalities were given by Garfinkel  
and Nemhauser \(1972a, Chapter 4\), and leroslow \(1978, 1979a,c\).  
Gomory \(1960b\) used a disjunctive argument to develop valid inequalities for mixed-  
integer regions. A general disjunctive approach for obtaining valid inequalities appears in  
Balas \(1975b\). The D-inequalities were studied by Blair \(1976\). leroslow \(1977, 1979a,c\)  
and Balas \(1979\) gave surveys of disjunctive methods.  
Valid inequalities that can be deduced from combinatorial structures and 0-1 variables  
appear throughout the text and, in particular, in Chapter II.2. References will be given in  
the notes for the corresponding sections.  
Section 11.1.2  
Chvatal \(1973a\) contains all of the results of this section with the exception of Theorem  
2.16, although a few of the results are given only implicitly.  
Blair \(1976\) also showed that the D-inequalities suffice for 0-1 problems \(Theorem 2.3\).  
The close connection between this theorem when *P* is empty with the inequality *Ox* .\:\:\:\:;; -1  
and the resolution method of propositional logic of Davis and Putnam \(1960\) is discussed  
in Blair, leroslow, and Lowe \(1986\). General conditions under which the convex hull can  
be obtained sequentially by imposing disjunctions one-by-one, as in the proof of Theorem  
2.5, were studied by Balas \(1979\); see Section 6 of that article and Exercise 9 of Section  
1.4.8.  
Theorem 2.16 is due to Schrivjer \(1980\). He showed that for any integer *k,* the linear  
inequality system consisting of all of the inequalities of rank equal to or less than *k* defines  
a rational polyhedron, and then he used total dual integrality \(see Section 111.1.1\) to show  
that there existed some *k* for which the system defines the convex hull of integer solutions.  
The example of the two-dimensional family of polyhedra of unbounded rank is from  
Chvatal \(1973a\). Related results are given in leroslow \(1971\) and leroslow and Kortanek  
\(1971\). 8\. Notes 255  
Section 11.1.3  
The fractional cuts are due to Gomory \(1958\). The connection between them and integer  
rounding in the space of the original variables is implicit in that article and Chvatal  
\(1973a\).  
Section 11.1.4  
Connections between valid inequalities for integer programs and superadditive functions  
originated with the work of Gomory \(1965, 1967, 1969, 1970\) on the group problem  
relaxation of a general integer program \(see Section 11.3.5\). The explicit use of superaddi-  
tive functions in the generation of valid inequalities for general \(pure\) integer programs  
was developed in a series of articles by Gomory and Johnson \(1972, 1973\) and Burdet and  
Johnson \(1974\), again in the context of the group problem.  
Araoz \(1973\) investigated superadditive valid inequalities for packing and covering  
problems and showed that the modular arithmetic requirement of the group relaxation  
was not essential to the superadditive theory. For general \(pure\) integer programs, the  
superadditive representation of all facet-defining inequalities \(Proposition 4.5 and Theo-  
rem 4.6\) appears in the articles by Burdet and Johnson \(1977\) and Jeroslow \(1978\). The  
function of Figure 4.5 was used by E. L. Johnson \(1974\), and the two-dimensional function  
given by \(4.5\) and exhibited in Figure 4.3 appears in Nemhauser and Wolsey \(1984\). Some  
other classes of superadditive functions that have been proposed for the purpose of  
generating valid inequalities are given by Burdet and Johnson \(1974, 1977\).  
Surveys by Jeroslow \(1978, 1979a,c\) and Johnson \(1979\), and a monograph by Johnson  
\(1980a\) provide comprehensive treatments of the use of superadditivity in integer and  
mixed-integer programming. These references are also relevant to the following three  
sections.  
Section 11.1.5  
A polyhedral description of superadditive valid inequalities for the group problem was  
given by Gomory \(1967,1969, 1970\). He also introduced the concept of master polytopes  
in these articles and showed how facets for lower-dimensional polytopes could be obtained  
from the master polytope by projection.  
Gomory's approach was extended to independence systems or packing problems and to  
dependence systems or covering problems by Araoz \(1973\) as well as to general pure-  
integer programs by Burdet and Johnson \(1977\). See also Johnson \(1979, 1980a, 1981a\) and  
Araoz and Johnson \(1981\).  
Superadditive inequalities for 0-1 problems were studied by Wolsey \(1977\), and those  
for multiple right-hand side problems were studied by Johnson \(1981b\).  
Section 11.1.6  
This section is based on Nemhauser and Wolsey \(1984\). The motivation for the MIR  
inequalities came from the mixed-integer cuts of Gomory \(1960b\).  
Schrijver gave us the example in Exercise 22, which shows that Theorem 6.5 is false  
unless each integer variable belongs to the set \{O, 1\}. This is related to the absence of finite  
convergence of Gomory's mixed-integer cutting-plane algorithm, as shown by White  
\(1961\). White's counterexample appears in Salkin \(1975\).  
Section 11.1.7  
The extension of the superadditive theory to mixed-integer programs began with the work  
ofE. L. Johnson \(1974\) on a mixed-integer group problem. 256 11.1. The Theory of Valid Inequalities  
Theorem 7.8, for bounded mixed-integer constraint sets, appears in Jeroslow \(1979b\).  
Its generalization to unbounded sets is given by Bachem and Schrader \(1980\) and by  
Bachem, Johnson, and Schrader \(1982\). Also see Blair \(1978\) and Jeroslow \(1985\).  
9. EXERCISES  
1. LetS = *\{x* E Z\~\: 4Xl + *X2* S 28, Xl + *4X2* s 27, Xl - *X2* S 1\}. Determine the facets of  
conv\(S\) graphically \(see Exercise 10 of Section I.4.8\). Then derive each of the facets  
of conv\(S\) as a C-G inequality.  
2. Let S = *\{x* E Z\~\: I9x, + *28x2* - I84x3 = 8\}. Derive the valid inequality *x, +*  
*X* 2 + *5x* 3 \~ 8 using modular arithmetic.  
3. For S = *\{x* E *B4\:* 9XI + *7X2* - *2X3* - *3X4* S 12, *2x,* + *5X2* + IX3 - *4X4* S IO\} show that  
4XI + *5X2* - *2X3* - *4X4* S 12 is a valid inequality by disjunctive arguments.  
4. Consider the node-packing problem on the graph of Figure 9.1. Show that L\[=I *Xi* S 2  
is a valid inequality, both combinatorially and algebraically.  
5. Prove the following\:  
i\) Let *P* = *\{x ERn\: Ax* s *b\}* \*" 0. *nx* s *no* is a valid inequality for *P* if and only if  
there exists *u* E *R'.;!* such that *uA* = nand *ub* s *no.*  
ii\) Let *P* = *\{x* E *R'\:\: Ax* s *b, X* s *d\}. nx* s *no* is a valid inequality for *P* if and only  
if there exist *u* E *R'\:* and wE *R1* such that *uA* + *w* \~ nand *ub* + *wd* s *no.*  
6. Let *Pi* = *\{x* E *R1\: Ai x* s *b* J for *i* = 1, 2\. Show that *nx* s *no* is a valid inequality for  
*PI* U *P2* if there exists *ui* E *R'\:* such that *uiAi* \~ nand *uibi* s *no* for *i* = 1,2. Under  
what restrictions on *PI* and *P2* does the converse hold?  
7. \(The Davis-Putnam Procedure\). Consider the satisfiability problem for S s *Bn*  
defined by  
*L Xj* + *L* \(1 - *x\)* \~ 1 for *k* = 1, ... , *K, x* E *Bn*  
*JECk JECk*  
where *Ck* n *Ck* = 0 and *Ck , Ck* s *N* for *k* = 1, ... *,K.*  
i\) Given *q* EN and a pair of constraints *k, I* such that *q* E *Ck* n *C"* show that  
*L Xj* + *L* \(1 - *Xj\)* \~ 1  
*jE\(CkUC,\)\\\{q\} jE\:\(CkUC,\)\\\{q\)*  
is a valid inequality for S.  
Figure 9.1 9\. Exercises 257  
8. 9. 10. ii\) Show that the inequality is aD-inequality.  
iii\) Show that if S = 0, it is possible to generate the valid inequality *Ox* \:s -1 by a  
finite number of replications of the procedure i.  
iv\) Show that the resulting algorithm is polynomial if I *Ck* U *Ck* I \:s 2 for all *k.*  
What is the rank of conv\(S\) in Exercise I?  
Prove Propositions 2.9 and 2.10.  
Show that the rank of *conv\(St\)* is *t* - 1, where *st* = *pt* n Z2 and  
11\. Show that if *P* = *\{x* E *RZ\: Xi* + *Xj* \:S 1 for 1 \:S *i \<j* \:S *n\}* and S = *P* n *En,* the rank of  
"£J=l *Xj* \:S 1 is O\(log *n\).*  
12. Use Theorem 2.5 to show that every valid inequality is a D-inequality for mixed 0-1  
programs.  
13. Consider the integer program *max\{2x* 1 + *5x2\: xES\},* where S is given in Exercise 1.  
Using the optimal basis of the corresponding linear program, the problem can be  
rewritten as  
*maxz*  
*z* = 38  
17  
3  
1 4 16  
*X2 -EX3* + *E X 4 3*  
1 1 2  
*- 3X3* + *3X4* + *X5* = 3  
*xEZ!.*  
Derive a Gomory fractional cut from each equation. Express each cut in terms of the  
original variables *\(xt, X2\).* Derive each cut as a rank 1 C-G inequality.  
14. For S = *P* n Z2 as given in Exercise 1 show that  
i\) Xl \:S 5,  
ii\) Xl + *2X2* \:S 15, and  
iii\) *2x* 1 + *5X2* \:S 36  
are superadditive valid inequalities.  
15. What conditions must be imposed on *F* so that "£7=1 *F\(a\)xj* \:S *F\(b\),* is a valid  
inequality for S = *\{x* E *zn\: Ax* \:S *b\}?*  
16\. Show that the following functions are superadditive\:  
i\) *G\(d\)* = max\{o\:, *F\(d\)\},* where 0\: \< 0 and *F* is superadditive.  
ii\) *G\(d\)* = maXhEZm *\{Fl\(h\)* + *F2\(d* - *h\)\},* where *Fl* and *F2* are superadditive on *zm.*  
iii\) G *a\(d\)* = max\{o\:, mineO, *d\)\}* for *d* E *R* 1 and 0\: \< O. 258 17. 11.1. The Theory of Valid Inequalities  
18\. i\) Draw G *a,* which was defined in iii of Exercise 16.  
ii\) Use G *a* to show that - *3x* 1 - *2x* 2 - *2x* 2 \:\:;;; - 3 is a valid inequality for  
S = *\{x* E *zl\:* - *7Xl* - *4X2* - *4X3\:\:;;;* - 6\}.  
iii\) Can you show by repeated use of G *a* that - *2x* 1 - *X2* - *X3* \:\:;;; - 2 is valid for S?  
iv\) ForS = *\{x* E *B3\: -7Xl* - *4X2* - *4X3\:S;;* - 6\} use *Ga* to showthat- Xl - *X2\:\:;;;* - 1 is  
valid for S.  
Suppose that we define the disjunctive rank of an inequality via the function *Fa.*  
What is the maximum disjunctive rank of conv\(S\), where  
i\) S = *\{x ERn\: Ax* \:\:;;; *b, x* \:s;; 1, *x* \~ O\} n *Zn,*  
ii\) S = *\{x ERn\: Ax\:\:;;; b, x\:s;; d, x* \~ O\} n *zn,*  
iii\) S = *\{x* E R\~\: *Ax* \:s;; *b\}* n *zn?*  
19\. Let *cf\>\(d\)* = *max\{cx\: Ax* \:s;; *d, x* E Z\~\} and suppose the problem is feasible for all  
*dE Rm.* Show that cf\> is superadditive on *Rm.*  
20\. Find the convex hull of S= *\(x* E *Z\:;* Xl + *2X2* + *3X3* + *4x4 \:5 4\).*  
21. Write an implicit polyhedral description of the set of valid inequalities for  
i\) Xl + *X2* + *X3* + *X4* = 4, *x* E *Z!.*  
ii\) Xl *+X2 +X3 +X4* \~ 4, *x* E *Z!.*  
iii\) *\(b\)Xl* + \(\~\)X2 + \(\~\)X3 + *\(\:\)X4* + \(\~\)X5 + \(\~\)X6 + \(\~\)X7 + *\(;\)Xg* \:\:;;; \(;\), *x* E *Z!.*  
22\. Show that the set  
*T* = *\{\(x, y\)* E Z\~ x *Rl\:* Xl + *X2* + *y\:s;;* 2, - Xl + *y\:s;;* 0, - *X2* + *y\:\:;;;* O\}  
and the valid inequality *y* \:\:;;; 0 give a counterexample to Theorem 6.5 when the  
constraints *x* \:s;; 1 are not present.  
23\. Given *T* = *\{\(xo, x, y\)* E Zl X Z\~ x R\~\: *Xo* + *LjEN ajxj* + *LjE\] gjYj* = *b\}* with *b* = lbJ  
*+ fo* and 0 *\<fo* \< 1, derive \(by a disjunctive and modular argument\) the Gomory  
mixed-integer cut  
24\. where *J+* = \{j E *J\: gj* \> a\}, *J-* = *J* \\ *J+,* andjj = *aj* - *lajJ* for j EN.  
Verify that the Gomory mixed-integer cut for  
*T'* = *\{\(x, y,* s\) E Z\~ x R\~ x *Rl\: uAx* + *uGy* + *us* = *ub\}*  
is equivalent to the superadditive valid inequality  
*L F\(aj\)xj* + *L F\(gj\)Yj* \:s;; *F\(b\)*  
*JEN jE\]*  
*forT* = *\{\(x, y\)* E Z\~ x R\~\: *Ax* + *Gy\:\:;;; b\}, whereF\(d\)* = *F\:x\(-Lui\<o uidi,Lui\>o uidJand*  
*a= ub -lubJ.* **11.2**  
# Strong Valid Inequalities  
**and Facets for Structured**  
# Integer Programs  
1. INTRODUCTION  
In the preceding chapter we presented a general theory of valid inequalities for integer  
and mixed-integer programs and techniques for generating all valid inequalities. How-  
ever, these general techniques can be quite inefficient in deriving facets or even lower-  
dimensional faces of the convex hull of a set of integral points.  
The theme of this chapter is to use structure to determine strong valid inequalities for  
the constraint sets of some .N9J\>-hard integer programming problems. The determination  
of families of strong valid inequalities is more of an art than a formal methodology. Thus  
our presentation will largely be a series of examples that convey the basic ideas. The  
mathematics enters in proving that classes of inequalities, which are often easily shown to  
be valid, are indeed strong in the sense that they define facets or faces of reasonable  
dimension. A related mathematical problem, which is considered in Part III, is to prove  
that a given family of inequalities represents all of the facets of the convex hull. We defer  
this topic because the results are limited almost exclusively to those combinatorial  
optimization problems for which polynomial-time algorithms are known.  
There are many interesting problems for which strong valid inequalities have been  
obtained. Only a small selection of these results can be given here, so we have picked a few  
prototype problems. To motivate some basic ideas, in this section we consider the node-  
packing polytope. In the following sections, we study the 0-1 knapsack polytope, the  
symmetric traveling salesman polytope, and a class of generic mixed-integer sets that we  
call 0-1 variable upper-bound flow models. The attention given to polyhedra for which the  
integer variables are binary reflects the fact that most of the known results are in this  
domain.  
In the preceding chapter, we derived some valid inequalities for the node-packing  
problem. Here we will establish the strength of the inequalities. Recall that a node packing  
in a graph G = *\(V, E\)* is a set of nodes such that no pair in the set is joined by an edge. Thus  
the set of node packings S is given by  
S = *\{x* E *Bn\: Xi* + *Xj* \~ 1 for all *\(i,j\)* E *E\},*  
where *n* = 1 *V* I. The vector *xES* is the characteristic vector of a packing; that is, *Xi* = 1 if  
node *i* is in the packing and *Xi* = 0 otherwise. Since S contains the zero vector and the *n*  
unit vectors, dim\(conv\(S\)\) = *n.*  
259 260 11.2. Strong Valid Inequalities and Facets for Structured Integer Programs  
2  
6 3  
5 4  
Figure 1.1  
A set C \~ *V* is called a *clique* if each pair of nodes in C is joined by an edge. Thus a node  
packing can contain no more than one node from each clique. For the graph of Figure 1.1,  
the maximal cliques yield the inequalities  
\(1.1\) Xl  
*X2*  
*X3*  
*X4*  
Xs  
*X6*  
\~  
corresponding to the cliques \{l, 2, 3\}, \{l, 3, 4\}, \{l, 4, 5\}, \{l, 5, 6\}, and \{l, 2, 6\}.  
When C is a maximal clique, the *clique constraint*  
\(1.2\) *L* Xj\~ 1  
jEC  
defines a facet of conv\(S\). This is an easy result to prove directly from the definition of a  
facet. A facet of conv\(S\) is of dimension *n* - 1 and thus contains *n* affine1y independent  
points ofconv\(S\). Moreover, as noted in Proposition 6.6 of Section 1.4.6, a facet contains  
*n* affinely independent points of S. Since the hyperplane LjEC *Xj* = 1 does not contain the  
origin, any set of affinely independent points on it are also linearly independent. Thus we  
will exhibit *n* linearly independent points of S that satisfy \(1.2\) at equality.  
Suppose, for simplicity of notation, that C = \{l, ... , *k\}.* Since C is maximal, for each  
*j* \$. C there is a node */\(j\)* such that */\(j\)* \~ *k* and *\{j, /\(j\)\}* is a node packing. The  
characteristic vectors of the packings \{l\}, ... , *\{k\}, \{k* + 1, */\(k* + I\)\}, ... , *en, /\(n\)\}* are easily  
shown to be linearly independent.  
The rows of the matrix given below are six linearly independent vectors which establish  
that Xl + *X* 2 + *X* 3 \~ 1 is facet for the graph of Figure 1.1. 1. Introduction 261  
Although there is an important class of node-packing problems for which the maximal  
clique constraints and nonnegativity give all the facets of conv\(S\), this is not true in our  
example. In particular, *Xl* = \~\(O 1 1 1 1 1\) is an extreme point of the polytope given by \(1.1\)  
and *x* \~ 0\. This can be seen by solving the linear program max LY=I *Xj* subject to \(1.1\) and  
*x* \~ 0. The unique optimal solution is *Xl.*  
To cut off *Xl,* we consider another family of valid inequalities. Suppose there is an  
*H* \~ *V* that induces a *chordless cycle,* that is, the nodes of *H* can be ordered as *\(i* I, *i* 2, ••• ,  
*ip \)* such that *\(i" is\)* E *E* if and only if *s* = *r* + 1 or *s* = 1 and *r* = *p.* Ifp is odd and at least 5,  
then *H* is called an *odd hole.* If *H* is an odd hole, then  
\(1.3\) 2\: *x.* \~ IHI - 1  
*jEH* J 2  
is satisfied by all node packings. Moreover, the clique constraints *Xi* + *Xj* \~ 1 for *i,* j E *H*  
do not imply \(1.3\).  
In our example, *H* = \{2, 3, 4, 5, 6\} is an odd hole and we obtain the constraint  
\(1.4\)  
which cuts off the solution *x* I .  
Since \(1.4\) is satisfied at equality by the five linearly independent characteristic vectors  
corresponding to the packings \{2, 4\}, \{2, 5\}, \{3, 5\}, \{3, 6\}, and \{4, 6\}, inequality \(1.4\) gives a  
facet of the convex hull of node packings for the subgraph with node set *H.* But it does not  
give a facet of conv\(S\) for the graph G, since there are no other packings that satisfy \(1.4\) at  
equality. If we added \(1.4\) to the clique constraints, we would obtain the new extreme point  
t\(1 22222\).  
Since \(1.4\) is a four-dimensional face of conv\(S\) but not a facet, it can perhaps be  
strengthened by tilting it to produce a facet. In other words, is there a valid inequality of the  
form  
\(1.5\)  
with *a* \> O? And if so, what is the largest value of *a* that preserves validity? To answer these  
questions, we must consider *x* I = ° and *Xl* = 1. When *x* I = 0, \(1.5\) is valid for any *a* \> 0.  
When XI = 1, we have *a* \~ 2 - *\(X2* + X3 + *X4* + Xs + *X6\)'* But Xl = 1 implies X2 = *X3* = X4 =  
*X* 5 = *X* 6 = 0, so *a* \~ 2. Thus  
\(1.6\)  
is a valid inequality. Moreover, it gives a facet of conv\(S\) since it is satisfied at equality by  
the characteristic vector of \{l\} and the characteristic vectors of the five packings given  
above that satisfy \(1.4\) at equality.  
We have just illustrated a general principle called *lifting* whereby a valid inequality for  
S n *\{x* E *Bn\: Xl* = o\} is extended to a valid inequality for S.  
Proposition 1.1. *Suppose* S s; *Bn, So* = S n *\{x* E *Bn\: x* I = 6\} *lor* £5 E \{o, I\}, *and*  
*n*  
\(1. 7\)  
*L TCjXj* \~ *TCo*  
*j=2*  
*is validlor So. 1iSI* = 0, *then* XI \~ ° *is validfor* S. *1iSI =1=* 0, *then* 262  
11.2. Strong Valid Inequalities and Facets for Structured Integer Programs  
\(1.8\)  
*n*  
*alXl* + I *njXj* \~ *no*  
*j=2*  
*is valid for* S *for any al* \~ *no* -" *where* ,= max\{L1=2 *njXj\: x* E SI\}. *Moreover, if*  
*al* = *no* - *'and* \(1.7\) *gives a face of dimension k of* conv\(SO\), *then* \(1.8\) *gives a face of*  
*dimension at least k* + 1 *ofconv\(S\). \[1f\(1.7\) gives afacet* ofconv\(\~\), *then* \(1.8\) *gives a*  
*facet ofconv\(S\).\]*  
*Proof Ifx* E *So,* then  
*n n*  
*alxl* + *L n\)x\)* = *L n\)x\)* \~ *no*  
\)=2 \)=2  
since \(1.7\) is valid for *So.*  
Ifx E Sl, then  
*n n*  
*alxI* + I *n\)x\)* = *al* + I *n\)x\)* \~ al +, \~ *no*  
*\)=2 j=2*  
by definition of the quantities al and ,.  
Since \(1.7\) gives a k-dimensional face of conv\(SO\), there exist *Xi* E So for *i* = 1, ... ,  
*k* + 1 that are affinely independent and satisfy \(1. 7\) at equality. Since *x\\* = 0, it follows that  
*Xi* satisfies \(1.8\) at equality for *i* = 1, ... , *k* + 1. Let, = *'L\)=2 njxj,* where *x\** E Sl. With  
*al* = *no* - " *x\** satisfies \(1.8\) at equality. Finally, since *xT* = 1, it follows that *x\** cannot be  
written as an affine combination of *\{Xl,* ... , *Xk+l\},* so the *k* + 2 vectors *\{X\*, Xl,* ... , *Xk+l\}*  
are affinely independent. •  
The lifting principle is also applicable to extending a valid inequality from SI to S.  
Using the same notation as in Proposition 1.1, we have the analogous result\:  
Proposition 1.2. *Suppose* \(1.7\) *is valid for* SI. *If* SO = 0, *then Xl* \~ 1 *is valid for* S. *If*  
SO *=1=* 0, *then*  
\(1.9\)  
*n*  
*YIX* I + *L njx\)* \~ *no* + *Y* I  
\)=2  
*is valid for* S *for any YI* \~ ,- *no, where* ,= max\{'L\)=2 *njXj\: X* E *SO\}. Moreover, if*  
*Yl* = ,  
- *no and* \(1. 7\) *gives a face of dimension k of* conv\(SI\), *then* \(1.9\) *gives a face of*  
*dimension at least k* + *lofconv\(S\).*  
When *a1* = *no* - 'in Proposition 1.1 or when YI = ,- *no* in Proposition 1.2, we say that  
the *lifting is maximum.*  
Propositions 1.1 and 1.2 are meant to be used sequentially. Given an *NI eN* = \{l, ... ,  
*n\}* and an inequality *LjEN! n\)xj* \~ *no* that is valid for S n *\{x* E *Bn\: Xj* = 0 for\} *EN* \\ *N 1\},* we  
lift one variable at a time to obtain a valid inequality  
\(1.10\) *L ajxj* + I *njx\}* \~ *no*  
*jEN\\N! JEN!*  
forS. 1\. Introduction 263  
The coefficients *\{ai\}* in \(1.10\) are dependent on the order in which the variables are  
lifted. So by considering different orderings of the elements of *N* \\ *Nb* we can get a family  
of valid inequalities for S.  
It is insightful to examine the lifting process in the polar space III = *\{n ERn\: nx* \~ 1 for  
all *xES* \~ *Bn\}.* If *L1=2 nixi* \~ 1 is valid for *So,* maximum lifting can be described by the  
one-dimensional optimization problem in III-space\:  
The geometry is illustrated in Figure 1.2 for the case *n* = 2. We suppose that III has the  
three extreme points *\{nO,* nl, *n2 \}.* Since\~ \> max\(n\~, *nD,* wehavethatn\~x2 \~ 1 givesafacet  
of conv\(SO\), where SO = S n *\{x\: Xl* = O\}. Maximum lifting is equivalent to moving from  
*\(0,* n\~\) in the direction \(1 0\) to obtain the extreme point *nO* of III or, equivalently, the facet  
of conv\(S\) defined by *n?x* I + n\~x2 \~ 1. Similarly, by a maximum lifting from *nIX* 1 \~ 1, we  
obtain the facet ofconv\(S\) defined by *JrtXI* + n\~x2 \~ 1. We also see that there is no way to  
generate the facet of conv\(S\) defined by *nix* 1 + n\~x2 \~ 1 by sequential lifting.  
To interpret sequential lifting geometrically, suppose we begin with the trivial inequality  
o \~ 1. Maximum lifting in the order \(1, 2\) yields the facet corresponding to the extreme  
point *n2*  
*,* and maximum lifting in the order \(2, 1\) yields the facet corresponding to the  
extreme point *nO.* Neither order gives nl.  
In principle, lifting is not restricted to choosing one coefficient at a time. If we observe  
that maximum sequential lifting is equivalent to finding an extreme point in a one-  
dimensional polyhedron, it is not surprising that in the simultaneous lifting of *k* coeffi-  
cients, the "best" liftings are obtained by finding the extreme points of a k-dimensional  
polyhedron. Hence if we start from the inequality 0 \~ 1 and allow the simultaneous lifting  
of\(nl *n2\),* we can indeed obtain *nO,* nl, and *n2*  
*.*  
As we have already seen, the values of the coefficients in \(1.10\) depend on the ordering  
of the variables in the sequential lifting. The following proposition, which will be useful in  
the next section, indicates how the coefficient of one variable depends on the ordering.  
\(0,0\) L--\_\_\_\_\_\_\_\_ -+ \_\_\_\_ 11"1  
Figure 1.2 264 11.2. Strong Valid Inequalities and Facets for Structured Integer Programs  
Proposition 1.3. *Let N* \\ *N\[* = \{I, 2, ... , *t\} and suppose when Proposition* 1.1 *is applied*  
*sequentially using maximum lifting in the order* \(iI, *i2 ,* ... , h-I' *ik,* ••. , *it\), the inequality*  
I *aixi* + I *nixi\:\:\:\:;; no*  
*iEN\\Nl JENl*  
*is obtained. Then for any order \(iI, i2 ,* ... , *i£\_\[, i£,* ... , *i;\) with i;* = *ii for j* = 1, ... , *k* - 1,  
*the resulting inequality*  
I *a;Xi* + I *n\}x\}\:\:\:\:;; no*  
*iEN\\Nl* iENl  
*obtained by maximum lifting has aL* \:\:\:\:;; *aik •*  
*Proof ik* = *i;* for some *s* \> *k.* Then  
*k-I*  
*ah* = *no* - max *IaijXjj* + I *nixi\: xES* n *\{x* E *Bn\: Xh* = 1  
*\{*  
*i=l iEN,*  
and *Xij* = 0 for j \> *k\}\}*  
*k-I*  
*= no* - max *IaijXij* + I *nix\}\: xES* n *\{x* E *Bn\: Xi's* = 1  
*\{*  
*i=l iEN,*  
and *x i'j* = 0 for *k* \~ j \~ *t,* j '\* *s\}* \}  
k-l *s-I*  
\~ *no* - max *IaijXjj* + I *nixi* + *Iaj,jXij\: xES* n *\{x* E *Bn\:*  
\{  
*i=! iENl \}=k*  
*Xi's* = 1 and *Xi'j* = 0 for *j* \> *s\}\}*  
# •  
Corollary 1.4. *In any sequential maximum lifting of the variables in N* \\ *NI , the mini-*  
*mum value of aik is obtained by lifting Xh last and the maximum value is obtained by lifting*  
*xhfirst.*  
Although this discussion has focused on maximum lifting, , can be hard to compute.  
Thus, in practice, *a* and yare generally determined from easily computable upper bounds  
on ,. We will illustrate these computations in Section 11.6.2.  
We have given two ways of showing that a valid inequality gives a facet of conv\(S\). The  
first approach was to apply the definition, the second approach was by maximum lifting of  
a lower-dimensional facet. We now consider a third approach, which is to apply Proposi-  
tion 3.6 of Section 1.4.3.  
We illustrate this approach by showing that \(1.6\) gives a facet of conv\(S\) in the node-  
packing example of Figure 1.1. Consider a valid inequality *LY=I nixi* \:\:\:\:;; *no* and suppose that  
it is satisfied at equality by the packings \{2, 4\}, \{2, 5\}, \{3, 5\}, \{3, 6\}, and \{4, 6\}. From \{2, 4\}  
and \{2, 5\} we obtain *n2* + *n4* = *n2* + *ns* = *no* or *n4* = *ns.* Similarly from \{2, 5\} and \{3, 5\} we  
obtain *n2* = *n3,* from \{3, 5\} and \{3, 6\} we obtain *ns* = *n6,* and from \{2, 4\} and \{4, 6\} we obtain  
*n2* = *n6.* Hence any equality that is satisfied by these five packings must be of the form 2. Valid Inequalities for the 0-1 Knapsack Polytope 265  
Now if the packing \{l\} also lies on the above hyperplane, we must have *reI* = *2a* and the  
equality must be of the form  
Finally, the inequality *a\(2x* I + *X2* + ... + *X6\)* \~ *2a* must hold for *x* = O. Thus *a* \> 0, and  
it suffices to take *a* = 1.  
Here we have applied Proposition 3.6 of Section 1.4.3 with *n* = 6, *k* = 0, and *Xl,* ••• *,x6*  
being the characteristic vectors of the six packings given above. The argument shows that  
all solutions to the linear system hi = ..to for *i* = 1, ... , 6 are of the form A = *are* and  
..to = *areo* with *a* E *R* I. Finally, we used *x* = 0 to establish that *a* \> O. Other applications of  
this technique will be given in Sections 3 and 4.  
We close this section with a pessimistic reminder regarding the possibility of obtaining  
all facets of the convex hull of a feasible set of points for an .N9P-hard optimization  
problem, but we add a note of optimism with respect to using the strong inequalities that  
can be obtained.  
In Proposition 7.4 of Section 1.5.7, it was established that for an .N9P-complete lower-  
bound feasibility problem, a good characterization of all of the facets of the convex hull of  
feasible solutions is not possible unless.N9P = *C€oJY9P.* Thus our use of structure to obtain a  
polyhedral representation of the constraint set is limited by the inherent complexity of the  
problem. For this reason the results of this chapter are only partial descriptions of the  
convex hull of the constraint set of the problem being studied. However, there are some  
experimental results which indicate that simple classes of strong valid inequalities that can  
be identified efficiently are extremely useful in solving a variety of integer programming  
problems by cutting-plane algorithms. In Chapter II.5, we will show how the results of this  
chapter can be incorporated in such cutting-plane algorithms.  
2\. VALID INEQUALITIES FOR THE 0-1 KNAPSACK POLYTOPE  
We consider the constraint set of a 0-1 knapsack problem  
\(2.1\)  
where *N* = \{l, ... , *n\}, aj* E Z! for j *EN,* and *b* E *Z!.* Note that S is an independence  
system \(see Section 11.1.5\). Since *aj* \> *b* implies *Xj* = 0 for all *XES,* we assume *aj* \~ *b* for  
allj *EN.* Thus dim\(conv\(S\)\) = *n.* It is convenient to order the coefficients monotonically  
so that *a* I \~ *a2* \~ ... \~ *an.* We represent elements of *Bn* by characteristic vectors so that  
for *R* £.; *N* the vector *xR* has components *xf* = 1 if j E *Rand xf* = 0 otherwise. If *XC* E S,  
we say that C is an *independent set;* otherwise C is a *dependent set.*  
As we observed in Section 11.1.5, the *n* constraints *x* \~ 0 give facets of conv\(S\). In  
addition, *Xj* \~ 1 gives a facet if \{j, *k\}* is an independent set for all *kEN* \\ *\{j\}.* We leave  
these results as exercises and go on to more interesting inequalities.  
Proposition 2.1. *If* C *is a dependent set, then*  
\(2.2\) *L Xj* \~ ICI - 1  
*jEe*  
*is a valid inequality for* S. 266 11.2. Strong Valid Inequalities and Facets for Structured Integer Programs  
*Proof* Suppose *XR* E Sand *LjEC Xf;?;* I C I. This means that *R* ;2 C so that *R* is  
dependent, which contradicts *XR* E S. •  
A dependent set is *minimal* if all of its subsets are independent. Note that if a dependent  
set C is not minimal, then *LjEC Xj* \~ I C I - 1 is the sum *ofLjEc' Xj* \~ I c' I - 1 and *Xj* \~ 1 for  
j E C \\ *C',* where C' is a minimal dependent set.  
*Example* 2.1\. S = *\{x* E *B 5 \: 79xI* + *53x2* + *53x3* + *45x4* + *45x5* \~ 178\}. The minimal  
dependent sets and corresponding valid inequalities are\:  
C I = \{l, 2, 3\} Xl + *X2* + *X3* \~ 2  
C2 = \{l, 2, 4, 5\} XI *+X2 +X4 +X5* \~ 3  
C3 = \{l, 3,4, 5\} *XI* + *X3* + *X4* + *X5* \~ 3  
C4 = \{2, 3,4, 5\} *X2* + *X3* + *X4* + *X5* \~ 3.  
While the constraints \(2.2\) are quite simple, they are nontrivial with respect to the  
polytope *P* ;2 S obtained by replacing *X* E *En* by *X* E R\~ and *Xj* \~ 1 for allj *EN,* that is,  
the linear programming relaxation with *P* = *\{x* E R\~\: *LjEN a jXj* \~ *b, Xj* \~ 1 for j E *N\}.* If  
*LjEN aj* \> *b,* then every nonintegral extreme point *x* of *Pis* of the form  
*Xj* = 1 for j E C \\ *\{k\}*  
*Xj* = 0 for j E *N* \\ C  
*Xk* = *\(b* - I *aj\)* / *ak* \> 0,  
*jEC\\\{k\}*  
where C is a dependent set, *k* E C, and C \\ *\{k\}* is independent. However, *x* does not satisfy  
the inequality \(2.2\).  
Proposition 2.1 applies to any independence system. We now begin to use some  
particular properties of the knapsack problem.  
The *extension E\(C\)* of a minimal dependent set C is the set C U *\{k EN* \\ C\: *ak* ;?; *aj* for  
allj E *C\}.* In Example 2.1, *E\(CJ* = *Cj* for *i* = 1,2,3 and *E\(C4 \)* = C4 U \{t\}.  
Proposition 2.2. *If* C *is a minimal dependent set, then*  
\(2.3\)  
I *Xj* \~ ICI - 1  
*jEE\(C\)*  
*is a valid inequality for* s.  
*Proof* Suppose *XR* E Sand *LjEE\(C\) xf* \~ IC 1 so that *IR* n *E\(C\)I* \~ IC I. Now  
*LjER aj* ;?; *LjERnE\(C\) aj* and by definition of *E\(C\)* we obtain *LjERnE\(C\) aj* ;?; *LjEC aj* \> *b,* which  
contradicts *XR* E S. •  
In Example 2.1, *LJ=I Xj* \~ 3 is a valid inequality obtained from Proposition 2.2 with  
*E\(C4\).* It dominates the inequalities \(2.2\) generated by *C2, C3,* and *C4•*  
In some instances the inequalities \(2.3\) give facets of conv\(S\).  
Proposition 2.3. *Let* C = *UI,* ... , *jr\} be a minimal dependent set withjl* \<h \< ... *\<jr' If*  
*any of the following conditions holds, then* \(2.3\) *gives afacet of* conv\(S\). 2\. Valid Inequalities for the 0-1 Knapsack Polytope 267  
a. *C=N.*  
b. *E\(C\)* = *N and* \(i\) \(C \\ \{jI,\}2\}\) U \{l\} *is independent.*  
c. C = *E\(C\) and* \(ii\) \(C \\ \{jl\}\) U *\{p\} is independent, where p* = min\{j\:\) *EN* \\ *E\(C\)\}.*  
d. C C *E\(* C\) C *N and* \(i\) *and* \(ii\).  
*Proof* The following *n* independent sets satisfy \(2.3\) at equality.  
*1. lj* = C \\ \{\}j\} for\}j E C. There are I C I of these.  
*2. lk* = \(C \\ \{jj,\}2\}\) U *\{k\}* for *k* E *E\(C\)* \\ C. *Ilk* n *E\(C\)* I = I C I - 1 *andlk* isindepen-  
dent by \(i\) and *ak* \:\:\:\:\:;; *al'* There are *IE\(C\)* \\ C I of these.  
*3. Ij* = \(C \\ \{it\}\) U \{j\} for\) *EN* \\ *E\(C\}. IIj* n *E\(C\)* I = I C I - 1 and *Ij* is independent  
by \(ii\) and *aj* \:\:\:\:\:;; *ap •*  
We leave it to the reader to show that the corresponding characteristic vectors are  
linearly independent. •  
In Example 2.1, Proposition 2.3 establishes that \(2.3\) with C = C I gives a facet of  
conv\(S\) since C I = *E\(CI \}* and \(CI \\ \{jl\}\) U *\{p\}* = \{2, 3, 4\} is independent. Also, since  
*E\(C4\)* = Nand *\(C4* \\ \{2, 3\}\) U \{l\} = \{l, 4, 5\} is independent, \(2.3\) with C = C4 gives a facet  
ofconv\(S\}.  
A simple consequence of Proposition 2.3 is\:  
Corollary 2.4. *IfC is a minimal dependent setfor Sand* \(CI, *C2 \) is any partition ofC with*  
CI *=1=* 0, *then LjEC1 Xj\:\:\:\:\:;;* I CI I -1 *gives afacet of* conv\(S\(CJ, *C2 », where*  
S\(Ct, *C2\)* = S n *\{x* E *Bn\: Xj* = 0 for\} *EN* \\ C, *Xj* = 1 for\} E *C2\}.*  
*Proof* For any *C2,* 0 S C2 C C, it follows that C I = C \\ C2 is a minimal dependent  
set for *S\(C!, C2\)* since  
*LjEC1 aj* \> *b* - *LjECz ah* and *LjEC1\\\{k\} aj* \:\:\:\:\:;; *b* - *LjEC2 aj* for all *k* E Ct. Now Proposition 2.3  
applies with S = *S\(C!, C2\)* and *N* = *E\(Cl \)* = C j • •  
We can use Corollary 2.4 and the lifting results of Section 1 to generate facets of conv\(S\).  
Proposition 2.5. *IfC is a minimal dependent setfor Sand* \(Cl\> *C2\) is any partition ofC*  
*with* CI *=1=* 0, *then* conv\(S\) *has afacet represented by*  
I *CijXj* + I *YjXj* + I *Xj\:\:\:\:\:;;* I C j I - 1 + I *Yh*  
jEN\\C *jECz* JEC1 *JEC2*  
*where Cij* \~ *Of or all\}* E *N\\* C *and where Yj* \~ 0 for all\} E *C2•*  
*Proof* We start with the inequality *LjEC1 Xj\:\:\:\:\:;;* I C j I - 1, which gives a facet of  
conv\(S\(CI, *C2»,* and do lifting by applying Proposition 1.1 for each\} *EN* \\ C and  
Proposition 1.2 for each\} E C 2. The nonnegativity of the coefficients is implied by their  
definitions in Propositions 1.1 and 1.2. • 268 11.2. Strong Valid Inequalities and Facets for Structured Integer Programs  
As we observed previously, the order of the variables in the lifting affects the coeffi-  
cients. However, we should begin with a j EN \\ C, because beginning with *k* E C2 is  
equivalent to starting with LjECtU\{k\} *Xj* \~ I C I I.  
a. C = \{l, 4, 5\} is a minimal dependent set and *E\(* C\) = \{l, 4, 5\}. By Proposition 2.3,  
*XI* + *X4* + *Xs* \~ 2 gives a facet ofconv\(S\).  
b. C = \{l, 4, 5\}, C I = \{4, 5\}, C2 = \{l\}. By Corollary 2.4, *X4* + *Xs* \~ 1 gives a facet of  
First we lift with respect to the variable *X* 3 by applying Proposition 1.1. This yields  
Hence *a3* = 1 andx3 + *X4* + *Xs* \~ 1 gives a facet *ofconv\{x* E *B3\: X3* + *X4* + *Xs* \~ n. Now we  
lift with respect to *X* I by applying Proposition 1.2. Hence  
Thus *2xI* + *X3* + *X4* + *Xs* \~ 3 gives a facet *ofconv\{x* E *B4\: 3xI* + *X3* + *X4* + *Xs* \~ 4\}. Finally,  
we lift with respect to *x* 2 by applying Proposition 1.1. Hence  
Thus *a2* = 0 and *2xI* + *X3* + *X4* + *Xs* \~ 3 gives a facet of conv\(S\).  
By symmetry, lifting in the order *\(X2, Xl, X3\)* yields the facet represented by  
*2xI* + *X2* + *X4* + *Xs* \~ 3. The orders *\(X2' X3, XI\)* and *\(X3, X2, XI\)* show that the original  
inequality *3Xl* + *X2* + *X3* + *X4* + *Xs* \~ 4 also gives a facet of conv\(S\). We have not consid-  
ered lifting *X* I first because, as explained before the example, this yields *Xl* + *X4* + *X* S \~ 2,  
which we already know gives a facet.  
To apply Proposition 2.5, we must solve IN \\ CII 0-1 knapsack problems. However,  
unlike the general 0-1 knapsack problem, these knapsack problems can be solved in  
polynomial-time by dynamic programming \(see Section 11.5.5\) because the objective  
coefficients are polynomial in *n.* Nevertheless, for computational purposes, it may suffice  
to get lower bounds on the *aj* and upper bounds on the *Yj.* We will return to these  
computational issues in Section II.6.2, where we will give an algorithm for solving general  
0-1 integer programs that uses strong valid inequalities derived from 0-1 knapsack  
problems.  
When C2 = 0 in Proposition 2.5, there is a formula that nearly determines all of the  
lifting coefficients.  
Proposition 2.6. *Let* C = \{j 1, ••• , *j,\} be a minimal dependent set with* j 1 \< *h* \< . . . \< *Jr.*  
*Let flh* = *LZ=* I *aA for h* = 1, ... , *r,' also let flo* = 0 *and A* = *fl,* - *b* ;?; 1. *Every valid inequality*  
*a/the/arm*  
\(2.4\) I *ajXj* + I *Xj* \~ I C I - 1  
*jEN\\C JEC* 2\. Valid Inequalities for the 0-1 Knapsack Polytope 269  
*that represents afacet of* conv\(S\) *satisfies the following conditions\:*  
i. *IfJ.1h* \~ *aj* \~ *J.1h+l* - *A, then aj* = *h.*  
ii. *If J.1h+l* - *A* + 1 \~ *aj* \~ *J.1h+l* - 1, *then* \(a\) *aj* E *\{h, h* + 1\} *and* \(b\) *there is at least one*  
*facet of the form* \(2.4\) *with aj* = *h* + 1.  
*Proof* The proof is based on lifting *LjEC Xj* \~ I C I - 1. Suppose, for *j\** E *N* \\ C, that  
*aj\** \~ *J.1h.* We will prove that *aj\** \~ *h* in any lifting in which *Xj\** is lifted last. Then from  
Corollary 1.4, it follows that *aj\** \~ *h* in allliftings.  
Suppose we have obtained the inequality  
\(2.5\) I *ajXj* + I *Xj* \~ I C I - 1  
*jEN\\\(CUU\*\}\) JEC*  
after determining all of the lifting coefficients except *aj.* Let  
\(2.6\)  
*G\(d\)* = max I *ajXj* + I *Xj*  
*jEN\\\(CUU\*\}\) JEC*  
I *ajXj* \~ *d*  
*jEN\\\(j\*\}*  
Then *aj\** = I C I - 1 - *G\(b* - *aj\*\).* Since \(2.5\) is valid when *xj\** = 0, we have *G\(b\)* \~ I C I - 1  
so that *aj\** \~ *G\(b\)* - *G\(b* - *aj\*\).*  
Now we show that *G\(b\)* - *G\(b* - *aj\*\)* \~ *h.* Consider \(2.6\) with *d* = *b* - *aj\*.* We have  
*r h*  
'" *a· =b+A-* '" *a·* \~b+A-a·\*\>b-a·\*  
L *1k* L *\)k* \) \)  
*k=h+l k=l*  
since *aj\** \~ *LZ=l alk* and *A\>* O. Hence there is no feasible solution with *Xjk* = 1 for *k* = *h* + 1,  
... , *r,* and since mink=l, ... , *h a jk* \~ maXk=h+l, ... , *r a jk* there exists an optimal solution *x* with  
*Xlk* = 0 for *k* = 1, ... , *h.* Define *x* by *Xjk* = 1 for *k* = 1, ... , hand *Xj* = *Xj* otherwise. Since  
Lt.l *ah* \~ *aj"* it follows that *x* is a feasible solution to \(2.6\) with *d* = *b.* Hence  
*h*  
*G\(b\)* \~ *G\(b* - *aj\*\)* + I *Xjk* = *G\(b* - *aj\*\)* + *h.*  
*k=l*  
Thus we have shown that *aj\** \~ *h* in allliftings when *aj\** \~ *J.1h.*  
Now suppose that *J.1h* \~ *ak* \~ *J.1h+l* - *A* and *Xk* is lifted first. We will show that *ak* = *h,* so  
by Corollary 1.4 we obtain *ak* \~ *h* in allliftings. From Proposition 1.1, *ak* = *\(r* - 1\) - \(,  
where  
\( = max\{ I *Xj\:* I *a jXj* \~ *b* - *a k, x* E *Br\}*  
*JEC JEC*  
= max\{r + 1 - *i\:* ± *aJt* \~ *b* - *ak \}*  
*1=1* 270 11.2. Strong Valid Inequalities and Facets for Structured Integer Programs  
*r*  
*l=h+2*  
I *aj,* = *b* - *\(J.1h+l* - *A\)* \~ *b* - *ak*  
since *ak* \~ *J.1h+l* - *A.* Also,  
*r*  
I *aj,* = *b* + *A* - *J.1h* \> *b* - *ak*  
*l=h+l*  
since *A* \> 0 and *ak* \~ */.1h.* Hence \(= *r* + 1-*\(h* + 2\) = *r* - h -1 and *Cik* = *\(r* -1\) - *\(r* - *h* -1\) = *h.*  
Putting these two results together establishes i and ii\(a\). To obtain ii\(b\), note that if  
*ak* \> */.1h+l* - *A,* it follows that *L/=h+2 aj,* \> *b* -- *ak,* which implies *Cik* = *h* + 1 if *Xk* is lifted first.  
# •  
*Example* 2.3. *3XlO* \~ 39\}.  
tion 2.6 yields  
S = *\{x* E *BlO\: 35xl* + *27x2* + *23x3* + *19x4* + *15x5* + *15x6* + *12x7* + *8xs* + *6X9 +*  
Let C = \{6, 7, 8, 9\}. Then /.10 = 0, /.11 = 15, *J.12* = 27, /.13 = 35, /.14 = 41, and *A* = 2\. Proposi-  
*Cij =* 0 if 0 \~ *aj* \~ 13  
o or 1 if *aj* = 14  
if 15 \~ *aj* \~ 25  
1 or 2 if *aj* = 26  
2 if27 \~ *aj* \~ 33  
2 or 3 if *aj* = 34  
3 if35 \~ *aj* \~ 39.  
Hence the only facet that can be obtained from lifting *X6* + *X7* + Xg + *X9* \~ 3 is represented  
by  
3. VALID INEQUALITIES FOR THE SYMMETRIC TRAVELING SALESMAN  
POLYTOPE  
A *Hamiltonian cycle* or *tour* ofa graph is a cycle that contains all of the nodes. Thus, given  
a graph G = *\(V, E\),* the edge set *E'* \~ *E* induces a *tour* if and only if the subgraph  
G' = *\(V, E'\)* is connected and each node is met by exactly two edges. Our reason for  
studying tours is that they are the feasible solutions to the symmetric traveling salesman  
problem.  
The results of this section are of two types. We develop inequalities that are valid for all  
graphs and prove that some of these inequalities are facets for complete graphs. Thus it is  
convenient to assume throughout the section that G is a *complete graph* on *m* nodes, that  
is, there is an edge between each pair of nodes so that *IE* I = *n* = *m\(m* - 1\)/2. The reader  
should observe, however, that all of the classes of valid inequalities given subsequently are  
derived without assumptions about which edges are in the graph. 3\. Valid Inequalities for the Symmetric Traveling Salesman Polytope 271  
We represent subsets of edges by their characteristic vectors *x* E *Bn* so that *E'* is  
represented by the vector *XE',* where *xr* = 1 if *e EE'* and *x;'* = 0 otherwise. Thus the set of  
feasible solutions S is the set of characteristic vectors whose edge sets induce tours. We will  
study conv\(S\) and another closely related polytope.  
Let *T* = *\{x* E *Bn \: x* \~ *x'* for some *x'* E S\}. Note that *T* is the independence system  
whose maximal members define S. Because *T\:\:J* S, any valid inequality for *T* is also valid  
for S. Since 0 E *T* and the *n* unit vectors are in *T,* dim\(conv\(T\)\) = *n.* Our reason for  
considering *T* is that conv\(T\) is full-dimensional and thus easier to analyze than conv\(S\),  
which is not. Later in this section, we will show that dim\(conv\(S\)\) = *n* - *m.*  
*T* is also of practical interest since we can construct an objective function such that *XO* is  
optimal over S if and only if *XO* is optimal over *T.*  
Proposition 3.1. *For any cERn and* ill\> max\{ I *Ce* I\: *e* E *E\}, the following statements are*  
*equivalent.*  
*1. XO is an optimal solution to the symmetric traveling salesman problem*  
min\{cx\: xES\}.  
*2. XO is an optimal solution to* max\{cx\: xES\}, *where ce* = ill - *cefor all e* E *E.*  
*3. XO is an optimal solution to* max\{cx\: *x* E *T\}.*  
*Proof* 1 \<=\> 2. *XO* is an optimal solution to min\{cx\: xES\} if and only if *XO* is an  
optimal solution to max\{-cx\: xES\}. But for any xES we have ill *LeEE xe* = *mill,* so 1  
and 2 are equivalent.  
2 \<=\> 3. Since *c e* \> 0 for all *e* E *E,* it follows that if *XO* is an optimal solution to  
max\{cx\: xED, then *XO* is a maximal element of *T.* But *XO* E S if and only if *XO* is a  
maximal element of *T. •*  
We begin our study of valid inequalities by first considering the lower- and upper-  
bound constraints  
\(3.1\)  
*xe* \~ 0 for all *e* E *E*  
\(3.2\)  
*Xe* \:\:\:s;; 1 for all *e* E *E,*  
which are obviously valid for *T* and S.  
Proposition 3.2. *For all e* E *E,* \(3.1\) *and* \(3.2\) *givefacets of conv\(T\).*  
*Proof* All of the inequalities \(3.1\) are facets since *T* is a full-dimensional indepen-  
dence system.  
For any *e, e'* E *E,* we have *x\(e,e'l* E *T.* The *n* vectors *x\(el* and *x\(e,e'l* for all *e' =1= e* are  
linearly independent and satisfy *Xe* = 1. Hence, all of the inequalities \(3.2\) are facets. •  
The relative complexity of conv\(S\) in comparison with conv\( *T\)* is already seen by  
observing that for *m* = 3, conv\(S\) contains the single point *x* = \(l 1 1\), so, for example,  
\(3.1\) is not even a supporting hyperplane for any *e* E *E.* It can be shown, however, that  
\(3.1\) yields facets of conv\(S\) for all *e* E *E* when *m* \~ 5, and all of the inequalities \(3.2\) yield  
facets for *m* \~ 4. 272 11.2. Strong Valid Inequalities and Facets for Structured Integer Programs  
We now introduce the two sets of constraints that are usually used in the integer  
programming formulation of the symmetric traveling salesman problem. For *U* \~ *V* let  
*J\(U\)* = *\{e* E *E\: e* has exactly one end in *U\}.* If *XES,* then  
\(3.3\)  
I *Xe* = 2 for all *v* E *V;*  
*eEJ\(\{v\)\)*  
and if *x* E *T,* then  
\(3.4\)  
I *Xe* \~ 2 for all *v* E *V.*  
eEJ\(\{v\)\)  
The constraints \(3.3\) and \(3.4\) are the *degree constraints* for Sand *T,* respectively.  
Proposition 3.3. *For all v* E *V, the inequality* \(3.4\) *gives afacet of conv\(T\).*  
*Proof* Suppose that *J\(\{v\}\)* = *\{e\[, e2,* ... *,em-I\}* and that *\{e\[, e2, en\}* forms a cycle.  
Consider the *n* vectors\: *x\{eJ,ej\)* for\} = 2, ... , *m* - 1; *x\{e2,e3\), x\{e1,e2,ej\}* for\} = *m,* ... , *n* - 1;  
and *x\{eJ,e3,en \).* Each of these vectors is in *T* and satisfies \(3.4\) at equality, and it is easy to  
check that they are linearly independent. •  
We now consider the dimension of conv\(S\).  
Proposition 3.4. dim\(conv\(S» = *n* - *m* = *m\(m* - 1\)/2 - *m.*  
*Proof* Let *Q* = *\{x* E *Bn\: x* satisfies \(3.3\)\}. The equation system \(3.3\) defines a con-  
straint matrix of rank *m.* Hence, by Proposition 2.4 of Chapter 1.4, we have  
dim\(conv\(Q» = *n* - *m.* Since conv\(S\) \~ conv\(Q\), it follows that dim\(conv\(S» \~ *n* - *m.*  
To prove that dim\(conv\(S» = dim\(conv\(Q» = *n* - *m,* it suffices to show that if the  
hyperplane *nx* = *no, n =1=* 0, contains the incidence vector of every tour, then *nx* = *no* is a  
linear combination of the constraints \(3.3\).  
The edge set of the graph G is *E* = *\{\(i,\}\)\: i* = 1, ... *,m* - I,\} = *i+* 1, ... *,m\}.* The  
variable *x e* for *e* = *\(i,\}\)* is written as *x ij.*  
Let\} E \{4, ... *,m\}* and *Pj3* be a path from\} to 3 through all of the points \{4, ... , *m\}.*  
Now consider the pairs of tours *T\}* = *Pj3* U \{\(I, i\), \(1, 2\), \(2, 3\)\} and *TJ* = *Pj3* U  
\{\(2, i\), \(1, 2\), \(1, 3\)\}, shown in Figure 3.1. Since *T\}* and *TJ* lie on the hyperplane *nx* = *no,* it  
follows that *nlj* + *n23* = *n2j* + *nl3* or *n2j* - *nlj* = *n23* - *nl3* for\} = 3, ... , *m.* Let AI = *n2j* - *1Clj*  
for\} = 3, ... , *m.* By an identical argument, we obtain the following for *i* = 1, ... , *m\:*  
Ai = *1Ci+IJ* - *1Cij* for\} \> *i* + 1 and *Ai* = *1Cj,i+1* - *1Cji* for\} \< *i.*  
j 1 j  
\~ 1 *Pj3*  
3 2 3 2  
*Tl T2*  
*j j*  
Figure 3.1 3\. Valid Inequalities for the Symmetric Traveling Salesman Polytope  
Thus for any coefficient *nil'* we have  
*nil* = *\(ni\)* - *ni-I,j\)* + *ni-I,j*  
*= \(nu* - ni-I\) + ... + *\(n2j* - *nlj\)* + *nlj*  
i-I  
*= L* At + *nlj*  
*t=1*  
i-I  
*= L At* + *\(nlj* - *nl,j\_l\)* + nlJ-I  
1=1  
i-I  
*t=1*  
i-I *j-I*  
*= L At* + *L At* + nl2  
t=1 *1=2*  
*= L At* + *\(nlj* - nl,j\_I\) + ... + \(n13 - n12\) + *nl2*  
273  
where *Ui* = \~\:\:l *At* for *i* \> 1 and *UI* = O. Let *a* = nl2 - *U2.* Hence  
*m-I m m-I m*  
*L neXe* = *L L ni\)xU* = *L L \(Ui* + *Uj* + *a\) xi\)*  
*eEE* i=1 *j=i+1* i=1 *j=i+1*  
= I *\[\(Ui* + \~\)\(\~ *Xji* + \~ *Xi\)\)\]*  
1=1 2 J\<l J\>l  
*= L U v* + - *L Xe ,*  
*vEV* 2 eEJ«\(v\}\)  
\[\( *a\)\(* \)\]  
which establishes that the constraint is a linear combination of degree constraints with  
*no* = *2LvE V U v* + *mao •*  
A cycle of G that does not contain all of the nodes is called a *subtour.* In a cycle, each  
vertex is of degree 2. Hence if *XE'* E *Bn* satisfies \(3.3\) for all *v* E *V,* then the subgraph  
G' = *\(V, E'\)* is either a tour or a set of disjoint subtours \(see Figure 3.2\). Such subgraphs  
are called *2-matchings.*  
We now introduce a set of constraints that are valid for *T* and are not satisfied by any  
subtours. For *W* s *V,* let *E\(W\)* = *\{e* E *E\:* both ends of *e* are in *W\}.* If *E'* \~ *E* and  
*IE'* n *E\( W\)* I \~ I *WI,* the subgraph G' = *\(V, E'\)* contains at least one subtour. This yields  
the *subtour elimination* constraints  
\(3.5\) *L x e* \~ I *WI* - 1 for all *W* C *V,* 2 \~ I *W* I \~ *m* - 1.  
*eEE\(J.V\)*  
Figure 3.2 274 11.2. Strong Valid Inequalities and Facets for Structured Integer Programs  
We have included the case I *WI* = 2 in \(3.5\), although it is not a subtour elimination  
constraint, since these are simply the upper-bound constraints *Xe* ;\:\:\:\:; 1 for *e* = *\(u,* v\) and  
*W* = *\{u, v\}.* Thus we no longer need to consider \(3.2\).  
In addition, if the degree constraints \(3.3\) are satisfied for all v E *V,* then \(3.5\) is  
superfluous for all *W* with I *W* I \~ *lm/2j* + 1. This is obvious when I *W* I \~ *m* - 2. In  
general, when each node is of degree 2 and *LeEE\(W\) Xe* \~ I *W* I, then every v E *W* is in a  
subtour and there can be no edges between *Wand V* \\ *W.* Hence *LeEE\(W\) Xe* = I *WI* and  
*LeEE\(v\\W\) Xe* = I *V* \\ *W* I. Thus it suffices to use \(3.5\) for either *Wor V* \\ *W.*  
Proposition 3.5. *The subtour elimination inequalities* \(3.5\) *give facets of* conv\(S\) *for*  
*m* \~ 4 *for all Wwith* 2 ;\:\:\:\:; I *WI* ;\:\:\:\:; *lm/2j.*  
*Proof* We show the result for *m* \~ 6 and 3 \~ I *W* I, where *W* = \{l, ... *,k\}* and  
*k* \~ *lm/2j.* The remaining cases are left as exercises. Note that the inequalities \(3.5\)  
represent proper faces since each of them is satisfied at equality by some tour and is a strict  
inequality for some other tour.  
We prove the result by showing that the conditions of Theorem 3.6 of Section 1.4.3  
hold. Here *nx* \~ *no* represents a subtour elimination inequality \(3.5\), *A =X* = *b=* represents  
the degree constraints \(3.3\), and we are concerned with solutions to the linear system  
*AxT ;* = *Ao,* where *\{Ta7\:\:t* is a set of tours that satisfy *nxT;* = *no.* Hence it suffices to  
demonstrate that all solutions *\(A, Ao\)* to *AxT*  
*,* = *Ao* for *i* = 1, ... , *n* - *m* are of the form  
*A* = om + *uA=, Ao* = *ana* + *ub=* for some *a* E *R* I and *u* E *Rm.*  
First observe that if *\(A, Ao\)* is a solution, there is a solution *\(X,* AD\) with X = *A* + *u 'A* =,  
where Ai\) = 1 for *j* = 2, ... *,k, A23* = 1, and Ai\) = 0 for *j* = *k* + 1, ... *,m.* To see this, we  
observe that the *m* x *m* node-edge incidence matrix  
*B=* 4  
*el2 e13 e23 el4 elm*  
1 1 0 I 1 1  
2 0 1 1 0 0  
# 1  
3 0 1 1 1 0 0  
# ------1------  
# 1  
0 1 I  
# 1  
# 1  
*m* I  
is nonsingular. Hence the appropriate *m* components of X can be fixed by solving the  
*m* x *m* system  
We now show in the following series of steps that\: *Au* = 1 if *i, JEW; Au* = 0 if *i* E *W,*  
*j* tf\:- *W;* and *AU* = *pfor i,j* tf\:- *W*  
Consider the two tours T, = PI U \{l, 3\} U *P2* U \{2, i\} and *T2* = PI U \{l, i\} U *P2* U \{2, 3\}  
shown in Figure 3.3 that are assumed to satisfy \(3.5\) at equality. We leave it to the reader to  
establish the existence of such tours. Since T, and *T2* contain *k* - 1 edges in *E\( W\),* we  
require *A'XTI* = *XXT2* = AD. Thus Ai3 + *A2i* = Aii + *A23,* so *A2i* = 1 if 4 ;\:\:\:\:; *i* \~ *k* and *A2i* = 0 if  
*k* \< *i* \~ *m.* 3\. Valid Inequalities for the Symmetric Traveling Salesman Polytope 1 3 3  
275  
2 2  
\~\~igure 3.3  
The remaining cases are similar. We stipulate tours *TI* and *T2* with *k* - 1 edges in *E\(W\)*  
and *n* - *k* - 1 edges in *E\( V* \\ W\) containing paths *PI* and *P2* with specified endpoints  
whose intermediate nodes are the remaining nodes of Wand *V* \\ *W,* respectively.  
Suppose 3 \~ *i* \~ *k* and consider two tours *TI* = *PI* U \{i, *j\}* U *P2* U \{l, 2\} and *T2* = *PI* U  
\{l, *j\}* U *P2* U \{2, i\} with *Ph P2* having endnodes 2, *j* and 1, *i,* respectively. This gives *.,1;2 +*  
*Au* = *A;j* + *Ali,* or *Au* = 1 for all *j* with 3 \~ *i* \< *j* \~ *k* and *Au* = 0 for all *j* with *k* \< *j* \~ *m.*  
The final case involves *p, q, r* ff\:. *W,* tours *TI* = *PI* U \{I, *p\}* U *P2* U *\{q, r\}* and *T2* = *PI* U  
\{l, *r\}* U *P2* U *\{p, q\}* and paths *Ph Pz* with endpoints 1, *q* and *p, r,* respectively. Then we have  
*Alp* + A\~r = *Air* + *A;q* so that *A;q* is a constant *P* for all *p, q* \$- *W.*  
Hence we have shown that *A' x* \~ = *AD* is of the form  
I *X* \~i + *P* I *x* P = I *WI* - 1 + *P\(* I *V* \\ WI - 1\)  
*eEEUV\) eEE\( V\\ 11'\)*  
for any *xTi* that satisfies \(3.5\) at equality.  
Now defining *u2* E *Rm* by *uT* = *P/2, i* E *W, UT* = *-P/2, i* E *V* \\ *W,* we have that  
*\(A'* + *u2A=\)xTi* = *\(A'* + *u2b=\)* is of the form  
\(1 + *P\)* I *X* \~i = I WI - 1 + *P\(* I *V* \\ WI - 1\) + *PI WI* - *PI V* \\ *WI*  
*eEE\(W\)*  
= \(1 + *jJ\)\(* I WI - 1\),  
so that \(1 + *jJ\)n* = *A'* + *u2A=* and \(1 + *p\)no* = \~ + *u2b=.* Hence Theorem 3.6 of Section 1.4.3  
applies with  
# •  
Let *p LP*  
= *\{x ERn\: x* satisfies \(3.1\), \(3.3\), and \(3.5\)\}. For *m* \~ 5, it can be shown that  
conv\(S\) = *p LP*  
*•* A subgraph on six nodes is shown in Figure 3.4. The reader can check that  
X\~l = \~ for *i* = 1, ... , 6, X\~i = 1 for *i* = 7, 8, 9, and X\~i = 0 otherwise is an extreme point of  
*p LP* since it is the unique optimal solution to min\{cx\: *x* E *pLP\},* where *C ei* = 1 for *i =*  
1, ... , 6, *cel* = 0 for *i* = 7, 8, 9, and *Cel* is suitably large otherwise. To define a polytope that  
contains conv\(S\) but not *xo,* we use a rank 1 C-G inequality. Use weights of 1 on the degree  
constraints for nodes 1, 2, and 3, weights of 1 for the constraints *xei* \~ 1, for *i* = 7, 8, 9,  
weights of \~ on *-xe,* \~ 0 for all other edges with one end in \{l, 2, 3\}, and round down the  
right-hand side. This yields 276 11.2. Strong Valid Inequalities and Facets for Structured Integer Programs  
e7 5  
e2 e5  
3 es 6  
el  
e3 e6  
2 e9 4  
Figure 3.4  
In general, let *H* be any subset of nodes with 3 \:\:\:\:; I *HI\:\:\:\:;* I *V* I - 1 and let *E* C *E* be an  
odd set of disjoint edges, each of which has one end in *H.* Then using weights of! on the  
degree constraints for all *v* E *H,* weights of! on *-xe\:\:\:\:;* 0 for all *e* E *6\(H\)* \\ *E,* weights of!  
on *Xe* \:\:\:\:; 1 for all *e* E *E,* and rounding yields that  
\(3.6\)  
is a valid inequality for *T.* Note that if I *E* I = 1, \(3.6\) is dominated by subtour elimination  
constraints, so we only consider \(3.6\) for I *E* I \~ 3.  
The inequalities \(3.6\) are called *2-matching inequalities* since they are needed to define  
the convex hull of 2-matchings. Now we have that conv\(S\) \~ pLPI  
= *\{x ERn\: x* satisfies  
\(3.6\)\} n *pLP.* In fact it can be shown that pLPI = conv\(S\) on all graphs with six or fewer  
nodes. But, for *m* \~ 7, more general inequalities are needed.  
A subgraph for generating a 2-matching inequality is shown in Figure 3.5. It resembles  
a comb with handle *H* and teeth *U'j* = *\{Ui, Vi\}* for *i* = 1, ... , *k,* where *k* \~ 3 is odd. We can  
restate \(3.6\) as  
• *H* •  
Figure 3.5 3\. Valid Inequalities for the Symmetric Traveling Salesman Polytope 277  
A general comb is shown in Figure 3.6. Here the teeth *JIJIj* for *i* = 1, ... , *k,* can contain  
more than two nodes and can have more than one node in common with the handle.  
Specifically a *comb* is a subgraph generated by a node set *\{H,* Wt, ... , *Wd* with the  
following properties\:  
1. I *H* n *JIJIj* I \~ 1 for *i* = 1, . . . , *k.*  
2. I *JIJIj* \\ *H* I \~ 1 for *i* = 1, ... , *k.*  
3. 2 \~ I *JIJIj* I \~ *m* - 2 for *i* = 1, ... , *k.*  
*4. JIJIj* n *Uj.* = 0 for *i* "\* *j.*  
*5. k* is odd and at least 3.  
**Proposition** 3.6. *For any subgraph ofG that is a comb, the comb inequality*  
\(3.7\)  
*k k k* + 1  
I *Xe* + I I *Xe* \~ *IH* I + I \(I *JIJIj* I - 1\) - -2-  
*cEE\(H\)* i=l *eEE\(W;\)* i=l  
*is valid for T.*  
*Proof* First weight the degree constraints for v E *H* by \~ and sum them. This yields  
\(3.8\) 1  
I *x e +- 2* I Xe\~ *IHI.*  
*eEE\(H\) eEO\(H\)*  
Now add -\~Xe \~ 0 for all *e* E *,J\(H\)* \\ U7=1 *E\( JIJIj\)* to \(3.8\) to obtain  
\(3.9\)  
1 *k*  
I *xe+-I* I Xe\~ IHI.  
*eEE\(H\)* 2 i=1 *eEO\(H\)nE\( W;\)*  
Consider the subtourelimination constraints for *UIj, H* n *UIj,* and *UIj* \\ *H,* respectively\:  
\(3.10\)  
I *Xe* \~ I *JIJIj* I - 1 for *i* = 1, ... , *k*  
\(3.11\)  
\(3.12\)  
*eEE\(W;\)*  
I *Xe* \~ IH n *JIJIj* I - 1 for *i* = 1, ... , *k*  
*eEE\(Hnw;\)*  
I *Xe* \~ I *UIj* \\ *HI* - 1 for *i* = 1, ... , *k.*  
*eEE\(W;\\H\)*  
Figure 3.6 278 11.2. Strong Valid Inequalities and Facets for Structured Integer Programs  
The edge *e* appears in \(a\) \(3.10\) and \(3.11\) if *e* E *E\(H* n *Wi\),* \(b\) \(3.10\) and \(3.12\) if  
*e* E *E\(Wi* \\ *H\),* and \(c\) \(3.10\) and the left-hand side of \(3.9\) with a coefficient of \~ if  
*e* E *J\(H\)* n *E\(Wi\).* Also note that \(3.11\) \[respectively, \(3.12\)\] is trivial when *IH* n *Wi* I = 1  
\[respectively, I *Wi* \\ *H* I = 1\]. Hence by multiplying each of the inequalities \(3.10\)-\(3.12\)  
by! and adding them to \(3.9\), the result is  
*k*  
I *Xe* + I I *Xe*  
*eEE\(H\)* i=l *eEE\(W';\)*  
1 *k*  
\~ *IH* I + 2 \~ \[\( I *Wi* I - 1\) + *\(IH* n *Wi* I - 1\) + \(I *Wi* \\ *HI* - 1\)\]  
1 *k*  
*= IH* I + 2 \~ \[\( I *Wi* I - 1\) + *\(IH* n *Wi* I - 1\) + \(I *Wi* I - 1 - *IH* n *Wi* I\)\]  
*k* 1  
*= IH* I + I \(Ilf'\: I - 1\) - *-k.*  
i=l 2  
Then, since *k* is odd, by rounding we obtain  
*k k k* + 1  
I *xe+I* I xe\~IHI+I\(IWiI-l\)--2-'  
*eEE\(H\)* i=1 *eEE\(W;\)* i=1  
Consider the comb C shown in Figure 3.7. The comb inequality \(3.7\) is  
12 3 + 1  
\~ *xe;* + *x e2* \~ I *HI* + I *WI* I - 1 + I U121 - 1 + I U'31 - 1 - -2-  
=4+2+2+ 1-2=7.  
# •  
W2  
Figure 3.7 3\. Valid Inequalities for the Symmetric Traveling Salesman Polytope 279  
The comb inequalities have coefficients in \{a, 1, 2\} and the 2's appear on *Xe* if  
*e* E *E\( fVi* n *H\)* for some *i.* These inequalities are rank 1 C-G inequalities with respect to  
the inequalities \(3.1\), \(3.3\), and \(3.5\).  
The comb inequalities can be generalized to obtain higher-rank C-G inequalities by  
considering generalized combs that have teeth which themselves are combs. Consider the  
graph of Figure 3.8. The handle *HI* has three teeth, namely, Wt, *W2,* and C, where Cis  
comb. We require that C n *HI* contain no vertices of *H* 2 and that each original tooth *fVi*  
has at least one node that is not contained in any handle. To derive a valid inequality for  
the graph of Figure 3.8, we proceed as we did in deriving the comb inequalities. Hence the  
following inequalities are weighted by \~ and summed, and then the resulting right-hand  
side is rounded down\:  
1. degree constraints for *HI;*  
2\. nonnegativity constraints for *e* E *J\(HI \)* \\ *\(E\(WI\)* U E\(\~\) U *E\(Ws»;*  
3. subtour elimination constraints for *Tfj, Tfj* n *HI* and *Tfj* \\ *HI* for *i* = 1, 2 and for  
Hin *Ws;*  
4. comb inequalities \(3.7\) for C and C \\ *HI.*  
The result for the graph of Figure 3.8 is  
2 S 2 4 *k+l*  
\(3.13\) I I *Xe* + I I *Xe* \~ I *IHiI* + I \(I W;I - 1\) + \(I *Wsl* - 2\) - -2-'  
i=1 *eEE\(H;\)* i=1 *eEE\(W;\)* i=1 i=I  
where *k* = 5. The left-hand side 0\[\(3.13\) is clear. The contributions to the right-hand side  
are, respectively, from  
1. IHtI,  
3\. !\[\(21 *WI* I - 3\) + \(21 \~ I -3\) + I *HI* n *Ws* I -1\] = I *WI* I - 1 + I \~ I - 1 - *i +*  
\~IH,nWsI,and  
4. H\(21ff31-2\)+\(21W41-2\)+\(IWsI-l\)+\(IWs\\H1 1-1\)\]-2+ *IH21.*  
Hence rounding yields  
*IHd* + IH21 + \~ \(lit; I - I\) + l-\~J  
4 *k* + 1  
*= IHII* + IH21 + \~ \(I *Jfj* 1- 1\) + \(I *Ws* 1 - 2\) - -2-'  
Figure 3.8 280 11.2. Strong Valid Inequalities and Facets for Structured Integer Programs  
Figure 3.9  
Inductively, we can build still more complicated generalized combs \(see Figure 3.9\).  
Besides properties 1-5 given in the definition of a comb, it is required for a graph with *k*  
teeth and *r* handles that\:  
*6. Hi* n *Hj* = 0 for *i =1= j;*  
*7. Uj* \\ Uf=1 *Hi =1=* 0 for *j* = 1, ... , *k;* and  
8. if *Hi* n *Uj =1=* 0 and *Hi* n *Uj* is deleted from the generalized comb graph, then the  
resulting graph is disconnected.  
When these conditions are satisfied it can be shown inductively that *the generalized*  
*comb* inequality  
\(3.14\)  
where *Wi* is the number of handles met by HIi, is valid for *T.* Moreover, for a complete  
graph we have the following theorem.  
Theorem 3.7. *The generalized comb inequalities* \(3.14\) *givefacets ofconv\(S\).*  
The proof of this theorem is much too long to give here.  
Theorem 3.7 generates a very large class of facets, but there are yet other classes. For  
example, the famous Petersen graph G = *\(V, E'\)* on 10 nodes \(see Figure 3.10\) does not  
contain a tour, which means \~eEE' *X e* \:\:\:\:\:; 9 is valid for the complete graph on 10 nodes. In  
fact, it can be proved to represent a facet of conv\(S\). But it does not belong to any of the  
families of valid inequalities introduced in this section.  
Figure 3.10 4\. Valid Inequalities for Variable Upper-Bound Flow Models 281  
The Petersen graph belongs to a certain infinite family of graphs G == *\(V, E'\)* that do not  
contain any tours. From some graphs in this class, we obtain facets of conv\(S\) represented  
by valid inequalities of the form *LeEE' Xe* \~ I *V* I - 1. Yet these graphs are not likely to have  
a good characterization. So one cannot expect to have a good characterization of the  
corresponding facets.  
Fortunately, such facets have not been necessary in the solution of many symmetric  
traveling salesman problems in the literature by algorithms that use cutting planes and  
branch-and-bound.  
4\. VALID INEQUALITIES FOR VARIABLE UPPER-BOUND FLOW MODELS  
We consider a single-node flow model with an exogenous supply of band *n* outflow arcs  
\(see Figure 4.1\). For each\} EN == \{l, ... , *n\}* the flow *Yj* E Rl on the\}th arc is bounded by  
the capacity *aj* ifarcj is open *\(Xj* = 1\) and ° otherwise. We call this relationship a *variable*  
*upper bound on the flow Yj.* Since the total outflow cannot exceed *b,* this model can be  
represented by the mixed-integer region  
\(4.1\)  
Our initial objective is to find strong valid inequalities for *T.* Consider the polytope  
used in the formulation of *T,* that is, *T* = *P* n *\{x* E *zn, Y ERn\}.* The fractional extreme  
points of *P* are characterized in the following proposition.  
Proposition 4.1. *All fractional extreme points of P are of the form*  
*Yj* = 0, *Xj* E \{O,l\} *for\}* \$. C.  
*where* C \~ *N is a dependent set of* S = *\{x* E *Bn\: LjEN ajXj* \~ *b\}, k* E C *and* C \\ *\{k\} is*  
*independent.*  
\~---b  
Figure 4.1 282 11.2\. Strong Valid Inequalities and Facets for Structured Integer Programs  
There are simple valid inequalities for *T* that cut off these fractional extreme points of  
*P.* Let *A* = 1\:jEC *aj* - *b* be the excess capacity of the arcs in a dependent set C. For *k* E C,  
the capacity of the set C \\ *\{k\}* is  
and for any C' C C, the capacity of the set C \\ C' is  
Thus we have proved  
Proposition 4.2. *lfC* 5; *N is a dependent set ofS and A* = *1\:jEC aj* - *b, then*  
\(4.2\) I *Yj* \~ *b* - I *\(aj* - *At* \(1 - *Xj\)*  
jEC *jEC*  
*is a valid inequality for T given by* \(4.1\).  
Since the point *\(x, y\)* given in Proposition 4.1 is such that *1\:jEC Yj* = *b, ak* - *A* \> 0, and  
*Xk* \< 1, it follows that *\(x, y\)* does not satisfy \(4.2\).  
*Example* 4.1. Consider the set *T* given by\:  
*T* = *\{x* E *B4 , Y* E *R!\:* I *Yj* \~ 9, *YI* \~ *5Xb Y2* \~ *5X2, Y3* \~ *X3, Y4* \~ *3X4\}.*  
*jEN*  
C = \{l, 2, 3, 4\}, *A* = 5\: \(4.2\) yields *YI* + *Yz* + *Y3* + *Y4* \~ 9  
\(the original inequality\)  
C = \{l, 2, 4\}, *A* = 4\: \(4.2\) yields *YI* + *Yz* + *Y4* \~ 9 - \(1 - *XI\)* - \(1 - *Xz\)*  
or *Y* I + *Yz* + *Y* 4 - *X* I - *X* 2 \:\:;\:;; 7.  
C = \{l, 2, 3\},  
*A* = 2\: \(4.2\) yields  
*YI* + *Y2* + *Y3* \~ 9 - 3\(1 - *XI\)* - 3\(1 - *X2\)*  
or *YI* + *Yz* + *Y3* - *3xI* - *3xz* \~ 3  
C = \{l, 2\},  
*A* = 1\: \(4.2\) yields *YI* + *Yz* \~ 9 - 4\(1 - *Xl\)* - 4\(1 - *X2\)*  
or YI + *Y2* - *4xI* - *4X2* \~ 1.  
*b*  
Figure 4.2 4\. Valid Inequalities for Variable Upper-Bound Flow Models 283  
Each of the inequalities of Example 4.1 can be shown to give a facet of conv\(T\).  
Moreover ifmaxjEc *aj* \> A, then the inequality \(4.2\) gives a facet of conv\(T\). We postpone  
the proof of this result to consider a more general model that also includes inflow arcs.  
Let  
*\(4.3\)* T=\{XEBn,YER\~\: *L Yj- L* yj\~b,Yj\~ajXjfOrjEN\},  
*jEN+ jEN-*  
where *N+* U *N-* = *N* \(see Figure 4.2\). Here *aj* E Rl for *j EN* and *b* E *R* I, that is, *b* can be  
negative. We say that C f; *N+* is a *dependent set* if *LjEC aj* \> *b.* Note, for example, that if  
*b* \< 0, every subset of *N+* is dependent.  
We can now generalize Proposition 4.2.  
Proposition 4.3. *If* C f; *N+ is a dependent set,* A = LjEC *aj* - *b, and L* f; *N-, then*  
\(4.4\) *L \[Yj* + *\(a* j - *At* \(l - *Xj\)\]* \~ *b* + *L AXj* + *L Yj*  
JEC *jEL jEN-\\L*  
*is a valid inequality for T given by* \(4.3\).  
*N I*  
*Proof* Let C+ = *\{j* E C\: *aj* \> *A\}.* Suppose a feasible point *\(x, y\)* E *T* is given and  
= \{j *EN\: Xj* = 1\}. Note that ifj \$\:. *N I*  
*,* then *Yj* = *Xj* = O.  
*Case* 1. C+ \\ *N I* = 0 and *L* n *N I* = 0.  
*= L Yj*  
jECnNI  
\~ *L Yj*  
*jEN+nN'*  
\~ *b* + *L Yj*  
*jEN-nN'*  
\~ *b* + *L Yj*  
*jEN-\\L*  
*= b* + *L Yj* + *L Axj*  
*jEN-\\L jEL*  
*Case* 2. *\(C+* \\ *N I \)* U *\(L* n *N I \)* \* 0.  
\(since C+ \\ N l  
= 0\)  
\(since C \~ *N+\)*  
\[by \(4.3\) and *Yj* = 0 ifj \$\:. *N I \].*  
\(since *L* n *N I*  
= 0\)  
\(since *L* n *N 1* = 0 and *Xj* = 0 for *j* \$\:. *N I*  
*\).*  
\~ *L aj* + *L aj* - A I C+ \\ *N I* I \(since *Yj* \~ *aj* for allj\)  
*jECnNI jEC+\\Nl*  
\~ *L aj-A+A* IL nNII  
JEC  
*= b* + *L Axj*  
*jEL* \(since *Xj* = 1 for *j* E *N I* and A = *L aj* - *b\)*  
JEC  
\~ *b* + *L Axj* + *L Yj* \(since *Yj* \~ 0 for *j EN\).*  
*jEL jEN-\\L* • 284 11.2. Strong Valid Inequalities and Facets for Structured Integer Programs  
*Example* 4.2\. The feasible set *T* is given by  
*Yl* + *Y2* + *Y3* + *Y4* \~ 9 + *Ys* + *Y6*  
Yl \~ *5xI, Y2* \~ *5X2, Y3* \~ *X3, Y4* \~ *3X4, Ys* \~ *3xs, Y6* \~ *X6,*  
*Y* ER\~,x *EB6.*  
Taking C = \(1, 2, 3\) and *L* = \{5\), we have *A* = 2 and \(4.4\) yields  
\[Yl + 3\(1 - *Xl\)\]* + *\[Y2* + 3\(1 - *X2\)\]* + *Y3* \~ 9 + *2xs* + *Y6,* or  
\(4.5\)  
Yl + *Y2* + *Y3* - *Y6* - *3X l* - *3X2* - *2xs* \~ 3.  
We can establish the dimension of the face formed by \(4.5\) by specifying a set of linearly  
independent points that satisfy it at equality\:  
C *N+\\C* L *N-\\L*  
*r* **\*** ,  
Yl *Xl Y2 X2 Y3 X3 Y4 X4 Ys XS Y6 X6*  
3 1 5 1 1 1 0 0 0 0 0 0 Zl  
5 1 3 1 1 1 3+£ 1 5 1 1-£ 1 0 0 5 1 1 1 5 1 0 0 1 1 4 1 5 1 0 0 0 0 5 1 1 1 0 1 0 0 5 1 1 1 £ 1 Z2  
Z3  
Zl  
Z2  
Z3  
24  
-4 Z  
5 5 0 0 2 1 *Ws*  
5 5 0 0 2+£ 1 *W s*  
3 5 0 0 0 0 0 *w 6*  
3+£ 5 0 0 0 0 £ *w6*  
where *e* is a small positive number.  
An ad hoc argument shows that these 12 points are linearly independent so that \(4.5\)  
gives a facet of conv\(T\).  
More generally, we have the following theorem.  
Theorem 4.4. !fmax.EC *a.* \> A, *and a.* \> *Afor j* E *L, then* \(4.4\) *gives afacet* ofconv\(T\),  
*J J J*  
*where T is given by* \(4.3\).  
*Proof* We prove the theorem by giving *2n* points of *T* that define the coefficients in  
\(4.4\) up to a scalar multiple; that is, the unique solution *\(n,* f.1..\) to *nxi* + *f.1..yi*  
= *no* for *i =*  
1, ... *,2n* is a scalar multiple of the coefficients in \(4.4\).  
Let Zi = *\(yi, Xi\)* E *T* for *i* = 1, ... *,2n.* For clarity, we write  
where *Cyi*  
*, IXi\)* are the *\(y, x\)* values for the arcs in C, *eyi*  
*, 2X i\)* are those for the arcs in  
*N+* \\ C, *Cyi, 3 X i\)* are those for the arcs in *L,* and *\(4yi, 4 X i\)* are those for the arcs in *N-* \\ *L.*  
Suppose that *al* = maXjEC *aj* \> *A.* Let *ei* be the ith unit vector, l' = \(1, 1, ... , 1\) and let *e*  
be a small positive number. By ly = *a,* we mean *lYj* = *aj* for all *j* E C. The points given 4\. Valid Inequalities for Variable Upper-Bound Flow Models 285  
below are the general versions of the points given in Example 4.2. We leave it to the reader  
to check that they are in *T* and satisfy \(4.4\) at equality. We first describe a set of 21 *N+* 1  
points\:  
i. *Zk* = *\(a* - *Aeb* 1, 0, 0, 0, 0, 0, 0\)  
*= \(a* - *\(A* - *e\)el -eek,* 1,0,0,0,0,0,0\)  
for *k* E C with *ak* \~ *A*  
for *k* E C with *ak* \< *A.*  
*11. Zk* = *\(a* - *akek,* 1 - eb 0, 0, 0, 0, 0, 0\) for *k* E C with *ak* \~ *A*  
*= \(a* - *\(A* - *ak\)el* - *akek,* 1 - *ek,* 0, 0, 0, 0, 00\) for *k* E C with *ak* \< *A.*  
iii. *zj* = *\(a* - *ale!,* 1 - *eI,* 0, *ej,* 0, 0, 0, 0\)  
for\} E *N+* \\ C.  
iv. ? = *\(a* - *ale!,* 1 - *e!, eej, ej,* 0, 0, 0, 0\)  
for\} E *N+* \\ C.  
Suppose these points satisfy *LjEN \(njxj* + */-ljYj\)* = *no.* By comparing Zl, *zj,* and *zj,* we see  
that *'!Cj* = */-lj* = ° for\} E *N+* \\ C. For each of the points *Zk,* we have *LjEC /-ljyj* = *'!Co* - *LjEC '!Cj.*  
It can then be seen that */-lj* = */-lo* for all\} E C. Moreover, since *LjEC yj* = *b,* we also have  
*!-lob* + LjEC *'!Cj* = *'!Co.*  
From the points *Zk,* we see that when *k* E C and *ak* \~ *A* we obtain  
*/-lO\(* I *aj\)* + I *'!Cj* = *'!Co·*  
*jEC\\\{k\} jEC\\\{k\}*  
Thus */-loeb* - *LjEC\\\{kl aj\)* + *'!Ck* = 0. Since *LjEC aj* = *b* + *A,* we obtain */-lo\(A* - *ak\)* = *'!Ck* when  
*ak* \~ *A.* On the other hand, when *ak* \< *A,* we have  
I *yj* = I *a j* + *a* I + *a k* - *A* = *b.*  
JEC jEC\\\{i,k\}  
Hence  
*!-lob* + I *'!Cj* = *'!Co*  
*jEC\\\{k\}*  
or *'!Ck* = 0.  
In summary, our inequality must be of the form  
Now we describe another set *of21L* 1 points\:  
v. for *k* E *L*  
*w k* = *\(a,* 1, 0, 0, *\(A* + *e\)eb eb* 0, 0\) From *wk* and *w k*  
*,* we obtain */-lk* = ° for *k* ELand that  
for *k* E *L.*  
or  
*'!Ck* = *!-lo\(b* - I *aj\)* = *-/-loA* for *k* E *L.*  
JEC 286 The final 21 *N-* \\ *L* 1 points are\:  
VI.  
11.2. Strong Valid Inequalities and Facets for Structured Integer Programs  
for *k* E *N-* \\ *L*  
*-k -*  
*W* = *\(a* - *\(A* - *a\)e\),* 1,0,0,0,0, *aeb ed*  
for *k* E *N-* \\ *L.*  
Comparing Z 1 and *w k*  
*,* we see that *Jrk* = ° for *k* E *N-* \\ *L.* Comparing *w k* and *w k*  
see that *a\(J.lo* + *J.ld* = ° or *J.lk* = -/10 for *k* E *N-* \\ *L.*  
*,* we  
Using the results ofv and vi, the inequality must be of the form  
\(4.6\)  
Now with *J.lo* = 1, we obtain \(4.4\).  
It remains to show that not all points *\(x, y\)* E *T* satisfy \(4.6\) at equality. Since *b\>* 0 is  
implied, the point given by *y.* = 0 for\) EN, *x.* = 1 for\) E *N+,* andx. = 0 for\) E *N-* is in *T,*  
and, when substituted in \(4.6\), one obtains z\~o on the left-hand side and *b* \> 0 on the right.  
Additional results along these lines are known. For example, if we require  
*LjEN+ Yj* - *LjEN- Yj* = *b* in \(4.3\), then \(4.4\) is, of course, still a valid inequality for *T.*  
Moreover, under some mild additional assumptions, \(4.4\) still gives a facet of conv\( *T\).*  
Also, some other valid inequalities for *T* given by \(4.3\) are known \(see Section 11.6.4\).  
The flow model with constraint set *T* given by \(4.3\) is much more general than it  
appears. With some additional simple constraints, it can be used to represent any linear  
inequality involving both continuous and 0-1 variables in which some of the continuous  
variables have simple upper bounds while the others have variable upper bounds.  
Suppose *T'* is the set of feasible solutions to  
*L \(ajzj* + *ajxj\)* + *L a\)z\)* + *L ajx\)* \~ *b*  
*jEll* \)Eh *jEh*  
° \~ *z\)* \~ *k\)x\)* for\) E 1\)  
° \~ *z\)* \~ *k\)* for\) E 12  
*x\)* E \{a, 1\} for\) E *J\]* U *J3 ,*  
In addition we assume for simplicity that \(X; \> 0 for\) E 13 and *alai* \~ 0 for\) E 1 1•  
Now let *1T* = \(j E 1 1\: *a\)* \> a\}, 11 = 1\) \\ 1t, 1\~ = \(j E 12 \: *a\)* \> o\} and 12 = 12 \\ *1i.* Define  
*Xj* E \{a, 1\} for\) E 12,  
\(4.7\) *Y\) =*  
*ajZj* + *a;x\)*  
*-\(a\)z\)* + *n;x\)*  
*a\)z\)*  
*-a\)z\)*  
*a;x\)*  
for\) E 11  
for\) E 11  
for\) E 12  
for\) E 12  
for\) E 13,  
and 4\. Valid Inequalities for Variable Upper-Bound Flow l\\tlodels  
*ajkj* + *a;*  
*-\(ajkj* + *a;\)*  
*ajkj*  
*-ajkj*  
# a;  
287  
*aj* = for\} EJt  
for\} E *J1*  
for\} E *J2*  
for\} E *J2*  
*for\}EJ3•*  
Now *T'* is given by the flow model constraints  
I *Yj* \~ *b* + I *Yj*  
*jEN+ jEN-*  
where *N+* = Jt u *J2* U *J 3* and *N-* = *J1* U *J2,* together with the additional constraints *Xj* = 1  
for\} E *J2, Yj* = *ajxj* for\} E *J3,* and \(4.7\). Thus \(4.4\) is a valid inequality for *T'.*  
We now give some examples of the use of\(4.4\) in different models.  
*Example* 4.3. *\(The 0-1 Knapsack Problem\:* S = *\{x* E *Bn\: LjEN ajXj* \~ *b\}* with *aj* E R\~ for  
j EN and *b* E *Rl\).* Here *N+* = Nand *Yj* = *ajxj* for\} EN. Let C be a minimal dependent  
set so that *A* = *LjEC aj* - *b* \> 0 and *aj* \> A for\} E C. Then \(4.4\) yields  
I *\(ajxj* + *\(aj* - A\)\(l - *Xj\)\)* \~ *b*  
*JEC*  
or  
A I *Xj* \~ *b* - I *a j* + A I C I = A\( I C I - 1\),  
*JEC JEC*  
which is precisely the constraint \(2.2\).  
*Example* 4.4 *\(Facility Location\).* Suppose  
where 0 \< *aj* \< *ao* for all\} E *N+.* Here *ao* is the capacity of a facility and *Xo* = 1 if and only if  
the facility is open. The flow from the facility to client\} is *Yh* and *aj* is the maximum  
requirement of client\}. Here *b* = 0, *N-*= \{a\}, and *Yo* = *aoXo.* Take C = \{j\} so that A = *aj* and  
take *L* = *N-.* Then \(4.4\) yields *Yj* \~ *ajXO* for\} E *N+,* that is, *Yj* = 0 if *Xo* = 0 and *Yj* \~ *aj* if  
*Xo* = 1.  
*Example* 4.5 *\(Machine Scheduling\).* Suppose that two jobs must be executed on the  
same machine. The ithjob for *i* = 1,2 has an earliest start time of *Ii* and a processing time  
of *Pi\>* O. The machine can only process one job at a time, and our objective is to model  
this restriction. 288 11.2. Strong Valid Inequalities and Facets for Structured Integer Programs  
Let J = 1 if job 1 is processed before job 2 and let J = 0 otherwise; for *i* = 1, 2, let *t* i be the  
time at which the machine begins to process job *i.* Then we have the model  
*-t* I + *t* 2 \~ PI - *w\(* 1 - J\)  
*ti* \~ *Ii* for *i* = 1,2 and J E *BI,*  
where *W* is a suitably large number so that the first constraint is valid when J = 1 and the  
second is valid when J = O.  
Suppose *12* + *P2* \> *11'* By substituting *Yi* = *t* i *-Ii* and *X3* = J, the first constraint becomes  
*whereYb Y2* \~ 0 andx3 E *BI. HereN+* = \{2\}, *N-* = \{l, 3\}, *Y3* = *WX3,* and *b* = II *-/2* - *P2* \< O.  
Take C = 0 so that *..1= -b* \> 0 and take *L* = \{3\}. Then \(4.4\) yields 0 \~ *-A* + *AX3* + *YI.*  
Translating back into the original variables, we obtain  
that is, tJ \~ II and if J = 0, then *tl* \~ *12* + *P2.*  
While the general inequalities \(4.4\) can be quite useful, still more valid inequalities may  
be obtained by using the structure of a problem. We illustrate this by considering the  
constraint set of an *uncapacitated lot-size problem* that involves production planning over  
a horizon of *T* periods \[see \(5.4\) of Section 1.1.5\].  
In period *t, t* = 1, ... , *T,* there is a given demand of *dt* E Rl that must be satisfied by  
production in period *t* and by inventory carried over from previous periods. The  
production in period *t iSYt,* 0 \~ *Yt* \~ *WXt,* where *W* is a large positive number, *andxt* E BI  
equals 1 if the plant operates during period *t* and equals 0 otherwise. Let *St* be the inventory  
at the end of period *t.* Thus we obtain the constraints  
*YI=dl+S I*  
*St-I* + *Yt* = *dt* + *St* for *t* = 2, ... , *T*  
\(4.8\)  
*Yt* \~ *WXt* for *t* = 1, ... , *T*  
*ST* = 0, S E *RI, Y* E *RI, x* E *BT.*  
The constraints for a single period, namely,  
are an equality-constrained version of the flow model \(4.3\). Thus from \(4.4\), we obtain the  
valid inequalities  
\(4.9\)  
which simply state the obvious facts that *St* \~ 0 when *X t* = 0 and *St* \~ *Yt* - *dt* when *X t* = 1.  
We now develop a more general set of inequalities for the system given by \(4.8\). 4\. Valid Inequalities for Variable Upper-Bound Flow Models Proposition 4.5. *For any* 1 \~ *I* \~ *T, L* = \{I, ... , !\}, *and* C £ *L,*  
289  
\(4.10\)  
*is a valid inequality for \(4.8\).*  
*Proof* Take any feasible solution *\(y,* s, *x\)* to \(4.8\). If *Xi* = 0 for all *i* E C, thenYi = 0 for  
all *i* E C and \(4.10\) reduces to s *I* \~ O.  
Now suppose that *Xi* = 1 for some *i* E C and let *k* = min\{i E C\: *Xi* = n. Hence *Yi* = 0  
for all *i* E C with *i* \< *k* and thus  
*I I I*  
*L Yi* \~ *L Yt* = *L dt* + *SI* - *Sk-I* \~ *L dt* + *SI*  
iEC *t=k t=k t=k*  
\(since *Xk* = 1\). •  
Note that when C = *\{l\},* \(4.10\) yields \(4.9\). There is, in fact, a much stronger result here  
whose proofwill not be given.  
Theorem 4.6. *The convex hull of solutions to* \(4.8\) *is given by the constraints* S E *RI,*  
*Y* E *RI, x* E *RI, Xt* \~ 1 *for all t, Sr=* 0, YI = *dl* + SI\> *and St-I* + *Yt* = *dt* + *stfor t* = 2, ... , *T*  
*and by the inequalities* \(4. *10\) for alii and* C *=1= 0.*  
*Example* 4.6\. is given by the inequalities  
Suppose *Cd!, d2 , d3 , d4\)* = \(4 2 7 3\). The convex hull of solutions to \(4.8\)  
*1=* 1, C = \{l\} YI \~ *4xI* + SI  
*1=* 2, C = \{2\} *Y2* \~ *2X2* + S2  
*1=* 3, C = \{2\} *Y2* \~ *9X2* + S3  
*1=* 3, C = \{3\} *Y3* \~ *7X3* + S3  
*I* = 3, C = \{2, 3\} *Y2 + Y3* \~ *9X2 + 7X3* + S3  
*1=* 4, C = \{2\} *Y2* \~ *l2x2* + S4  
*1=* 4, C = \{3\} *Y3* \~ *10x3* + S4  
*1=4,C=\{4\} Y4* \~ *3X4* + S4  
*1=4,* C = \{2, 3\} *Y2* + *Y3* \~ *12x2 + 10x3* + S4  
*I* = 4, C = \{2, 4\} *Y2 + Y4* \~ *l2x2 + 3X4* + S4  
*I* = 4, C = \{3, 4\} *Y3* + *Y4* \~ *10x3 + 3X4* + S4  
*1=4,* C = \(2, 3, 4\) *Y2* + *Y3* + *Y4* \~ *l2x2 + 10x3 + 3X4* + S4  
YI=dl+sl  
*St-1* + *Yt* = *dt* + *St* for *t* = 2,3,4  
S4 = 0, S *ER!, Y ER!, x ER!, Xt* \~ 1 for *t* = 1, ... , 4. 290 11.2. Strong Valid Inequalities and Facets for Structured Integer Programs  
The reader is asked to check that the *I* = 1, C = \{l\} inequality is equivalent to *x,* = 1,  
since *y,* = 4 + *s* 1 and *x* 1 \~ 1. It then follows that all the other inequalities that could be  
generated with 1 E C are superfluous. All the inequalities given above with 1 \$. C give  
facets except the last four. The last four inequalities are superfluous because *S4* = O.  
Although most production planning problems are much more complicated than our  
simple model in that they involve plant capacities, multiple items, and multistage  
production, they frequently have the system \(4.8\) as part of their formulation. Hence the  
theoretical results for the system \(4.8\) can be used in improving the formulation of more  
realistic production planning problems \(see Section 11.6.4\).  
5\. NOTES  
Section 11.2.1  
The idea of using structure to obtain strong valid inequalities for ffg\}-hard integer  
programs has its roots in the work of Dantzig, Fulkerson and Johnson \(1954, 1959\) on the  
traveling salesman problem and in the work of Gomory \(1965, 1967, 1969, 1970\) on the  
group problem.  
Facet-defining inequalities for the node-packing problem were given by Padberg \(1973\),  
Nemhauser and Trotter \(1974\), Chvatal \(1975\), Trotter \(1975\), and Giles and Trotter  
\(1981\).  
Gomory \(1969\) introduced the idea of lifting in the context of the group problem. Its  
computational possibilities were emphasized by Padberg \(1973\), and the approach was  
generalized by Wolsey \(1976\), Zemel \(1978\), and Balas and Zemel \(1984\).  
The significance of having a partial description of the convex hull of integer solutions is  
strongly emphasized in the survey by Padberg \(1979\).  
Section 11.2.2  
Facet-defining inequalities for the knapsack polytope were studied simultaneously by  
Balas \(1975a\), Hammer, Johnson and Peled \(1975\), and Wolsey \(1975\). Proposition 2.6 is  
due to Balas \(1975a\). Also see Balas and Zemel \(1978\), Padberg \(1980b\), and Zemel \(1986\).  
The problem of extending these results to two or more general constraints remains an  
important open question.  
Section 11.2.3  
The study of the convex hull of tours for the symmetric traveling salesman problem is  
largely due to Grotschel and Padberg \(1979a,b, 1985\). The proof of Proposition 3.4 is due  
to Maurras \(1975\). A different proof is given by Grotschel and Padberg \(1979a\).  
Subtour elimination constraints were introduced by Dantzig, Fulkerson and Johnson  
\(1954, 1959\) and were shown to define facets of the convex hull of tours by Grotschel and  
Padberg \(1979b\).  
Comb inequalities in which each tooth contains only one node of the handle are due to  
Chvatal \(1973b\). Chvatal's combs were generalized and were shown to define facets by  
Grotschel and Padberg \(1979b\). The inequalities \(3.14\) are due to Grotschel and Pulley-  
blank \(1986\). They called them *clique-tree inequalities* and proved that they define facets  
of the convex hull of tours.  
The facet-defining inequality obtained from the Petersen graph is due to Chvatal  
\(l973b\). The Petersen graph is the smallest of a large class of graphs known as *hypo-*  
*hamiltonian graphs* that give facets for which no good characterization is known \(see 6\. Exercises 291  
Grotschel, 1980b\). Another such class of graphs has been studied by Papadimitriou and  
Yannakakis \(1984\).  
Other polyhedral results for the symmetric traveling salesman problem have been  
obtained by Cornuejols and Pulleyblank \(1982\), Cornuejols, Naddef and Pulleyblank  
\(1983\), and Cornuejols, Fonlupt and Naddef \(1985\).  
Facets for the convex hull of tours on a directed graph have been studied by Grotschel  
and Padberg \(1975\) and Grotschel and Wakabayashi \(1981a,b\). Grotschel and Padberg  
\(1985\) surveyed these results.  
Section 11.2.4  
The basic results for the variable upper-bound flow model are from Padberg, Van Roy and  
Wolsey \(1985\). Martin and Schrage \(1985\) obtained similar inequalities using different  
arguments. Van Roy and Wolsey \(1986\) have generalized these results to handle variable  
lower bounds.  
The facet-defining inequalities for the lot-size model \(4.8\) were developed in Barany et  
al. \(1984\). Extensions to handle capacities are given in Leung and Magnanti \(1986\) and  
Pochet \(1988\), and those to treat backlogging are given in Pochet and Wolsey \(1988\). Valid  
inequalities for more general fixed-cost network problems are given in Van Roy and  
Wolsey \(1985\).  
6. EXERCISES  
1. Use clique inequalities, odd hole inequalities, and lifting to derive facets for the  
convex hull of node pac kings for the graph in Figure 6.1.  
2\. Prove Proposition 1.2.  
3. Consider the uncapacitated facility location problem \(UFL\) introduced in Sec-  
tion 1.1.3, with  
*T* = *\{x* E *Bn, y* E *R';!n\:* I *Yij* = 1 for *i* EM, *Yij* \~ *Xj* for all *i* E *M,j EN\}.*  
*JEN*  
i\) Show that dim\(conv\(1\) = *mn* - *m* + *n.*  
ii\) Show that *Y ij* \~ *Xj* define facets of conv\( *T\).*  
Figure 6.1 292 11.2. Strong Valid Inequalities and Facets for Structured Integer Programs  
4\. Let G = *\(V, E\)* be a graph where each node has degree at least 3. Consider the set  
S = *\{x* E *B* I *E* I \: *xi* - *L x e* \~ 0 for all *j* E c5\( *v\)* and *v* E *v\}*  
eEc5\(v\)\\\{j\}  
where c5\( *v\)* denotes the set of edges incident to node *v.*  
i\) Show that the inequalities *Xe* \~ 1 define facets of conv\(S\).  
ii\) Show that the inequalities *xi* - *LeEJ\(v\)\\\{j\} Xe* \~ 0 define facets of conv\(S\).  
5\. Consider the linear ordering problem of determining a permutation *n\:* \{l, ... ,  
*n\}* ... \{l, ... , *n\}* formulated as  
max *L cijc5ij*  
*ij*  
c5ij + c5ji = 1 for all *i* \< *j*  
c5 j,h + ... + c5j,j, \~ I C I - 1 for all cycles C = \{jJ, ... *,jr\}*  
c5 E *Bn\(n-I\),*  
where c5ij = 1 if *i* precedes *j.*  
i\) Show that the inequalities with I C I ;\:\:\: 4 are unnecessary in the description of  
the problem.  
ii\) Show that for I C I = 3, the inequalities define facets.  
6. For S = *\{x* E *Bn\: LjEN ajxj* \~ *b\},* showthatxj;\:\:\: 0 andxj \~ 1 define facets ofconv\(S\)  
when *a* E Z\~ and *aj* + *ak* \~ *b* for allj, *kEN* withj '\* *k.*  
7\. For Example 2.3 use Propositions 2.3 and 2.6 to find as many facets as you can. Use  
these results to solve  
max *12xI* + *5X2* + *8X3* + *7X4* + *5xs* + *5X6* + *4X7* + *3xg* + *2X9* + *XIO*  
*35xl* + *27x2* + *23x3* + *19x4* + *15xs* + *15x6* + *12x7* + *8xs* + *6X9* + *3xIO* \~ 39  
*X EBIO*  
as a linear programming problem.  
8\. Let S = *\{x* E *B6\: 27xJ* + *23x2* + *17x3* + *12x4* + *8xs* + *2X6* \~ 40\}.  
i\) Describe as many facet-defining inequalities as possible for S based on Proposi-  
tion 2.3 and Corollary 2.4.  
ii\) What inequalities are obtained for S from Proposition 2.6?  
9\. Let S = *\{x* E *Bn\:* LiE! *LiEQ; ajXj* \~ *b, LjEQ; Xi* \~ 1 for *i* E *I\}* with *N* = *UiE1Qi'*  
i\) Show that if C is a minimal dependent set with I C n Qi I \~ 1, C n *Qi* = \(j\(i\)\}  
when C n *Qi* '\* 0, and  
*iCC\)* = *E\(C\)* u. U \{1\:cnQr\*'I2'J\}  
\(j E *Qi\: aj;\:\:\: aj\(i\)\},*  
ii\) then *LjEE\(C\) Xj* \~ I C I - 1 is a valid inequality for S.  
Specify conditions under which this valid inequality defines a facet of conv\(S\). 6\. Exercises  
293  
10. Let  
\~ 1  
11. 12. 13. \(see Exercise 14 of Section 1.1. 8\).  
i\) Derive facets for conv\(S\).  
ii\) Can you show that these facet-defining inequalities give conv\(S\)?  
Consider the symmetric traveling salesman polytope for the complete graphs on 5  
and 7 nodes, respectively. Try to write down all of the facet-defining inequalities and  
see if you can give a proof that you have them all.  
Give a nontrivial lower bound on the number of facets of the symmetric traveling  
salesman polytope for complete graphs with *n* = 5, 7,10, 100, and 1000 nodes.  
Prove the validity of the generalized comb inequalities \(3.14\),  
14. Prove that *LeEE' Xe* \~ 9 is valid for the complete graph on 10 nodes, where  
G = *\(V, E'\)* is the Petersen graph, by showing it to be a C-G inequality.  
15. Prove Proposition 4.1.  
16. i\) Use Proposition 4.3 to derive valid inequalities for  
17. ii\) Which of these inequalities define facets?  
Under what conditions does \(4.4\) define a facet of  
18. Consider the capacitated facility problem with feasible region  
*T* \~ \{X E *En, y* E *R,\:,n\:* t *y ij* \~ *ai* for i E *M,* t *Y ij* .;; *bjxj* for *j* E *N* l  
Let *I* \~ *M* and Zj = LiE! *Yij* so that the Zj satisfy  
*2\: Zj* = 2\: *ai* and  
*jEN* iE!  
i\) ii\) Derive valid inequalities for *T.*  
Can you show that the inequalities define facets? 294 **19.** 11.2. Strong Valid Inequalities and Facets for Structured Integer Programs  
Consider the mixed 0-1 region with lower and upper bounds  
with *lj,a* j \~ 0 and the region  
where *Yj* is a variable representing production time, and *Pj* is the associated set-up  
time.  
i\) Show the equivalence between *T* and *T'.*  
ii\) Derive valid inequalities for *T* \(or *T'\).*  
20\. For the fixed-cost networks shown below, show that the proposed inequalities are  
valid.  
a\)  
b\)  
J----\~ 2 \)----\~\~  
6 2 10 2 5 **6\. Exercises 295**  
# c\)  
4 2 3 6  
*Y12* + *Y13* \:\:\:\:; *6X 12* + *4X13* + *Y27* + *Y36* + *Y37·* **11.3**  
# Duality  
**and Relaxation**  
1. INTRODUCTION  
In the preceding two chapters we studied polyhedral descriptions of the set of feasible  
solutions to linear inequalities in nonnegative integer variables. Now we introduce an  
objective function and consider the integer optimization problem  
\(IP\) ZIP = *max\{cx\: xES\},* S = *\{x* E Z\~\:Ax.;\:; *b\},*  
where *c* is an n-vector with integral coefficients and *\(A, b\)* is an *m* x *\(n* + 1\) matrix with  
integral coefficients.  
The theme of this chapter is to develop a theory for determining ZIP, or at least a good  
upper bound on ZIP, without explicitly solving IP. This can be considered to be a theory of  
optimality, since a tight bound on ZIP provides the fundamental way of proving optimality  
of a feasible solution to IP. Suppose we are given an *XO* E S that is claimed to be an optimal  
solution to IP. How can we decide whether this claim is true?  
Our previous results provide one answer. Consider the linear program  
*z\** = *max\{cx\: x* E conv\(S\)\}.  
Then *XO* is an optimal solution to IP if and only if *cxo* = *z\** \(see Theorem 6.4 of  
Section I.4.6\). Although this answer is correct, it depends on knowing conv\(S\), which is an  
assumption we do not make here.  
Observe that the answer just given tells us if *XO* is optimal or not; that is, *cxo* = *z\** is a  
necessary and sufficient condition for the optimality of *xO.* Suppose we just ask for a  
sufficient condition for the optimality of *xO.* We prefer to focus on sufficiency rather than  
necessity because if a sufficient condition is satisfied, the optimality claim is proved.  
Here is a simple, but rather naive, sufficient condition. Consider the linear program  
\(1.1\) ZLP = *max\{cx\: x* E *P\},*  
where *P* = *\{x* E R\~\: *Ax* .;\:; *b\}.* Then *XO* E S is an optimal solution to IP if *cxo* = hp. We said  
this condition is naive because without further assumptions, it is unlikely to hold.  
An equivalent sufficient condition arises from considering the linear programming  
dualofIP\:  
296 1. Introduction 297  
\(1.2\)  
where *PD* = *\{u* E *R'\:'\: uA* ;,- *c\}.* But now we can phrase the sufficient condition in a subtly  
different way; that is, *XO* is optimal to IP ifthere is a *UO* E *PD* such that *cxo* = *uOb.*  
Problem \(1.1\) is called a *relaxation* ofIp, and problem \(1.2\) is called a *\(weak\) dual* ofIP.  
Relaxation and duality are the two fundamental ways of determining *ZIP* and upper  
bounds on *ZIP.* These notions will be made precise after we give an example.  
*Example* 1.1\. Consider the maximum cardinality node-packing problem on the graph  
shown in Figure 1.1. We use the clique constraints, that is no more than one node can be  
chosen from each clique, to obtain the integer programming formulation  
ZIP = max Xl *+X2 +X3 +X4 +Xs +X6*  
*XI+X2* +X6\~1  
*X2+X3* \~ 1  
*X3+X4* \~ 1  
*X4+XS* \~ 1  
XS+X6\~ 1  
The solution *x?* = *xg* = *xg* = 1, *xJ* = 0 otherwise, is feasible. We want to prove that it is  
optimal. The relaxation \(1.1\) is obtained by replacing *x* E *B6* by *x* E R\~, and the dual of  
this linear program is  
*Ul* + *U2* ;,- 1  
*U2* + *U3* ;,- 1  
+ *us;'-* 1  
*u ;,-0.*  
2 3  
1\<\] \>,  
6 5  
Figure 1.1 298 11.3. Duality and Relaxation  
A feasible solution to the dual is *u?* = u\~ = u\~ = 1, *u?* = 0 otherwise. Since  
\~J-l *xJ* = \~r-l *u?* = 3, it follows that XO is an optimal solution to IP and that *ZIP* = *ZLP* = 3.  
The reader can check that in this example the clique constraints and nonnegativity are not  
sufficient to give the convex hull of node packings. However, for the given objective  
function, we have the good fortune that the linear programming relaxation has an integral  
optimal solution. This example shows that it is not necessary to have the convex hull of  
feasible solutions to obtain or prove the optimality of an integral solution.  
Another simple argument that does not use linear programming also establishes the  
optimality of *xo.* Consider any set of cliques such that each node is contained in at least one  
of them, for example, C 1 = n, 2, 6\}, C2 = \{3, 4\}, and C 3 = \{4, 5\}. Such a set of cliques is  
called a *clique cover.* Any node packing contains no more than one node from each of the  
cliques in a clique cover. Hence we obtain the max-min relationship that the maximum  
number of nodes in any node packing is equal to or less than the minimum number of  
cliques in any clique cover. Thus from the cover \{Cl\> *C2, C3\},* we obtain ZIP \~ 3. This is an  
example of a combinatorial duality, which is a principle that is fundamental to the solution  
of combinatorial optimization problems.  
A *relaxation* ofIP is any maximization problem  
\(RP\)  
with the following two properties\:  
\(Rl\)  
S s; *SR*  
\(R2\)  
*ex* \~ *ZR\(X\)* for *xES.*  
Proposition 1.1. *fjRP is irifeasible, so is* IP. *fjIP isjeasible, then ZIP* \~ *ZR.*  
*Proof* From \(Rl\), if *SR* = 0, then S = 0, so the first statement holds.  
Now suppose that *ZIP* is finite and let *XO* be an optimal solution to IP. Then  
*ZIP* = *exo* \~ *ZR\(XO\)* \~ *ZR,* where the first inequality follows from \(R2\) and the second one  
follows from \(Rl\). Finally, if *ZIP* = 00, \(Rl\) and \(R2\) imply that *ZR* = 00. •  
If *x·* E S satisfies *ex\** \~ *ZIP* - E for some fixed E \> 0, then we say that *x\** is an *E-optimal*  
*solution* to IP. Since it is sometimes too costly to find a provably optimal solution, we may  
have to be satisfied with a provably E-optimal solution for a given tolerance E. Although  
the upper-bound *ZR* may fail to prove optimality, RP may allow us to establish  
E-optimality. In particular, ifx\* E S satisfies *ex\** \~ *ZR* - E, thenx\* is an E-optimal solution.  
The most common way to obtain a relaxation is to satisfy \(Rl\) by dropping one or more  
of the constraints that define S and to satisfy \(R2\) by setting *ZR\(X\)* = *ex.*  
The *linear programming relaxation* of IP is \(1.1\). The so-called *group relaxation* is  
obtained by dropping certain nonnegativity conditions. In many problems, the con-  
straints can be partitioned into a set of simple ones that can be handled easily and  
complicated ones. A relaxation is obtained by removing the complicated constraints and  
including them in the objective function in such a way that \(R2\) is satisfied. This  
technique is called *Lagrangian relaxation.* The latter two approaches will be considered in  
Sections 5 and 6 of this chapter.  
Dropping constraints is not the only way to satisfy \(Rl\). We can combine equalities by  
taking linear combinations an9 inequalities by taking nonnegative linear combinations. A 1\. Introduction 299  
right-hand side *b* of a constraint can be replaced by a set of right-hand sides that contains  
*b.* In particular, if S = *\{x* E R\~\: *Ax* \~ *b\}* and S = *UdEB\{X* E R\~\: *Ax* \~ *d\},* where *B* s; *Rm,*  
then S s; S.  
Adding and/or changing variables can also be used to obtain relaxations. For example,  
we obtain a relaxation if S = *\{x* E *ZZ\: Ax* \~ *b\}* is replaced by S' = *\{\(x, x'\)* E Z\~+P\:  
*Ax* + *A x'* \~ *b\}* since S = *\{x* E Z\~\: *\(x,* 0\) E *S'\}.* Such a relaxation can be useful if matrix  
*\(A, A'\)* is easier to work with than *A.* These ideas for relaxation will be used in the  
algorithms to be developed subsequently.  
A distinct disadvantage of using relaxation to obtain bounds is that only an optimal  
solution to the relaxed problem guarantees an upper bound on ZIP. Duality eliminates this  
difficulty since the dual problem is defined so that any dual feasible solution yields an  
upper bound on ZIP.  
A *weak dual* ofIP is any minimization problem  
\(DP\)  
that satisfies  
\(D1\) *ZD\(U\)* \~ *cx* for all *xES* and *u* E *SD'*  
Analogous to Proposition 1.1, we have the following proposition.  
Proposition 1.2. *ffDP isfeasible, then* ZIP \~ *ZD. ffDP has an unbounded objective value,*  
*then* IP *is infeasible.*  
A *strong dual* of IP is a weak dual that also satisfies  
\(D2\) If S '\* 0 and ZIP is bounded from above, then there exists  
*UO* E *SD* and *XO* E S such that *ZD\(UO\)* = *cxo.*  
By solving a strong dual we find ZIP, since ZIP = *ZD* when both problems have finite  
optimum values. By solving a weak dual we can approximate ZIP from above. We call  
*Il.D* = *ZD* - ZIP the *absolute value of the duality gap.*  
Weak duals are easy to construct. For example, by taking the dual of a linear  
programming relaxation ofIP we obtain a weak dual to IP.  
Combinatorial structures are used to construct dual problems. A typical combinatorial  
optimization problem exemplified by the node-packing problem is the following. Let *V =*  
n, 2, ... , *n\}* be a finite set and let *ce* = \{C b C 2, ••• , C *m\}* be a finite collection of subsets of  
*V.* A subset *VO* s; *V* is called a *packing* if I *VO* n Ci I \~ 1 for *i* = 1, ... , *m.* A subset *ceO* s; *ce*  
is called a *cover* if UC,E'€" Cj = *V.* Suppose *VO* is any packing and *ceO* is any cover. Then  
IVOI \~ I IvonCI \~ lceol,  
\(i\: C;E'€"\)  
where the first inequality follows from UC,E'€" Ci \:\:2 *VO* and the second one follows from  
I *VO* n C I \~ 1 for all *i.* In other words, the cardinality of any packing is equal to or less  
than the cardinality of any covering, so the minimum covering problem is a weak dual of  
the maximum packing problem. A fundamental problem of combinatorial optimization  
is to characterize packing and covering problems for which strong duality holds.  
The general relationship between duality and relaxation is given in the following  
proposition. 300 11.3. Duality and Relaxation  
Proposition 1.3. *If a problem is dual to a relaxation of* IP, *then it is also dual to* IP.  
*Proof* Suppose ZOR = *min\{zoR\(u\)\: u* E SOR\} is dual to RP. Then *ZR\(X\)* .;; *ZOR\(U\)* for all  
*x* E *SR* and all *u* E SOR. By relaxation, *cx* .;; *ZR\(X\)* for all *xES* S *SR.* Hence *cx* .;; *ZOR\(U\)*  
for all *xES* and *u* E SOR. •  
As with relaxations, algorithms generally use a weak dual to obtain bounds and  
iteratively refine the dual to strengthen the bounds.  
2. DUALITY AND THE VALUE FUNCfION  
Here we consider a family of integer programs  
*\(2.1\) z\(d\)* = *max\{cx\: x* E *S\(d\)\}, S\(d\)* = *\{x* E z\~\: *Ax.;; d\}* for *d* E *D,*  
where *A* and c are fixed and *d* is a parameter in *D* s *Rm.* Depending on our need we may  
take *D* = *R m* or *D* = *zm* or *D* = *\(d* E *R m \: S\(d\)* \*' 0\}. The function *z\(d\)* for dE *D* is called  
the *value function* of IP. We say that *z\(d\)* = -00 if *S\(d\)* = 0 and that *z\(d\)* = + 00 if the  
objective value is unbounded from above.  
The following propositions give some elementary properties ofthe value function.  
Proposition 2.1. *The value function ofIP is non decreasing over Rm.*  
# •  
Proposition 2.2. *Z\(O\)* E\{O,oo\}. *Ifz\(O\)* = *oo,thenz\(d\)=± ooforalldERm.Ifz\(O\)*= 0,  
*then z\(d\)* \< 00 *for all* dE *Rm.*  
*Proof* See Proposition 6.7 of Section 1.4.6. •  
Problems with *z\(d\)* = ± 00 for all *d* E *R m* \(e.g., max\{xl\: *2Xl* - *X2';; d, x* E Zm reduce  
to feasibility problems. Thus, for simplicity of exposition, it is convenient to ignore them  
here. Hence, unless otherwise specified, we assume *z\(O\)* = 0, so *z\(d\)* \< 00 for all *d* E *Rm.*  
Proposition 2.3. *The valuefunction ofIP is superadditive over D* = *\{d* E *Rm \: S\(d\)* \*' 0\}.  
*Proof* Suppose *Xi* E Z\~ and *AXi.;; di* for *i* = 1, 2. Then *\(Xl* + *x 2 \)* E Z\~ and  
*A\(xl* + *x 2 \)* .;; *d l* + *d2*  
*.* Thus if *c.xi* = *z\(di\)* for *i* = 1,2, then  
# •  
The problem of finding an upper bound on the optimal value ofIP can be generalized to  
the problem offinding a function *g\(d\)\: Rm* ... *R* I such that *g\(d\)* ;;;. *z\(d\)* for all *d* E *Rm* \(see  
Figure 2.1\). Thus a dual problem to IP can be formulated as  
\(2.2\) min\{g\(b\)\: *g\(d\);;;. z\(d\)* for *d* E *Rm, g\: Rm* ... *Rl\}*  
or, equivalently, as  
\(2.3\) min\{g\(b\)\: *g\(d\);;;. cx* for *x* E *S\(d\)* and *d* E *Rm\}.* 2. Duality and the Value Function 301  
--------------\~\~----------------------------d  
Figure 2.1  
This dual is strong since there are feasible solutions with *g\(b\)* = *z\(b\);* for example,  
*g\(d\)* = *z\(d\)* when *z\(d\)* \> - 00 and *g\(d\)* \:;\:\: 0 otherwise.  
Some restrictions on *g* are needed to obtain a useful dual problem. Since *z\(d\)* is  
nondecreasing, it is natural to assume that *g\(d\)* is nondecreasing. Then *g* satisfies  
*g\(d\)* \~ *ex* for *x* E *S\(d\)* if and only if *g\(Ax\)* \~ *ex* for *x* E Z\~. Thus when *g* is nondecreas-  
ing, \(2.3\) can be stated as  
*ming\(b\)*  
\(2.4\)  
*g\(Ax\)* \~ *ex* for *x* E Z\~  
*g* nondecreasing.  
Now suppose that *g* is linear; that is, *g\(d\)* = *ud* with *u* E *R'\:'.* Thus we require *uAx* \~ *ex*  
for all *x* E Z\~. This last condition is equivalent to *uA* \~ e. Thus we obtain the weak dual  
\(2.5\) *min\{ub\: uA* \~ *C, u* E *R'\:'\},*  
which is the dual of the linear programming relaxation ofIP.  
Linear functions are generally too restrictive to obtain strong duality. In the following  
example, we first consider the value function and the linear dual and then we give two  
illustrations of strong dual functions.  
*Example* 2.1  
*z\(d\)* = max 3XI + *6X2* + llx3 + *12X4*  
Xl + *2X2* + *3X3* + *4X4'\:\:; d*  
*xEZ!* 302 11.3\. Duality and Relaxation  
*ex*  
30  
20 •  
*g\(d\)* = *'y'd* -----O!\~  
# •  
# •  
10 *z\(d\)* = *3\[dJ* + 2 *\[V3dJ*  
# •  
\~----------------------------------------------Ax  
o 2 3 4 5 6 7 8 9  
Figure 2.2  
Figure 2.2 gives a plot of *\(Ax, ex\)* for the feasible points. The upper envelope of these  
points gives the value function  
*z\(d\) =*  
\~1 *d, d* = 0, 3, 6, ...  
11  
3 + 3 *\(d* - 1\), *d* = 1, 4, 7, ...  
11  
6 + 3 *\(d* - 2\), *d* = 2, 5, 8, ....  
*z\(d\)* = z\(ldJ\) for *d* positive and not integral, and *z\(d\)* = - 00 if *d* \< 0\. We can also express  
the value function over R\~ by *z\(d\)* = *3\[dj* + *2\[1dj,* which shows that *z* is superadditive over  
R\~.  
*ex*  
30  
20  
*g\(d\)* = *3d+2, OSds4*  
10 *z\(d\}*  
\~------------------------------------d  
o 2 3 4 5 6 7 8 9  
Figure 2.3 2. Duality and the Value Function 303  
Figure 2.2 also shows the function *g\(d\)* = *lfd,* which is the optimal dual solution wheng  
is restricted to be linear; that is, *u* = If is the optimal solution to \(2.5\). We see this  
graphically by observing that any line through the origin with slope \< If is not dual feasible  
and that if the slope is greater than If, *ud* \> *lfd* for all *d* E *Rl.* Note that the optimal linear  
function only provides a strong dual when *d* is an integer multiple of 3.  
Figure 2.3 shows *zed\)* and the function  
where *gl\(d\)* = *3d* + 2 and *g2\(d\)* = *4d* - 2. We can see from the picture that *go* is dual  
feasible.  
We now give an algebraic justification of its dual feasibility. We have  
*gl\(Ax\)* = 3xI + *6X2* + *9X3* + *12x4* + 2  
*= ex* + 2\(1 - *x 3\)*  
\~ *ex* for *x* E *\{Z!\: X3* \~ 1\}  
and  
*g2\(Ax\)* = 4xI + *8X2* + *12x3* + *16x4* - 2  
\~ *ex* + *\(X3* - 2\)  
\~ *ex* for *x* E *\{Z!\: X3* \~ 2\}.  
*Hencego\(Ax\)* \~ *ex* for all *x* E *Z!.* Note thatgo \(4\) = *z\(4\)* so that strong duality is obtained  
*ford* = 4.  
Figure 2.4 shows *zed\)* and the superadditive function *F\(d\)* = *3d* + l\~dl. Note that  
*F\(1\)* = 3 = eJ, *F\(2\)* = 7 \> *e2, F\(3\)* = 11 = *e3,* and *F\(4\)* = 14\> *e4.* Thus *F\(aj\)* \~ *ej* for *j =*  
1, ... , 4 and hence superadditivity implies  
4 4  
*F\(Ax\)* \~ I *F\(aJxj* \~ I *ejXj* for *x* E *Z!.*  
j\~1 j\~1  
*F\(d\)*  
30  
20  
10  
\~---------------------------------------------------d  
o 2 3 4 5 6 7 8 9  
Figure 2.4 304 11.3. Duality and Relaxation  
Thus *F* is dual feasible. Strong duality is obtained for *d* = 4 since *F\(* 4\) = 14.  
The three functions used in the example illustrate important classes of dual functions  
that are used in integer programming algorithms. Linear functions are the simplest, but  
they do not generally yield strong duality. The function *go\(d\)* exemplifies the type of dual  
function used to prove optimality in branch-and-bound algorithms with linear program-  
ming relaxations. Superadditive functions are used to prove optimality in cutting-plane  
algorithms.  
3\. SUPERADDITIVE DUALITY  
There are two important reasons for restricting the function *g* to be superadditive in the  
dual problem \(2.4\)\:  
a. The purpose of the dual problem is to estimate the value function from above, and  
the value function is superadditive over the domain for which it is finite.  
b. If *g* is superadditive, the condition *g\(Ax\)* ;;. *cx* for *x* E ZZ is equivalent to *g\(aj\)* ;;. *Cj*  
for *j* EN. This is true since *g\(Aej\)* ;;. *cej* is the same as *g\(aj\)* ;;. *Cj* for *j* EN; and if *g*  
is superadditive, then *g\(aj\)* ;;. *Cj* for *j* EN implies  
*g\(Ax\);;.* I *g\(aJxj;;'* I *CjXj* = *CX*  
*jEN jEN*  
for *x* E *ZZ.*  
Condition b enables us to state a *superadditive dual problem* independent of *x.*  
*w* = *minF\(b\)*  
*F\(aj\)* ;;. *Cj* for *j* EN  
\(SDP\)  
*F\(O\)* = 0  
*F\: Rm ... Rl,* nondecreasing and superadditive.  
We now establish results analogous to linear programming duality for the primal  
problem IP and the dual problem SDP.  
Proposition 3.1. *\(Weak Duality\). IfF isfeasible to* SDP *and x isfeasible to* IP, *then cx* \~  
*F\(b\).*  
*Proof*  
I *CjXj* \~ I *F\(aJxj* since *Cj* \~ *F\(aj\)* for *j* EN and *x* E R\~  
*jEN jEN*  
\~ *F\(Ax\)*  
\~F\(b\)  
since *F* is superadditive, *F\(O\)* = 0, and *x* E Z\~  
since *F* is nondecreasing. •  
Weak duality allows us to take care of the case of an unbounded primal objective  
function that we dismissed earlier. 3\. Superadditive Duality 305  
Corollary 3.2 *\(Unbounded Primal Objective Function\). then the superadditive dual is infeasible.*  
*IfIP isfeasible and z\(b\)* = 00,  
*Proof* If *z\(b\)* = 00, then *z\(O\)* = 00, so no dual solution can satisfy *F\(O\)* = O. •  
Weak duality establishes that if *F* is a feasible solution to the superadditive dual, then *F*  
provides an upper bound on the value function for all dE *Rm.* It remains to be shown that  
the objective min *F\( b\),* which we did not use in the proof of weak duality, yields strong  
duality.  
For the remainder of this section, we assume that *P* = *\{x* E R\~\: *Ax* \~ *b\}* contains  
explicit bound constraints so that we can use Corollary 2.14 of Section n.1.2.  
Theorem 3.3 *\(Strong Duality\).*  
1. *IfIP isfeasible, then* SDP *isfeasibleand w* = *z\(b\).*  
*2. If* IP *is infeasible, then the dual objective function is unbounded from below*  
\(w = -00\).  
*Proof* 1. *LjEN CjXj* \~ *z\(b\)* is a valid inequality for S. Hence Theorem 4.6 of Sec-  
tion n.l.4 implies that there exists a superadditive and nondecreasing function *F* \(with  
*F\(O\)* = 0\) such that *LjEN F\(aJxj* \~ *F\(b\)* is valid for S and dominates *LjEN CjXj* \~ *z\(b\).* This  
means that *F\(aJ* \~ Cj for *j* EN and *F\(b\)* \~ *z\(b\).* Hence *F* is dual feasible. But then  
*F\(b\)* \~ *z\(b\),* so *F* is an optimal dual solution with *w* = *F\(b\)* = *z\(b\).*  
2. Since *P* contains explicit bound constraints, there exists *u* E *R';* such that *uA* \~ c \(see  
Proposition Ll of Section n.Ll\). Let *Fl\(a\)* = *ua* for all *a* E *Rm.* By Corollary 2.14 of  
Section n.l.2, we have *Ox* \~ - 1 is a C-G inequality for S. Hence there is a superadditive  
and nondecreasing function *F2* with *F2 \(aj\)* \~ 0 for *j* EN and *F2 \(b\)* \~ - l. Thus for any  
*A* E *Ri,* it follows that *Fl* + *AF2* is a feasible dual solution; also, *Fl\(b\)* + *AFlb\) ....* -00 as  
*A ....* 00. •  
The familiar *complementary slackness* property oflinear programming duality carries  
over to superadditive integer programming duality. In particular, if *XO* is an optimal  
solution to IP and *FO* is an optimal superadditive dual solution, then  
\(3.1\)  
We prove \(3.1\) as a corollary to a slightly more general result.  
Theorem 3.4. *If XO is an optimal solution to* IP *and FO is an optimal solution to the*  
*superadditive dual, then*  
*for all x* E Z\~ *such that x* \~ *xc.*  
*Proof*  
*cxO* = *cx* + *c\(XO* - *x\)* \~ I *FO\(aj\)xj* + I *FO\(aJ\(xJ* - *Xj\)*  
*JEN JEN*  
\~ *FO\(Ax\)* + *FO\(A\(xO* - *x»* \~ *FO\(Ax\)* + *FO\(b* - *Ax\)*  
\~ *FO\(b\)* = *cxo.* 306 11.3. Duality and Relaxation  
Hence the second equality holds. The first equality holds since we also have *FO\(Ax\)* \~ *cx*  
and *FO\(A\(xO* - *x»* \~ *c\(XO* - *x\). •*  
Note that \(3.1\) is trivial when *xJ* = O. When *xJ* \~ 1, we obtain \(3.l\) from *FO\(Aej\)* = *cej'*  
The next two results describe optimal solutions to the superadditive dual. The first  
result comes from a superadditive description of conv\(S\) and linear programming duality.  
Theorem 3.5. *If* S \* 0 *and* max\{cx\: xES\} \< 00, *then there exist au* E R\~ *and finite*  
*rank C-G functions Fk for k* = 1, ... , *t with t* \< *n such that pc* = Lk\~l *UkFk is an optimal*  
*solution to the superadditive dual.*  
*Proof* By Proposition 4.5 of Section 11.1.4 there exist C-G functions *Fk* for  
*k* = 1, ... , *t* such that  
conv\(S\) = *\{x* E R\~\: 2\: *Fk\(aj\)xj* \< *Fk \(b\)* for *k* = 1, ... , *t\}.*  
*JEN*  
Now apply linear programming duality. •  
The value function *zed\)* ofIP would be a feasible \(and hence optimal\) solution to SDP  
except for the fact that it is superadditive only on the domain *D* where IP is feasible. The  
following theorem tells us that z can always be extended to a \(finite-valued\) superadditive  
function over *Rm.*  
Theorem 3.6. *There are C-G functions pi for i* = 1, ... *,q such that zed\) =*  
mini\~l", *,q Fi\(d\)for all d with zed\)* \> - 00.  
We will not prove this theorem. We observe, however, that *F\(d\)* = minH"" *q Fi\(d\)* is  
superadditive over *Rm* since it is the minimum of a finite number of superadditive  
functions. The functions *pi* are optimal solutions to the superadditive dual for certain  
values of *d.* Hence, implicit in the result is that it is possible to calculate *zed\)* for all *d* E *D*  
by solving IP for only a finite number of *d ED.*  
*Example* 3.1 *\(Example* 2.1 *continued\).* We showed in the previous section that  
*F\(d\)* = *3d* + l\~dj is an optimal dual solution for *d* = 4. This solution can be obtained by  
applying Theorem 3.5 as explained below. It can be shown that conv\(S\) is given by the  
inequalities  
*Xl* + *2X2* + *3X3* + *4X4* \< 4  
*X2* + *2X3* + *2X4* \< 2  
*xER!*  
and thus is generated from *P* by the functions *FI\(d\)* = *d* and *Fl\(d\)* = l\~dj. For  
c = \(3 6 11 12\), an optimal solution to the dual of the linear program  
max\{cx\: *x* E conv\(S\)\} is *u* = \(3 1\). Hence *F* = *3FI* + *Fl* is an optimal solution to the  
superadditive dual. Note that the optimal solution to IP is *x* = \(1 0 1 0\). Since  
*F\(l\)* = CI and *F\(3\)* = *C3,* the complementary slackness conditions are satisfied.  
Theorem 3.6 is trivial for this example. We take *F\(d\)* = 3ldj + 2ltdj for *d* E *R* I and note  
that *F\(d\)* = *zed\)* whenever *zed\)* is finite. 3\. Superadditive Duality 307  
*Example* 3.2\. Section 11.1.1.  
This integer program has the constraint set of the example presented in  
max *7Xl* + *2X2*  
*-Xl* + *2X2* \~ 4  
*SXl* + *X2* \~ 20  
*-2Xl* - *2X2* \~-7  
*xEZ;.*  
The superadditive dual is  
*minP* \(=n  
# PCD\>7,  
*F\(O\)* = 0, *F* superadditive and nondecreasing.  
1. A dual feasible solution is *F\(d\)* = ndl + *\*d2* + *Od3•* This is the linear solution  
obtained from an optimal dual solution to the linear programming relaxation. It  
yields the bound Z IP \~ 30n.  
2. Rounding yields the better dual solution *F\(d\)* = \[ndl + *\*d21.*  
3. An optimal dual solution \(see Section 4 of Chapter 11.1\) is given by the complicated  
function *F 12.*  
*Example* 3.3. We reconsider the node-packing example of Section 1 \(see Figure 1.1\) with  
the objective function c = \(l 3 3 3 3 3\). Solutions to the dual of the linear pro-  
gramming relaxation correspond to assigning nonnegative weights *Ui* to the cliques Ci so  
that for all\} E *V* the sum of the weights over all cliques containing node *j* is at least *Cj.*  
Given the cliques Cl = \{l, 2, 6\}, C2 = \{2, 3\}, C3 = \{3, 4\}, C4 = \{4, *S\},* and Cs = *\{S,* 6\}, we see  
that a feasible solution is *U* = \(1 2 1 2 2\), which yields the superadditive dual  
feasible solution *Fl\(d\)* = *d l* + *2d2* + *d 3* + *2d4* + *2ds.* Thus we obtain ZIP \~ *Fl\(b\) =*  
1 + 2 + 1 + 2 + 2 = 8. Now the odd hole induced by the nodes \{2, 3, ... , 6\} yields the valid  
inequality *X2* + *X3* + ... + *X6* \~ 2, which is generated by the superadditive function  
*F 2\(d\)* = Hdl + ... + *1dsl.* Note that *F 2\(al\)* = 0 and *F 2\(a\)* = 1 for *j* \> 1. Hence a feasible  
dual solution is given by *F\(d\)* = *3F2\(d\)* + *d l.* Since *F\(b\)* = 3 x 2 + 1 = 7, we have ZIP \~ 7.  
To show that *F* is an optimal dual solution, we observe that *XO* = \(l 0 1 0 1 0\) is a  
feasible node packing and *cxo* = 7.  
Neither the extended value function of Theorem 3.6 nor the C-G function of Theo-  
rem *3.S* are useful for computing bounds. The value function is not available, even after  
the problem is solved, and the C-G function depends on having a linear inequality  
description of conv\(S\). Both functions, in a sense, provide more information than we  
need. The extended value function is optimal for all *d* E *R m* for which IP is feasible, and  
the C-G function is a nonnegative linear combination of the same C-G functions for all c.  
Unfortunately, we do not know how to characterize a locally optimal function \(e.g., one  
that is optimal only in a neighborhood of a particular band C of interest\). Thus for 308 11.3. Duality and Relaxation  
algorithmic purposes, we must restrict the class of dual feasible functions to computable  
ones that do not necessarily yield strong duality. In the following sections we will consider  
some classes of dual feasible functions that are useful algorithmically.  
To complete this section, we state without proof the analogous result on superadditive  
duality for mixed-integer programs.  
Theorem 3.7. *Let T* = *\{x* E Z;, *Y* E R\~\: *Ax* + *Gy* \~ *b\} and z\(b\)* = max\{cx + *hy\: \(x, y\)* E  
n. *A strong dual to the mixed-integer programming problem is*  
*w* = *minF\(b\)*  
*F\(aj\)* \~ *Cj*  
*forj* EN  
*F\(gj\)* \~ *hj*  
*forj EJ*  
*F* nondecreasing and superadditive,  
*F\(d\)* = lim *F\(Ad\).*  
'\\\>0, A  
*F\(O\)* = 0  
*Ifz\(O\)* = 0 *and T\** 0, *then z\(b\)* = *w. Ifz\(O\)* = 0 *and T=* 0, *the dual is irifeasible or its*  
*objective value is unbounded. Ifz\(O\)* = 00, *the dual is infeasible.*  
4. THE MAXIMUM-WEIGHT PATH FORMULATION AND SUPERADDITIVE  
DUALITY  
Consider the integer programming problem *ZIP* = *max\{cx\: xES\},* where S = *P* n Z\~,  
*p* = *\{x* E R\~\: *Ax* \~ *b\},* and *\(A, b\)* \~ 0 with integral coefficients. By using the polyhedral  
characterization of superadditive functions developed in Section 11.1.5, the superadditive  
dual problem can be written explicitly as a linear program\:  
*w* = *minF\(b\)*  
*F\(aj\)* \~ *Cj forj* EN  
\(SDLP\)  
*F\(dl \)* + *F\(d2\)* - *F\(dl* + *d2\)* \~ 0 for *dJ, d2, d l* + *d2* E *D\(b\)*  
*F\(O\)* = 0, *F\(d\)* \~ 0 for *d* E *D\(b\),*  
where *D\(b\)* = *\{d* E Z\~\: *d* \~ *b\}* and *F* is a vector with *ID\(b\)* I coordinates.  
*Example* 4.1 *\(Example* 2.1 *continued\).* For the knapsack problem  
max 3XI + *6X2* + llx3 + *12X4*  
Xl *+2X2* + *3X3* + *4X4* \~ 4  
*xEZ!,*  
SDLPis 4\. The Maximum-Weight Path Formulation and Superadditive Duality min *F\(4\)*  
*F\(1\)* ;;;. 3  
*F\(2\)* ;;;. 6  
*F\(3\)* ;;;. 11  
*F\(4\)* ;;;. 12  
*2F\(1\)* - *F\(2\)* ... 0  
*F\(1\)* + *F\(2\)* - *F\(3\)* ... 0  
*F\(1\)* + *F\(3\)* - *F\(4\)...* 0  
*2F\(2\)* - *F\(4\)...* 0  
*F\(O\)* = 0, *F\(d\);;;.* 0 for *d* E *D\(4\).*  
309  
Since SDLP is a linear program that is strongly dual to IP, we can use linear program-  
ming duality to express IP as a linear program. The purpose of this section is to study the  
structure of this dual pair oflinear programs. We will discover that IP can be formulated as  
the linear program of finding a maximum-weight path joining two specified nodes in a  
directed graph and that SDLP can be interpreted as the dual of this maximum-weight path  
problem. We will establish the duality after formulating IP as a maximum-weight path  
problem.  
To formulate IP as a maximum-weight path problem when S is an independence  
system, consider the digraph \~ = *\(V, .stl\),* where *V* = *D\(b\)* = *\{d* E Z';\: *d ... b\},* and  
*.stl =.stl1* U *.stl2,* where  
.stl 1 = *\{\(d, d* + *aj\)\: d, d* + *aj* E *D\(b\), j* EN\} and *.stl2* = *\{\(d; b\)\: dE D\(b* \)\}.  
Since *aj* \> 0 for allj EN and *b* ;;;. *d* for all *dE V,* \~ has no cycles.  
The arc *e* = *\(d, d* + *aj\)* forj EN is called a *variablej arc* and is assigned weight *We* = *Cj.*  
For *d* \* *b,* node *d* represents the subset of feasible solutions *S\*\(d\)* = *\{x* E Z\~\: *Ax* = *d\}*  
since every path from node 0 to node *d* has the property that *l\:.jEN ajxj* = *d,* where *Xj* is the  
number of variable *j* arcs in the path. The weight of any such path is *l\:.jEN CjXj.* Arcs  
*\(d, b\)* E *Sli2* are called *slack arcs* and are assigned a weight ofO. If there is aj such that  
*d* + *aj* = *b,* then a variable *j* arc and a slack arc join the same pair of nodes. Node *b*  
represents the set of all feasible solutions since every path from node 0 to node *b* has the  
property that *l\:.jEN ajxj'" b,* where *Xj* is the number of variable *j* arcs in the path. Hence  
any maximum-weight path from node 0 to node *b* corresponds to an optimal solution to  
IP.  
11  
O-'------\~\~----\~\~\~----\~.\_----\~\~4  
12  
Figure 4.1 310 11.3. Duality and Relaxation  
Table 4.1.  
Path  
\[\(0, 1\), \(1, 2\), \(2, 3\), \(3, 4\)\] \[\(0, 1\), \(1, 2\), \(2, 4\)\] \[\(0, 1\), \(1, 3\), \(3, 4\)\] \[\(0, 2\), \(2, 3\), \(3, 4\)\] \[\(0, 1\), \(1, 4\)\] \[\(0, 3\), \(3, 4\)\] \(4 °  
\(2 1  
\(2 1  
\(2 1  
\(1 °  
° ° \(1 °  
\[\(0,4\)\] \(0 °  
*x*  
1  
# °  
° ° 1  
0\)  
0\)  
0\)  
0\)  
0\)  
0\)  
1\)  
Weight  
12  
12  
12  
12  
14  
14  
12  
The digraph for Example 4.1 is shown in Figure 4.1. The slack arcs have been omitted.  
Actually they are unnecessary in the example since each slack arc is "parallel" to a non-  
slack arc of positive weight. The paths from node ° to node 4, along with the correspond-  
ing feasible solutions, are given in Table 4.1. We see that there is at least one path  
corresponding to each feasible solution and that the weight of the path equals the value of  
the corresponding solution. In general, many paths correspond to the same feasible  
solution because each path is an ordering of the set of arcs that represent the solution.  
We now give the standard flow formulation of the maximum-weight path representa-  
tion ofIP. Each arc is represented by a binary variable with the interpretation that an arc is  
in the solution if and only if the corresponding variable equals 1. The variable for the arc  
*\(d,d* + *aj\)* is *Yj\(d\),* and the variable for a slack arc *\(d, b\)* is *Yo\(d\).*  
The constraints that are satisfied only by paths from node ° to node bare\:  
i. Exactly one arc leaves node 0, that is,  
\(4.1\) *- L Yj\(O\)* - *Yo\(O\)* = -1.  
*JEN*  
11. Exactly one arc enters node *b,* that is,  
\(4.2\)  
iii. For *d* \* 0, *b,* the number of arcs that enter node *d* equals the number that leave, that  
is,  
*\(4.3\) L y/d* - *aj\)* - *L y/d\)* - *Yo\(d\)* = ° for *d* \* 0, *b.*  
\(jEN\: *d-ap,O\)* \(jEN\: *d+ar"b\)*  
Note that \(4.1\) implies that the number of arcs entering each node *d* \* ° is already  
constrained to be ° or 1. Finally note that \(4.1\) also implies that it suffices to allow the  
variables to be nonnegative integers. Thus we obtain an integer program representation of  
the maximum-weight path formulation ofIP given by  
\(MP\) ZMP= max *L*  
*\(jEN,dED\(b\)\: d+aF"b\)*  
subject to \(4.1\)-\(4.3\) and  
\(4.4\) *yAd\)* E Zl for allj and *d.* 4\. The Maximum-Weight Path Formulation and Superadditive Duality 311  
We have *ZMP* = *ZIP* as explained above; also, *ZIP* = *w* by strong duality \(Theorem 3.3\).  
We now consider the dual of the linear programming relaxation of MP, which is given  
by  
\(4.5\) ZLP = min *\(u\(b\)* - *u\(O\)\)*  
*u\(d* + *aj\)* - *u\(d\)* \~ Cj for\} *EN, dE D\(b\), d* + *aj* \~ *b*  
*u\(b\)* - *u\(d\)* \~ 0 for *d* E *D\(b\), d,\* b,*  
where *u\(d\)* is the dual variable for the node *d* constraint. Note that if *u* is a feasible  
solution to \(4.5\) with *u\(O\)* '\* 0, then so is *u\** where *u\*\(d\)* = *u\(d\)* - *U\(O\)* for all *d* E *D\(b\).*  
Since *u\*\(b\)* = *u\*\(b\)* - *u\*\(O\)* = *u\(b\)* - *u\(O\),* we can set *u\(O\)* = O.  
In Example 4.1, \(4.5\) is  
min *u\(4\)*  
*u\(l\)* \~ 3  
*u\(2\)* \~ 6  
*u\(3\)* \~11  
*u\(4\)* \~ 12  
*-u\(1\)* + *u\(2\)* \~ 3  
*-u\(l\) + u\(3\)* \~ 6  
*-u\(l\)* + *u\(4\)* \~ 11  
*- u\(2\)* + *u\(3\)* \~ 3  
*-u\(2\)* +u\(4\)\~ *6*  
*- u\(3\)* + *u\(4\)* \~ 3.  
We have omitted *u\(O\)* = 0 and the constraints *u\(b\)* - *u\(d\)?3* 0, which are superfluous here.  
Now we return to SDLp, introduced at the beginning of this section. Consider the  
constraints  
\(4.6\)  
We are going to relax \(4.6\) in three different ways, depending on *d 1* and *d2•*  
i. If *d 1* = *aj* for some\} *EN,* then we replace \(4.6\) by *F\(aj* + *d2\)* - *F\(d2\)* \~ *Cj.* This is a  
relaxation since *F\(aj\)* \~ *Cj.*  
ii. If *d 1* + *d2* = *b,* then we replace \(4.6\) by *F\(d2\)* - *F\(b\)* \~ O. This is a relaxation since  
*F\(d1\)* \~ O.  
iii. Otherwise we drop \(4.6\).  
Finally, we omit the non negativity constraints.  
This yields the relaxation ofSDLP\: 312  
11.3. Duality and Relaxation  
*WR* = min *F\(b\)*  
\(4.7\)  
*F\(aj\)* \~ *Cj*  
for\} *EN*  
*F\(d* + *aJ* - *F\(d\)* \~ *Cj*  
*F\(b\)* - *F\(d\)* \~ °  
*F\(O\)* = 0.  
for\} *EN, dE D\(b\), d* + *aj* \~ *b*  
for *d* E *D\(b\), d,\* b*  
By relaxation, *WR* \~ w.  
Now observe that \(4.5\) and \(4.7\) are identical. Hence *WR* = ZLP. Thus  
and all of these objective values are equal.  
In conclusion, we have interpreted the superadditive dual of an integer program whose  
constraints generate an independence system as the linear programming dual of a  
maximum-weight path formulation of the integer program.  
The maximum-weight path formulation is oflimited use in computation because of the  
size of the digraph. The number of nodes *ID\(b\)* I grows exponentially with the number of  
constraints *m* and, even for fixed *m,* grows linearly with the size of the coefficients of *b* E  
*Z,\:,.* Generally, its use is restricted to knapsack problems that have constraint coefficients  
of modest size.  
Despite these practical limitations, the fundamental idea is applicable to any integer  
program. In particular, any integer programming constraint set S = *\{x* E Z\~\: *Ax* \~ *b\}* can  
be represented by a digraph with the property that directed walks from node ° to node *d* E  
*zm* correspond to solutions with *Ax* = *d.* As before, an arc *e* = *\(d, d* + *aj\)* is a variable\} arc  
and is assigned weight *We* = *Cj.* However, when matrix *A* has negative coefficients, we must  
determine a finite subset *D\(b\)* c *zm* to which we can restrict *d.* Note that *D\(b\)* = *D\(b\)*  
does not suffice since *d* + *aj* \~ *d* is no longer true.  
To determine *D\(b\),* we use the result that if S,\* 0 there is an *OJ* which depends *onA, b,*  
and C, such that *Xj* \~ *OJ* for all\} *EN* in some optimal solution \(see Theorem 4.1 of  
Section 1.5.4\). *Thenxj* \~ *OJ* for\} E Nimpliesd-\~ *Ax* \~ *d+,* wheredi = *OJ LjEN* min\(O, *au\)*  
and *di* = *OJ LjEN* max\(O, *au\)* for *i* = 1, ... , *m.* ThusD\(b\) = *\{d* E *zm\: d-* \~ *d* \~ *d+\}.*  
5. MODULAR ARITHMETIC AND THE GROUP PROBLEM  
In this section, we consider relaxations of the maximum-weight path formulation that  
reduce the size of the digraph. In fact, we will be able to choose the number of nodes,  
although the quality of the bounds produced by the relaxation will generally deteriorate as  
the digraph gets smaller.  
Consider the set S = *\{x* E Z\~\: *LjEN ajxj* = *b\},* where *aj* E ZI for\} *EN* and *b* E *ZI.*  
Suppose *k* E zl and we relax S to  
This means that multiples of *k* can be subtracted from each coefficient and the right-hand  
side of the original constraint. Thus *xES k* if and only if *x* E Z\~ and for each \(Ilo, *AI,* ... ,  
*An\)* E *zn+l* there exists a w' E ZI such that 5\. Modular Arithmetic and the Group Problem  
*L \(aj* - *kAj\)Xj* = *\(b* - *kAo\)* + *kw'.*  
*jEN*  
313  
By choosingA;- = *\[alkj* forj EN andAo = *\[b/kj,* we have that *X* E *Sk* ifand only if *X* E Z\~  
satisfies  
\(modulo *k\),*  
where *¢k\(d\)* = *d* - *k \[d/kj;* that is, *¢id\)* = *d\(mod k\)* is the remainder when *d* is divided  
*byk.*  
We can now represent S *k* by directed walks in a digraph that contain only *k* nodes. The  
graph is *qj;k* = *\(Vb slh\),* where  
The arc *e/d\)* = *\(d, ¢k\(d* + *aj»* is called a *variablej arc.* A directed walk in *qj;k* from node 0  
to node *¢k\(b\)* with *Xj* variablej arcs generates a feasible solution *x* E *Sk;* conversely, any  
*xES k* generates directed walks from node 0 to node *¢k\( b\).*  
Any directed walk from node 0 to node *¢k\(b\)* can be decomposed into a dipath from  
node 0 to node *¢k\(b\)* and \(possibly\) directed cycles. Correspondingly, any *x* E *Sk* can be  
decomposed into *x\** + l\:l=l *Xi,* where ¢k\(l\:jEN *a jxj\)* = *¢k\( b\)* and *x\** generates dipaths from  
node 0 to node *¢k\(b\)* and where *¢k\(l\:jEN ajxJ\)* = 0 and *Xi* generates directed cycles, for  
i = 1, ... *,t.*  
Later in this section, we will consider the problem of finding a maximum-weight dipath  
from node 0 to node *¢k\(b\)* in qj;b where the arcs have nonpositive weights. Hence the  
solution will be a dipath corresponding to an *x\** E S *k.* However, there is no guarantee that  
dipaths correspond to elements of S; that is, it could be the case that some, or even all,  
elements of S correspond to directed walks from node 0 to node *¢k\(b\)* that contain  
directed cycles.  
*e4\(5\)*  
Figure 5.1. *eM\)* = *\(d,* ¢M + *aj».* 314 11.3. Duality and Relaxation  
*Example* 5.1\. S = *\{x* E *Z!\: 78xl* - *68x2* + *37x3* + *X4* = 141\} and *k* = 6. We have  
*CP6\(al\)* = 0, *CP6\(a2\)* = 4, *CP6\(a3\)* = *CP6\(a4\)* = 1, and *CP6\(b\)* = 3. Hence S6 = *\{x* E *Z!\: OXl +*  
*4X2* + *X3* + *X4* = 3 \(mod 6\)\}.  
The digraph \~6 is shown in Figure 5.1. Table 5.1 gives all of the solutions corresponding  
to dipaths, and Table 5.2 gives some directed cycles. More cycles can be generated by  
replacing any of the variable 3 arcs by variable 4 arcs.  
Any *x* in Table 5.1 plus a nonnegative integer multiple of an *x* in Table 5.2 is in *S6;* for  
example,  
\(4 3 1 2\) = \(0 0 1 2\) + 4 \(1 0 0 0\) + \(0 3 0 0\).  
Any such *x* is in S if and only if\:EjEN *ajxj* = *b;* for example,  
\(9 11 5 2\) = \(0 0 1 2\) + 9 \(1 0 0 0\) + 3 \(0 3 0 0\) + 2 \(0 1 2 0\).  
Now we introduce weights on the arcs of \~k. For any positive integer *k* and *PER* l, the  
problem  
\(5.1\)  
*zk\(b\)* = max *L CjXj* - *pw*  
*jEN*  
*L ajxj* - *kw* = *b*  
*jEN*  
xEZ\~, *WEZ l*  
is a relaxation of our original problem  
\(IP\)  
This is true because any feasible solution to IP can be extended to a feasible solution to  
\(5.1\) of the same value by putting *w* = O. We now show how \(5.1\) can be formulated as a  
maximum-weight path problem from node 0 to node *CPk\(b\)* in the digraph \~k.  
We can eliminate *w* from the objective function by substituting  
*w* = *\(L ajxj* - *b\)/k*  
*jEN*  
and we have already shown how to describe the relaxed solution set S *k.* Hence \(5.1\) can be  
reformulated as the *group prob\{em*  
Table 5.1.  
Acyclic Paths  
\[romOto 3 *x*  
*e3\(0\), e3\(1\), e3\(2\) e3\(0\), e3\(1\), e4\(2\) e3\(0\), e4\(1\), e4\(2\) e4\(0\), e4\(1\), e4\(2\) e2\(0\), e2\(* 4\), *e3\(2\) e2\(0\), e2\(4\), e4\(2\)* \(0 0 3 0\)  
\(0 0 2 1\)  
\(0 0 1 2\)  
\(0 0 0 3\)  
\(0 2 1 0\)  
\(0 2 0 1\) 5. Modular Arithmetic and the Group Problem Table 5.2.  
315  
Simple Cycles *x*  
*el\(O\)* \(1 0 0 0\)  
*e2\(0\), e2\(4\), ei2\)* \(0 3 0 0\)  
*e2\(0\), e3\(4\), e3\(5\)* \(0 1 2 0\)  
*e3\(0\), e3\(1\), e3\(2\), e3\(3\), e3\(4\), e3\(5\)* \(0 0 6 0\)  
\(GP\) \(mod *k\)*  
xEZ\~.  
We use the term *group* because of the modulo addition in the constraint, which is  
equivalent to addition in the cyclic abelian group of order *k.*  
If c. - *\{3a'/k* \> 0, we can choose *x.* so that *¢k\(a.\)x.* = 0 and \(c. - *\{3a./k\)x.* is arbitrarily  
\} \} \}\}\}\} J \(  
large. Thus if *Sk* \* 0, we can choose *x* so that GP has an unbounded optima value. Hence  
we impose the condition *Cj* - *pa\) k* \~ 0 for\} E *N* or  
which implies that *zk\(b\)* \~ *Pb/k.* Thus we can restate GP as  
*zk\(b\)* = *ub* + max I *\(Cj* - *uaj\)xj*  
*jEN*  
\(mod *k\)*  
xEZ\~,  
where *u* = *P/k* and *PI* \~ *u* \~ *P2.* Note that *PI* \~ *u* \~ *P2* if and only if *u* is a dual feasible  
solution to the linear programming relaxation of \(5.1\). Moreover, the term *ub,* which is  
independent of *x,* is minimized by an optimal dual solution to the linear programming  
relaxation of\(5.1\); that is, *u* = *PI* if *b* \> O. With *u* = PI, we obtain  
*zk\(b\)* = zLP\(b\) - min I - *CjXj*  
*jEN*  
\(mod *k\)*  
xEZ\~,  
where *Cj* = *Cj* - *Plaj* \~ 0 for\} *EN. Thuszk\(b\)* \~ *zLP\(b\),* and we also observe that GPyields  
a minimum-weight correction to the linear programming relaxation subject to the  
constraint *x* E *Sk.* 316 II.3. Duality and Relaxation  
The correction term  
is the same for all *dE ¢,/\(b\).* Hence *zk\(b\)* is the sum ofalinearterm in *b* and a correction  
term that is cyclic with period *k.*  
An interesting observation is that *'11k\(d\)* is superadditive; that is, for any *d l*  
*, d2* E ZI we  
obtain  
The inequality holds because the left-hand side can be interpreted as the weight of a  
maximum-weight path from node 0 to node d l + *d2* that is constrained to contain node *dl •*  
*Example* 5.1 *\(continued\).* Suppose c = \(4 -4 1 0\). Then !9 = PI = *cdal* .s\:;;  
*P/6* .s\:;; *C2/a2* = *P2* = f.;. Since *b* \> 0, we choose *u* = *P/6* = PI and obtain the group problem  
*OXI* + *4X2* + *X3* + *X4* = 3  
\(mod 6\)  
*xEZ!.*  
To find a maximum-weight path from node 0 to node 3 in \~6, we can eliminate the loop  
arcs *eI\(d\)* since CI = 0, and we can also eliminate the *e3\(d\)* arcs since they are parallel to the  
*e4\(d\)* arcs and *C3* \< *C4.* This yields the digraph shown in Figure 5.2, where the number on  
each arc is its weight times 39.  
A path of maximum weight is *\(e4\(0\), e4\(1\), e4\(2»,* corresponding to the solution  
*x* = \(0 0 0 3\). Since *r.jEN ajxj* = 3, it follows that *x* \$. S. The weight of this path is  
*'l'6\(b\)* = -f9, so we obtain the upper bound of *z6\(b\)* = *zu.\(b\)* + *'l'6\(b\)* = \[2\(141\) - 61/39 = 7rr.  
-2  
-20  
Figure 5.2 5. Modular Arithmetic and the Group Problem 317  
5 Linear function ZLP *\(d\)*  
4 Step function Z6 *\(d\)*  
8  
7  
6  
3  
2  
\~---------------------------------------------------------d  
o 60 120 180  
Figure 5.3  
It is clear from the digraph of Figure 5.2 that, for any *d* E ZI, only the variable 4 arcs  
are used in a maximum-weight path from node 0 to node *¢6\(d\).* Thus *If/6\(d\)* = *-f9¢6\(d\)* for  
all *d* E ZI. The functions *ZLp\(d\)* and *z6\(d\)* are shown in Figure 5.3 for dE Zl.  
So far, the development of the group problem has been done for an arbitrary positive  
integer *k.* Now we consider a meaningful choice of *k* that is motivated by trying to  
enhance the possibility of an optimal solution to GP being feasible to IP.  
Suppose bE Zl and *cl/al* = *max\{cjlaj\: aj* \> *O,j EN\}.* With *k* = all OP can be  
restated as  
\(5.2\)  
*j* = 2, .,. , *n.*  
Note that any feasible solution to \(5.2\) yields an integer value for *Xl.* This is true since  
*LjEN\\\(!\) ajxj* = *ta* 1 + *¢alb\)* for some *t* E ZI. Thus  
*Xl* = J.- *\(b* - *L ajXj\)* = *b* - *¢a,\(b\)* - *t,*  
*a* 1 *jEN\\\(!\) a* 1  
which is integer for all bE ZI since *¢a,\(b* - *¢a,\(b\)* = O. However, *Xl* maybe negative. This  
should not be surprising since, in this case, \(5.1\) is 318  
11.3. Duality and Relaxation  
*za,\(b\)* = max *L CjXj-CIW*  
*jEN*  
*L ajxj* - *a* I *W* = *b*  
*JEN*  
which is the same as  
*za,\(b\)* = max *L CjXj*  
*JEN*  
*L ajxj* = *b*  
*jEN*  
*X I EZI ,Xj EZlforj=2,* ... *,no*  
When can we be sure that *LjEN\\\{l\} ajxj* will be small enough to guarantee *x* I ;. O? To  
answer this question, we use the fact that *Xj* for *j* = 2, ... , *n* is determined by solving a  
maximum-weight path problem on a graph with *a* I nodes and nonpositive weights on the  
arcs. Hence there is a maximum-weight path with no more than *a* I - 1 arcs; that is, the  
corresponding solution *\{xJ\}* satisfies LjEN\\\(l\} *xJ* \~ *a* I - 1. Thus  
where *7i* = *max\{aj\:j* E *N\\\{l\}\},* so that *x?;' \(l/al\)\(b* - *\(al* - *l\)a\).* Consequently if  
*b* ;. *\(a* I - *l\)a* and *k* = *a* \[, then GP yields an optimal solution to IP. Thus the relaxation GP  
is asymptotically exact in the sense that for suitably large *b* we obtain *za,\(b\)* = *z\(b\).*  
*Example* 5.1 *\(continued\).* Suppose *u* = *PI* = *ct/al* and *k* = *al* = 78. Here the digraph is  
too large to draw, but an optimal solution is easy to deduce. We have *¢78\(a2\)* = 10,  
*¢78\(aJ\)* = 37, *¢78\(a4\)* = 1, and *¢78\(b\)* = 63\. With *\(e2* \(\:3 *\(4\)* = *-i9\(20* 35 2\), an optimal  
solution is *xg* = 0, x\~ = 1, x\~ = 26, which yields  
*x?* = \~ *\(b* - *L* ajX\~\) = \~ \(141 - 37 - 26\) = 1.  
*a* I *jEN\\\(l\)· 78*  
Hence we obtain a feasible, and thus optimal, solution to IP given by *XO* = \(1 0 26\).  
Note that 141 \< 77·37 = *\(al* - *1\)a;* that is, the condition *b* ;. *\(al* - *l\)a* is by no means  
necessary for an optimal solution of the group problem to solve IP.  
The approach we have taken here generalizes straightforwardly to integer programs  
with more than one constraint. Consider an equality-constrained version ofIP\:  
\(IP\) max\{cx\: *Ax* = *b, x* E Z\~\},  
where *\(A, b\)* is an integral m x *\(n* + 1\) matrix and C is an integral *n-vector.* We relax IP to  
the so-called *group problem*  
\(GP\) *zK\(b\)* = max *cx* - *pw*  
*Ax* - *Kw* = *b* 5\. Modular Arithmetic and the Group Problem 319  
*whereK* = *\(kJ,* ... ,kp\)isan *m* X *p* integer matrix withp ..; *m* andpisap-vector. Our goal  
is to determine the canonical form or simplest possible path representation ofGP.  
Suppose that IP has bounded optimum value. To ensure that GP also has bounded  
optimum value, recall that a feasible integer program is bounded only if its linear  
programming relaxation is dual feasible. Thus we require that the dual of the linear  
programming relaxation ofGP has a feasible solution, that is, there exists a *u* E *R m* such  
that *uA* \~ c and *uK* = *p.* Given such a *u,* we can rewrite GP as  
*zK\(b\)* = *ub* + max\(c - *uA\)x*  
*x* E *SK\(b\),*  
where *SK\(b\)* = *\{x* E z\~\: *Ax* - *Kw* = *b* for some *w* E *ZP\}* and c - *uA* ..; O.  
To obtain a unique canonical representation of the maximum-weight path representa-  
tion ofGp, we use the Smith normal form of matrix *K* \(see Theorem 4.11 of Section 1.7.4\),  
which is stated here in greater generality. We say that a square integral matrix *R* is  
*unimodular* if I det *R* I = 1.  
Theorem 5.1. *\(Smith Normal Form\). Given an m* x *p integer matrix K of rank p* ..; *m,*  
*there exist unimodular integer matrices Rand* C, *where R is m* x *m and* C *is p* x *p such that*  
*RKC* = \~. *Matrix* \~ *is of the form Oij* = 0 *for all i* \* *j and i* \> *p; and the elements Ou* = *oJor*  
*i* = 1, ... *,p, are positive integers such that 0; is a divisor of 0;+1 for i* = 1, ... *,p* - 1.  
*Moreover, matrix* \~ *is unique.*  
For dE *zm,* let *¢fi\(d\)* = *d,* where *d;* = *d;\(mod 0;\)* for *i* = 1, ... *,p,* and *d;* = *d;* for *i* \> *p.*  
We can now give a canonical form of S\~b\) and, hence, ofGP.  
Theorem 5.2. *A canonical representation ofSK\(b\) is given by\:*  
*that is,for i* = 1, ... , *p, the ith equation must be satisfied* mod *Ob andfor i* \> *p the equation*  
*is an ordinary equality.*  
*Proof* We have  
*SK\(b\)* = *\{x* E Z\~\: *Ax* - *Kw* = *b* for some wE *ZP\}*  
*= \{x* E Z\~\: *RAx* - *RKw* = *Rb* for some *w* E *ZP\}*  
since *R* is a nonsingular matrix.  
Now if *Cw'* = *w,* where C is a unimodular integer matrix, then *w* E *ZP* if and only if  
*w'* E *ZP.* Hence  
*SK\(b\)* = *\{x* E Z\~\: *RAx* - *RKCw'* = *Rb* for some *w'* E *ZP\}*  
*= \{x* E Z\~\: *RAx* = *Rb* + \~w' for some *w'* E *ZP\}*  
*= \{x* E Z\~\: *¢fi\(RA\)x* = *¢fi\(Rb\)* \(mod *M\}.* • 320 11.3. Duality and Relaxation  
Therefore GP can be stated as  
*zK\(b\)* = *ub* + max *L \(Cj* - *uaj\)xj*  
*jEN*  
\(GP\)  
\(mod .1\)  
xEZ\~.  
To make each of the *m* equations modular, we choose *K* to be *m* x *m* and nonsingular.  
Then .1 is an *m* x *m* diagonal matrix with *6i* E Z\~ for *i* = 1, ... , *m* and n7!l *6i* = 1 det *K I.*  
Corollary 5.3. *I! K is an m* x *m nonsingular integer matrix,* GP *is a maximum-weight*  
*path problem on a digraph with* n;\:; 1 *6i* = 1 det *K* 1 *nodes.*  
Here *qjJK* = *\(V K, sl/x\),* where  
*VK* = *\{cp",,\(d\)\: dE Rm\}* = *\{d* E *Z'\{'\: di* \< *6i* for *i* = 1, ... *,m\}* and  
*.sdK* = *\{\(d, cp",,\(d* + *Raj»\: dE VK,j* EN\}.  
Note that if *6i* = 1, the ith modular equation holds trivially for all *x* E Z\~ and can be  
omitted from the formulation. Correspondingly, in the digraph *qjJK,* if *6i* = 1, then *di* = 0 for  
all *d* E *VK •* In particular, if *6m* = 1 det *K* I, we obtain a cyclic group as in the case of a single  
constraint problem.  
By choosing *u* = *uo,* an optimal solution to the dual of the linear programming  
relaxation of IP, we obtain the minimum value of *ub* = *uOb* = *hp\(b\).* In this case,  
*zK\(b\)* = *hp\(b\)* + *If/K\(b\),* where  
*If/K\(b\)* = -min\{ *L* - *\(Cj* - *uOa\)xj\: x* E *SK\(b\)\}.*  
*jEN*  
So as before, *zK\(b\)* \~ *zLP\(b\)* and GP yields a minimum-weight correction to the linear  
programming relaxation subject to *x* E *SK.* We also see that *If/K\(d\)* is a cyclic function; that  
is, *If/K\(d\)* = *If/K\(d* + *Kw\)* for all *d, wE zm.*  
By using the same argument as in the single constraint case, we obtain the following  
corollary.  
Corollary 5.4. *If/K\(d\) is superadditive!or dE zm.*  
If, in addition, we choose *K* as an optimal basis matrix for the linear programming  
relaxation of IP, we may enhance the possibility of obtaining a feasible solution to IP.  
Suppose *A* = *\(AB , AN\)* \(where *AB* is *m* x *m* and nonsingular\), *x* = *\(XB, XN\),* and  
C = *\(cB, CN\)' LetK* = *AB* and suppose that *A lilb* \~ 0 andcN - *cBAlilAN* \~ O. If.1 is the Smith  
Normal Form *ofAB ,* then GP is  
\(5.3\)  
*ZAB\(b\)* = *zLP\(b\)* + max\(cN - *cBAlilAN\)XN*  
*cp",,\(RAN\)XN* = *cp",,\(Rb\)* \(mod .1\)  
Problem \(5.3\) is the group problem originally considered by Gomory. Note that if x\~ optimal solution to \(5.3\), it yields an optimal solution to IP if x\~ = *Alil\(b* - ANX\~\) is an  
\~ O. 5. Modular Arithmetic and the Group Problem 321  
We have seen that choosing *K* = *AB* is equivalent to dropping non negativity on *XB,*  
which leads to the following corollary.  
Corollary 5.5. \(5.4\)  
*The group problem* \(5.3\) *is equivalent to the integer program*  
*ZAB\(b\)* = max *CBXB* + *CNXN*  
*ABxB* + *ANxN* = *b*  
*where AB is an optimal basis matrixfor the linear programming relaxation ofIP.*  
Note that a Gomory group problem can be generated from any dual feasible basis  
matrix *AB* and that Corollary 5.5 holds for any such basis. Dual feasibility \(i.e.,  
*CN* - *cBAliA N* \~ 0\) is necessary to *boundzA.\(b\)* from above.  
Gomory began with problem \(5.4\) and derived the canonical form \(5.3\). The motiva-  
tion for considering problem \(5.4\) is that at an optimal solution to the linear programming  
relaxation ofIP, the constraints *XB* \~ 0 are inactive-if there is degeneracy they may be  
tight. Thus, there is some hope that they will be inactive in an optimal solution to IP. In fact  
when *b* is "suitably large", this is true. This asymptotic behaviour of GP is given by the  
following proposition.  
Theorem 5.6. *Let AB be any dualfeasible basis to the linear programming relaxation ofIP*  
*and let \(f\)* = *max;,j* I *\(A"BIAN\)lj* I. *The group problem* \(5.3\) *defined by AB solves* IP *for all*  
*b* E *zm such that A"B' b* \~ *\(f\)* I det *AB* 11, *where 1 is the vector of aliI* s. *An optimal solution to*  
IP *is given by* x\~ = *A"Bl \(b* - ANX\~ *\)for some* x\~ *that is an optimal solution to \(5.3\).*  
*Proof* It suffices to show that there exists an optimal solution *xRr* to \(5.3\) such that  
x\~ = *A"Bl\(b* - *ANxRr\)* \~ O. Since GP is a maximum-weight path problem on a digraph with  
I det *AB* I nodes, it follows that there is an optimal solution *xRr* such that *Lj xRrj* \< I det *AB I.*  
Hence *A "BIA NXRr* \< *\(f\) IdetAB* 11. Since, by assumption, *A "BIb* \~ *\(f\)* Idet *AB* 11, itfollows that  
x\~\~O. *•*  
*ExampleS.2*  
max 7Xl + *2X2*  
-Xl + *2X2+ X3* 2  
5xl+ *X2 +X4* 19  
*-2x,- 2X2 +Xs* =-5  
xEZ\~.  
An optimal LP basis is given by *AB* = *\(al. a2, as\),* and *u* = *cBA"B'* = \(n- It is readily checked that the Smith Normal Form of *AB* is  
# o  
1  
# o  
-If 0\). 322 11.3. Duality and Relaxation  
with  
*c* 0 D and c\~O 0 D *R* = -\~ 0 0  
We have  
RA\~ \(\~ -2 -1 0 D and Rb\~\(;n -6 -2 0  
11 5  
Since t51 = t52 = 1, only the bottom row of *R \(A, b\)* is needed to define the group problem.  
Because c - *uA* = \(0 0 --rl- -if 0\), the group problem is  
*5x3* + *X4* = 7  
*\(X3' X4\)* E Z\~.  
\(mod 11\)  
This problem can be solved on a digraph with 11 nodes, and the optimal solution is x\~ x\~ = O. Thus  
= 8,  
x\~= ;\~ *=Ajib-8Ajia3=TI* ;\~ -TI \~ = -\~ .  
*\( X?* \) 1 \(36\) 8 \(-1\) \(4 \)  
Since x\~ \< 0, it follows that *XO* is not feasible to IP. Had we taken *b* = \(.\:\~\), obtained the same group problem. But in this case we have  
1 \(36\) 8 \(-1\) \( 4 \)  
x\~ = *Aji\(b* - *8a3\)* = TI \~\~ - TI \~ = \~ \~ 0,  
we would have  
so *XO* is an optimal solution to the original problem.  
Dropping nonnegativity on the basic variables yields the problem offinding an optimal  
integral solution in the cone generated by the active constraints. This is shown graphically  
in Figure 5.4, where *Xl* = 4, *X2* = -1 is the optimal integral point in the "cone" defined by  
the constraints *-Xl* + *2X2* \~ 2 and *5xl* + *X2* \~ 19.  
An interesting application of the asymptotic behavior of GP is that it can be used to  
show that by solving a finite number of group problems, *zIP\(b\)* can be obtained for all but  
a finite number of points in *zm.* Hence, as we remarked in Section 3, the complete value  
function of IP can be found by solving a finite number of integer programs. 6. **Lagrangian Relaxation and Duality**  
# 323  
• • • \\-0  
2  
# •  
*5xl+X2\:s19*  
• • • •  
01----....... ---\_------11....----+ .......  
-----Xl  
2 3  
-1 • • •  
• • • •  
**Figure** 5.4\. The optimal solution is *Xl* = 4, *X2* =-1.  
# 6\. LAGRANGIAN RELAXATION AND DUALITY  
Consider an integer program  
ZIP = max\{cx\: xES\}, where S = *\{x* E Z\~\: *Ax* \~ *b\},*  
which can be rewritten as  
ZIP = max *ex*  
AIX\~bl  
\(IP\)  
\(complicating constraints\)  
\(nice constraints\)  
xEZ\~,  
where *A* = \(\~\:\) and *b* = *\(g\:\).* We suppose *thatA 2x* \~ *b2* are *m* - ml "nice constraints", say  
those of a network problem. By dropping the *m* 1 complicating constraints *Al X* \~ *b* 1 we  
obtain a relaxation that is easier to solve than the original problem. There are many  
problems for which the constraints can be partitioned in this way. We will give some  
examples later. 324 11.3. Duality and Relaxation  
The idea of dropping constraints can be embedded into a more general framework  
called *Lagrangian relaxation.* It is convenient to consider a generalization oflP\:  
ZIP = max *ex*  
IP\(Q\)  
A1X\~bl  
*xEQ.*  
However, when we are discussing results that are specific to IP, it is assumed that  
*Q* = *\{x* E Z\~\: *A 2X* \~ *b2\}* oF 0. Again it has to be understood that the problem obtained  
from IP\(Q\) by dropping the complicating constraints is much easier to solve than IP\(Q\).  
Now for any *A* E *R'\:",* consider the problem  
LR\(A\) ZLR\(A\) = max\{z\(A, *x\)\: x* E *Q\},* where *Z\(A, x\)* = *ex* + *A\(b l*  
- *A* IX\).  
The problem LR\(A\) is called the *Lagrangian relaxation* of IP\(Q\) with respect to  
*A* IX \~ *b* 1. This terminology is used because the vector *A* plays a role in LR\(A\) similar to the  
role of Lagrange multipliers in constrained continuous optimization problems.  
LR\(A\) does not contain the complicating constraints. Instead we have included these  
constraints in the objective function with the "penalty" term *A\(b l*  
- *A* IX\). Since *A* \~ 0,  
violations of *A* IX \~ bl make the penalty term negative, and thus intuitively *A* IX \~ bl will  
be satisfied if *A* is suitably large.  
Proposition 6.1. LR\(A\) *is a relaxation ofIP\(Q\)for all A* \~ o.  
*Proof* If *x* is feasible in IP\(Q\), then *x* E *Q* and hence *x* is feasible for LR\(A\). Also,  
*,.Z\(A, x\)* = *ex* + *A* \(b l - *A* IX\) \~ *ex* for all *x* feasible in IP\(Q\) since *A* IX \~ bl and *A* \~ o. •  
As a consequence of Proposition 6.1, ZLR\(A\) \~ ZIP for all *A* \~ O. The least upper bound  
available from the infinite family of relaxations \{LR\(A\)h?o is ZLR\(A\*\), where *A\** is an optimal  
solution to  
\(LD\)  
Problem LD is called the *Lagrangian dual* of IP\(Q\) with respect to the constraints  
A1X\~bl.  
The following example from Section 11.1.1, but with the constraints Xl \~ 2, *X2* \~ 4  
added, will be used throughout this section to illustrate the concepts and results presented.  
*Example* 6.1  
max 7Xl + *2X2*  
-Xl + *2X2* \~ 4  
5Xl+ *X2* \~ 20  
-2X l- *2X2* \~ -7  
\~ -2  
*X2* \~ 4  
*xEZ;.* 6\. Lagrangian Relaxation and Duality The Lagrangian relaxation with respect to -Xl + *2x* 2 \~ 4 is  
325  
max\(7 + *A\)Xl* + \(2 - *2A\)X2* + *4A*  
5Xl + *X2* \~ 20  
-2Xl - *2X2* \~ -7  
XEZ\~,  
where Q is the finite set of points  
\(see Figure 6.1\).  
The example suggests at least two different viewpoints. The first is to view  
*Z\(A, x\)* = *\(c* - *M* l\)X + *Ab 1* as an affine function *ofx* for *A* fixed. It then follows that *hR\(A\)*  
can be determined by solving the linear program  
*ZLR\(A\)* = *max\{z\(A, x\)\: x* E conv\(Q\)\},  
4  
3  
11=0  
2  
11=2  
o\~------------------------\~--\~------ o 2  
Figure 6.1 326 11.3. Duality and Relaxation  
where as usual we assume that conv\(Q\) is a rational polyhedron.  
In the example \(see Figure 6.1\),  
conv\(Q\) = *\{x* E R\~\: *-Xl';;;* -2, *X2* .;;; 4, *-Xl* - *X2* .;;; -4, *4Xl* + *X2* .;;; 16\}.  
Thus  
hR\(O\) = max\{7xl + *2X2\: X* E conv\(Q\)\} = *z\(O, x 7\)* = 29  
*ZLR\(1\)* = max\{8xl + *OX2* + 4\: *X* E conv\(Q\)\} = *z\(1, X8\)* = 36;  
and as one increases A from 0, *ZLR\(A\)* first decreases until A = \<!- and then it increases. In  
general we obtain  
1  
*ZLR\(A\)* = *Z\(A, x 7\)* = 29 - A for 0.;;; A .;;; 9  
*ZLR\(A\)* = *Z\(A, X8\)* = 28 + 8A 1  
for A? 9.  
Hence *Zw* = *ZLR\(\<!-\)* = *z\(\<!-, x 7\)* = *z\(\<!-, X8\)* = 28\~ and A\* = \<!-.  
All of these calculations can be seen in Figure 6.1, where we have shown the objective  
function max\(c - *AA I\)X* + *Ab l* for different values of A.  
*z\(\>., xi\)*  
36  
30  
24  
\~r-----------\~\~ .. -e\~--------------------------2  
\~=-------------------------------------------------\>.  
2  
Figure 6.2. The numbers assigned to each line denote the following; \(I\) 18 + *2.1;* \(2\) 20; \(3\) 22 - *2.1;* \(4\) 23 + *5.1;*  
\(5\) 25 + *3.1;* \(6\) 27 + *A;* \(7\) 29 - *A;* \(8\) 28 + *8.1.* 6\. Lagrangian Relaxation and Duality 327  
The second viewpoint is to consider *ZLR\(A\)* to be determined by maximization over a set  
of discrete points, that is,  
*ZLR\(A\)* = max *Z\(A,* xt  
*x'EQ*  
and to observe that for fixed *Xi, Z\(A, Xi\)* = *exi* + *A\(b l* - *A IX* i\) is an affine function of *A.* See  
Figure 6.2, where we have drawn the affine functions *Z\(A, Xi\)* for *Xi* E Q.  
In Figure 6.2 one can read off the value of *ZLR\(A\)* for any value of *A.* We see that *hR\(A\)* is  
piecewise linear and convex \(the heavy lines in Figure 6.2\) and that *Zw* = 28\~.  
Formally, one solves the linear program  
*ZLR\(A\)* = min\{w\: *w* \~ *Z\(A, Xi\)* for i = 1, ... , 8\},  
which shows that *ZLR\(A\)* is the maximum of a finite number of affine functions and is  
therefore piecewise linear and convex.  
We now study how the solution of the Lagrangian dual relates to the solution of the  
original problem IP\(Q\). Returning to Figure 6.1, note that when *A* = *t* we obtain  
28\~ = *z\(t, X* 7\) = *z\(t, x 8\)*  
*= z\(§,* \~X7 + *§x8\)* since *Z\(A, x\)* is affine in *x*  
*= z\(§J* \(34\) + § \(4 0\)\)  
*= z\(§,* \(ZJ! Jj\)\) = *z\(§, x\*\)* with *x\** = \(ZJ! Jj\)  
*= ex\** + §\(4 + x\~ - *2x;\)*  
*= ex\** since *x\** satisfies *-x* I + *2X2* = 4.  
In other words, by taking a convex combination of points in Q, in the example *x7* and  
*x 8*  
*,* we obtain a point *x\** in conv\(Q\) satisfying the complicating constraint, for which  
*ex\** = *Zw.* This shows that for the example we obtain  
*Zw* = max\{ex\: *A IX* \~ *b* I, *x* E conv\(Q\)\}.  
We now formalize the results suggested by the example. The major result is that the  
primal linear programming problem offinding a convex combination of points in Q that  
also satisfy the complicating constraint *A* I *X* \~ *b* I is dual to the Lagrangian dual.  
**Theorem** 6.2. *Zw* = max\{ex\: *A IX* \~ *bl*  
*Proof*  
*, x* E conv\(Q\)\}.  
= max \(c - *AA I\)X* + *Ab* I  
xEconv\(Q\)  
\(since the objective function is linear\)  
= max *\[ex* + *A\(b l*  
- *A IX\)\].*  
xEconv\(Q\) 328 11.3. Duality and Relaxation  
Hence  
= min max *\[cx* + *A\(b l* - *A* IX\)\].  
",,0 xEconv\(Q\)  
If Q = 0 the inner max equals -00 for all A. Hence ZLD = -00 as desired. Otherwise, let  
*\{xk* E R\~\: *k* E *K\}* and *\{ri* E R\~\: *j* E *J\},* respectively, be the sets of extreme points and  
extreme rays of conv\( *Q\).* Thus  
I I *\{ooif\(C-MI\)ri\>O forsomejEJ*  
x\~\~v1Q\) *\[cx* + *A\(b* - *A x\)\]* = *cxk* + *A\(b l*  
\_ *A IXk\)* for some *k* E *K* otherwise.  
Hence  
A\~O,  
which can be restated as  
ZLD = min 17  
\~,A  
\(6.1\)  
\~ *cri forj* E *J*  
A\~ O.  
Thus by linear programming duality, we obtain  
*ZLD* = max c\( I *o/xk* + I *piri\)*  
*kEK* iE!  
= 1  
\(6.2\)  
*uk, pi* \~ 0 for *k* E *K* and *j* E *J*  
= max\{cx\: *A* IX \~ *b l*  
*, X* E conv\(Q\)\}.  
# •  
Corollary 6.3. ZLD *can be calculated/rom the linear programs* \(6,1\) *or \(6.2\).*  
The reader familiar with linear programming decomposition will recognize the linear  
program \(6.2\) as the reformulation obtained when Dantzig-Wolfe price decomposition is  
applied to 6\. Lagrangian Relaxation and Duality 329  
where conv\( *Q\)* = *\{x* E R\~\: *A 2X* \~ *b2\}* and *A* are the "dual prices" associated with the  
constraints *A IX* \~ *b l*  
*.* It follows that \(6.1\) is the dual of the Dantzig-Wolfe reformulation.  
Alternatively, \(6.1\) is the reformulation obtained by applying resource or Benders'  
decomposition to the dual linear program  
\(see the next section\).  
Corollary 6.4. ZLR\(A\) *is piecewise linear and convex on the domain over which it isjinite.*  
*Proof* ZLR\(A\) is finite if and only if *A* lies in the polyhedron *\{A* E *R';"\: AA* I *rj* \~ *crj* for  
*j* E *J\}.* On this polyhedron, ZLR\(A\) = *Ab l* + maXkEK \(c - *AA I\)Xk* and is the maximum ofa  
finite number of affine functions. Convexity follows from Proposition 4.1 of Section I.2.4 .  
# •  
Since  
we have that  
ZIP = max *cx* \~ ZLD = *max\{cx\: A IX* \~ *bl*  
*xES*  
*, X* E conv\(Q\)\}.  
The duality gap ZLD - ZIP depends on the relative sizes of conv\(S\), conv\(Q\) n  
*\{x\: A IX* \~ *bl\},* and the objective coefficients c.  
Corollary 6.5. ZIP = *zLDfor all* c *if and only if*  
When *Q* = *\{x* E Z\~\: *A 2x* \~ *b2\},* it is also of considerable interest to compare ZLD with  
ZLP = *max\{cx\: Ax* \~ *b, x* E R\~\}. In Example 6.1, ZIP = 28 \< ZLD = 28\~ \< *ZLP* = 30n-.  
Corollary 6.6. ZLD = ZLP *for all* c *if all the extreme points of \{x* E R\~\: *A2x* \~ *b2\} are*  
*integral.*  
*Proof* Under the hypothesis of the corollary we obtain  
*\{x* E R\~\: *A 2x* \~ *b2\},* and the result follows from Theorem 6.2.  
conv\(Q\) =  
# •  
In Example 6.1, a natural choice of the "complicating constraints" is  
Thus  
Obviously *\{x* E *R;\: -XI* \~ -2, *X2* \~ 4\} only has integral extreme points, so that, by Corol-  
lary 6.6, this Lagrangian relaxation would terminate with ZLD = ZLP = 30-fi-. 330 11.3. Duality and Relaxation  
In summary,  
conv\(S\) s conv\(Q\) n *\{X* E R\~\: *A IX* \~ *b l \}* S *\{X* E R\~\: *Ax* \~ *b\}*  
and thus *ZIP* \~ *Zw* \~ *ZLP.* But because some faces of the respective polyhedra can coincide,  
we may obtain *ZIP* = *Zw* or *Zw* = *ZLP* for particular c even if the conditions of the two  
previous corollaries do not hold. Figure 6.3 illustrates this. The inner polytope is conv\(S\).  
The outer polytope is *\{x* E R\~\: *Ax* \~ *b\}.* The inner polytope, together with the shaded  
region, is conv\(Q\) n *\{x* E R\~\: *A IX* \~ *bl\}.* Four different objective functions are indicated,  
and the results are summarized as follows\:  
Objective Functions  
cl  
c2  
c3  
c4  
Objective Values  
ZIP = ZLD = ZLP  
ZIP \< ZLD = ZLP  
ZIP \< ZLD \< ZLP  
ZIP = ZLD \< ZLP  
It is possible to characterize problems where *ZIP* = *Zw* in terms of a complementarity  
condition. We will obtain this result as a corollary to the following theorem.  
Theorem 6.7. *ZIP* \~ *Zw* - e *if and only if there exists A\** \~ 0 *and x\** E S *such that*  
*A\*\(bl*  
- *A IX\*\)* \~ oj, *Z\(A\*,X\*\)* \~ *ZLR\(A\*\)* - *02, and 01* + *02* \~ e.  
*Proof* To show sufficiency, we have  
\~ *ZIP* + e \(since *x\** E *S\).*  
# /11  
/ I  
# / I  
# / I  
**//** \)  
# ,  
# I  
# I  
Figure 6.3 6\. Lagrangian Relaxation and Duality 331  
To show necessity, let *x\** be an optimal solution of IP\(Q\) and let\},,\* be an optimal  
solution ofLD. We have  
ZLD = ZLR\(\}"\*\) = *z\(\}"\*, x\*\)* + ZLR\(\}"\*\) - *z\(\},,\*, x\*\)*  
*= cx\** + \}"\*\(b l - *A* lX\*\) + hR\(\},,\*\) - *z\(\},,\*, x\*\)*  
= ZIP + \}"\*\(b l - *A* lX\*\) + ZLR\(\}"\*\) - *z\(\}"\*, x\*\).*  
Hence ZIP \~ ZLD - *e* implies  
\}"\*\(bl - *A* lX\*\) + \(ZLR\(\},,\*\) - *z\(\},,\*, x\*\)\)* .\:;; *e.*  
# •  
By putting 61 = 62 = *e* = ° in Theorem 6.7, we obtain necessary and sufficient condi-  
tions for the duality gap to be 0.  
Corollary 6.8. ZIP = ZLD *if and only if there exists* \}" \* \~ ° *and x\** E S *such that*  
\}"\*\(bl - *A* lX\*\) = ° *and* ZLR\(\}"\*\) = *z\(\},,\*, x\*\).*  
Theorem 6.7 can also be helpful in identifying \(nearly\) optimal solutions to IP\(Q\). For  
example, in the process of solving LR\(\}"\) we may find an *xES* that is nearly optimal in  
LR\(\}"\) and nearly satisfies complementary slackness.  
Corollary 6.9. *Ifx\** E S *satisfies* \}"\(bl - *A* lX\*\) .\:;; 61 *and z\(\}", x\*\)* \~ *ZLR\(\},,\)* - 62 *for some*  
\}" \~ 0, *then cx\** \~ ZIP - 15 1 - 152•  
InExample6.1,x6 = \(33\) E S. ForA = *Ts,* weobtainzLR\(\},,\) = 28t!, \}"\(b l - *A* lX6\) = *Ts* = 610  
and *ZLR\(\},,\)* - *z\(\}", x 6\)* = l\~ = 62. Hence *cx6* \~ ZIP - It!.  
The complementary slackness conditions are also useful in right-hand-side parametrics  
as shown in the following corollary to Theorem 6.7.  
Corollary 6.10. *Let x\* be an optimal solution to* LR\(\}"\*\), *where* \},,\* \~ 0, *and define*  
*d\** = *A* lX\*. *Then x\* is an optimal solution to*  
max\{cx\: *A* IX'\:;; *d l*  
*, X* E *Q\}*  
In Example 6.1, *x7* = \(3 4\) is an optimal solution to LR\(i\\i\). Hence *x7* is an optimal  
solution when the first constraint is -Xl + *2X2* .\:;; 5.  
Lagrangian relaxation and duality also apply to equality constraints. Suppose that  
*A* IX = b l in Problem IP\(Q\). Then defining LR\(\}"\) as before, we have the following  
proposition.  
Proposition 6.11. *If A* IX = *bl in* IP, *then* LR\(\}"\) *is a relaxation ofIP for all* \}" E *Rml.*  
The only difference between the equality and inequality cases is that in the equality case  
the multipliers are unrestricted in sign.  
We now give one problem to illustrate the formulation of Lagrangian relaxations.  
Others will be given later when we discuss computation. 332 11.3. Duality and Relaxation  
*Example* 6.2 *\(A Flow Problem with Budget Constraint\).* Suppose there is a set of *n* jobs  
to be assigned to a set of *n* workers, with *N* = \{l, ... *,n\}.* Suppose that *cij* is the value of  
assigning worker *i* to job *j,* that *t* ij is the cost of training worker *i* to do job *j,* and that we  
have a training budget of *b* units. We wish to maximize the total value of the assignment  
subject to the budget constraint, that is,  
max 2\: 2\: *cijxij*  
iEN *jEN*  
*jEN*  
iEN  
*2\: x ij* = 1 *fori* E *N*  
\(1\)  
*2\: xij* = 1 for *j* EN  
\(2\)  
\(3\)  
First we observe that the problem is .N'9P-hard. Ifwe then wish to choose a Lagrangian  
relaxation, there are four options to consider. Note that in each option the relaxed  
problem LR\(A\) is considerably easier to solve than the original problem.  
*1. Lagrangian relaxation with respect to* \(3\). Then LR1\(A\), *A* E *Rl,* is an assignment  
problem with objective function  
*2. Lagrangian relaxation with respect to* \(1\) *and* \(2\). Then *LR2\(u, v\), U ERn, vERn,* is  
a knapsack problem with objective function  
*2\: Ui* + 2\: *Vj* + 2\: 2\: *\(Cij* - *Ui* - *Vj\)Xij.*  
iEN *jEN* iEN *jEN*  
*3. Lagrangian relaxation with respect to* \(1\) *or* \(2\), *say* \(1\). Then *LR3\(U\), U ERn,* is a  
knapsack problem with generalized upper-bound constraints and objective function  
*2\: Ui* + 2\: 2\: *\(Cij* - *Ui\)Xij.*  
iEN iEN *jEN*  
*4. Lagrangian relaxation with respect to* \(1\) *or* \(2\) *and* \(3\), *say* \(1\) *and* \(3\). Only  
generalized upper-bound constraints remain. Thus the Lagrangian L\~\(u, *A\),*  
*U ERn, A* E *Rl,* with objective function  
*Ab* + 2\: *Ui* + 2\: 2\: *\(Cij* - *Ui* - *Atij\)Xij*  
iEN iEN *jEN*  
is trivial to solve. For each *j,* an *i* is chosen to maximize *cij* - *Ui* - *Atij,* and the  
corresponding *x* ij is set to 1.  
In choosing a relaxation there are two major questions to consider\: How strong is the  
bound *zw,* and how difficult to solve is the Lagrangian dual \(LD\)? We defer discussion of  
the latter question until we discuss computation, and now we just consider the bounds. 6\. Lagrangian Relaxation and Duality 333  
When *Q* is a set of assignment constraints or a set of generalized upper-bound  
constraints, Corollary 6.6 applies and *zLo* = *zto* = ZLp. Since  
C *Q2* = *\{x* E *En'\:* I I *ti\}Xi\}';; b\}*  
*iEN JEN*  
and  
conv\(Q2\) C *\{x* E *R\:'\:* I I *ti\}Xi\}';; b, Xi\}';;* 1 for *i,j EN\},*  
*iEN JEN*  
we have  
and each of the inequalities is strict for some objective function.  
We now consider two ways of strengthening the Lagrangian dual of problem IP. The  
first approach yields a dual whose optimal value equals  
This dual is obtained by applying Lagrangian duality to the reformulation ofIP given by  
ZIP = maxcxl  
\(RIP\)  
*AIXI* .;; *b* I  
*A 2X2* .;; *b 2*  
Xl *\_X2* = 0  
Xl E *Z\:, X2* E *Z\:.*  
Taking Xl - *x2 \:\:=* 0 as the complicating constraints, we obtain the Lagrangian dual of  
RIP\:  
*zcso* = min \{max\{\(c - *u\)xl* + *ux2\}\}*  
*u*  
Xl E *Z\:, X2* E Z\:  
= min \{max CIXI + max *C2 X2\}*  
*C I+C2=C*  
*AIXI';;b l*  
*, A2X2.;;b2*  
xIEZ\~, *x2EZ\:,*  
where *u* = *c2•* 334 11.3. Duality and Relaxation  
From Theorem 6.2, we obtain a polyhedral interpretation of the dual.  
**Corollary 6.12**  
*and* Zcso \~ *Zw.*  
We have used the terminology CS since the technique has been called *cost splitting.* The  
technique is useful when\:  
1. conv\{x E z\~\: *A IX* \~ *bl\}* C *\(x* E R\~\: *A IX* \~ *bl\},* so for some objective functions *e* we  
obtain *Zcso* \< *Zw.*  
2. The sets of constraints *A iX* \~ *bi* are simple to deal with separately; that is, the  
difficulty is caused by their interaction.  
In Example 6.2, we could take *A IX* \~ *b l* to be constraint set \(1\) and \(3\) and take  
*A 2x* \~ *b2* to be constraint sets \(2\) and \(3\). This yields Zcso \~ ZLo with the inequality strict  
for some objective functions.  
Another approach that dominates the Lagrangian dual is the "surrogate" dual. Starting  
from IP\(Q\), with weights *A* E *R';"* for the complicating constraints, consider the problem  
SD\(A\)  
The problem SD\(A\) is called the *surrogate relaxation* ofIP\(Q\) with respect to *A IX* \~ *bl*  
*.*  
SD\(A\) contains a single "complicating" constraint. For instance, when *Q* = Z\~ the surro-  
gate relaxation is a knapsack problem. The *surrogate dual* ofIP\(Q\) is the problem  
\(SD\)  
**Proposition** 6.13\. LR\(A\) *is a relaxation ofSD\(A\)for A* \~ 0 *and Zw* \~ *Zso.*  
*Proof* is feasible in SD\(A\) we obtain *A\(b l*  
The feasible region ofSD\(A\) is contained in that ofLR\(A\). In addition, when *x*  
- *A IX\)* \~ 0 and hence  
*Z\(A, x\)* = *ex* + *A\(b l* - *A IX\)* \~ *ex.* •  
Although the surrogate dual can be used computationally, it does not have such nice  
theoretical properties as the Lagrangian dual.  
We close this section by relating Lagrangian duality to the general duality theory of  
Section 2. Given the initial problem IP\( *Q\),* we define its value function *zQ* by  
for all *d l* E *Rm,.* Note that when *Q* = *\(x* E Z\~\: *A 2X* \~ *b2\},* it follows that *ZQ* is a projection  
of the **IP** value function *Z* onto *d2* = *b2•* Thus *zQ\(dl\)* = *z\(dl, b2\)* for all *d l* E *Rm,.* Now using  
a similar approach to that of Section 2, with *SQ\(dl\)* = *\(x* E *Q\: A IX* \~ *d l\}* in place of *Sed\),*  
we obtain as the equivalent of\(2.4\) the dual problem 6\. Lagrangian Relaxation and Duality  
min *g\(b l \)*  
\(6.3\) *g\(Alx\)?cx forxEQ*  
g non decreasing, g\: *Rm,* --\> *R I.*  
**Example 6.1 \(continued\).** The dual problem is  
*w* = min *g\(4\)*  
*g\(-xI* + *2X2\)? 7xI* + *2X2* for *x* E *Q*  
g nondecreasing, g\: *R* I .... *R* I  
335  
or  
min *g\(4\)*  
*g\(2\)* ? 18, Xl = \(2 2\)  
*g\(4\)* ? 20, *x 2* = \(2 3\)  
*g\(6\)* ? 22, *x 3* = \(2 4\)  
*g\(-I\)* ? 23, *X4* = \(3 1\)  
g\(l\) ? 25, *x 5* = \(3 2\)  
*g\(3\)* ? 27, *x 6* = \(3 3\)  
*g\(5\)* ? 29, *x 7* = \(3 4\)  
*g\(-4\)* ? 28, *x 8* = \(4 0\)  
g nondecreasing.  
It is readily seen from Figure 6.4 that  
if dl \<-4  
if -4.\:;; d l \< 5  
if d l ? 5.  
Since *zQ\(O\)* = *z\(O, b2\),* we cannot expect *zQ\(O\)* = 0\. Hence the simplest class of functions  
that are candidates for the dual \(6.3\) are affine functions *g\(dl \)* = An + *Adl*  
*, A* E *R';".* In  
particular, if we take g to be the affine function supporting *ZQ* and passing through the  
points \(-4 28\) and \(5 29\) \(see Figure 6.4\), then g is clearly dual feasible and  
*g\(4\)* = 28\~ = *Zw.*  
This leads us to examine the restricted dual  
WOR = min *g\(b l \)*  
\(6.4\) *g\(A IX\)? cx* for *x* E *Q*  
g affine and nondecreasing, g\: *Rm,* .... *R* I,  
which can be rewritten as 336 11.3. Duality and Relaxation  
32  
*g\(dl \)*  
*xl*  
28  
X B  
*.X\*=X6*  
*• x 5*  
24  
.x4  
20 *• x 2*  
*·x3*  
*• xl*  
16 *dl*  
-4 -2 0 2 4 6  
Figure 6.4  
When *Q* = *\{x* E Z\~\: *AZx* \~ bZ\}, it is also interesting to examine the dual \(2.4\) in *Rm,*  
where we restrict the dual functions *g\: R m*  
*.... RI* to be of the form *g\(dl*  
*,* dZ\) = *\)"dl* + *gz\(dZ \)*  
with dZ E *Rm-m"* and\)" E *R';".* This gives the alternative dual  
\(6.5\) *Wo* = min *\)"b l* + *gz\(bZ\)*  
*AA IX* + *gz\(AZx\)* \~ *cx* for *x* E Z\~  
\),. \~ 0, gz nondecreasing, gz\: *Rm,* .... *R* I,  
which can be rewritten as  
*Wo* = min *wo\(\),.\),*  
;.,,0  
where  
We now compare these two restrictions \[i.e., \(6.4\) and \(6.5\)\] of the general dual with the  
Lagrangian dual.  
Theorem 6.14. *given by\:*  
*The relationships among the Lagrangian dual and the restricted duals are*  
a. ZLR\(\),.\) = *WOR\(\),.\) for all* \),. \~ 0. *Hence the Lagrangian dual and the restricted dual*  
*\(6.4\) are equivalent.*  
b. *IJQ* = *\{x* E *Z1\: AZx* \~ bZ\}, *then* ZLR\(\),.\) = *WOR\(\),.\)* = *wo\(\),.\). Hence the Lagrangian dual*  
*and the two restricted duals* \(6.4\) *and* \(6.5\) *are equivalent.* 7\. Benders' Reformulation  
*Proof*  
337  
a. *WDR\(A\)* = min\).o\{Ao + *Ab 1\:* Ao + *MIX;;;' ex* for *x* E *Q\}*  
*= Ab l* + max *\(ex* - *MIX\)* = *hR\(A\).*  
*xEQ*  
b. Wo\(A\) = *Ab l* + min *gz\(bZ\)*  
*g2\(A 2 x\);;;.* \(e - *M* I\)X for *x* E Z\~  
*g z* nondecreasing.  
Using \(2.4\), we obtain  
*WD\(A\)* = *Ab l* + max\{\(e - *M* l\)X\: *A 2x* \~ *bZ*  
*, x* E Z\~\}  
= ZLR\(A\).  
# •  
The reader should now verify, for Example 6.1, that *g\(d1\)* = 28\~ + \~dl is an optimal  
solution to the restricted dual \(6.4\) and that  
is an optimal solution to the restricted dual \(6.5\). Both evidently give the same objective  
value *Zw* = 28\~.  
7\. BENDERS'REFORMULATION  
In the preceding section we gave a method for handling complicating constraints. We now  
consider the dual notion of complicating variables. In particular, in the mixed-integer  
program  
z = max *ex* + *hy*  
\(MIP\)  
*Ax+Gy* \~ *b*  
*x EX* f; Z\~,y ER\~,  
we can view the integer variables *x* as complicating variables to what would otherwise be a  
linear program, and we can view the continuous variables *y* as complicating variables to  
what would otherwise be a pure-integer program. For example, in a fixed-charge network  
flow problem where the integer variables represent decisions about which arcs to use in a  
network, the problem in the y-space is an ordinary network flow problem once *x* is  
specified.  
The procedure described below shows how MIP can be reformulated as a problem in  
*X* x *R* I; that is, there is only one continuous variable. However, this formulation generally  
contains a huge number of linear constraints. Since one expects only a small subset of  
these constraints to be active in an optimal solution, a natural relaxation is obtained by  
dropping most of them. 338 11.3. Duality and Relaxation  
As a first step, we suppose that the integer variables *x* have been fixed. The resulting  
linear program is  
LP\(x\) *ZLP\(X\)* = max *\{hy\: Gy* \~ *b* - *Ax, y* E R\~\}  
and its dual is  
min *\(u\(b* - *Ax\)\: uG* \~ *h, u* E *Rr;'\}.*  
We can characterize whether LP\(x\) is infeasible or has a bounded optimal value or has  
an unbounded optimal value by using the representation of the dual polyhedron in terms  
of its extreme points and extreme rays. Let *\{Uk ERr;'\: k* E *K\}* be the set of extreme points  
of *Q* = *\{u ERr;'\: uG* \~ *h\}* and let *\{v j* E *Rr;'\:\}* E *J\}* be the set of extreme rays of  
*\{u ERr;'\: uG* \~ O\}.NotethatifQ =1= 0, then\{vj *ERr;'\:\}* EJ\}isalsothesetofextremerays  
of *Q.* From Theorem 4.10 of Section 1.4.4 we can characterize *ZLP\(X\).*  
Proposition 7.1. *Thefunction ZLP\(x\) is characterized asfollows\:*  
i. *IfQ* = 0, *then ZLP\(X\)* = 00 *ifvj\(b* - *Ax\)* \~ *Of or all* j E J, *and* ZLp\(X\) = -00 *otherwise.*  
ii. *IfQ* =1= 0, *then ZLP\(X\)* = *minkEK uk\(b* - *Ax\)* \< 00 *if* \~\(b - *Ax\)* \~ 0 *for all* j E J, *and*  
*ZLP\(X\)* = -00 *otherwise.*  
An immediate consequence of Proposition 7.1 is that when *Q* =1= 0, MIP can be stated as  
\(7.1\)  
*Z* = max *\(cx* + min *uk\(b* - *AX»\)*  
*x kEK*  
vj\(b-Ax\)\~O for\}EJ  
xEX.  
This yields the Benders' representation ofMIP given by the following theorem.  
Theorem 7.2. MIP *can be reformulated as*  
z=max1\]  
1\] \~ *cx* + *uk\(b* - *Ax\)* for *k* E *K*  
\(MIP'\)  
*vj\(b* - *Ax\)* \~ 0 for\} E *J*  
*X EX,1\]ER 1•*  
*Proof* If there is no *x* E *X* such that *vj\(b* - *Ax\)* \~ 0 for all\} E *J,* then *ZLP\(X\)* = -00 for  
all *x* E *X* and *Z* = -00. If there is an *x* E *X* such that *vj\(b* - *Ax\)* \~ 0 for all\} E *J* and  
*Q* = 0, then *K* = 0 so that *Z* = 00; otherwise MIP' is equivalent to \(7.1\). •  
MIP' is Benders' reformulation. Since it typically has an enormous number of  
constraints, a natural approach is to consider relaxations obtained by generating only  
those constraints corresponding to a small number of extreme points and extreme rays.  
An algorithm based on such a relaxation will be discussed in the next chapter. 7\. Benders' Reformulation  
339  
2  
L-----\~----------------------------Ul 234  
Figure 7.1  
*Example 7.1*  
max *5xI* - *2X2* + *9X3* + *2YI* - *3Y2* + *4Y3*  
*5xI- 3X2* + *7X3* + *2YI* + *3Y2* + *6Y3* \~-2  
*4xI* + *2X2* + *4X3* + *3YI* - *Yz* + *3Y3* \~ 10  
*Xj* \~ 5 for *j* = 1, 2, 3  
*x* E *Z!, Y ER!.*  
Here we suppose that *X* = *\{x* E *Z!\: Xj* \~ 5 for *j* = 1, 2, 3\}.  
In Figure 7.1 we show the polyhedron *\{u* E R\~\: *uG* ;;;, *h\}*  
*2uI* + *3U2;;;' 2*  
*3uI* - *u2;;;,-3*  
*6Ul* + *3U2;;;' 4*  
*u ER;.*  
The extreme points of this polyhedron are *u1* = \(1 0\), *u2* = \(\~ 1\), *u3* = \(0 \~\), and  
*u4* = \(0 3\), and its extreme rays are VI = \(1 0\) and *v2* = \(1 3\).  
The resulting reformulation of the mixed-integer program is 340 11.3. Duality and Relaxation  
z = max 'I  
'I \~ *5Xl* - *2X2* + *9X3* + \(-2 - *5Xl* + *3X2* - *7X3\)*  
'I \~ *5Xl* - *2X2* + *9 X 3* + 1\(-2 - *5Xl* + *3X2* - *7X3\)* + t \(10 - *4Xl* - *2X2* - *4X3\)*  
'I \~ *5Xl* - *2X2* + *9X3* + 1 \(10 - *4Xl* - *2X2* - *4X3\)*  
'I \~ *5Xl* - *2X2* + *9 X 3* + 3\(10 - *4Xl* - *2X2* - *4X3\)*  
\~o  
\(-2 - *5Xl* + *3X2* - *7X3\)* + 3\(10 - *4Xl* - *2X2* - *4X3\)* \~ 0  
*Xj* \~ 5 for\) = 1, 2, 3  
*x* E *zI,* 'I E *R* 1.  
The reader should check that an optimal solution is *x* = \(0 3 1\) and 'I = 3.  
*Example* 7.2 *\(Uncapacitated Facility Location\).* tion ofMIP given by  
Here we use the alternative formula-  
z = max *cx* + 'I'  
'I' \~ *uk\(b* - *Ax\)* for *k* E *K*  
*vj\(b -Ax\)* \~ 0 for\) E *J*  
*x EX,* 'I' *ERI.*  
We consider the formulation given in Section 1.1.3\:  
z = max - I *jjXj* + I I *CuYu*  
*JEN iEI JEN*  
I *Y u* = 1 for *i* E *I*  
*JEN*  
*-Xj* + *Yu* \~ 0 for *i* E *I,\) EN*  
*x* E *Bn, Y* E R\~rn,  
where *N* = \{l, ... , *n\}* and *1=* \{l, ... , *m\}.*  
In this case, *LP\(x\)* is  
*ZLP\(X\)* = max I I *CuYu*  
*iEI JEN*  
I *Y u* = 1 for *i* E *I*  
*JEN*  
Yu\~Xj *foriEI,\)EN*  
*Y* E *Rr;zn.*  
Now rather than applying the Benders' reformulation directly, we will take advantage of  
the fact that *LP\(x\)* can be decomposed into *m* subproblems. For *i* E *I,* let 8\. Notes  
341  
*zLP\(X\)* = max I *CijYij*  
*jEN*  
I *Yij=* 1  
*jEN*  
*Y ij* .;;; *Xj* for j E *N*  
yER\~  
and note that *hp\(X\)* = *I.iEI zLp\(x\).*  
Clearly, *LPi\(X\)* is feasible and bounded for *x* E *Bn/\{o\}.* Hence to describe *zLP\(x\),* it  
suffices to find the extreme points of  
where *Wi* = *\(Wi!,* ••• , *Win\)'* It is easily seen that these extreme points are  
Hence  
As a result we can write the Benders' reformulation\:  
*z* = max - I *jjXj* + I t7i  
*jEN iEI*  
*t7i* .;;; *Cik* + I *\(Cij* - *CiktXj* for *i* E *I* and *kEN*  
*jEN*  
\(7.2\)  
which has no more than *mn* + 1 constraints. The standard Benders' reformulation,  
obtained directly from LP\(x\) without decomposition, has an exponential number of  
constraints.  
8. NOTES  
Section 11.3.1  
The concepts of relaxation and weak duality might best be attributed to the folklore of the  
field. Geoffrion and Marsten \(1972\) were among the first to use the term *relaxation*  
explicitly in the context of discrete optimization. Nemhauser \(1985\) gave an annotated  
bibliography of the uses of duality in integer programming and combinatorial optimiza-  
tion.  
Section 11.3.2  
The value function ofa discrete optimization problem appeared in Everett's \(1963\) rather  
informal treatment of Lagrangian relaxation and duality. Its importance was brought out 342 11.3. Duality and Relaxation  
in much greater depth in Geoffrion's \(1974\) treatment of Lagrangian duality for integer  
programs.  
The value function of a knapsack problem was studied and shown to be superadditive  
by Gilmore and Gomory \(1966\). Gomory \(1965, 1967\) extended these results to group  
problems.  
The value functions of pure- and mixed-integer programs have been studied extensively  
by Blair and Jeroslow \(1977, 1982, 1984, 1985\).  
The general dual problem \(2.4\) comes from Wolsey \(1981a\) and Tind and Wolsey \(1981\).  
Section 11.3.3  
An explicit statement of a superadditive dual appears in Johnson \(1973\) in the context of a  
cyclic group problem. Obviously, superadditive duality is closely related to the superaddi-  
tive characterization of all valid inequalities \(see the notes for Sections 11.2.4-11.2.7\).  
Blair and Jeroslow \(1977\) use the superadditivity of the value function to study the  
sensitivity of the optimal value as *b* varies. Jeroslow \(1978\), Wolsey \(1981b\), and Blair and  
Jeroslow \(1982\) studied the representation of the value function by a finite number of  
C-G functions. Cook, Gerards et al. \(1986\) generalized these results and also derived  
upper bounds on the Chvatal rank as a function of *n,* independent of the data.  
Section 11.3.4  
This longest-path view of integer programs is based on Gilmore and Gomory's \(1966\)  
dynamic programming recursion for the knapsack problem. Also see Shapiro \(1968a\).  
Section 11.3.5  
The group problem was introduced by Gomory \(1965\), and the results of this section are  
from that article. Also see Shapiro \(1970\) and Wolsey \(1971a,b\).  
The literature on methods for solving the group problem and using it as relaxation for  
solving the general integer programming problem will be given in the notes for Sec-  
tion 11.6.1.  
Section 11.3.6  
Lorie and Savage \(1955\) proposed a simple heuristic for 0-1 integer programming that is  
equivalent to a Lagrangian relaxation with respect to all of the linear constraints.  
Nemhauser and Ullman \(1968\) showed that with respect to this relaxation, the problem of  
finding an optimal set of multipliers is equivalent to solving the dual of the linear  
programming relaxation; they also showed that this set of multipliers yields the same  
bound as that obtained from the linear programming relaxation.  
Everett \(1963\) introduced the concept of Lagrangian relaxation for structured discrete  
optimization problems, and he proved Corollaries 6.5 and 6.6, Theorem 6.7, and Corol-  
laries 6.8-6.10 without explicitly using Theorem 6.2.  
Brooks and Geoffrion \(1966\) established the connection between Lagrangian relaxation  
and column generation methods for solving large-scale linear programs \[see Dantzig and  
Wolfe 1960\]. Geoffrion \(1974\) formalized the ideas of Lagrangian duality for general  
integer programs and, among other things, proved the main theorem \(Theorem 6.2\).  
Related articles are Shapiro \(1971\) and Fisher and Shapiro \(1974\).  
Several approaches to closing the duality gap that arises in Lagrangian duality have  
been proposed \[see, e.g., Bell and Shapiro \(1977\)\].  
The use of Lagrangian duality in solving structured combinatorial optimization  
problems was stimulated by Held and Karp's \(1970, 1971\) very successful application of it 9. Exercises 343  
to the traveling salesman problem. Some of these applications will be elaborated on in  
Sections 11.5.4 and II.6.1-1I.6.3, and several others will be cited in the notes for Sec-  
tion II.5.4.  
The idea of using cost splitting \(Corollary 6.12\) to obtain a Lagrangian dual problem  
equivalent to a linear program over the convex hull of the integer points in the intersection  
of two polyhedra appears in Ribeiro and Minoux \(1985\), Jornsten and Nasberg \(1986\), and  
Trick \(1987\).  
In a somewhat different manner, this approach was used by Nemhauser and Weber  
\(1979\) to solve set partitioning problems using a matching relaxation and by Edmonds  
\(1970\) and Frank \(1981\) in matroid intersection problems \(\~ee Section 111.3.5\).  
Surveys of the theory, computational aspects, and applications of Lagrangian duality  
are given by Shapiro \(1979b\) and Fisher \(1981\).  
Surrogate duality is due to Glover \(1968b, 1975\) and Greenberg and Pierskalla \(1970\).  
Karwan and Rardin \(1979\) discussed the relationship between surrogate and Lagrangian  
duality. Fisher, Lageweg et al. \(1983\) applied surrogate duality to job shop scheduling  
problems.  
Section 11.3.7  
Resource or Benders' decomposition for mixed-integer programming is described in  
Benders \(1962\). Lemke and Spielberg \(1967\) described a variation of Benders' algorithm  
that is designed for 0-1 MILPs. Geoffrion \(1970, 1972\) extended Benders' decomposition  
to handle a more general class of none on vex optimization problems. Magnanti and Wong  
\(1981\) described techniques for obtaining stronger Benders-type reformulations. Wolsey  
\(198lc\) and Holm and Tind \(1985\) provided theoretical extensions to the decomposition of  
integer programs. Van Roy \(1983, 1986\) proposed a procedure called cross-decomposition,  
which simultaneously uses Lagrangian and Benders' decomposition.  
9. EXERCISES  
1. Formulate the packing and covering problems discussed in this chapter as integer  
programs and thereby show that they are dual problems. Do you know any cases  
where strong duality holds?  
2. Find a maximum-weight node packing on the graph shown in Figure 9.1. The  
numbers on the nodes are the weights. Give a short proof that this packing is optimal.  
Figure 9.1 344 11.3. Duality and Relaxation  
3\. A *restriction* ofIP is any maximization problem  
4\. where \(a\) *ST* c;; S, and \(b\) *ZT\(X\)* \~ *cx* for *x EST.*  
i\) ii\) What use is a restriction ofIP?  
What can be said about its dual?  
i\) Calculate the value function of the knapsack problem  
*z\(d\)* = max *7Xl* + *4X2* + *X3*  
5Xl+3x2+2x3\~d  
xEZ\~.  
ii\) Show that z is superadditive and nondecreasing for *d* E Zl.  
iii\) Express z in such a way that there is a short proof that it is superadditive and  
nondecreasing.  
5. Let  
6. \(P\) z = *max\{cx\: Ax* \~ *b, x EX\}*  
and  
\(P;\) Zi = *max\{cx\: Ax* \~ *b, x* E *Xi\}* for *i* = 1, ... , *n.*  
Show that if *Fi* is dual feasible for \(Pi\) for *i* = 1, ... *,n,* and *X* = U7\~1 *Xi,* then  
*F* = maxi *\{F;\}* is dual feasible for \(P\).  
Show that the problem *minxEB' fix\)* has a dual problem given by  
max *Yo* + I *Yj*  
*JEN*  
*Yo* + I *Yj* \~f\(xS\) *jES*  
for all S c;; *N*  
Hint\: Take *\:Ji* = *\{F\: F\(d\)* = *Yo* + *yd\},* the class of affine functions.  
7. i\) Show that the superadditive dual of  
\(IP\) *max\{cx\: Ax* \~ *b, x* E *Z"\}*  
is  
min *F\(b\)*  
\(SD\) *F\(aj\)* = *Cj* for *j EN*  
*F* superadditive and nondecreasing with *F\(O\)* = O. 9\. Exercises  
345  
ii\) Show that if *F* is feasible in \(SD\), then *F\(Ax\)* = - *F\( -Ax\)* for all *x* E *zn.*  
iii\) Show that *LjENF\(aj\)xj* \~ *F\(b\)* is a valid inequality for IP.  
iv\) Show that ifIP is feasible for all *d,* the value function  
*zed\)* = max\{cx\: *Ax* \~ *d, x* E *zn\}*  
is dual optimal for all *d.*  
8\. i\) Give the superadditive dual of  
*z* = max 7Xl + *4X2* + *1x3*  
5Xl+3x2+2x3\~b  
xEZ\~  
\(see exercise 4\).  
ii\) Find at least two dual feasible solutions when *b* = 13.  
iii\) Use these solutions to obtain bounds when *b* = 15.  
9. i\) Give the superadditive dual of  
max 5Xl + llx2 + *16x3* + *20X4*  
Xl+ *2X2+ 3X3+* 4x4\~14  
*xEZ!.*  
ii\) Use the superadditive description of conv\(S\) \(see Example 3.1\) to find an  
optimal dual solution.  
10. i\) Formulate the problem  
XEZ\~  
11. as a shortest-path problem.  
ii\) Solve the problem by Dijkstra's shortest-path algorithm.  
iii\) Give a dual feasible solution.  
Use the group problem to solve  
max 7Xl + *4X2* + *X3*  
5Xl+3x2+2x3\~b  
xEZ\~  
for *b* = 217, 495, and 621. 346 11.3. Duality and Relaxation  
12\. Use the group problem to solve  
max *2Xl* + *5X2*  
*4Xl* + *X2* \~ 28  
xl+4x2\~27  
\(See exercises 1 and l3 of Section 11.1.9\).  
13\. Use Lagrangian duality to solve the problem of exercise 10 with *b* = \(\~\).  
i\) ii\) What bound is obtained by dualizing the first constraint?  
What bound is obtained by dualizing the second constraint?  
iii\) For what values of *b* is the optimal solution easily obtained? \(See Corollary 6.10.\)  
14. Apply Lagrangian relaxation to the integer program in exercise 12.  
i\) Show that if any two constraints are dualized, the value of the Lagrangian dual  
equals the value of the linear programming relaxation.  
ii\) Find a different objective function for which i is false.  
iii\) Show that if any single constraint is dualized, the value of the Lagrangian dual is  
an improvement on the value of the linear programming relaxation.  
iv\) Apply cost splitting to get a better Lagrangian dual.  
v\) Demonstrate i-iv graphically.  
15\. Consider two different Lagrangian duals for the generalized assignment problem\:  
max I I *CijXij*  
*i j*  
for *i* EM  
I *lixij* \~ *bj* for *j* EN  
*i*  
*x EBmn.*  
Discuss their relative merits according to the following three criteria\:  
i\) ease of solution of the subproblem,  
ii\) ease of solution of the Lagrangian dual,  
iii\) strength of the upper bound obtained by solving the dual. 9. Exercises 347  
16\. Discuss the merits of different Lagrangian duals for the capacitated facility location  
problem  
mIll I I *hijYij* + I *CjXj*  
*iEM jEN jEN*  
I *Y ij* \~ *ai* for *i* EM  
*jEN*  
I *Yij* \~ *bjxj* for\) EN  
*iEM*  
*Yij* \~ *min\(ai, bJxj* for *i EM,\)* E *N*  
17\. Consider the problem of processing *n* jobs on one machine. Let *Pj* denote the  
processing time of job\), let *rj* denote the earliest start time, and let *Wj* denote the  
weight associated with job\). The problem is to minimize *LjEN Wjtj,* where *tj* is the  
start time of job\). Without release dates *\(rj* = 0 for all\)\), the optimal job ordering is  
given by Smith's rule\: Processthejobsin order 1, ... , *n,* where *WI/PI* \~ ... \~ *wn/Pn.*  
How can Lagrangian relaxation be used to obtain a lower bound for the problem  
with release dates?  
18\. Consider the capacitated lot-sizing problem, that is, the uncapacitated problem  
formulated in Section 1.1.4 \(see also Section 11.2.4\) with additional capacity con-  
straints on the production levels *Yt* \~ *UtXt* for *t* = 1, ... , *T.* After dualizing these  
constraints, the Lagrangian subproblem is an uncapacitated problem that can be  
solved rapidly by dynamic programming \(see Section 11.5.5\).  
The Lagrangian dual is equivalent to a linear programming problem. Describe this  
linear programming problem in polyhedral terms.  
19\. Solving the Lagrangian duals in exercise 15 is equivalent to solving the dual problem  
*F\( ei* \) \~ *cij* for all *i* and\)  
*liej*  
*FEYft*  
for certain classes of functions Yft, where *ei* is the *ith* unit vector. For each of your  
proposed Lagrangians, what is Yft?  
20\. Describe the class of dual functions that correspond to the cost splitting and  
surrogate duals, respectively. Show that neither dual dominates the other.  
21. Suggest how to find optimal multipliers in the surrogate dual.  
22\. Apply Benders' reformulation to the fixed-charge network problem described in  
Section 1.1.4. Discuss possible advantages of such a reformulation. 348 11.3. Duality and Relaxation  
23\. Apply Benders' reformulation to UFL \(without separating the subproblems by  
client\) and compare this formulation with \(7.2\).  
**24.** Write out explicitly the Benders' reformulation of the mixed-integer program  
max 2XI + *X2* + *3X3* + 7YI + *5Y2*  
9XI + *4X2* + *14x3* + 35YI + *24Y2* \~ 80  
-Xl - *2X2* + *3X3* - 2YI + *4Y2* \~ 10  
*X* E Z\~, *Y* ER\~. **11.4**  
**General Algorithms**  
1. INTRODUCTION  
Here we discuss approaches for finding an optimal, or E-approximate, solution of the  
linear integer programming problem  
\(IP\) ZIP = *max\{ex\: xES\}.*  
For simplicity, in this introductory discussion, we assume S *=1=* 0 and ZIP \< 00. Therefore,  
to solve an instance of IP, an algorithm must produce a feasible solution *XO* E S and an  
upper bound WO on the value of all feasible solutions such that *exo* = woo A general iterative  
scheme for finding *XO* and WO is shown in Figure 1.1.  
Many integer programming algorithms focus on the dual step by systematically  
reducing the upper bound w\* but generally not producing an *xES* until w\* = ZIP.  
Relaxation algorithms are of this type. At each iteration a relaxation of IP is solved and if  
an optimal solution of the relaxation does not yield an optimal solution of IP, the  
relaxation is refined. A general relaxation algorithm is the following.  
General Relaxation Algorithm  
*Initialization\:* Set *t* = 1, w\* = 00, and z\* = -00. Choose S1 ;2 Sand z1 *\(x\)* \~ *ex* for *xES.*  
*Iteration t\:*  
*Step* 1\: Solve the relaxation ofIP\:  
zk = *max\{zk\(x\)\: x* E Sk\}.  
*Step* 2\: *Optimality test.* Let the solution be *Xl.* If *Xl* E Sand zk = *ext,* then  
w\* = *ext* = z\* and *xt* is an optimal solution to IP.  
*Step* 3\: *Refinement.* Set w\* = zk, z\* = *ext* if *xt* E S, and *t* \~ *t* + 1. Choose *Sit!* to satisfy  
S S *st;!* s Sk and *zt;!\(x\)* to satisfy *ex* \~ *zt;!\(x\)* \~ *zk\(x\)* for *xES* with either *Sit! =1=* Sk  
or *zit!\(x\) =1= zk\(x\).*  
Note that in this algorithm, the sequence of upper bounds satisfies *zt;!* \~ zk for all *t.* In  
many specific instances of the general relaxation algorithm, *zk\(x\)* = *ex* for all *t* so that  
optimality is achieved as soon as an *xt* E S is produced. In this case, the refinement step  
satisfies *Sit!* C Sk for all *t.* It is then desirable to choose *st;!* s Sk \\ *ext\};* otherwise  
*zt;!* = zk.  
349 *Input*  
*t=* 1  
Upper bound *w\** \(possibly + \(0\)  
Feasible solution *x\** E S \(may be omitted\)  
*Output*  
Optimal solution *x\**  
ZIP *=z\**  
y  
*z\** = \{ *ex\** if *x\** is specified  
- 00 otherwise  
*Optimality test*  
*z\*=w\**  
y  
*Dual iteration*  
*Dual step*  
*w\*-* min *\(w\*, wt\)*  
*N*  
*Primal interation*  
*Primal step*  
*N*  
*xt* ES  
and  
*ext\> z\**  
?  
*N*  
y  
*z\** - *ext*  
*x\** - *xt*  
**Figure 1.1**  
**350** 1\. Introduction 351  
An important example of this type of relaxation algorithm is *a/ractional cutting-plane*  
*algorithm.* Here we assume that S = *\{x* E Z\~\: *Ax* \~ *b\}.* Note that to specify the algorithm,  
it suffices to give the initial relaxation and the rule for constructing *Rpt+l* from *RPt.*  
Fractional Cutting-Plane Algorithm \(FCPA\)  
*Initialization\: z1\(x\)* = *cx* for all *x* E R\~; S1 = *\{x* E R\~\: *Ax* \~ *b\}.*  
*Refinement\: zitl\(X\)* = *zk\(x\)* for all *x* E R\~; Sitl = Sk n *\{x* E R\~\: *ntx* \~ *nb\},* where *\(nt, nb\)*  
is a valid inequality for S such that *ntxt* \> *nb.*  
Observe that max\{cx\: *x* E Sk\} is a linear program whose optimal dual solution *ut* is  
readily extended to the feasible solution *\(ut*  
*,* 0\) to the dual ofmax\{cx\: *x* E *Sit!\}.* The dual  
variable for the new constraint *nt x* \~ *nb* equals zero. Thus it is desirable to solve the  
sequence of linear programs  
*max\{cx\: x* E Sk\}  
by a dual algorithm. Hence we can interpret the fractional cutting-plane algorithm as a  
dual linear programming algorithm for solving IP. The dual *ofLpt* is weakly dual to IP. The  
generation of a valid inequality corresponds to the generation of a column in the dual  
space and, consequently, to a relaxation of the dual ofLPl.  
Figure 1.2 illustrates the application of FCPA to the two-variable integer programming  
problem introduced in Chapter 11.1.  
We will not discuss here the very important question of how to choose the valid  
inequality *\(nt, nb\)* that separates *xt* from Sitl. In Section 3, we will give an FCPA for general  
integer programs that uses C-G inequalities. In Chapters 11.5 and 11.6 we will show how  
strong valid inequalities can be used in FCPAs for some structured integer programs and  
the general 0-1 integer program.  
4  
3  
2  
O\~--------------2-------3--\~---'4--------Xl  
Figure 1.2 352 11.4. General Algorithms  
Another very important type of relaxation algorithm uses an enumerative approach.  
We say that *\{Si\: i* = 1, ... , *k\}* is a *division* of S if U7=l *Si* = S. A division is called a *partition*  
if Si n Sj = 0 for *i, j* = 1, ... , *k, i* \*' *j.*  
Proposition 1.1 *Let*  
*where* \{Si\}7=l *is a division of* S. *Then* ZIP = maXi=l, ... , *k* z\{P.  
Proposition 1.1 expresses the familiar concept of *divide and conquer.* In other words, if it  
is too difficult to optimize over S, perhaps the problem can be solved by optimizing over  
smaller sets and then putting the results together.  
The division is frequently done recursively as shown in the tree of Figure 1.3. Here the  
sons of a given node \[e.g., *\(Sl1, S12, S13\)* are the sons of Sl\] represent a division of the  
feasible region of their father.  
When S \~ *Bn,* a simple way of doing the recursive division is shown in Figure 1.4. Here  
Sb1  
•• *• bk* = S n *\{x* E *Bn\: Xj* = Jj E CO, 1\} for *j* = 1, ... , *k\},* and the division is a partition of  
S.  
Carried to the extreme, division can be viewed as total enumeration of the elements of  
S. Total enumeration is not viable for problems with more than a very small number of  
variables. To have any hope of working, the enumerative approach needs to avoid dividing  
the initial set into too many subsets.  
Suppose S has been divided into subsets \{Sl, ... , *Sk\}.* Ifwe can establish that no further  
division of Si is necessary, we say that the enumeration tree can be *pruned* at the node  
corresponding to Si or, for short, that Si can be pruned.  
Proposition 1.2. *The enumeration tree can be pruned at the node corresponding to* Si *if*  
*anyone of the following three conditions holds.*  
1. *Infeasibility\:* Si = 0.  
*2. Optimality\: An optimal solution* ifIpi *is known.*  
*3. Value dominance\:* z'p \~ ZIP.  
8 22  
Figure 1.3 1\. Introduction 353  
s  
sa  
SOOO SOOl SOlO SOIl S100 S101 *S110* SIll  
Figure 1.4  
We would like to be able to apply Proposition 1.2 without necessarily having to solve IPi.  
To accomplish this, we use relaxation or duality. Let Rpi be a relaxation oflpi with Si s; Sk  
and *zk\(x\)* \~ *cx* for *x* E *Si.*  
**Proposition** 1.3. *The enumeration tree can be pruned at the node corresponding to* Si *if*  
*anyone of the following three conditions holds.*  
1. Rpi *is infeasible.*  
*2. An optimal solution xk to* Rpi *satisfies xk* E Si *and zk* = *cxk.*  
*3. zk* \~ \~IP' *where* \~IP *is the value of some feasible solution ofIP.*  
*Proof* Condition 1 implies Si = 0. Condition 2 implies thatxk is an optimal solution  
to IPi. Condition 3 implies z\~p \~ ZIP. •  
Let Dpi be \(weakly\) dual to IPi.  
**Proposition 1.4.** *The enumeration tree can be pruned at the node corresponding to* Si *if*  
*one of the following two conditions holds.*  
1. *The objective value ofDpi is unboundedfrom below.*  
2. Dpi *has afeasible solution of value equal to or less than* \~IP.  
*Proof* Condition 1 implies Si = 0. Condition 2 implies zip \~ ZIP. •  
Comparing Propositions 1. 3 and 1.4, we see that Rpi must be solved to optimality before  
value dominance can be applied, but value dominance may be applicable with respect to  
dual feasible solutions that are not optimal. On the other hand, Rpi may yield a feasible  
solution to Ipi that establishes or improves the lower bound \~IP. 354  
*Example 1.1*  
11.4. General Algorithms  
ZIP = max - *100Xl* + *72x2* + *36x3*  
*- 2Xl* + *X2* \~ 0  
*-4Xl* + X3\~0  
*Xl* + *X2* + *X3* \~ 1  
*xEB3.*  
A division is shown in the tree of Figure 1. 5.  
We use linear programming relaxation and Proposition 1.3 for pruning. The infeasibil-  
ity condition holds for SO since  
The optimality condition holds for S110 and S111 since these sets contain the unique  
solutions \(l 1 0\) and \(l 1 1\), respectively. Since zf\~o \< zl\~l = 8, we have ZIP = zl\~l = 8.  
Now we can apply the value dominance criterion to SIO since *zj?* = - 64"\< ZIP. Hence  
*X O* = \(1 1 1\) is an optimal solution to IP, and ZIP = 8. -  
When relaxations are used for pruning, the enumerative approach fits into the context  
of the general relaxation algorithm. Suppose we have just solved a relaxation ofIPi. In the  
refinement step, we first divide Si, say Si = Uj=l *Sij.* Then we form relaxations for the sets  
*Sij* in such a way that Uj=l S\~ C Sk.  
An enumerative relaxation algorithm is frequently called *branch-and-bound* or *implicit*  
*enumeration.* We now give a general branch-and-bound algorithm for solving IP. In the  
description of the algorithm, \:£ is a collection of integer programs \{lpi\}, each of which is of  
the form zip = *max\{cx\: X* E Si\} where Si \~ S. Associated with each problem in \:£ is an  
upper bound *Zi* \~ *zip.*  
8  
80  
Figure 1.5 2. Branch-and-Bound Using Linear Programming Relaxations 355  
General Branch-and-Bound Algorithm  
*Step* 1 *\(Initialization\)\:* 2 = \{IP\}, SO = S, *ZO* = 00, and \~IP = - 00.  
*Step* 2 *\(Termination test\)\:* If 2 = 0, then the solution *XO* that yielded \~IP = *cxo* is optimal.  
*Step* 3 *\(Problem selection and relaxation\)\:* Select and delete a problem Ipi from 2. Solve  
its relaxation RPi. Let *zk* be the optimal value of the relaxation and let *xk* be an optimal  
solution if one exists.  
*Step* 4 *\(Pruning\)\:* a. If zk \~ *ZIP,* go to Step 2. \(Note if the relaxation is solved by a dual  
algorithm, then the step is applicable as soon as the dual value reaches or falls below \~IP'\)  
b. If *xk* \$ *Si,* go to Step *S.*  
c. If *xk* E Si and *cxk* \> *ZIP,* let ZIP = *cxk.* Delete from 2 all problems with *Zi* \~ ZIP. If  
cx\~ = *zk,* go to Step.f; otherwise go to Step *S. -*  
*Step* 5 *\(Division\)\:* Let *\{Sij\}j=!* be a division of *Si.* Add problems *\{lPij\}j=!* to 5£, where *zU* = *zk*  
for\} = 1, ... *,k.* Go to Step 2.  
Commercial codes for general mixed-integer programming problems use linear pro-  
gramming relaxations and division. We will study this class of algorithms in the next  
section. In Chapters II.S and 11.6 we will consider some special purpose branch-and-bound  
algorithms that use different relaxations or duals and other division tactics. We will also  
present cutting-plane algorithms that sometimes fail to find strong valid inequalities and  
then resort to branch-and-bound to complete the solution.  
2\. BRANCH-AND-BOUND USING LINEAR PROGRAMMING RELAXATIONS  
Here we consider the general integer programming problem  
\(IP\) ZIP = *max\{cx\: xES\},* where S = *\{x* E Z\~\: *Ax* \~ *b\}.*  
We study its solution by a branch-and-bound algorithm that uses linear programming  
relaxations. This is the basic algorithm used by all commercial codes for solving mixed-  
integer programming problems. Merely for simplicity of notation, we confine the presen-  
tation to IP. Essentially, however, all of the ideas carryover unchanged to the mixed-  
integer program  
\(MIP\) *ZMIP* = *max\{cx* + *hy\: \(x, y\)* E *T\},* where *T* = *\{x* E *Z1, y* E R\~\: *Ax* + *Gy* \~ *b\}.*  
This setting is simple but general enough to enable us to discuss various properties of  
branch-and-bound algorithms such as types of divisions, tree development strategies,  
finiteness of the resulting tree, the smallest possible tree, and so on.  
In the initial relaxation, S is replaced by S\~p = *\{x* E *R1\: Ax* \~ *b\}.* We also take  
*z R\(X\)* = *cx* in each relaxation.  
Pruning Criteria  
When solving linear programming relaxations, the pruning criteria of infeasibility,  
optimality, and value dominance given in Propositions 1.3 and 1.4 are directly applicable.  
Suppose the linear programming relaxation at node *i* of the enumeration tree is 356 11.4. General Algorithms  
If Lpi has an optimal solution, we denote the one found by *Xi.*  
The pruning conditions are\:  
1. SLp = 0 \(infeasibility\);  
*2. Xi* E Z1 \(optimality\); and  
3. zh \~ \~IP where \~IP is the value ofa known feasible solution to IP \(value dominance\).  
Note that if Lpi is solved by a dual algorithm, we may be able to prune before an  
optimal solution to Lpi is found. Also, we may wish to use the weaker condition  
zh \~ \~IP + E for some given tolerance E \> O.  
Division  
Since we use a linear programming relaxation at each node, the division is done by adding  
linear constraints. An obvious way to do this is to take S = S1 U S2 with  
S1 = S n *\{x* E *R1\: dx* \~ *do\}* and S2 = S n *\{x* E *R1\: dx* \~ *do* + 1\}, where *\(d, do\)* E *zn+1.* If  
*Xo* is the solution to the relaxation  
z\~p = max\{cx\: *x* E *R1, Ax* \~ *b\},*  
we can choose *\(d, do\)* so that *do* \< *dxo* \< *do* + 1. This is highly desirable since it yields  
*Xo* \$. Sh U S\[p and therefore gives the possibility that for *i* = 1, 2 we will obtain  
zh = max *\{cx\: x* E SLP\} \< z\~P.  
In practice, only very special choices *of\(d, do\)* are used.  
i. *Variable dichotomy.* Here *d* = *ej* for some *j EN.* Then *Xo* will be infeasible in the  
resulting relaxations ifxJ \$. Z1 and *do* = *lxJJ* \(see Figure 2.1\). Note that *ifxj* E *B1,* then the  
left branch yields *Xj* = 0 and the right branch yields *Xj* = 1.  
An important practical advantage of this division is that only simple lower- and upper-  
bound constraints are added to the linear programming relaxation. Thus it is only  
necessary to keep track of the bounds, and the size of the basis does not increase.  
ii. *GUB dichotomy.* Suppose the problem contains the generalized upper-bound con-  
straint *I\:jEQ Xj* = 1 for some *Q* \~ *N.* The division is shown in Figure 2.2. Note that *XO* will  
be infeasible in the resulting relaxations if 0 \< *I\:jEQ\\ xJ* \< 1, where *Q* 1 is a nonempty subset  
*ofQ.*  
iii. Assuming that *Xj* is bounded \(0 \~ *Xj* \~ *kj\),* we can consider each integral value of *Xj*  
separately \(see Figure 2.3\). This approach, however, is not used in commercial integer  
programming codes.  
Note that each of the divisions i-iii is a partition.  
We now consider the size of the enumeration tree. For most of the remainder of this  
section, we will assume that the division is done by variable dichotomy.  
Figure 2.1 2\. Branch-and-Bound Using Linear Programming Relaxations 357  
Figure 2.2  
Proposition 2.1. *If P* = *\{x* E R\~\: *Ax* \:\:\:\:;; *b\} is bounded, an enumeration tree developed on*  
*variable dichotomies will be finite provided that at each node i that requires division, a*  
*dichotomy of the form \(xi\:\:\:\:;; lxJJ, Xi* \~ *lxJJ* + 1\) *is chosen where xJ is not integral. In*  
*particular, if wi* = r max *\{Xi\: X* E *P\}\], no path of the tree can contain more than LiEN Wi*  
*edges. .*  
*Proof* Once we have added the constraint *xi\:\:\:\:;; d* for some *dE* \{O, ... , *wi* - 1\}, the  
only other constraints that can subsequently appear on a path from the root to a leaf of the  
tree are *xi* \~ *d'* for *d'* E \{O, ... , *d* - 1\} and *xi* \~ *d* for *dE* \{l, ... , *d\}.* It follows that the  
largest number of constraints involving *Xj* will occur by adding *Xi* \:\:\:\:;; *d* for all *d* E \{O, ... ,  
*Wi* - 1\}, or *Xi* \~ *d* for all *dE* \{l, ... *,wi\},* or *Xi* \~ *d* for and E \{l, .. ; *,a\} andxj\:\:\:\:;; d* for all  
*d* E *\{a,* ... , *Wi* - I\}. In each of these cases, we require *Wi* constraints on *x\)* and hence  
*LiEN Wi* in total on any path. •  
We can use Proposition 2.1 and the upper bounds given in Theorem 4.1 of Section 1.5.4  
to enforce the finiteness of the enumeration tree even when *P* is not bounded.  
The size of the enumeration tree is very dependent on the quality of the bounds  
produced by the \(linear programming\) relaxation. In particular, we have the following  
proposition.  
Proposition 2.2. *If node t of the enumeration tree with constraint set Sf is such that*  
*max\{cx\: X* E S\~\} \> *ZIP, then node t cannot be pruned.*  
Proposition 2.2 indicates that, regardless of how we develop the tree, the bounds  
\(quality of relaxations\) are the primary factor in the efficiency of a branch-and-bound  
algorithm. Nevertheless, tree development strategies, such as which subproblem  
corresponding to an unpruned node should be considered next and which fractional  
variable should be selected for the dichotomous division, are also important. We now  
consider these problems.  
Figure 2.3 358 11.4. General Algorithms  
N ode Selection  
Given a list\:£ of active subproblems or, equivalently, a partial tree of un pruned or *active*  
nodes, the question is to decide which node should be examined in detail next. Here there  
are two basic options\: \(1\) *a priori rules* that determine, in advance, the order in which the  
tree will be developed; and \(2\) *adaptive rules* that choose a node using information  
\(bounds, etc.\) about the status of the active nodes.  
A widely used \(essentially\) a priori rule is *depth-jirst search plus backtracking,* which is  
also known as *last in, first out* \(LIFO\). In depth-first search, if the current node is not  
pruned, the next node considered is one of its two sons. Backtracking means that when a  
node is pruned, we go back on the path from this node toward the root until we find the  
first node \(if any\) that has a son that has not yet been considered. Depth-first search plus  
backtracking is a completely a priori rule if we fix a rule for choosing branching variables  
and specify that the left son is considered before the right son. An example of depth-first  
search plus backtracking with left sons first is given in Figure 2.4. The nodes are numbered  
in the order in which they are considered. An underlined node is assumed to have been  
pruned.  
Depth-first search has two principle advantages\:  
1. The linear programming relaxation for a son is obtained from the linear program-  
ming relaxation of its father by the addition of a simple lower- or upper-bound  
constraint. Hence given the optimal solution for the father node, we can directly  
reoptimize by the dual simplex algorithm without a basis rein version or a transfer of  
data.  
2\. Experience seems to indicate that feasible solutions are more likely to be found deep  
in the tree than at nods near the root. The- success of a branch-and-bound algorithm  
is very dependent on having a good lower-bound g\:IP for value dominance pruning.  
The default option in most commercial codes is depth first when the current node is not  
pruned. At least one son is considered immediately. Rules for choosing a son will be  
discussed later. However, when a node is pruned, the next node is not generally deter-  
mined by the backtracking strategy. Before explaining how this selection is done, we  
mention one other essentially a priori rule, which is the opposite of depth-first search. The  
Figure 2.4 2. Branch-and-Bound Using Linear Programming Relaxations 359  
level of a node in an enumeration tree is the number of edges in the unique path between it  
and the root. In *breadth-first search,* all of the nodes at a given level are considered before  
any nodes at the next lower level. While this means of node selection is not practical for  
solving general integer programs using linear programming relaxations, it has some  
interesting properties, one of which is its use in heuristics.  
Several reasonable criteria can be given for choosing an active node\:  
a. Choose a node that has to be considered in any case. By Proposition 2.2, if there is a  
unique node with the largest upper bound it must be considered. This argument mitigates  
for the rule *best upper bound;* that is, when a node has been pruned, next select from all  
active nodes one that has the largest upper bound. Thus if *it* is the set of active nodes,  
select an *i* E *it* that maximizes *Zi.*  
b. Choose a node that is more likely to contain an optimal solution. The reason for this  
is that once we have found an optimal solution, even if we are unable to prove immedi-  
ately that it is optimal, we will have obtained the largest possible value of ZIP' This is very  
important for subsequent pruning. Later we will give a simple procedure for estimating  
zip. Suppose *Zi* \~ *Zi* is an estimate of zip. The rule *best estimate* is to choose an *i* E *it* that  
maximizes *Zi.*  
c. Although trying to find an optimal solution is highly desirable, it may be more  
practical to try to find quickly a feasible solution *x* such that *cx* \> \~IP' The criterion  
\(2.1\)  
*-i*  
Z - ZIP  
max *-i* ","  
iE.2 Z - Zl  
which we call *quick improvement,* attempts to achieve this objective. Note that node *i* with  
Zi \> \~IP will be preferred to node\} with zj \~ \~IP' Moreover, preference will be given to  
nodes for which *Zi* - Zi is small. One expects that such nodes will yield a feasible solution  
quickly. Quick improvement is used in some commercial codes as the default option once  
a feasible solution is known.  
**Branching Variable Selection**  
Suppose we have chosen an active node *i.* Associated with it is the linear programming  
solution *Xi.* Next we must choose a variable to define the division. We restrict it to the  
index set *N i*  
= \{j *EN\: xj* \$. Zl\}. Empirical evidence shows that the choice of a\} E *Ni* can  
be very important to the running time of the algorithm. Frequently, there are a few  
variables that need to be fixed at integer values and then the rest turn out to be integer-  
valued in linear programming solutions. Because robust methods for identifying such  
variables have not been established, a common way of choosing a branching variable is by  
*user-specified priorities.* This means that an ordering of the variables is specified as part of  
the input and that branching variables are selected from *N i* according to this order. For  
example, a 0-1 variable corresponding to whether a project should be done would be given  
higher priority than 0-1 variables corresponding to detailed decisions within the project.  
Other possibilities involve *degradations* or *penalties.* Degradation attempts to esti-  
mate the decrease in Zi that is caused by requiring *Xj* to be integral. Suppose  
*Xj* = *xj* = *lxjj* + *Ij* and/j \> O. Then by branching on *xj,* we estimate a decrease of  
*Dt* = *pji Ij* for the left son and *Dt* = *pt\(1* - *Ij\)* for the right son. The coefficients *\{Pji, pt\}*  
can be specified as part of the input or estimated in several different ways \(e.g., by using  
dual information at the node or by using information on previous branchings involving  
*Xj\).* 360 11.4. General Algorithms  
Penalties involve more elaborate calculations to determine the coefficients *\{pji, pt\}*  
and yield a lower bound on the decrease in *Zi.* They were used in early commercial codes  
but are not in favor now because they are too costly to compute relative to the value of the  
information they give. An illustration of penalty calculations will be given in Example 2.1.  
Given *\{Dji, Dt\}* for *j* E *N i*  
*,* a common way to choose the branching variable is by the  
criterion  
\(2.2\) max *min\{Dj --\:i*  
*, Dj"!-i\}.*  
*JEN'*  
The idea is that a variable whose smallest degradation is largest is most important for  
achieving integrality. When *Dji* = *fj* and *Dji* = 1 - *fj,* criterion \(2.2\) is called *maximum*  
*integer infeasibility.*  
Other rules are also used, for example, *maXjENi* max\{Dji, *Dt\}.* Here the idea is that one  
branch may easily be pruned by value dominance.  
When the branching variable is chosen by \(2.2\), it is recommended that we next  
consider the subproblem corresponding to the son that yields the smaller degradation.  
Thus we select the subproblem corresponding to the left son if and only if *Df* \:\:\:\:\:; *Dt.*  
Now we can compute *Zi* by assulning that the degradations for each variable are  
independent. Thus if *Dji* \~ *Dt,* we estimate  
*Zi* = *zh* - *Df* - I *min\{Dki*  
*, Dti \}.*  
*kENi\\\{j\}*  
Note that if we are required to branch to the right son of node *i,* the estimate becomes  
*Zi* = *zLp* - *Dt* - I *min\{Dki*  
*, Dti\}.*  
*kENi\\\{j\}*  
*Example* 2.1  
ZIP = max *7xJ* + *2X2*  
*- XI* + *2X2* \~ 4  
*5xI* + *X2* \~ 20  
*- 2x* I - *2x* 2 \:\:\:\:\:; -7  
xEZ\~.  
We introduce slack variables *\(X3, X4, X5\)* E R\~. Although the slack variables will be integral  
when *X* I, *X* 2 are integral, there is no need to require them to be integral.  
Solving the linear programming relaxation gives the optimal basic solution  
3 16 332  
ZLP *+ TIX 3 + TIX 4* 11  
1 2 36  
*Xl - -X3* 11 *+ TIX 4* 11  
5 1 40  
*X2 + TIX 3 + TIX 4* 11  
8 6 75  
*TIX 3 + TIX 4 + X5=* 11  
*\(Xl, X2\)* E Z\~, *\(X3, X4, X5\)* E R\~. 2\. Branch-and-Bound Using Linear Programming Relaxations 361  
Thus z\~P = 30n and *Xo* = \(1f *WOO* ff\).  
Since *\(x?,* x\~\) \$. Z2, one must branch on either Xl or *X2.* We use \(2.2\) to choose between  
them.  
Suppose we consider the criterion of maximum infeasibility; then *Djo* = *fJ* and  
*DjO* = 1 - *fJ* for *j* = 1, 2. Hence DJ o = It, DtO = ft, *D"2°* = n, and *D 2 0* = n. By \(2.2\), we obtain  
*. \(D-O D+O \)* \(3 4\) 4 *D+O*  
m\~£ mIn *j, j* = max TI' TI = TI = 2·  
Hence we would branch on *X2 \(X2* \~ 3, *X2* \~ 4\) and examine the right son.  
Now we illustrate the use of penalties to determine the branching variable. From the  
representation of the optimal solution, we see that if *x* 3 or *x* 4 increases, *x* 2 decreases. Hence  
we can set *P* 2° = 00 • Following this approach, *Z* LP decreases by \~ \(\~\) per unit decrease in *x* 2  
if *X3 \(X4\)* is made basic; hence we can set *P20* = mina, \~\) ==\~. Similarly, pta = 3 and  
p\~ = \~ = 8. Hence  
Now  
*. \(D-o D+O\)* \(24 21\) 24 *D- D+*  
mr£ mIn *j, j* = max TI' 55 = TI = 1 = 1·  
Thus we would branch on *x* 1. Empirical evidence indicates that these calculations are not  
worthwhile for large problems.  
We choose to branch on *X2* \(see Figure 2.5\). Adding the constraint *X2* \~ 4 *\(X2* - *t =*  
*4, t* \~ 0\) to the current optimal solution gives the node 1 relaxation. The full set of  
equations is given by  
3 16 332  
ZLP + TIX 3 *+ TIX 4* 11  
1 2 36  
*Xl - -X3* 11 + TIX 4 11  
5 1 40  
*X2* + *TIX3 + rrX4* 11  
8 6 75  
*TIX 3 + rrX4* + Xs TI  
5 1 4  
TIX 3 *+ rrX4 +t=--* 11  
*x, t* \~ O.  
Note that in a computer system the bound constraints would not be added explicitly. The  
dual simplex algorithm shows immediately that this problem is primal infeasible \(see the  
last constraint\). Hence node 1 is pruned by infeasibility. 362 11.4. General Algorithms  
The only remaining node on the candidate list \(node 2\) corresponds to the original IP  
with *X2* \~ 3 *\(X2* + s = 3, s \~ 0\) added. The resulting linear programming relaxation is  
3 16 332  
ZLP + TIX3 *+ rrX4* 11  
1 2 36  
*Xl - -X3* 11 + TIX4 11  
5 1 40  
*X2 +* TIX3 + TIX4 11  
8 6 75  
TIX3 + TIX4 *+ X5* 11  
5 1 7  
*- -X3* 11 *- -X4* 11 + s = 11  
After one iteration of the dual simplex algorithm we obtain the optimal solution  
7 3  
ZLP *+ -X4* 5 + -s 5  
1 1  
Xl *+ -X4* 5  
- -s 5  
*X2* + s  
2 8  
*-X4* 5 *+ X5+* -s 5  
1 11  
*X3 + -X4* 5  
- -s 5  
Thus zip = 29\~ and *X2* = \(\~ 3 7 0 2f\). "5  
149  
5  
17  
# 5  
3  
29  
5  
7  
5  
Fathomed by  
infeasibility  
Feasible solution Feasible solution  
*z* 3 = 27 = *Z Z* 4 = 28 = *Z* = *Z*  
LP - IP LP - IP IP  
*x4* = \(4 080 1\)  
. Figure 2.5 2. Branch-and-Bound Using Linear Programming Relaxations 363  
Since XI is not integral, we branch on *X* I and examine the left son first since *IT* \< 1. The  
tree is shown in Figure 2.5. Adding *XI* \~ 3 and reoptimizing by the dual simplex method  
gives an integral solution *X3* = \(3 3 1 2 5\) with *zip* = 27. Hence node 3 is pruned and  
we set ZIP = 27.  
The-only remaining node is node 4\. By solving the linear programming relaxation of  
the node 4 problem, we obtain *X4* = \(4 0 8 0 1\) and *ztp* = 28. Hence node 4 is  
pruned and ZIP = 28. The list of active nodes is now empty, so the algorithm terminates  
with the optimal solution *X* = *X4* and ZIP = 28.  
*Example* 2.2  
ZIP = max *77xI* + *6X2* + *3X3* + *6X4* + *33xs* + *13x6* + 1l0x7 + *21xs* + *47x9*  
*774xI* + *76x2* + *22x3* + *42x4* + *21xs* + *760X6* + *818x7* + *62xs* + *785x9* \~ 1500  
*67xI* + *27x2* + *794x3* + *53x4* + *234xs* + *32x6* + *797x7+ 97xs* + *435x9* \~ 1500  
*xEB9*  
We solved this problem by a branch-and-bound algorithm contained in a mathematical  
programming system. The tree is shown in Figure 2.6. Additional information about the  
nodes of the tree is given in Table 2.1. The linear programming relaxations are solved in the  
order given by the node numbers.  
The algorithm begins by solving the initial linear programming relaxation. As indicated  
in Table 2.1, its value is z\~p = 225.7 and there are two fractional variables. Associated with  
each fractional variable are two reduced costs. One is the reduced cost of the nonbasic  
variable that becomes basic if the fractional basic variable goes to its upper bound of 1; the  
other is the reduced cost of the nonbasic variable that becomes basic if the fractional  
variable goes to its lower bound ofO. By multiplying each of these costs by the distance that  
Figure 2.6 364 11.4\. General Algorithms  
the fractional variable must move to achieve the corresponding bound, we estimate  
upward and downward costs. \(lfthe reduced cost is less that 0.1, the algorithm uses 0.1 in  
place of the reduced cost.\) The smaller of the upward and downward costs is used for the  
estimated cost of a fractional variable, and then the fractional variable with the largest  
estimated cost is chosen for branching. In the example, it is *Xl.*  
Now we choose a direction for branching by comparing the upward and downward  
estimated costs for *Xl,* and we branch in the direction of the smaller of the two estimated  
costs. In the example, we set x I = 1 and solve the resulting linear program to obtain the  
solution at node 1. The algorithm next decides whether to consider the opposite branch  
*X* I = 0 or to branch from node 1. This is done by comparing the estimated solution value *i 1*  
with the estimated solution value at node 0 had the degradation been computed using the  
downward cost for x 1. The larger of these two values determines the next node. In the  
example, we solve the problem with *X* I = O.  
In general, after considering the first branch from a node, the algorithm either considers  
the opposite branch or branches down from the node just created. If the first branch is  
pruned, the algorithm next considers the opposite branch. If the first branch is not pruned,  
the algorithm chooses between the two possibilities as indicated above.  
When both branches of a node have been considered in turn, there are three possibili-  
ties. If neither has been pruned, the algorithm selects the node corresponding to one of  
them. This selection is made by the criterion of higher estimated solution value until an  
integral solution has been found; thereafter it is made by the quick improvement rule  
given by \(2.1\). In the example, node 2 is chosen for division before node 1 because  
*i 2*  
= 175.9\> 162.6 = *i 1*  
*•* If one of the two branch nodes has been pruned, the algorithm  
selects the node corresponding to the other. If both have been pruned, all active nodes are  
considered according to the criterion of highest estimated value until an integral solution  
has been found; thereafter they are considered according to the quick improvement rule.  
Table 2.1.  
LP Solution Number of Variable Chosen First Estimated  
Nodei Value *\(zLp\)* Fractional Variables for Branching Direction Solution *Zi*  
0 225.7 2 XI 200.2  
1 217.8 2 *X7* 1 162.6  
2 204.8 2 *X9* 0 175.9  
3 185.1 2 *X6* 0 175.9  
4 177.1 1 *X3* 0 175.9  
5 176 0  
6 *122.2a*  
7 *42.4a*  
8 176.0 2 *X9* 0 142.8  
9 *155.3a*  
10 *170.6a*  
11 186.4 2 *X7* 132.3  
12 *148a*  
13 *154.3a*  
14 *167.6a*  
*a* The node was terminated without necessarily achieving primal feasibility because the LP value fell below the  
value of a feasible integer solution.  
*b* The first feasible integer solution is found at node 5. It is *x 5*  
= \(0 1 0 1 1 0 1 1 0\). 2. Branch-and-Bound Using Linear Programming Relaxations 365  
In the example, we branch down from node 2 to node 5, where we find the integral  
solution *x* = \(0 1 0 1 1 0 1 1 0\) of value 176. The opposite branch node 6 is  
pruned because of its linear programming bound. Now nodes 1, 2, and 3 are candidates  
and node 1 is selected by the quick improvement rule. The rest of the calculation is self-  
explanatory.  
Generalized **U pper-Bound** Constraints  
Many integer programs with binary variables have *generalized upper-bound constraints* of  
the form  
\(2.3\) *L Xj* = 1 for *i* = 1, ... *,p,*  
*jEQ,*  
where the *Q/s* are disjoint subsets of *N.* Here we explore the branching scheme given in  
Figure 2.2, which has proved to be a very efficient way of handling these constraints and is  
widely used in mathematical programming systems.  
Suppose in a solution ofa linear programming relaxation we have 0 \< *Xk* \< 1 for some  
*k* E *Qi'* Conventional branching on *Xk* is equivalent to *Xk* = 0 or *LjEQ,\\\{k\} Xj* = 0 since the  
latter equality is equivalent to *Xk* = 1. Nowunless there is a good reason for singling out *Xk*  
as the variable that is likely to equal 1, the *Xk* = 1 branch probably contains relatively few  
solutions as compared to the *Xk* = 0 branch. If this is the case, almost no progress will have  
been made since the node with *x k* = 0 corresponds to nearly the same feasible region as  
that of its father.  
It appears to be more desirable to try to divide the feasible region of the father roughly  
equally between the sons. To accomplish this, we consider the branching rule  
\(2.4\) *L Xj* = 0 or  
*jEQ\)*  
The conventional rule is the special case of\(2.4\) with *QJ* = *\{k\}.* We can use \(2.4\) for any *Ql*  
such that *k* E *Q\}* and *LjEQ! Xj* \< 1. It seems reasonable to take *Q\}* and *Q* \\ *Ql* of nearly  
equal cardinality.  
A simple implementation of the branching rule \(2.4\) is obtained by indexing the  
variables in \(2.3\) as XiI' *Xi z'* ••• *,Xi,.* The choice of *Q\]* is then specified by an index *j,*  
1 \~j \~ *t* - 1, and *QJ* = \{iJ, ... , *i\).*  
*Example 2.3*  
ZIP = max 50Xl + *47x2* + *44x3* + *41x4* + *38xs* + *36x6* + *31x7* + *29x8* + *27x9*  
+ 25xlO + 23x ll + *21xl2* + *20x!3*  
13  
*j=l*  
*L* \(21 - *j\)Xj* \~ 22  
The solution is shown in the tree of Figure 2.7, where beside each node the solution of  
the linear programming relaxation is given. 366 11.4. General Algorithms  
# 0000.  
ZLP = 55, Xl = 1, X13 = %, *Xj* = 0, otherwise  
I I I  
zLP = 50, Xl = 1, *Xj* = 0,  
otherwise  
2 2 2  
zLP = 54o/t1, xl = \~1' xl2 = ';1,  
*X\}* = 0, otherwise  
3 3 3  
zLP = 51, *x7* = 1, *Xj* = 0, otherwise  
\(optimal solution\)  
Infeasible  
Figure 2.7  
We leave it to the reader to show that if conventional branching had been used at node  
2, a much larger enumeration tree would have resulted.  
Piecewise Linear Functions  
In Section I.1.4 we showed how a piecewise linear *functionf\(y\)* \(see Figure 2.8\) could be  
represented by a linear function with constraints on the variables.  
For any *y* = I\:J=o *a\)I,j,* where  
\(2.5\)  
*t*  
*j=O*  
I *Aj* = 1 and *Aj* E R\~ for *j* = 0, ... , *t,*  
we have  
*t*  
*fey\)* = *If\(a\)Aj,*  
*j=O*  
provided that no more than two A/s are positive; and if *Aj* \> ° and *Ak* \> 0, then *k* = *j* - 1 or  
*j* + 1.  
*f\(y\)*  
\~----\~----\~--------------\~----------\~---------y  
Figure 2.8 3\. General Cutting-Plane Algorithms  
367  
*k*  
E  
*j=O*  
*t*  
E \}..\)=o  
*j=k+2*  
Figure 2.9  
As noted in Section 1.1.4, these conditions on the A/S can be represented using linear  
constraints and binary variables. But, within the scope of a branch-and-bound algorithm,  
it is more efficient to enforce the nonlinear constraints through branching. The approach  
is similar to the treatment of generalized upper-bound constraints.  
If *Ak* \> 0, then either An = ... = *Ak-l* = ° or *Ak+l* = ... = *At* = 0. Hence  
k-l  
\(2.6\)  
I A; = ° or  
*t*  
I *Aj* = ° for *k* = 1, ... , *t* - 1.  
*j=O*  
*j=k+l*  
Moreover,if1 = *\(10,* ... , 1t\)satisfies\(2.5\)with1k \> ° and 1, \> ° for some *I* \~ *k* + 2, we can  
use \(2.6\) with index *k* + 1 for branching \(see Figure 2.9\). It is important to note that the  
solution 1 is infeasible along both branches.  
Branching strategies for using the constraints \(2.4\) and \(2.6\) are left for the reader to  
develop.  
3\. GENERAL CUTTING-PLANE ALGORITHMS  
We begin this section with a fractional cutting-plane algorithm \(FCPA\) for pure-integer  
programs that uses Gomory cuts. The main result is that the Gomory FCPA is finitely  
convergent. We then extend the algorithm to mixed-integer programs. The last topic of  
this section is a primal cutting-plane algorithm for pure-integer programs. It progresses by  
generating adjacent extreme points of the convex hull of feasible integral solutions.  
Consider an equality-constrained integer program  
max *\{ex\: x Ese\},* where *se* = *\{x* E Z\~\: *Ax* = *b\},*  
which for the rest of this section will be written as  
\(IP\) *max\{xo\: \(xo, x\)* E *SO\},* where SO = *\{xo* E *ZI, X* E Z\~\: *Xo* - *ex* = 0, *Ax* = *b\}.*  
We suppose that an optimal basis for the linear programming relaxation has been  
obtained, so IP can be written as  
*maxxO*  
\(3.1\)  
*XBi* + I *aijxj* = *aiO* for *i* = 0, 1, ... , *m*  
*jEH*  
*XBo* E Z, *XBi* E Zl for *i* = 1, ... , *m, Xj* E Zl for *j* E *H,*  
where *Xo* = *x B o ' X Bi* for *i* = 1, ... *,m* are the basic variables and where *Xj* for 368 11.4. General Algorithms  
*j* E *HeN* = \{l, ... , *n\}* are the nonbasic variables. Since the basis is primal and dual  
feasible, we have *aiD* \~ 0 for *i* = 1, ... *,m* and *aO\)* \~ 0 for *j* E *H.*  
Suppose in \(3.1\) there exists an *i* such that *aiD* \$\:. Zl. The results of Section II. 1.3 yield the  
following proposition.  
Proposition 3.1. *\(Gomory fractional cut\) !faiO* f£. Zl, *then*  
*L fijx\)* = *fiD* + *Xn+h Xn+1* E Zl,  
*\)EH*  
*where fij* = *aij* - *laijJ* for *j* E *Hand fiD* = *aiD* - laiDJ, *is a valid equality for* so.  
*Example* 3.1. Our standard example written in equality form is  
*maxxo*  
*Xo* - *7xI* - *2X2* o  
*- XI* + *2X2* + *X3 4*  
*5xI* + *X2* + *X4 20*  
*- 2x* I - *2x* 2 + *X* 5 - 7  
*Xo* E *ZI, x\)* E Zl for *j* = 1, ... , 5.  
An optimal solution to the linear programming relaxation is  
3 16 332  
*Xo + TIX3* + *TIX4* 11  
1 2 36  
*XI - TIX3* + *TIX4* 11  
5 1 40  
*X2* + *TIX3* + TIX4 11  
8 6 75  
TIX 3 + TIX 4 + *X5 =* U'  
where *X3* = *X4* = o.  
Generating the fractional cut from row 0, we obtain  
In terms of the original variables, the cut is *2x* I + *X2* \~ 10.  
The Gomory FCPA is just the general FCPA given in Section 1, with all of the generated  
valid inequalities being Gomory cuts.  
*Initialization\:* Set *t=* 1, *z1\(x\)* = *Xo,* S1 = *\{xo* E *R* 1, *X* E R\~\: *Xo* - *ex* = 0, *Ax* = *b\}.*  
*Iteration t\:* 3\. General Cutting-Plane Algorithms 369  
*Step* 1\: *Solution of the linear programming relaxation.* Solve  
max\{xo\: *\(xo, x\)* E Sk\}.  
*IfRpl* is feasible and has an optimal solution, suppose the solution is *\(xb, Xl\).* \(See the  
remark below ifRp1 has unbounded optimal value.\)  
*Step* 2\: *Optimality test.* If *Xl* E Z\~, then *Xl* is an optimal solution.  
*Step* 3\: *Infeasibility test. IfRpl* is infeasible, then IP is infeasible.  
*Step* 4\: *Addition of a cut.* Choose a row *X Bi* + *LjcHt* a\~jxj = a\~o with a\~o ff Z 1. Let  
*L fUXj* - *Xn+t* = *fiO, Xn+t* E Zl  
*jcHt*  
be the fractional Gomory cut for the row. Set  
*Step* 5\: *t* +- *t* + 1.  
When the cut is added, the new basis, which includes *x n+t* as a basic variable, is dual  
feasible. Primal feasibility is violated only by *X n+ l* \< O. Hence it is natural to solve *Rpt+l* by  
the dual simplex method.  
If *Rpl* is unbounded, then by Corollary 6.8 of Section 1.4.6 we have that IP is either  
unbounded or infeasible. Moreover, by Theorem 4.1 of Section 1.5.4, ifIP is feasible, then  
there is a feasible solution with *LicN Xi* \~ *d,* where *d* is a suitably large integer. Hence we  
can add the constraint *LicN Xj* \~ *d* to RPI. Then IP is unbounded if and only if the  
modified problem has a feasible solution.  
*Example* 3.1 *\(continued\).* relaxation Rp1 is  
As noted above, the solution of the linear programming  
*\(xA,x'\)* = *\(X6,* xl, ... ,xl\) = C\~2 \~\~ \~\~ 0 0 \~n.  
Also, *X2* + *nX3* + TIX4 = 1? Generating the fractional cut from this row yields  
An optimal solution to Rp2 is  
\( 2 2 *Xo, X* b ••• *,X6* = -5- 5 3 \~ 0 2\: 0\).  
2\) \(149 17  
Also, *Xo* + \~X4 + \~X6 = 29\~. Generating the next fractional cut from this row yields 370 11.4. General Algorithms  
An optimal solution to Rp3 is \(29 1f i ¥ ° 1f 1 0\). Also, *X2* - *!X4* + *iX 7* = i.  
From this row, the fractional cut is  
Xg E Zl.  
The optimal solution to Rp4 is \(28 4 0 8 0 1 3  
0\), which is also optimal  
to IP.  
In terms of the variables Xj, and *X2,* the three added cuts are *X2* \~ 3,  
*2x,* + *X2* \~ 9, and *3x,* + *X2* \~ 12 \(see Figure 3.1\).  
Finite Convergence  
We now give some additional specifications on the Gomory FCPA which guarantee that it  
converges finitely. We suppose that  
*\{x* E *R1\: Ax* = *b\}* \~ *\{x* E *R1\: L xi* \~ *d\}*  
*iEN*  
for some suitably large *d* E Zl. As noted above, this is without loss of generality.  
The convergence argument depends on a lexicographic decreasing sequence of solution  
vectors *\{Xh, Xl\}, t* = 1, 2, ... , which can be obtained, as will be explained soon, by solving  
a sequence oflinear programs by a lexicographic dual simplex algorithm. Recall that *x* \~ *Y*  
if there exists a *k* such that *Xk* \< *Yk* and *Xi* = *Yi* for *i* \< *k.* Also, *x* 1;. *Y* if *x* \~ *Y* or *x* = *y.*  
The algorithm finds the lexicographically largest element in Sa or shows that So is  
empty. Since the objective value is the first component of *\(xo, x\),* a lexicographically  
largest element is optimal. Let *cj* = max\(O, cJ and *Cj* = mineO, *ci\)'* Since ° \~ *xi* \~ *d* for  
*j* E *N,* it follows that if *\(xo, x\)* E SO we obtain  
*\(d L Cj,* 0, ... , 0\) 1;. *\(xo, x\)* 1;. *\(d L ci, d,* ... , *d\).*  
*iEN iEN*  
3  
2  
o 2 3 4  
Figure 3.1 3\. General Cutting-Plane Algorithms 371  
Let *ci* = *\(d LjEN cj, d,* ... *,d\).* We will show that the cuts in the Gomory FCPA can be  
chosen so that after *t* cuts have been added it will follow that *\(xo, x\)* E SO implies  
*\(xo, x\)* l;;. *at,* where *at* E *zn+1* and *at* !\:\:. *at-I.* It then follows that the total number of cuts is  
bounded.  
The Lexicographic Dual Simplex Algorithm  
Consider a basic solution to the linear programming relaxation of\(3.1\) written in the form  
maxxo  
\(3.2\)  
*Xi* + I *aUxj* = aiQ for *i* = 0, 1, ... , *n*  
*jEH*  
*Xi* ?\:\: 0 for *i* = 1, ... , *n,*  
where *H* is the index set of non basic variables. The representation \(3.2\) contains a row for  
each variable. Thus for *i* E *H* we have the trivial identity *Xi* - *Xi* = 0, that is,  
*aii* = -1, *ajQ* = 0, and *au* = 0 for\} E *H* \\ \{i\}. The basic solution obtained from \(3.2\) is  
*Xi* = *aiQ* for *i* = 0, ... , *n.*  
Since the constraint set is bounded, there is a dual feasible basis, that is, a basis with  
*aOj* ?\:\: 0 for all \} E *H.* Thus if *aOj* \> 0 or *\(alj,* ... , *anJ* \~ 0, we have *aj* = *\(aoh alj,* ... ,  
*anj\)* \~ O. However, *ifaOi* = 0 and *\(alh* ... , *anj\)* f 0 we add the redundant equation  
\(3.3\) *y* + I *Xj* = *d, y* ?\:\: 0  
*jEH*  
as the second row of\(3.2\). Now \(1, *alh* ... , *anJ* \~ 0 so that *\(aOh* 1, *a lh* ... , *anj\)* \~ O. Hence  
we assume that we have a basic solution to the linear program that satisfies *aj* \~ 0 for  
*\}EH.*  
Proposition 3.2. *If aiQ?\:\:* 0 *for i* = I, ... *,n and aj* \~ 0 *for* \} E *H, then*  
*\(xo, x\)* = *ao* = *\(aoo, alO,* ... , *ano\) is the lexicographically largest feasible solution to \(3.2\)*  
*and is optimal.*  
*Proof* By hypothesis, *\(xo, x\)* = *ao* is feasible. Moreover, *ao* is the lexicographically  
largest feasible solution since any other feasible solution is of the form *ao* - *LjEH ajxj*  
and *aj* \~ 0 with *Xj?\:\:* 0 for all \} E *H.* Finally, the lexicographically largest solution  
maximizes *Xo. •*  
We now give a finite simplex algorithm for finding the lexicographically largest feasible  
solution to \(3.2\).  
Proposition 3.3. *Suppose \(xo, x\)* = *aoP* - *LjEHP a\)Xj is a basic solution with dj* \~ 0 *for*  
\} E *HP andafo* \< O. *A dual simplex pivot that makes Xi nonbasic yields ag+ 1* !\:\:. *al.*  
*Proof* Suppose *k* E *HP* and *x k* is the variable to become basic. Then *afk* \< 0 and  
a\~+l = a\~ - \(afo *I* afk\)a\~ !\:\:. *ab* since a\~ \~ 0 and \(afo *I afk\)* \> O. •  
Thus we need to give a rule for choosing the variable to enter the basis so that af+l \~ 0  
for all\} E *HP+l.* 372 11.4. General Algorithms  
Proposition 3.4. *Suppose amP* \< 0, *O!j* !\> 0 *for\}* E *HP, and Xi is chosen as the variable to*  
*leave the basis. Let* Hf = \{j E *HP\: \(fij* \< a\}. *If* Hf = 0, *then there is no feasible solution.*  
*Otherwise choose k* E Hf *to satisfy*  
\(3.4\)  
1 *-P L* 1 *-P It II· HP* \\ *\{k\}*  
*\_P ak* \> *-P aj or a \)* E i  
*aik aij*  
*and also choose Xk as the variable to become basic. Then ar* I !\> 0 *for all\}* E *HP+* 1 •  
*Proof* If Hf = 0, then *Xi* = afo - *LjEHP* a\~xj \< 0 for all feasible solutions since afo \< 0  
and a\~ \~ 0 for all\} E *HP.*  
Now suppose *k* E *Hf* is chosen to satisfy \(3.4\). Note that because the system of  
equations contains the identitiesxj - *Xj* = 0 for all\} E *HP,* a\~ cannot be a scalar multiple of  
*af* for any\} E *Hf\\\{k\}.* Hence \(3,4\) uniquely determines *k.* We have  
a. af'tl = - \(l / afk\)a\~ !\> 0 since *afk* \< 0 and a\~ !\> o.  
b. For\) E *HP* \\ *\{k\},* we have *ar*  
*1*  
= *CPj* - \(a\~ / afk\)a\~. There are two cases. If  
. h *-p+l L* o· *-P L* 0 *-P L* 0 d *\(-P* / *-P\)* 0 If· *HP* \\ *\{k\}*  
*\)* E *HP* \\ *Hf,* t en *a j* \> SInce *a k* \> *,a j* \> an *a* ij *a ik* \:\:\:\:; . *\)* E i ,  
then *at1* !\> 0 by \(3.4\). •  
Theorem 3.5. *If we begin with a basic solution satisfying aJ* !\> 0 *for all\}* E SO *and apply*  
*the dual simplex pivoting rule given in Proposition* 3.4, *then in afinite number of pivots we*  
*either show that* \(3.2\) *has no feasible solution or find the lexicographically largest solution.*  
*Proof* Since the sequence *\{aiD* is lexicographically decreasing, no basis can be  
repeated. •  
Now we return to the Gomory FCPA and suppose that we have found *\(Xb, xt\),* the  
lexicographically largest solution to *RPt.* If *\(Xb, xt\)* E *zn+l,* we have solved IP. So suppose  
this is not the case.  
Proposition 3.6. *Let \(Xb, xt\) be the lexicographically largest solution to Rpt and suppose*  
.xi E *Zlfor i* = 0, ... , *s* - 1 *and Xs* fJ. *ZI. Let at* = *\(xb,* ... *,Xs-I\> \[XsJ, d,* ... , *d\). If\(xo, x\)*  
*is afeasible solution to* IP, *then \(xo, x\)* 1; *at.*  
*Proof* If *\(xo, x\)* is feasible to IP, then *\(xo, x\)* E *zn+l, Xj* \:\:\:\:; *d* for\} *EN,* and  
*\(xo, x\)* J, *\(xb, xt\).* The vector *at* is the lexicographically largest vector that satisfies these  
properties. •  
Now all we need to do is produce a Gomory cut so that *\(xb+1*  
*, xt+l\),* the lexicographically  
largest solution to *Rpt+1*  
*,* satisfies *\(xb+1*  
*, xt+l\)* !\( *at.* Then, either *\(xb+1*  
*, xt+l\)* E *zn+l* and we are  
done or *at*  
*+*  
*1* !\( *at.*  
Proposition 3.7. *Let \(Xb, xt\) and at be defined as in Proposition* 3.6. *By adding the cut*  
\~ *I' I'* ZI *d* .. . *b· \(t+l* 1+1\) *L t*  
\~jEHljsjXj - *Xn+t* = *jsO, Xn+t* E +, *an reoptlmlzlng, we 0 tam Xo ,X* \:\:\:\:; *a.*  
*Proof* It suffices to consider the first pivot. In this pivot, the variable that becomes  
nonbasic *isxn+t sincexn + t* = - *Iso* \< 0 *andxt* \~ O. *Letxk* be the variable to become basic and  
let *\(Xb, .\:e\)* be the solution after one pivot. Then 3\. General Cutting-Plane Algorithms 373  
where a\~ !;.. ° and *Iso* / *Isk* \> 0. There are two cases.  
i. There exists an *i* \~ *s* - 1 such that a\~k =\#= 0\. Since a\~ !;.. 0, its first nonzero component  
a\~k is positive and *q* \~ *s* - 1. Hence, we obtain x\~ = x\~ for *i* = 0, ... , *q* - 1 and  
x\~ \< x\~. Thus *\(Xb, xt\)* \~ *cl.*  
ii. Here a\~k = ° for *i* = 1, ... , *s* - 1. Since *Isk* =\#= ° and a\~ !;.. 0, we have a\~k \~ *Isk* \> 0.  
Hence x\~ = x\~ for *i* = 0, ... *,s* - 1 and x\~ \~ lx\~J. Hence *\(Xb, xt\)* k; *at. •*  
We preserve the order of the original equations by putting the equations for the cuts at  
the end. Moreover, since the slack variable x *n+t* for the *tth* cut becomes nonbasic after the  
cut is added, we have the trivial equation *Xn+t* - *Xn+t* = 0. If *Xn+t* becomes basic in a  
subsequent pivot, its value is positive and the cut is no longer active. At this point, we drop  
the cut, and hence *X n+\(,* from the problem. This implies that, for computational purposes,  
we only need to keep the *n* + 1 equations Xi + *LjEHI* a\~jxj = a\~o for *i* = 0, ... *,n.* Note that  
these equations will, in general, contain slack variables from cuts. The remaining equa-  
tions are trivial identities. By Proposition 3.4, the vectors *\{aJ\}jEH* are lexicographically  
positive, so the addition and deletion of cut equations does not affect the properties of the  
lexicographic dual simplex method.  
Theorem 3.8. *If the Gomory* FCPA *is executed by choosing thefractional cut from the row*  
*of lowest index whose corresponding variable is not an integer, and the resulting linear*  
*program is solved to obtain a lexicographically largest solution \(i.e., by the lexicographic*  
*dual simplex method\), then after at most \(d* + 1 \)n+l *\(dLjEMCl* - *Cj* + 1\)\) *cuts, the algorithm*  
*finds an optimal solution or shows that* IP *is infeasible.*  
*Proof* vectors *y* E zn+l that satisfy  
By Propositions 3.6 and 3.7, the number of cuts is bounded by the number of  
*\( d* ,2 *Cj,* 0, ... , 0\) k; *y* k; *\(d* ,2 *cl, d,* ... , *d\).*  
*JEN JEN*  
In addition, we have added another factor of *\(d* + 1\) to the bound to accommodate the  
upper-bound constraint *LjEN Xj* \~ *d. •*  
*Example* 3.1 *\(continued\).* Here we apply the finite Gomory FCPA. After pivoting, each  
cut row is discarded.  
The solution to Rpl is *\(xb,* Xl\) = \(3on- if *WOO* tT\). Since Xo + rrX3 + 1fX4 = 30n,  
we add the cut *ftX3* + nX4 = n + *X6,* X6 E Zl.  
The solution to Rp2 is \(30 \~ .if j ° .If\). Since x I + *1X4* - tX6 = .if, we add the cut  
tx 4 + 1X 6 = 1 + X 7, X 7 E Z l.  
The solution to Rp3 is \(29t 1 1 1 ° 5\). Since Xo + *1X4* + \~X7 = 291, we add the cut  
tX4 + *!X7* = ! + Xg, Xg E Zl.  
The solution to Rp4 is \(28\~ \~ f ° \~ \~\). Since Xo + *§X3* + \~X8 = 28\~, we add the  
cut *-§X3* + \~X8 = \~ + *X9,* X9 E Z\~.  
Reoptimizing yields the optimal solution \(28 4 ° 8 ° 1\).  
Note that the bounds Xl \~ 4andx2 \~ 4 are easy to obtain from theoriginalinequalities. 374 11.4. General Algorithms  
Hence *Xo* \~ 36, *X3* \~ 8, *X4* \~ 20, and *Xs* \~ 9. Thus any solution is lexicographically  
equal to or less than aO = \(36 4 4 8 20 9\). Now the solution to RP'  
yields a' = \(30 4 4 8 20 9\) \~ *an.* The successive linear programming solutions  
yield a 2  
= \(30 3 4 8 20 9\) \~ *a',* a 3 = \(29 4 4 8 20 9\) \~ *a2*  
*,* a 4 =  
\(28 4 4 8 20 9\) \~ *a 3*  
*,* and *as* = \(28 4 0 8 0 1\) \~ *a 4*  
*•* Thus we see how the  
lexicographic upper bound is reduced at each iteration.  
There is a nice interpretation of the sequence *\{ak\}k=o* on an enumeration tree \(see Figure  
3.2\). Here we have enumerated all possible integral values for the variables where  
*x* = *\(x" X2, X3\)* E B3 and 0 \~ *Xo* \~ 3. Note that the leaves of the tree, read from left to right,  
give the possible values of *\(xo, x\)* in increasing lexicographic order. Now suppose the  
solution ofRP' gives *X6* = 3 and 0 \< xl \< 1. Figure 3.2 shows the integral vectors that are  
eliminated by this solution and also shows those eliminated by Rp2 if *X6* = 3, XI = 0, and  
o \< x\~ \< 1. Each cut eliminates at least the rightmost leaf that is still a candidate. Hence we  
can think of the cutting-plane procedure as a lexicographic search through the integral  
vectors until the lexicographically largest one that is feasible to IP is found. This suggests  
that it is important to choose a cut so that the subsequent pivot yields a large lexicographic  
decrease in *710.* Insofar as we know, strategies of this type have not been systematically  
investigated. Perhaps some such strategy would improve the reputably poor performance  
of fractional cutting-plane algorithms.  
Extension to Mixed-Integer Programming  
The Gomory FCPA extends straightforwardly to mixed-integer programs. Suppose, in the  
solution of the linear programming relaxation of an Mlp, *Xi* E Zl is a basic variable given  
by  
*Xi* + I *aijXj* + I *aijYj* = *aiD,*  
*hEHI jEH\\HI*  
where *HI* is the index set of nonbasic integer variables and where *aiD* \$. Z'. Here we use the  
Gomory mixed-integer cut  
\(see Proposition 7.4 of Section 11.1.7\). Everything else remains as above except for the  
finite convergence argument.  
In mixed-integer programming, it is not reasonable to assume that the objective  
variable *Xo* is integer-valued. Hence we cannot use the objective row for obtaining cuts.  
But our finite convergence argument depended on deriving a cut from the lowest-index  
fractional variable. In fact, by excluding *Xo* as a candidate the convergence argument fails  
to hold \(see the example given in exercise 12\).  
The only way we know to salvage finite convergence is to scale the problem so that *Xo* is  
integral. But this is definitely unsatisfactory for computational purposes.  
Primal Cutting-Plane Algorithm  
A disadvantage of fractional cutting-plane algorithms is that no feasible solution is found  
until the algorithm terminates. Here we sketch a cutting-plane algorithm that circumvents 3\. General Cutting-Plane Algorithms 375  
Figure 3.2  
this problem. Unfortunately, it is not a practical algorithm because it tends to require an  
exorbitant number of cuts.  
Suppose we have a nonoptimal extreme point of the convex hull of feasible integral  
solutions. The idea of a primal cutting-plane algorithm is to use cuts to enable pivoting to  
an adjacent extreme point of the convex hull whose objective value is greater.  
The geometry is shown in Figure 3.3, where S = *\{x* E *Z2\: Ax* \~ *b\}* and the outer  
polytope is *P* = *\{x* E *R* 2\: *Ax* \~ *b\}.* If *x* I happens to be an integral extreme point of *P,* then  
it must also be an extreme point of conv\(S\). Given a basic representation of *x* , in which the  
active constraints are *Xk* = ° and *Xk'* = 0, our objective is to pivot from *Xl* to *x 2* or to *x 3*  
*•*  
However, a standard simplex pivot will yield a fractional extreme point of *P,* either *X4* or  
*x5*  
*•* To pivot from *Xl* to *x2*  
*,* the polytope that contains conv\(S\) must contain the facet-  
defining inequality *a\*x* \~ *b\** and any other valid inequality defining a face that supports  
conv\(S\) at *x 2*  
*,* say *aOx* \~ *bOo* By first adding the constraint *Xp* = *b\** - *a\*x,* a degenerate  
pivot that *makesxp* nonbasic *andxk* basic can be performed, and we still have the extreme  
point *Xl.* We then add the constraint *x p '* = *bO* - *aOx* and make *xp'* nonbasic and *Xk'* basic.  
This yields the extreme point *x 2*  
*•* Thus, in two dimensions, we need the facet of conv\(S\)  
that defines the edge joining *Xl* and *Xl* to be able to pivot from *Xl* to *x 2*  
*•*  
Y Objective function  
*a\*x* = *b\**  
Figure 3.3 376 11.4. General Algorithms  
Analogously, in *n* dimensions we need *n* - 1 valid inequalities that contain the one-  
dimensional face joining Xl to *X2* and another valid inequality that defines a face which  
supports conv\(S\) at *X2* to be able to pivot from *x* I to *X2.* These very stringent requirements  
explain why a primal cutting-plane algorithm for general integer programming is likely to  
be very slow. Besides the problem of finding an initial integral point, it will be necessary to  
produce valid inequalities that contain the one-dimensional faces \(edges\) on a path from  
the initial point to an optimal point. In contrast, a fractional cutting-plane algorithm can  
succeed with a much weaker family of cuts, and a nondegenerate pivot occurs immedi-  
ately after the addition of each cut.  
We now study how these primal cuts can be derived algebraically. Consider a basis for  
the linear programming relaxation of IP given by \(3.1\) in which the coefficients *au* are  
integral for *i* = 0, 1, ... , *m* and all\} E *H.* A basis that satisfies these conditions is available  
if *A* = *\(A* " *J\)* and *b* \~ 0. Otherwise, a Phase I procedure may be required.  
*aOk* \< ° for *k* E *H.* Consider a primal pivot in which *Xk* becomes basic. Suppose  
If the basis is dual feasible, the integral solution *\(xo, x\)* = *ao* is optimal. So suppose  
*aiO*  
min \_.  
i=l, ... *,m\:aik\>O aik*  
If *ark* = 1, we can pivot on the row *XB,+ LjEHarjXj =aro* and maintain integrality. If  
*ark* \* 1, we add a C-G cut derived from the inequality *LjEH arjXj* \~ *aro.* In particular,  
multiply this inequality by 1 / *ark\>* 0 and then round to obtain  
Adding a slack variable yields the equation  
\(3.5\) *arj aro*  
*Xn+l* + *Xk* + I =- *Xj* = =-  
*jEH\\\{k\} ark ark*  
l-J l- J  
# ,  
*Xn+1* E Zl.  
Figure 3.4 3\. General Cutting-Plane Algorithms 377  
Since *ark\>* 0, *larO/arkJ* \~ *arO/ark* and the coefficient *Ofxk* in \(3.5\) is one, we can pivot  
on the row \(3.5\) and maintain integrality.  
The geometry of the cut is shown in Figures 3.4 and 3.5. In Figure 3.4, assume we are at  
the point Xl and we want to pivot along the line *Xz* = O. An ordinary simplex pivot would  
yield the fractional point *X3.* Instead we introduce the cut Xl + *laz* / *adxz* \~ lb/ad, which  
enables us to pivot to the integral point *xZ.* The cut is obtained by adding  
*\(taz* / ad - *az* / *al\)xZ* \~ 0 to Xl + *Caz* / *al\)xZ* \~ *b/al* and then rounding. Thus the cut gives  
the convex hull of the region *\{x* E *R Z \:* Xl + ta2 / *adx2* \~ *b/al\}'*  
In Figure 3.5, there are no feasible integral points along the line *a21Xl* + *a22X2* = *b2 ,*  
so it is not possible to move from *X* 1 along this line. However from an appropriate  
nonnegative linear combination of *ailXl* + *ai2X2* \~ *bi* for *i* = 1, 2, we obtain the inequality  
alxl + *a2x2* \~ *b,* with 0 \< *b* \< *a h Az* \< O. Now we proceed as above to obtain the  
inequality Xl + la2 / *adX2* \~ *b/al* and then the cut Xl + *\[az* / *adx2* \~ *\[b/ad.*  
*Example 3.2*  
max *Xo* = XI + *2xz*  
-4xl+ *X2+X3* = 0  
7Xl + *4X2* + *X4* = 14  
xEZ!.  
An integral solution to the linear programming relaxation is  
*Xo* = 0 + *X* I + *2x 2*  
*X3* = a + *4x* 1 - *Xz*  
*x4=14-7xl-4xz*  
Xl = *X2* = O .  
• .I'iE---Xl + 1l\~21al JX2 s *\{jlal*  
\~--\~\~----\~\~-----------------Xl  
Figure 3.5 378 11.4. General Algorithms  
2  
4th cut  
\~------------\~------------\~\~----------Xl  
o 2  
Figure 3.6  
Suppose we choose to increase *X2.* The variable that becomes nonbasic is *X3,* and no cut is  
required since the coefficient of *X2* is one. This degenerate pivot yields the equations  
Now we make *x* I basic and derive the cut *x* I - *X* 3 \~ 0 from *23x* I - *4x* 3 \~ 14. We then do  
a degenerate pivot on the row *Xs* = 0 - XI + *X3.* Three more cuts and pivots are required  
\(see Figure 3.6\). The last pivot is the only nondegenerate one and it yields the optimal  
solution *\(x" X2\)* = \(1 1\).  
It is a fact that with an appropriate choice of pivot columns and rows from which to  
generate the cuts, a finite algorithm can be obtained. But for the reasons stated above,  
primal cutting-plane algorithms are likely to be slower than fractional cutting-plane  
algorithms. 4\. Notes 379  
4\. NOTES  
Section 11.4.1  
Geoffrion and Marsten \(1972\) proposed a somewhat different framework for discrete  
optimization algorithms.  
Enumerative methods go under the general names of *branch-and-bound, implicit*  
*enumeration,* and *divide-and conquer.* The latter term is frequently used in the computer  
science literature, and the first two terms are commonly used in the mathematical  
programming/operations research literature. Algorithms that focus on pruning by bounds  
that are obtained from relaxations or dual solutions are generally referred to as *branch-*  
*and-bound algorithms,* a term coined by Little et al. \(1963\). Those that focus on pruning  
based on logical tests and inequalities derived from Boolean implications \(see Section  
I.1.6\) are called *implicit enumeration algorithms,* a term apparently coined by Geoffrion  
\(1967\). Many algorithms use both of these ideas, so a classification of enumeration  
algorithms along these lines is not particularly relevant. Furthermore, although logical  
testing is important, pruning by bounds has emerged as the fundamental tool of enumera-  
tive algorithms.  
Land and Doig \(1960\) gave the first branch-and-bound algorithm for general integer  
programs. However, the popularity of this approach increased substantially after the  
publication of the branch-and-bound algorithm for the traveling salesman problem by  
Little et al. \(1963\) because it demonstrated that large \(at that time\) problems could be  
solved by controlled enumeration. Balas \(1965\) gave the first implicit enumeration  
algorithm for general 0-1 IP's  
General expositions and survey articles on enumerative methods are by Lawler and  
Wood \(1966\), Agin \(1966\), Mitten \(1970\), Tomlin \(1970\), Geoffrion and Marsten \(1972\),  
Beale \(1979\), Garfinkel \(1979\), and Spielberg \(1979\).  
Sensitivity and parametric analysis for integer programs has been discussed by Geof-  
frion and Nauss \(1977\), Shapiro \(1977\), Nauss \(1979\), Holm and Klein \(1984\), Schrage and  
Wolsey \(1985\), and Cook et al. \(1986\).  
Parallel processing presents new opportunities for computational advances in discrete  
optimization. Kindervater and Lenstra \(1985, 1986\) give an annotated bibliography and  
an introduction to parallelism in combinatorial optimization. In an empirical study, Pruul  
\(1975\) simulated parallel computation and showed that by exploring several nodes of an  
enumeration tree simultaneously it is possible to reduce substantially the total number of  
nodes that need to be considered. His results have been summarized in Pruul et al. \(1988\).  
Another developing area is interactive optimization. Fisher \(1985\) surveyed results and  
opportunities for using interactive methods in discrete optimization.  
Section 11.4.2  
Almost all general MILP codes use a branch-and-bound framework with linear program-  
ming relaxations. As noted above, the first algorithm of this type was described by Land  
and Doig \(1960\). They proposed the division scheme shown in Figure 2.3. The now  
commonly used variable dichotomy scheme \(Figure 2.1\) was proposed by Dakin \(1965\).  
Penalties were introduced by Driebeek \(1966\) and were sharpened by Tomlin \(1971\).  
The treatment of generalized upper-bound constraints by the division scheme shown in  
Figure 2.2, together with the indexing scheme described below \(2.4\), was introduced by  
Beale and Tomlin \(1970\). They called such sets *specially ordered sets.* This terminology is  
now widely used, and the concept is very important in the global maximization of 380 11.4. General Algorithms  
piecewise linear nonconcave functions. Beale and Forrest \(1976\) developed this approach,  
which enables the implementation of the division scheme \(2.6\) without the explicit use of  
auxiliary integer variables.  
Various strategies for exploring the enumeration tree, together with experimental  
comparison, are given by Benichou et al. \(1971, 1977\), Breu and Burdet \(1974\), Forrest et  
al. \(1974\), Gauthier and Ribiere \(1977\), and Mitra \(1973\). The strategy described in the text  
is used in the commercial code SCICONIC as described by Beale \(1979\).  
Eleven commercial mixed-integer programming systems, including all of the available  
options for branching, node, and variable selection as well as other characteristics of the  
codes, have been described by Land and Powell \(1979\). This article also includes a  
comparison of two of these codes on a small number of test problems, as well as brief  
descriptions of several "academic" codes. Powell \(1985\) is an annotated bibliography that  
updates the Land and Powell article.  
The XMP system of Mars ten \(1981\) has been updated to include branch-and-bound for  
general MIPs. It is a highly modular system, which makes it very useful for research, and is  
available in both microcomputer and main-frame versions. Several other linear program-  
ming systems for microcomputers also have branch-and-bound capabilities. One of the  
most widely used is the LINDO system of Schrage \(1986\).  
The art involved in using commercial codes to solve large scale MIPs is discussed in the  
context of solving practical problems by Beale \(1983\) and Suhl \(1985\).  
leroslow \(1974\), Ibaraki \(1976, 1977\), Rinnooy Kan \(1976\), and Fox et al. \(1978\) have  
presented some theoretical results on node selection and branching strategies. 1 eroslow  
gives a family of problems for which the number of nodes that must be searched is  
exponential with respect to the size of the problem, regardless of which strategies are used.  
Section 11.4.3  
Gomory \(1958, 1960a, 1963a\) shows that the FCPA is finitely convergent with appropriate  
use of the lexicographic dual simplex algorithm. The proof given here is based on that  
given by Nourie and Venta \(1982\), which is really just a reinterpretation ofGomory's proof  
that provides additional insight into the nature of the convergence.  
Given a fractional LP solution, Gomory and Hoffman \(1963\) showed that the cuts  
*Lj Xj* \~ 1, where the sum is taken over all nonbasic variables cannot yield a finite FCPA.  
Bowman and Nemhauser \(1970\) proved that the stronger cuts given in exercise 11 yield a  
finite algorithm.  
The mixed-integer cutting-plane algorithm and its finite convergence under the  
assumption that the objective function variable must be an integer is given in Gomory  
\(1960b\). Without this assumption, no finite cutting-plane algorithm for MILPs is known.  
A primal cutting-plane algorithm for general integer programs was proposed by Ben-  
Israel and Charnes \(1962\). A finitely convergent primal cutting-plane algorithm was given  
by Young \(1965\), and simplified versions were obtained by Glover \(1968a\) and Young  
\(1968\). Because of poor computational experience, this line of research has been very  
inactive. An exception is a primal cutting-plane algorithm for the traveling salesman  
problem developed by Padberg and Hong \(1980\). Although this algorithm has been  
moderately successful, it seems to be inferior to an FCPA for the traveling salesman  
problem \(see Section 11.6.3\).  
Another strategy for cutting-plane algorithms is to maintain integrality and dual  
feasibility and then to use cuts to obtain primal feasibility. A finite algorithm of this type  
has been given by Gomory \(1963b\). Other such algorithms have been obtained by Glover  
\(1965, 1967\). 5\. Exercises 381  
5\. EXERCISES  
1. Solve the integer program  
by branch-and-bound.  
Examine the procedure graphically.  
Investigate how the branch-and-bound tree changes with different branching  
strategies.  
2\. Solve the integer knapsack problem  
by branch-and-bound.  
3\. Solve the problem of exercise 2 with the additional constraint *x* E *B4.*  
4\. Propose various ways to estimate degradations. Test them on the above problems.  
5\. Solve Example 2.3 using a branch-and-bound algorithm, with the conventional  
branching rule. Draw the branch-and-bound tree.  
6\. Modify the general branch-and-bound algorithm if, instead of an optimum solution,  
we only want to find a feasible solution within a given percentage, say *p%* of the  
optimum value.  
7\. Consider the integer program  
max *- Xn+!*  
*2x* I + *2x* 2 + . . . + *2x n* + *X n+1* = *n*  
*x* E *Bn+!.*  
Show that any branch-and-bound algorithm using the linear programming relaxa-  
tion to compute upper bounds will require the enumeration of an exponential  
number of nodes when *n* is odd.  
8\. Consider a mixed-integer program with one integer variable. Show that the branch-  
and-bound tree for this problem will have no more than three nodes. Why? 382 11.4. General Algorithms  
9. Consider the integer program of exercise 1. The optimal linear programming basis  
gives the information  
maxz  
\(lP\)  
1 6  
Z *+-X3+-X 4* 5 5  
= 38  
4 1 17  
*Xl + T5X3* - *T5X 4* 3  
1 4 16  
*X2* - *T5X3* + *T5X4* 3  
1 1 2  
*--X3+-X 4+XS* 3 3 3  
xEZ\~  
i\) Solve using Gomory's FCPA.  
ii\) Solve using Gomory's finitely convergent FCPA.  
iii\) Use the solutions ofi and ii to give optimal dual solutions ofIP.  
iv\) . Use iii to find an upper bound on the optimal value ofIP when  
10. What is the maximum number of Gomory cuts needed to verify that a 0, 1 IP is  
feasible or infeasible. *\(Hint\:* Consider the enumeration tree.\)  
11. i\) Let S = *\{x* E *ZZ\: LjEN ajxj* = *b\}* with *aj,* bE *Rl,* and *b* \$. *Z!.* Show that  
*LjEN\* Xj* \~ 1 is a valid inequality for S, where *N\** = \{j *EN\: aj* \$. Zl\}.  
ii\) Show that a finitely convergent FCPA is obtained by using these cuts in place of  
Gomory cuts.  
iii\) Carry out several iterations on the IP of exercise 7, and compare the correspond-  
ing enumeration trees.  
12\. i\) Use Gomory mixed-integer cuts to solve the integer program  
max *y*  
Xl+X2+y\~2  
*-Xl* +y\~O  
*- X2* + *Y* \~ 0  
*X* E Zi, *Y* E *Z!.*  
ii\) Replace the constraint *y* E Z! in i by *y* E *Rl* to give a mixed-integer program  
\(see exercise 22 of Section 11.1.9\). What happens now using the Gomory mixed-  
integer cuts?  
13\. Describe a Phase 1 procedure for a primal cutting-plane algorithm. **11.5**  
# Special-Purpose  
# Algorithms  
**1. INTRODUCTION**  
The algorithms presented in the previous chapter have the great advantage of robustness.  
They can, in principle, be applied to all linear integer programs. However, there is often a  
heavy price to pay for this generality.  
Three major reasons why a problem class may not be solved satisfactorily by a general  
algorithm are\:  
l. size of the formulation,  
2. weakness of the bounds, and  
3. speed of the algorithm.  
On the other hand, when instances of a class of highly structured integer programs are  
to be solved, the structure can often be used to improve the performance substantially in  
one or all of the three areas cited above. In this chapter we will show how structure can be  
used either to devise special-purpose algorithms or to improve the performance of general  
algorithms for several classes of problems.  
Integer programming formulations frequently have a very large number of variables or  
constraints. For example, in the strong formulation of the uncapacitated facility location  
problem described in Section I. 1.3, an instance with *n* = 50 locations and *m* = 200 clients  
has more than *mn* = 10,000 variables and more than *mn* = 10,000 constraints. Similarly  
for the traveling salesman problem on *m* nodes, the formulation given in Chapter 1.1 has  
*O\(m2\)* variables and *O\(2m\)* subtour elimination constraints.  
The computation of bounds requires the *choice of a relaxation.* Typically this choice  
involves a tradeoff between the strength of the bound obtained from the relaxation and the  
speed with which it can be calculated. For the symmetric traveling salesman problem there  
is a large hierarchy of relaxations. For the uncapacitated facility location problem the  
tradeoff is between the linear programming relaxation of the weak form ulation, which can  
be solved by a formula but gives weak bounds, and the linear programming relaxation of  
the strong formulation, which gives very good bounds but is much harder to solve.  
Very often the relaxation is embedded in a branch-and-bound algorithm. Here struc-  
ture may make it possible to find nearly optimal solutions to the dual of the relaxation  
very quickly, providing the upper bounds needed for the branch-and-bound algorithm.  
Structure may also help us to find good feasible solutions quickly, which are also  
383 384 11.5. Special-Purpose Algorithms  
important in pruning the branch-and-bound tree. Furthermore, it is often the case that  
nearly optimal primal and dual solutions are a satisfactory solution to the problem.  
Structure frequently suggests decomposition. Both Lagrangian \(row\) decomposition  
and Benders \(column\) decomposition have been introduced in Chapter 11.3. For a given  
problem, several Lagrangian relaxations may be available. In addition, algorithms for the  
Lagrangian dual provide alternative ways to solve large linear programs. However, both  
row and column decompositions typically lead to a large number of variables or con-  
straints, so again the question of finding effective algorithms for large-sized problems is an  
issue. Finally for some structures, dynamic programming provides a decomposition  
which makes it possible to solve problems by a recursive algorithm that uses dominance to  
eliminate nonoptimal solutions.  
Thus if we want to make efficient use of structure, we must deal effectively with the  
following three issues.  
1. A choice of formulation and "strong" linear programming \(or combinatorial\)  
relaxation must be made.  
2. The chosen relaxation typically has a very large number of constraints \(and possibly  
columns\). An algorithm that finds an optimal \(or a good dual feasible\) solution to  
the relaxation as quickly as possible has to be selected. It also may be desirable to  
have a heuristic for finding good primal feasible solutions rapidly.  
3. Since the solution to the relaxation rarely solves the original problem, a procedure is  
needed, typically embedding the relaxation into branch-and-bound, to arrive at an  
optimal solution to the original problem.  
In the following four sections we will present some methods that take advantage of  
structure. First we discuss strong cutting-plane \(or constraint generation\) algorithms.  
Then we present some ways of quickly finding nearly optimal dual and primal feasible  
solutions. Next we discuss the algorithms that can be used in combination with Lagran-  
gian and Benders' decomposition, as well as some of the problems that arise in their  
implementation. Finally we describe dynamic programming and illustrate its application  
to certain discrete optimization problems.  
The first four sections of the next chapter are each devoted to a particular structured  
problem, knapsack problems, 0-1 problems, the symmetric traveling salesman problem,  
and fixed-charge flow problems, respectively. For each problem we exhibit how some of  
the practical choices are made that lead to a relatively efficient special-purpose algorithm.  
We will use the uncapacitated facility location problem as an example of a structured  
problem throughout this and the next three sections. As a starting point we know the two  
formulations presented in Chapter I.1, the so-called "strong formulation"  
z = max I I C *ijY ij* - I *jjXj*  
*iEi JEN JEN*  
I *Yij* = 1 for *i* E *J*  
\(UFL\)  
*JEN*  
*Yij* - *Xj* \~ 0 for *i* E *J,j* EN  
*yERr;n, xEBn,*  
where *J* = \{l, ... , *m\}* and *N* = \{l, ... , *n\},* and the "weak formulation" 1\. Introduction  
385  
\(WUFL\)  
*z* = max *L L cijYij* - *L jjXj*  
iEi *jEN jEN*  
*L Yij* = 1 for *i* E *I*  
*jEN*  
*L Y* ij - *mXj* \~ 0 for *j* E *N*  
iEi  
We have already remarked on the size of these formulations. Another important  
observation is that once we have decided which facilities Q \<;; *N* are to be opened, then the  
optimal allocation of the clients to the facilities is obvious. In particular, client *i* is served  
by a facility *k* E Q such that *Cik* = maXjEQ *cij'* Hence there exists an optimal solution to  
UFL and WUFL with *Y* E *Bmn.*  
Proposition 1.1. *The value %pening/acilities at* Q \<;; *N is*  
*v\(Q\)* = *L* max cij - *L jj.*  
iEi JEQ jEQ  
Now suppose we wish to make a choice between the linear programming relaxations of  
UFL and WUFL. Note that there is a closed-form solution to the linear programming  
relaxation ofWUFL.  
Proposition 1.2. *When fJ* ;;" 0 *for all j* E *N, there exists an optimal solution \(x\*, y\*\) to the*  
*linear programming relaxation of* WUFL *with YUi* = 1, *where ji* = arg maXjEN \(cij - *fJlm\)*  
*for all i* E *I, Yu* = 0 *otherwise, and where x;\*= \(l/m\)* "LiEi *Yu for all j* E *N.*  
From Proposition 1.2 it follows that *"LjEN xj* = 1, independent of the number offacilities  
opened in an optimal solution. Hence the fixed costs *jj* for *j* E *N* are largely ignored, and  
the relaxation cannot provide a good upper bound on the optimal value *z.*  
In contrast, the upper bound provided by the linear programming relaxation ofUFL is  
usually very strong and no larger than the bound obtained from the linear programming  
relaxation of WUFL. The strength of this bound will be discussed further in Section 3.  
*Example 1.1.* Consider the uncapacitated location problem with the following data\:  
*m =6, n* = 5,  
C = *\(cij\) =*  
12 13 6 0 1  
8 4 9 1 2  
2 6 6 0 1  
3 5 2 10 8  
8 0 5 10 8  
2 0 3 4  
/ = \(jj\) = \(4 3 4 4 7\).  
Using Proposition 1.2, we obtain the optimal solution to the LP relaxation of WUFL  
given by *yi2* = *Y23* = *Y32* = *Y44* = *YS4* = *Y64* = 1, *X2* = t xj = t *X4* == ! with value 481. 386 11.5. Special-Purpose Algorithms  
The LP relaxation of UFL has an optimal solution \(derived later\) given by  
*y71* = *y72* = *Y21* = *Y23* = *Y!2* = *Y!3* =\~, *Y44* = *Y54* = *Y64* = 1, *xT* = *xi* = *x!* = \~, *X4* = 1 with value  
ZLP = 41\~. The optimal value ofUFL is *Z* = 41.  
From now on, we only consider the strong LP relaxation of UFL and concentrate on  
developing fast algorithms that solve it exactly or approximately.  
2. A CUITING-PLANE ALGORITHM USING STRONG VALID  
INEQUALITIES  
Here we consider linear programs of the form  
ZLP\(.'J'\) = max *ex*  
*Ax\<b*  
\(LP *\(.'J'»*  
*nx* \< *no* for *\(n, no\)* E.'J'  
xER\~,  
where .'J' generally contains a large number of constraints. Linear programs of this form  
anse as\:  
i. relaxations of an integer program max\{ex\: *xES\},* where S = *\{x* E *ZZ\: Ax* \< *b\}*  
and .'J' is a set of "strong" valid inequalities for S; and  
11. linear programs with a large number of constraints.  
As an example of i, S is the set of solutions to a 0-1 knapsack problem, .'J' represents the  
set of cover inequalities, and LP\(.'J'\) represents the linear program consisting of the  
knapsack constraint and the cover inequalities.  
As an example of ii, LP\(.'J'\) is the LP relaxation of UFL, where *Ax* \< *b* are the  
constraints of the weak formulation WUFL without integrality and .'J' represents the *mn*  
variable upper-bound constraints *Yu* - *Xj* \< 0 for all *i* E *I, j EN.*  
The question we need to answer is how to solve the linear program LP\(.'J'\). The "brute  
force" approach of adding all the constraints in .'J' a priori is impractical when the number  
of inequalities in .'J' is very large. Furthermore, most of the inequalities in .'J' are  
unnecessary for the solution of LP\(.'J'\). However, a priori addition of a subset of the  
inequalities from .'J' may be very desirable.  
A more general approach uses the inequalities of .'J' as cutting planes. Only those  
inequalities in .'J' that are likely to be active in the neighborhood of an optimal solution to  
LP\(.'J'\) are generated.  
Fractional Cutting-Plane Algorithm \(FCPA\) for LP\(.'J'\)  
*Initialization\:* Sk = *\{x* E R\~\: *Ax* \< *b\}.* Set *t* = 1.  
*Iteration t\:*  
*Step* 1\: Solve the relaxation of LP\(.'J'\)  
z\~ = max *\{ex\: x* E S\~\} 2. A Cutting-Plane Algorithm using Strong Valid Inequalities 387  
and let *Xl* be an optimal solution. \(Note that the dual solution complementary to  
X l - I is a dual feasible solution to LPI. Hence Lpi can be solved by a dual algorithm  
starting from the point Xl-I.\)  
*Step* 2\: *Optimality Test.* If *nxl* .\:\(; *no* for all *\(n, no\)* E *f!f,* then *Xl* is an optimal solution to  
LP\( *f!f\).* Stop.  
*Step* 3\: *Refinement.* Let *f!fl* C *f!f* be a set of one or more inequalities *\(n, no\)* with *nxl* \> *no*  
and  
S\~I = Sk n *\{x* E R\~\: *nx* .\:\(; *no* for *\(n, no\)* E *f!fl\}.*  
*Step* 4\: *t* \<- *t* + 1.  
Given the solution *Xl* to the relaxation LpI  
, we must show that *Xl* is a feasible solution to  
LP\(f!f\) or find a valid inequality *\(n, no\)* E *f!f* for which *nxl* \> *no.* This is the separation  
problem that we introduced in Section 1.6.3.  
The Separation Problem for *f!f*  
Given a point *x\** E R\~, show that *x\** satisfies all the valid inequalities in *f!f,* or find one or  
more valid inequalities \(7\(, *no\)* E *f!f* with *nx\** \> *no.*  
Based on the polynomial equivalence of "separation" and "optimization" \(see Theo-  
rem 3.3 of Section 1.6.3\), we make the following observations.  
1. Under the assumption that we can check whether *x\** satisfies the constraints *Ax* .\:\(; *b*  
in polynomial time, LP\(f!f\) can be solved in polynomial time if and only if the  
separation problem for LP\(f!f\) can be solved in polynomial time.  
ii. If we are dealing with an integer program max\{cx\: *Ax* .\:\(; *b, x* E Z\~\} that is *.Nr!P-*  
hard, and *f!f* represents one or more families offacets, there may be some families of  
facets for which the separation problem is in *r!P,* but there will be others for which  
the separation problem is .Nr!P-hard. More precisely, based on Proposition 7.4 of  
Section 1.5.7, we cannot expect to have a good characterization of all the facet  
classes for the problem, and hence there will certainly be problem instances for  
which FCPA will terminate with a solution *Xl* that is not integral.  
To demonstrate the FCPA with separation, we return to the uncapacitated facility  
location problem. Suppose that the number of constraints in formulation UFL is too large  
to solve its linear programming relaxation directly. So we start with the LP relaxation of  
the formulation WUFL and let *f!f* consist of the *mn* constraints *Yij.\:\(; Xj* for all  
*i* E *I* and\) *EN.*  
Now given a point *\(x\*, y\*\)* E R\~ x *R';",* the separation problem for *f!f* is to find whether  
one or more of the *mn* variable upper-bound constraints is violated. This is easily done by  
enumeration, and a violation occurs if and only if maXiE! *yij* \> *xj* for some\) E *N.* Several  
implementations are possible, depending on the number of violated constraints added. In  
the implementation given below, for each\) for which a violation occurs, we add one most  
violated constraint.  
*Example* 1.1 *\(continued\).* We implement the FCPA with separation.  
*Iteration* 1. It has been seen earlier that the optimal solution ofLpl is *yb* = *y13* = *yj2 =*  
y\~4 = y\~4 = *Y64* = 1, *xi* = t, xj =\~, x\~ = \~ with *zLp* = 48t. Since Xl is not integral we apply the 388 11.5. Special-Purpose Algorithms  
separation algorithm to *\(Xl, y l\)* and find that three constraints from *fJi,* namely  
*Y12";; X2, Y23";; X3,* and *Y44";; X4,* are violated. These constraints are now added to *S1.*  
*Iteration* 2. The linear program Lp2 is solved, giving the solution *Ytl* = 0.8, *Y12* = 0.2,  
Y\~l = 1, *yj2* = 1, *ya4* = 0.4, *yas* = 0.6, Y\~4 = 1, Y\~4 = 1, *x1* = 0.3, x\~ = 0.2, *xa* = 0.4, x\~ = 0.1  
with Zf.p = 44.9\. The separation algorithm now generates the four violated inequalities  
hI";; *Xl\> Y32";; X2, Y54";; X4,* andY45";; *Xs·*  
*Iteration* 3. The linear program Lp3 has an optimal solutionY?I= 1, Y\~l = 0.2, Y\~3 = 0.8,  
Y\~3 = 1, *Yl4* = 1, Y\~4 = 1, Y\~4 = 1, *x?* = 0.2, x\~ = 0.8, *xl* = 1, and Z\[p = 42.8\. The separation  
algorithm now generates the violated inequalities Yu ,,;; *Xl* and *Y33* ,,;; *X3.*  
*Iteration* 4. The linear program Lp4 has an optimal solution *ytl* = *yt2* = *Y!l* = *Y!3* = yj2 =  
yj3 =\~, Y\~4 = Y\~4 = Y\~4 = 1, xt = *x!* = *x1* =\~, x\~ = 1, and z\~p = 41.5. No violated inequalities  
are generated by the separation algorithm, so *\(x\\ y4\)* is an optimal solution of the LP  
relaxation of UFL.  
Note that only 9 out of the 30 possible variable upper-bound constraints *Yij";; Xj* have  
been added in the course of the algorithm. Since the optimal solution *\(x\\ y4\)* of LP\(fJi\) is  
not integral, we need to proceed further. One approach described below is to embed the  
FCPA into a branch-and-bound algorithm. Another is to enlarge the family *fJi* of strong  
valid inequalities \(see exercise 5\).  
A Strong Cutting-Plane/Branch-and-Bound Algorithm for **IP.**  
Given the problem  
\(IP\) max\{cx\: *Ax* ,,;; *b, x* E Z\~\}  
and a class *fJi* of "strong" valid inequalities, we use the following 2-phase algorithm.  
*Phase* 1\. Solve LP\(fJi\) by the FCPA. On termination, let *Xl* be an optimal solution of  
LP\(fJi\). If *Xl* E Z\~, stop. *Xl* solves IP. Otherwise, go to Phase 2.  
*Phase* 2. Let *fJi'* C *fJi* be the cuts generated in Phase 1, that is *fJi'* = U\~=l *fJiS.* Solve the  
reformulation ofIp,  
\(IP'\) *max\{cx\: Ax,,;; b, 1CX";; 1CO* for *\(1C, 1Co\)* E *fJi', x* E Z\~\},  
by branch-and-bound.  
*Example* 1.1 *\(continued\).* The branch-and-bound tree for the problem IP' with the nine  
inequalities added is shown in Figure 2.1.  
On the first branch, node 2, *x* 1 is set to the value 1; the linear program has an optimal  
solution with *x* integer of value 40. The only remaining node is 3, where *x* I is set to zero.  
Here again the linear program has an optimal solution with *x* integer of value 41. The  
corresponding solution is *X2* = *X3* = *X4* = 1, *Y12* = *Y23* = *Y32* = *Y44* = *YS4* = *Y64* = 1. Since the  
tree has no active nodes, the solution found at node 3 is optimal.  
41  
41  
40  
40  
Figure 2.1 2\. A Cutting-Plane Algorithm using Strong Valid Inequalities 389  
It may be possible to eliminate some variables from IP'. Given an optimal solution to  
LP\(@'\), the reduced prices *Cj* are nonpositive for all nonbasic variables *Xj* at their lower  
bound, and nonnegative for all nonbasic variables at their upper bound. We suppose that z  
is the value of the best feasible solution known. -  
Proposition 2.1. *If Xj is nonbasic at its lower \(upper\) bound in the solution of* LP\(@,\),  
*Xj* E *Zi, and* ZLP\(S1'\) + *Cj* ..;; \~ \(hp\(S1'\) - *Cj* ..;; \~\), *there exists an optimal solution to the integer*  
*program with Xj at its lower \(upper\) bound.*  
This means that the set of variables needed in the branch-and-bound phase is  
*N\** = *N* \\ *U* EN\: *Xj* is nonbasic, hp\(S1'\) - *ICj* I ..;; \~\}.  
*Example* 1.1 *\(continued\).* We exhibit the use of Proposition 2.1.  
Suppose we have observed that the solution in which location 1, 2, and 4 are open has  
value *v\(\{1,* 2, 4\}\) = 40\. Since *Yij* equals 0 or 1 in an optimal solution, the reduced prices for  
the LP relaxation of UFL can be used to fix any variable with reduced price  
I *Cij* I \~ 41.5 - 40 = 1.5. In this case we can set *Y13* = *Y14* = *Y15* = *Y22* = *Y24* = *Y25* = *Y31* = *Y34* = *Y35*  
*= Y41* = *Y42* = *Y43* = *Y52* = *YS3* = *YS5* = *Y62* = *Y6S* = 0 before entering the branch-and-bound phase.  
*Example* 2.1\. This is an instance of UFL with 33 facilities and clients. Each of the 33  
cities is a client and a potential location for a facility. Here *cij* is the negative of the distance  
between cities *i* and\}. The distances are given in Table 2.1, and the geographic locations  
are shown in Figure 2.2. The fixed costs are 2000 for each facility.  
Figure 2.2. Thirty-three city problem\: I, Chicago, Ill.; 2, Indianapolis, Ind.; 3, Marion, Ohio; 4, Erie, Pa.; 5,  
Carlisle, Pa.; 6, Wana, w.v.; 7, Wilkesboro, N.C.; 8, Chattanooga, Tenn.; 9, Barnwell, S.c.; 10, Bainbridge, Ga.;  
11, Baton Rouge, La.; 12, Little Rock, Ark.; 13, Kansas City, Mo.; 14, La Crosse, Wis.; 15, Blunt, S.D.; 16, Lincoln,  
Neb.; 17, Wichita, Kan.; 18, Amarillo, Tex.; 19, Truth or Consequences, N .M.; 20, Manuelito, N .M.; 21, Colorado  
Springs, Colo.; 22, Butte, Mont.; 23, Lewiston, Idaho; 24, Boise, Idaho; 25, Twin Falls, Idaho; 26, Salt Lake City,  
Utah; 27, Mexican Hat, Utah; 28, Marble Canyon, Ariz.; 29, Reno, Nev.; 30, Lone Pine, Calif.; 31, Gustine,  
Calif.; 32, Redding, Calif.; 33, Portland, Ore. \~  
'" co  
**Table 2.1. Data** for **the** 33-City **Problem**  
0  
2 184 0  
3 292 195 0  
4 449 310 215 0  
5 670 540 380 288 0  
6 516 357 232 200 211 0  
7 598 514 434 566 436 381 0  
8 618 434 493 787 814 642 295 0  
9 881 697 719 790 632 697 224 320 0  
10 909 964 955 1020 974 952 541 341 318 0  
II 978 892 1031 1246 1352 1180 843 538 747 441 0  
12 654 597 803 1018 1154 1104 766 461 749 634 380 0  
13 504 503 722 937 1043 806 986 722 1042 954 784 404 0  
14 276 460 568 725 946 817 874 894 1214 1185 1218 660 452 0  
15 780 964 1072 1229 1450 1321 1378 1326 1646 1672 1410 1030 626 476 0  
16 529 644 789 1004 1184 1001 1214 950 1270 1213 1043 632 219 436 419 0  
17 805 698 917 1132 1238 1055 1m 842 1162 1027 779 473 195 637 634 256 0  
18 1181 1007 1226 1441 1547 1364 1375 1080 1134 1138 783 611 563 1046 759 624 368 0  
19 1548 1444 1630 1845 1984 1801 1726 1431 1685 1477 1134 1033 906 1389 1094 967 711 404 0  
20 1547 1454 1668 1883 1994 1811 1879 1584 1776 1632 1267 1053 944 1427 1196 1005 749 442 251 0  
21 1239 1167 1353 1568 1707 1524 1584 1313 1633 1498 1151 979 614 988 600 525 471 368 512 507 0  
22 1538 1733 1830204522082090 2136207823982332 2110 1782 1378 1300 760 1229 1382 1319 1163 930 910 0  
23 1999 2158 2291 2448 2669 251525972500282026752336 2164 1707 1860 1375 1582 1658 1545 1389 1156 1237 436 0  
24 1716 18752008 2165 2386 2232 2488 2217 2537 2392 2053 1881 1422 1577 1106 1244 1375 1262 1106 873 954 483 283  
25 1580 1738 1872 2029 2250 2095 23522081 2401 2256 1917 1745 1286 1473 988 1147 1239 1126 970 737 818 379 419  
26 1425 1569 1717 1874 2109 1926 2115 1844 2164 2019 1680 1508 1118 1335 862 913 1002 889 733 500 581 430 656  
27 1560 1549 185220092089 19062063 1792 2112 1967 1456 1274 1032 1540 1068 1007 944 665 521 282 491 768 994  
28 1918 1744 1963 2178 2284 2101 2174 1879 2071 1892 1562 1348 1239 1722 1258 1300 1044 737 526 295 802 816 1022  
29 2065 21022357 2514264224592626235526752530 2191 2019 1673 1905 1432 1570 1507 1320 1109 878 1124 842 715  
30 2284 2131 2326 2441 2671 2488 2418 212324372259 1929 1715 1606 1924 1451 1589 1411 1104 893 662 1143 1004 981  
31 2340234825432658 2888 2705 2869 2598 2918 2773 2434 2262 1916 2148 1675 1813 1750 1468 1102 871 1367 1085 958  
32 2247232725392696286726842851 258029002755 24162244 1898 2130 1657 1795 1732 1545 1334 1103 1349 1014 814  
33 2163 23222455 26122833 2679 2761 2664 2984 2839 2500 2328 1871 2024 1539 1746 1822 1709 1553 1320 1391 693 346  
0  
136 0  
373 237 0  
711 575 358 0  
739 603 386 545 0  
432 465 533 849 739 0  
698 599 589 768 523 266 0  
675 1033 778 1092 982 243 349 0  
531 1015 760 1047 964 225497266 0  
447 583 820 1158 1355 581 847 710 444 0  
2 3 4 5 6 7 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  
Source\: *Rand-McNally Road Atlas,* 38th edition, Rand-McNally Company \(1962\). ....  
\\C  
...  
**Table 2.2.** The *\(i ,j\)* Pairs for which Variable Upper-Bound Constraints Have Been Added  
*j=1* 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 t  
i= 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 1  
14 3 6 5 8 10 12 11 17 15 13 21 20 27 18 33 23 22 30 29 2  
2 3 6 4 9 10 7 11 12 15 14 18 19 22 25 26 27 20 32 30 33 23 3  
2 5 3 7 8 9 10 16 1 13 21 19 18 28 15 33 23 22 26 20 31 29 30 4  
3 1 2 7 10 11 8 13 14 17 16 20 18 19 23 22 24 28 21 30 33 29 32 31 5  
14 4 1 3 2 9 11 10 15 13 16 17 28 21 27 24 29 33 30 19 31 32 6  
10 8 5 9 3 12 17 18 16 14 11 21 33 31 30 20 19 32 22 7  
16 9 6 4 5 2 8 1 22 21 19 27 30 20 32 29 24 28 33 31 8  
13 4 14 1 18 10 15 11 16 26 23 22 27 9  
4 5 7 9 11 14 15 17 27 25 28 26 10  
15 6 7 4 3 5 2 12 16 11  
5 13 6 16 17 15 12  
16 17 13 28 15 13 392 11.5. Special-Purpose Algorithms  
Table 2.3.  
\{j\: *Xj* = 1\}\: 20,24.  
\{j\: *Xj* = 1\}\: 3,7,8,13, 16.  
*\{\(i,i\)\: Yij* = 1\}\: \(18,20\), \(19, 20\), \(20, 20\), \(21, 20\), \(22, 24\), \(23, 24\), \(24, 24\), \(25, 24\), \(26, 24\), \(27,  
20\), \(28, 20\), \(29, 24\), \(30, 20\), \(31, 24\), \(32, 24\), \(33, 24\).  
*\{\(i,i\)\: Yij* = !\}\: \(1,3\), \(1,13\), \(2, 3\), \(2, 8\), \(3,3\), \(3, 7\), \(4, 3\), \(4, 7\), \(5, 3\), \(5, 7\), \(6,3\), \(6, 7\),  
\(7,7\), \(7, 8\), \(8, 7\), \(8,8\), \(9, 7\), \(9, 8\), \(10,7\), \(10, 8\), \(11, 8\), \(11,13\), \(12, 8\),  
\(12, 13\), \(13, 13\), \(13, 16\), \(14, 13\), \(14, 16\), \(15, 13\), \(15, 16\), \(16, 13\), \(16, 16\), \(17, 13\),  
\(17, 16\).  
When the FCPA/branch-and-bound algorithm is applied, the initial LP relaxation  
\(with no variable upper-bound constraints\) has value -2000. After adding 230 variable  
upper-bound constraints \(see Table 2.2\), the linear programming relaxation of UFL is  
solved with value -20,346. The solution is given in Table 2.3.  
When branch-and-bound is applied, an integral solution of value -20,393 is found at  
node 2, and an integral solution of value -20,363 is found and proved optimal at node 3  
\(see Figure 2.3\). The optimal solution is shown in Table 2.4  
We have tacitly assumed that the number of variables in LP\(\~\) does not cause  
computational difficulties. But this may not be the case. For example, if *m* = 500 and  
*n* = 100 in UFL, then FCPA could not be used directly to solve LP\(@'\).  
To handle a very large number of variables in addition to, perhaps, a very large number  
of constraints, we used a standard linear programming technique. We first solve LP\(\~\)  
with a suitably chosen subset of variables eliminated; that is, we choose *N'* C *N* and set  
*Xj* = 0 for *j* E *N'.* After solving the restricted version of LP\(\~\), we check for optimality  
with respect to LP\(\~\) by calculating the reduced prices of the variables *Xj* withj E *N'* . If all  
of these reduced prices are nonpositive, LP\(\~\) is solved. Otherwise, we delete from *N'* all  
*j* such that the reduced price *ofxj* is positive. We then continue with the solution ofLP\(\~\).  
Note that after adding variables it is preferable to reoptimize with a primal algorithm since  
the current solution is primal feasible. Hence if this technique is used within an FCPA, it is  
desirable to have primal and dual linear programming algorithms available.  
-20363 -20393  
-20363 -20393  
Figure 2.3  
Table 2.4.  
\{j\: *Xj* = 1\}\:  
\{i\: *Yi,7* = 1\}\:  
\{i\: *Yi.13* = 1\}\:  
\{i\: *Yi,20* = l\}\:  
\{i\: *Yi,24* = 1\}\:  
7,13,20,24  
3,4,5,6,7,8,9,10  
1,2, 11, 12, 13, 14, 15, 16, 17  
18,19,20,21,27,28,30  
22,23,24,25,26,29,31,32,33 3\. Primal and Dual Heuristic Algorithms 393  
3. PRIMAL AND DUAL HEURISTIC ALGORITHMS  
Heuristic or approximate algorithms are designed to find good, but not necessarily  
optimal, solutions quickly. For a varit\:ty of problems with structure, it is easy to devise  
heuristic algorithms to find primal and dual feasible solutions. It is particularly desirable  
to find both primal and dual feasible solutions since the dual solution provides an upper  
bound on the deviation from optimality of the primal solution. Depending on the quality  
of the solution required, an approximate solution can be the final answer to a problem or  
can be an input to an exact algorithm. The lower and upper bounds provided by  
approximate solutions can be of great help in decreasing the running time of branch-and-  
bound algorithms.  
Though it is difficult to describe completely general heuristic algorithms, three ideas are  
applicable in a wide variety of cases. The first is that of a "greedy", alternatively called a  
"steepest ascent/descent" or "myopic", algorithm.  
Greedy algorithms are frequently applied to the maximization of set functions. Let  
*v\(Q\)* be a real-valued function defined on all subsets of *N* = \{l, ... , *n\}* and consider the  
problem max\{v\(Q\)\: Q £; *N\}.*  
A Greedy \(Heuristic\) Algorithm for Maximizing a Set Function  
*Initialization\:* QO = 0, *t* = 1.  
*Iteration t\:*  
*Step* 1\: Letj, = arg *maxjEN\\Q,-1* V\(Q'-l U *U\}\)* with ties broken arbitrarily.  
*Step* 2\: Ifv\(Qt-1 U *U,\}\)* \~ V\(Q'-l\), stop. Q'-l is a greedy solution.  
*Step* 3\: *Ifv\(Qt-l* U *U/\}\)* \> V\(Q'-l\), set Q' = Q'-l u *U,\}.*  
*Step* 4\: If Q' = *N,* stop. *N* is a greedy solution. Otherwise let *t* +- *t* + 1.  
The idea of this greedy algorithm is simple. Given a set *Q',* the next element chosen is  
one that gives the greatest immediate increase in value, provided that such an element  
exists. Moreover, once an element is chosen, it is kept throughout the algorithm. Recall  
that we used the greedy algorithm to find an optimal solution to the minimum-weight  
spanning tree problem \(see Section 1.3.3\). In general, however, we cannot expect the  
greedy algorithm to yield an optimal solution.  
In the uncapacitated facility location problem, we obtain  
*v\(Q\)* = *iEI JEQ jEQ*  
\{ 2\: max *cij* - 2\: *jj* for 0 C Q £; *N*  
- 00 for Q = 0 \(since Q = 0 is infeasible\).  
*Example* 1.1 *\(continued\). Iteration* 1. QO = 0.  
We apply the greedy heuristic described above.  
*j\:* 2 3 4 5  
*v\(Qo* U *U\}\)\:* 31 25 27 21 14 394 n.s. Special-Purpose Algorithms  
*Iteration* 2. Ql = \{l\}, V\(Ql\) = 31.  
\}\: - 2 3 4 5  
V\(QI U \{j\)\)\: 35 33 38 29  
*Iteration* 3. Q2 = \{l, 4\), *V\(Q2\)* = 38.  
\}\: - 2 3 5  
*V\(Q2* U \{j\}\)\: 40 39 31  
*Iteration* 4. Q3 = \{I, 2, 4\), *V\(Q3\)* = 40.  
\}\: - 3 5  
*V\(Q3* U \{j\)\)\: 37 33  
Since *V\(Q3* U \{j\)\) \~ *V\(Q3\)* for all\} \$. *Q3,* it follows that Q3 = \{l, 2, 4\) is a greedy solution  
with value 40.  
There are generally several greedy heuristics for a given problem, and common sense  
must be used to convert the "greedy" idea into a reasonable greedy heuristic. An equally  
valid greedy approach for the uncapacitated facility location problem is to start with all  
facilities open and then, one-by-one, close a facility whose closing leads to the greatest  
increase in profit.  
For the 0-1 packing problem max\{cx\: *Ax* \~ 1, *x* E *Bn \),* where *A* is a 0-1 matrix, one  
greedy approach is to recursively set that variable to one for which the resulting solution is  
still feasible and for which *Cj* is as large as possible. However, examination of a few  
examples quickly leads to the idea that *Cj* should be divided by the number of l's in the  
column *aj;* that is, the "improved" greedy criterion is to choose a column for which the  
average increase in profit per row covered, *Cj* / *LiEM ail\>* is maximum.  
The second important idea is that of "local search" or "interchange" heuristics. As the  
name implies, a heuristic of this type takes a given feasible solution and, by making only  
limited changes in it, tries to find a better feasible solution.  
A **k-Interchange Heuristic for** max\{c\(x\)\: *xES* S *Bn\).*  
Given a positive integer *k, k* \~ *n,* let  
*Initialization\:* Find a point Xl E S.  
*Iteration t\:* Given a point *Xl* E S, if there is a point *x'* E *Nk\(XI\)* n S with *c\(x'\)* \> *c\(xl\),*  
then let *Xt+l* = *x'* and *t* \<-- *t* + 1. Otherwise stop; *x'* is a k-interchange solution.  
Clearly the amount of work per iteration in this algorithm depends crucially on *k,* and  
for the heuristic to be fast we typically limit *k* to values of 1, 2, or 3. Observe that when  
*k* = *n,* the algorithm asks for an examination of all the points in *Bn.* Again, depending on  
the problem structure, it is usual to make variations in the definition of *Nk\(X\).* For the 3\. Primal and Dual Heuristic Algorithms 395  
uncapacitated facility location problem, one reasonable choice \(given a set *Q* of open  
facilities\) is to look at the neighborhood in which either \(a\) one of the existing facilities is  
closed or \(b\) one new facility is opened, or where both a and b occur simultaneously, that  
is,  
N2\(Q\)=\{F\~N\: IF\\QI \~ 1 and IQ\\FI \~ 1\}.  
The third general principle is that often primal and dual heuristic solutions can be  
found in pairs. The complementary slackness conditions \(see Section 1.2.2\) are one way of  
pairing heuristic solutions.  
We use the 0-1 packing problem max\{cx\: *Ax* \~ 1, *x* E *Bn\}* to illustrate this idea. The  
dual of its linear programming relaxation is min\{L7!1 *Ui\: uA* \~ c, *u* E *R';'\}.* Given a  
heuristic solution *u\** to this dual, let *N\** = \{j *EN\:* L7!1 *u7ai\}* = *c\).* Then the choice of an  
associated primal heuristic solution is restricted to the vectors *x* with *Xj* = 1 only if\} E *N\*.*  
Moreover, if such a primal feasible vector *x\** can be found that also satisfies *LjEN a i\}xj* = 1  
for all *i* with *u7* \> 0, then by complementary slackness it follows that *x\** and *u\** are optimal  
solutions.  
The pairing of primal and dual heuristics is now demonstrated for the uncapacitated  
facility location problem. First we consider the dual of the linear programming relaxation  
ofUFL\:  
hp = min 2\:\: *Ui* + 2\:\: *tj*  
*iEf JEN*  
*Ui+ wi\}* \~ci\} *foriEI,\}EN*  
- 2\:\: *wi\}* + *tj* \~ *-fJ* for\} EN  
*iEf*  
Wi\},tj\~O *foriEI,jEN.*  
We can eliminate constraints and variables from this formation by observing that\:  
a. For given *wi\},* the only constraints on *tj* are nonnegativity and *tj* \~ *LiEf Wi\}* - *fJ,* and  
hence in any optimal solution we have *tj* = \(LiE1 *wi\)* - *jj\)+,* where *x+* denotes  
max\(x, 0\).  
b. Forgiven *Ui,* we have that *LjEN \(LiEf Wi\)* - *fJt* is minimized by setting *wi\}* as small as  
possible, that is *wi\}* = *\(ci\)* - *Uit.*  
Hence the dual can be rewritten as  
\(3.1\) ZLP = min *w\(u\),* where *w\(u\)* = 2\:\: *Ui* + 2\:\: \(2\:\: *\(ci\)* - *Uit* - *fJ\)+.*  
*uER'"* iE1 *JEN iEf*  
Alternatively, if we assume that *fJ* \~ 0 for all *j* E *N,* it is easy to see that the constraints  
*Xj* \~ 1 can be dropped from the linear programming relaxation ofUFL. Thus *tj* disappears  
from the dual and it becomes  
ZLP = min 2\:\: *Ui*  
*iEf*  
\(3.2\)  
2\:\: *\(ci\)* - *Uit* \~fJ *iEf*  
for *j EN.* 396 11.5. Special-Purpose Algorithms  
The two condensed duals \(3.1\) and \(3.2\) are of interest because they only depend on  
*u* E *Rm.* The dual \(3.1\) is particularly useful since it gives an upper bound for any *u* E *Rm.*  
Now we consider the association of primal and dual solutions. Given a primal solution  
with Q s *N* being the set of open facilities, one way to associate a dual solution is to take *Ui*  
equal to the second largest C *ij* over\} E Q. The motivation for this lies in the complemen-  
tary slackness condition *\(Yij* - *x\)wij* = O. For\) E Q, we have *Xj* = 1 in the primal solution;  
and for each *i,* there is one *Y ij* = 1 with\} E Q. Hence if the complementary slackness  
condition is to hold, we must have no more than one *wij* \> 0 for each *i* E *I* and\} E Q.  
Since *wij* = *\(cij* - *Ui\)+'* this leads to the heuristic choice of *Ui* suggested above. Taking the  
greedy solution Q = \{l, 2, 4\} obtained for Example 1.1, the associated dual solution is  
*U* = \(12 4 2 5 8 2\). Using the formula in \(3.1\), we obtain *w\(u\)* = 33 +  
\(0 + 2 + 6 + 5 + 0\) = 46.  
Now conversely, suppose we are given a dual solution *U* that is feasible to \(3.2\) and we  
wish to associate a primal feasible solution with it. The linear programming complemen-  
tarity conditions suggest associating a primal solution in whichxj = 0 *ifLiEI \(cij* - *Uit \<jj.*  
Let *leu\)* = \{j *EN\: LiEI \(cij* - *uJ+* = *jj\}.* The best solution that satisfies complementarity is  
obtained by solving  
max \{I max *cij* - I *jj\}.*  
*Qf;J\(U\) iEI jEQ jEQ*  
However, this problem may not be much easier to solve than the original problem UFL.  
Therefore we take as a primal heuristic solution *Q\(u\)* any minimal set *Q\(u\)* S *leu\)*  
satisfying  
\(3.3\) max Ci = max Ci *jEQ\(u\)* 1 *jEJ\(u\) 1*  
for all *i* E *I.*  
The following proposition tells us when *\(Q\( u\), u\)* are optimal to UFL and the dual of its  
linear programming relaxation, respectively.  
**Proposition** 3.1. *Given a u that isfeasible to* \(3.2\) *with Ui* \~ *maXjEJ\(U\) cijfor i* E *I, and a*  
*primal solution Q\(u\) defined by* \(3.3\), *let ki* = 1 \(j E *Q\(u\)\: Cij* \> uJ I. *Ifki* \~ 1 *for all i* E *I,*  
*then* Q\( *u\) is an optimal set of open facilities.*  
*Proof*  
*v\(Q\(u»* = I max *cij* - I *jj.*  
*iEI jEQ\(u\) jEQ\(u\)*  
If *k i* = 0, then  
max *cij* = *Ui* = *Ui* + I *\(Cij* - *Uit;*  
*jEQ\(u\) jEQ\(u\)*  
and if *k;* = 1, then  
max *cij* = *Ui* + I *\(Cij* - *Uit.*  
*jEQ\(u\) jEQ\(u\)* 3\. Primal and Dual Heuristic Algorithms 397  
Hence, if *ki* \~ 1 for all *i* E *I,* then  
*v\(Q\(U»* = I I *\(Cij* - *uJ+* - I *jj* + I *Ui*  
iE! *jEQ\(u\) jEQ\(u\)* iE!  
= I \(I *\(Cij* - *Uit* - *jj\)* + I *Ui*  
*jEQ\(u\)* iE! iE!  
= I *Ui* \(by definition of *J\(u»*  
iE!  
= hp \(since *U* is feasible in \(3.2».  
Since *v\(Q\(u»* achieves the upper bound of ZLP, it follows that *Q\(u\)* is an optimal set of  
open facilities. •  
Now we present a heuristic algorithm for the dual problem \(3.2\) that uses the ideas of  
greedy and interchange. After finding a dual solution, the algorithm constructs a primal  
solution from \(3.3\) and then uses Proposition 3.1 to check optimality.  
Dual Descent \[A Greedy Algorithm for \(3.2\)\]  
Begin with *u7* = *maXjEN cij* for *i* E *I.* Cycle through the indices i E *I* one-by-one attempt-  
ing to decrease *Ui* to the next smaller value of *cij'* If one of the constraints  
\(3.4\) I \(Cij-Uit\~jj *forjEN*  
iE!  
blocks the decrease of *Ui* to the next smaller *cij,* then decrease *Ui* to the minimum value  
allowed by the constraint. When all of the *u/s* are blocked from further decreases, the  
procedure terminates.  
A possible improvement of this greedy heuristic is obtained by modifying the order in  
which the *u/s* are considered as candidates to decrease. The reasoning is the same as in the  
case of the 0-1 packing problem. Let *Hi\(u\)* = \{j *EN\: cij* - *Ui?* O\}. Rather than just cycling  
through the *u/s,* we choose *Us* next if *IHs\(u\)* I \~ *IHj\(u\)* I for all *i* E *I,* since this implies the  
smallest increase in *LjEN* LiE! *\(Cij* - *Uit* per unit decrease in LiE! *Ui.* This discussion also  
justifies decreasing *Uj* only to the next smaller *cij* rather than to the smallest permissible  
value.  
Now suppose that dual descent terminates with a solution *u\** and that the associated  
primal solution *Q\(u\*\)* given by \(3.3\) fails to verify the optimality conditions of Proposition  
3.1. Then there exists an *i* such that *k i \>* 1. In an attempt to find an improved dual  
solution, we adopt the neighborhood search idea.  
*Interchange Step.* Increase some *uifor* which *k i* \> 1 to its previous value. Use the resulting  
*U* as the starting solution and reapply the dual greedy algorithm terminating with *u'*  
satisfying LiE! *u;* \~ LiE! *ui.*  
If *u\** = *u',* stop. *u\** is the heuristic solution.  
If *u\** satisfies the optimality conditions, stop.  
Otherwise, repeat the interchange step.  
*Example* 1.1 *\(continued\).* Applying dual descent yields the results shown in Table 3.1.  
For the first five steps, *Uj, i* = 1, ... , 5, is decreased to the second maximum in the row. 398 n.s. Special-Purpose Algori\~hms  
Table 3.1.  
*U jj* - *LiEf \(Cu* - *Uir*  
Step 1\: 2 3 4 5 6 j\: 2 3 4 5  
0 13 9 6 10 10 4 4 3 4 4 7  
1 12 9 6 10 10 4 4 2 4 4 7  
2 12 8 6 10 10 4 4 2 3 4 7  
3 12 8 6 10 10 4 4 2 3 4 7  
4 12 8 6 8 10 4 4 2 3 2 7  
5 12 8 6 8 8 4 4 2 3 0 7  
6 10 8 6 8 8 4 2 0 3 0 7  
7 10 6 6 8 8 4 0 0 0 7  
Now *U6* cannot be decreased because the constraint \(3.4\) for\) = 4 would be violated. Next,  
*U I* is decreased toward the third maximum in the row but is only decreased by two units  
because constraint \(3.4\) becomes tight for\) = 2. Finally, *U2* is decreased until \(3.4\) becomes  
tight for\) = 1. This completes the dual descent with *u* = \(10 6 6 8 8 4\), *w\(u\)* = 42,  
and *J\(u\)* = \{l, 2, 4\}. Now we associate a primal solution as described in \(3.3\) and obtain  
*Q\(u\)* = *J\(u\),* with a primal solution value of 40.  
The proposed modification given above to the order of decreasing the *u/s* produces the  
same result. The solution *\(u, Q\(u\)\)* fails to satisfy the optimality conditions since  
*k* 1 = IJ E Q\( *u\)\: eli\> u* I\} I = 2. To apply the interchange step, we increase *u* I back to its  
previous value of 12 and then restart the dual descent. In Table 3.2, we see that *U3* can now  
be decreased by one unit and then no further move is possible except to decrease *U* I again.  
Hence *u* = \(11 6 5 8 8 4\) and *w\(u\)* = 42 as before, but now *J\(u\)* = \{2, 3, 4\}. From  
\(3.3\) we obtain the associated primal solution *Q\(u\)* = *J\(u\)* with the improved primal value  
of41.  
There is a branch-and-bound algorithm for the un capacitated facility location problem,  
called DUALOC, that obtains primal and dual feasible solutions at each node of the  
branch-and-bound tree using dual descent, the primal heuristic given by \(3.3\), and  
interchange. If a node is not pruned by these heuristics, then branching is accomplished by  
taking a\) *EN* and considering the two problems with *Xj* = ° and *Xi* = 1.  
*Example* 2.1 *\(continued\).* When DUALOC is applied, the algorithm iterates six times  
through the interchange step. The values of the corresponding dual lower bounds and  
primal upper bounds are shown in Table 3.3. Hence before entering the branch-and-  
bound phase we have -20,503 \~ z \~ -20,340\. Branching on *X3* = 0, an optimal integer  
solution of value -20,363 is found. Branching on *X3* = 1, an optimal integer solution of  
value -20,393 is found. Hence an optimal solution has value -20,363 \(see Figure 2.2 and  
Table 2.4\).  
Table 3.2.  
*U jj* - *LiEf \(Cu* - *Uit*  
Step 1\: 2 3 4 5 6 j\: 2 3 4 5  
8 12 6 6 8 8 4 2 2 1 0 7  
9 12 6 5 8 8 4 2 1 0 0 7  
10 11 6 5 8 8 4 1 0 0 0 7 3\. Primal and Dual Heuristic Algorithms Table 3.3.  
Iteration Dual Bound Primal Bound  
1 -20,294 -20,503  
2 -20,326 -21,553  
3 -20,337 -20,503  
4 -20,338 -21,021  
5 -20,340 -20,503  
6 -20,340 -20,853  
399  
Analysis of Heuristics  
We have emphasized the importance of finding both primal and dual feasible solutions,  
particularly when a primal feasible solution is taken as an approximation to an optimal  
solution. The dual solution provides an upper bound on the deviation from optimality of  
the primal solution and thus gives an a posteriori evaluation of the quality of the primal  
solution.  
In addition to this evaluation of an instance, it is frequently possible to give an a priori  
evaluation of a heuristic algorithm over all instances. One way to obtain results of this type  
is by *worst-case analysis.*  
The essential idea of worst-case analysis is simple. Consider a maximization problem  
consisting of instances\:  
P\(I\) *z\(I\)* = max\{cf\(x\)\: *x* E *Sf* '\*' 0\} for *J* E§;.  
Suppose we have a heuristic algorithm *\(H\)* that finds a feasible solution of value *ZH\(I\).*  
Worst-case analysis is based on calculating some maximum deviation between *z\(I\)* and  
*ZH\(J\).* The analysis depends crucially on the function that is used to measure deviation.  
Perhaps the simplest function is just the absolute difference *z\(I\)* - *ZH\(I\).* But for most  
problems the maximum value ofthe absolute difference is not a meaningful measure since  
it can be made arbitrarily large by scaling the objective function. \(However, in Part III, we  
will consider a class of integer programs in which the objective function coefficients are all  
1 and the difference between the optimal value of the linear programming relaxation and  
the optimal value of the integer program always is less than or equal to 1\).  
Relative values, which are independent of objective function scaling, are usually a more  
meaningful measure of deviation. To consider relative values, it is convenient to assume  
that *Cf\(X\)* "" 0 for all *x* E *Sf* and all *J* E .10. We say that heuristic algorithm *H* has a *worst-*  
*case relative performance* or *performance guarantee* of *rH* if  
*rH* = *inf\{r\(I\)\: ZH\(I\)* = *r\(I\)z\(I\)\}*  
*fE.J*  
= sup\{r\: *zH\(H\)* "" *rz\(I\)* for all *J* E .1o\}.  
If a heuristic algorithm is not completely specified \(e.g., as a result of the absence of a tie-  
breaking rule\), we assume the worst possible outcome. By definition, 0.\:\:;; *rH'\:\:;;* 1 and *H*  
guarantees to find a feasible solution of value at least *rH* x 100% of the maximum value for  
all instances. Note that *rH* = 1 - f\>H, where  
*CH* = inf\{ c\: C "" *z\(I\)* Z\(\~H\(I\) for all *J* E .10 with *z\(I\)* \> 0 \},  
is the largest possible *relative error.* 400 11.5. Special-Purpose Algorithms  
To keep the same scale for mmlmlzation problems of the form  
*z\(I\)* = *min\{clx\)\: x* E SI '\*' 0\} with *Cl\(X\)* \~ o for all *x* E *SlandI* E\~, we use the reciprocal  
ratio and define  
*rH* = *sup\{r\: z\(l\)* \~ *rZH\(J\)* for all *I* E \~\}.  
Thus 0 \~ *rH* \~ 1 and *H* guarantees to find a solution of value at most *rIJ* x 100% of the  
minimum value for all instances. It is not at all unusual to obtain dramatically different  
results for maximization and minimization versions of what otherwise would be the same  
problem.  
Worst-case analysis is a very conservative approach since it takes only one bad instance  
to give a poor result. The alternative approaches of a probabilistic or statistical analysis will  
be considered briefly at the end of this section.  
There is one general principle that is used to obtain nearly all results on worst-case  
analysis. Consider a dual heuristic \(DH\) that produces an upper bound for *P\(J\);* that is,  
*ZDH\(l\)* \~ *z\(l\)* for all *I* E\~. Let  
*rDH* = *sup\{r\: ZH\(J\)* \~ *rZDH\(l\)* for all *I* E\~\}  
so that *rH* \~ *rDH.* Now if there is a simple relationship between *Hand* DH it is often  
possible to calculate *rDH* directly and hence a lower bound on *rHo* Moreover, in some cases,  
we can find an instance *10* with *ZH\(J°\)* = *rDHzDH\(l°\)* and *ZDH\(l°\)* = *z\(IO\)* so that *rH* = *rDH.*  
*Example* 3.1. We give a simple illustration of this approach by analyzing a greedy  
heuristic for the maximum-weight matching problem. This is the special case of maxi-  
mum-weight packing in which matrix *A* is the node-edge incidence matrix of a graph  
G = *\(V, E\).* Thus *L7!! aij* = 2 for allj, and the greedy heuristic, introduced earlier in this  
section, chooses edges of maximum weight such that each chosen edge does not meet any  
of the edges chosen previously. The algorithm stops when no such edge exists. \(We assume  
that all edge weights are positive.\) Let *EH* be the set of edges chosen by the greedy heuristic.  
Then *EH* is a maximal matching; that is, *e* E *E* \\ *EH* meets an *e'* E *EH.*  
The dual of the linear programming relaxation of the matching problem is  
min *L Ui*  
iEV  
Ui+Uj\~Cij *for\(i,j\)EE*  
*Ui* \~ 0 for *i* E *V,*  
where C *ij* is the weight of edge \(i, *j\).* Consider the dual solution *u H* given by  
*if\(i,j\)* E *EH*  
otherwise.  
Figure 3.1 3\. Primal and Dual Heuristic Algorithms **401**  
We claim that *uH* is dual feasible. Suppose *u\{l* + *u1* \< *cij* for some *\(i,\)* E *E.* Then  
*\(i,\)* \$. *EH.* Hence either *i* or\) or both are met by an *e* E *E H*  
*,* and one ofthese edges, say  
*\(i,\},\),* has been chosen before *\(i,\)* was considered. Hence *u\{l* = cij' ;;. *cu,* which contra-  
dicts *u\{l* + *u1* \< *cij'*  
Now  
ZOH = *L u\{l* = 2 *L cij* and *ZH* = *L* cij \~ Z \~ ZOH,  
iEV *\(i,j\)EEH \(i,j\)EEH*  
so *rH* ;;. rOH ;;. !. In the graph of Figure 3.1, in which each edge has weight equal to 1, by  
choosing *e* = \(2, 3\) first, we obtain *EH* = \{\(2, 3\)\}, *uH* = \(0 1 1 0\), ZOH = Z = 2, and  
*ZH* = !ZOH' Hence *rH* = rOH =\~.  
Although a solution whose value is only half of the optimal value is unlikely to be  
satisfactory, one must remember that a performance guarantee of 50% means that 50% is  
the worst possible outcome, and it is likely that for most instances the relative error will be  
much smaller. Moreover, it is not unusual for a heuristic to have performance guarantee of  
zero. Indeed, this is the case when the greedy heuristic is applied to the set-packing  
problem.  
Consider the family of set-packing problems  
for *k* = 1, 2, ... ,  
where  
and h is the *k* x *k* identity matrix; *lk* and *Ok* are *k* x 1 matrices of alII's and all O's  
respectively. Since \:E7!, *ail* = *k* \< \:E7!, *aij* = *k* + 1 for all\) \> 1, the greedy heuristic first sets  
*x,* = 1 and then stops. The optimal solution is *x,* = 0 and *Xj* = 1 for *j* = 2, ... , *k* + 1.  
Hence *zH\(k\)* / *z\(k\)* = 1 / *k* and *rH* = O. So, in the worst case, the greedy heuristic is  
arbitrarily bad for the set-packing problem.  
This raises a question in computational complexity. Suppose PP \* *,NPP.* Given an *,NPP-*  
hard optimization problem and 0 \< *r* \< 1, does there exist a polynomial-time heuristic  
algorithm *\(H\)* with *rH* ;;. *r?* The answer to this question depends on the problem. In fact,  
both extremes are possible.  
There are some problems for which the approximation problem is ,NPP-hard for *rH* ;;. *r*  
for any *r* \> O. We will show below that this is the case for a minimization version of a *p-*  
facility location problem. For the problem of finding the minimum number of colors  
needed to color the nodes of a graph such that no pair of nodes joined by an edge have the  
same color, the approximation problem is ,NPP-hard for *r* ;;. \~. But no polynomial-time  
heuristic algorithm *H* is known that yields *rH* ;;. *r* for any *r* \> O.  
At the other extreme, there are ,NPP-hard optimization problems such that for any  
*0\< r* \< 1, there is an algorithm with performance *rH;;' r,* whose running time is polyno-  
mial in the length of the input and in 1 / \(1 - *r\).* We call such an algorithm a *fully*  
*polynomial approximation scheme.* In Section 11.6.1, we will give a fully polynomial 402 11.5. Special-Purpose Algorithms  
approximation scheme for the knapsack problem. A more modest result is a *polynomial*  
*approximation scheme,* which is an algorithm with performance *rH* \~ *r,* whose running  
time is polynomial in the length of the input for any fixed *r,* 0 \< *r* \< 1.  
To illustrate some of these results, we now consider the analysis of some heuristics for a  
p-facility variation of the uncapacitated facility location problem. Here there are no fixed  
costs, but we can open no more than *p* facilities. Suppose c *ij* \~ 0 is the cost of assigning  
client *i* to facility *j* so that the objective function is  
*v\(Q\)* = I min *cij* for 0 C *Q* \~ *N.*  
iE! *jEQ*  
We call the problem  
z=min\{v\(Q\)\: 1 \~ IQI \~p\}  
the *p-facility minimization problem.*  
Proposition 3.2. *The p-facility minimization problem with performance guarantee*  
*rH* \~ *r is \}\(r;j/J-hardfor any r* \> O.  
*Proof* Given a graph G = *\(V, E\),* we say that G has a node cover of its edges of size  
I *U* I if there is a *U* \~ *V* such that every edge of G is incident to a node of *U.* Given an  
integer *k* \< I *V* I, the problem of determining whether a graph has a node cover of size *k* is  
\}\(r;j/J-complete \(see exercise 14 of Section 1.5.9\). We now show that a polynomial-time  
algorithm for the p-facility minimization problem with performance guarantee *rH* \~ *r* for  
any *r* \> 0 implies a polynomial-time algorithm for the node-cover problem.  
Consider the family of p-facility minimization problems with *1= E, N* = *V, p* = *k,* and  
if *ei* is incident to node *j*  
otherwise.  
Now G contains a node cover of size *k* if and only if z = I *E* I. Hence an algorithm with a  
performance guarantee of *r* \> 0 yields a solution with *v\(Q\)* \~ IE *I/r.* This implies that G  
contains a node cover of size *k* since any feasible solution that does not cover all of the  
edges has cost of at least  
IE 1- 1 + IE 1+ 1 \> ill.  
*r r* •  
A dramatically different result is obtained for the *p-facility maximization problem.*  
Here *cij* \~ 0 is the profit obtained from assigning client *i* to facility *j* so that  
*v\(Q\)* = I max *cij* for 0 C *Q* \~ *N,*  
iE! *jEQ*  
and the problem is  
z=max\{v\(Q\)\: 1 \~ IQI \~p\}. 3\. Primal and Dual Heuristic Algorithms 403  
We analyze the greedy heuristic for this problem. To accommodate the constraint  
I *Q* I \~ *p* in the greedy heuristic for maximizing a set function, we just modify the stopping  
rule\: If *t* = *p,* stop; *QP* is a greedy solution. Otherwise, *t* \<- *t* + 1. Moreover, since C *ij* ;\:;. 0 for  
all *i* and j, we can assume that the greedy heuristic produces a solution *Ql* with *t* = *p.*  
Let *PI* = V\(QI\) and *PI* = *V\(QI\)* - *v\(Qt-1\)* for *t* = 2, ... *,p.* Thus the value of the greedy  
solution is ZG = Lf\~l *Pl'*  
Theorem 3.3  
ZG \( *\(P* - 1 *\)P\) e* - 1  
*z;\:;'* 1 - *P* ;\:;. *-e-* \~ 0.63 for *p* = 1, 2, ...  
*\(e is the base of the natural logarithm\). Moreover, for each p there is an instance for which*  
*the bound is tight; that is, rG* = 1 - *«p* - 1\) / *p\)P for a p-facility maximization problem.*  
We prove Theorem 3.3 using a series of propositions.  
Proposition 3.4. *If SeT* C *Nand k* fl. *T, then* v\( *T* U *\{k\}\)* - v\( T\) \~ *v\(S* U *\{k\}\)* - *v\(S\).*  
*Proof* We have maxp\:=s *cij* \~ *maxjET cij'* Hence  
*max\(Cik* - mas x *cij'* 0\) ;\:;. *max\(Cik* - maT X *Cij, 0\)*  
JE JE  
and  
*v\(S* U *\{k\}\)* - *v\(S\)* = I *max\(cik* - max *Cij,* 0\) ;\:;. I *max\(cik* - max *Cij, 0\)*  
iEi JES iEi *JET*  
*= v\(T* U *\{k\}\)* - *v\(T\).* •  
This property of set functions is known as *submodularity* and is the essential property  
used to prove Theorem 3.3. In fact, Theorem 3.3 can be generalized to the maximization  
of submodular set functions \(see Section III.3.9\).  
Proposition 3.5. Z \~ *PPI and* Z \~ Ll\~l *Pi* + *PPt+Iior t* = 1, ... *,p* - 1.  
*Proof* Let *Q\** be an optimal solution, that is, *Z* = *v\(Q\*\).* Since *QI* is the set obtained  
after *t* steps of the greedy algorithm, we have *V\(QI\)* = L\:\~l *Pi.* Suppose *Q\** \\ *Ql* = \{j\}, ... ,  
*jk\}'* We have *k* \~ *p* since I *Q\** I \~ *p.* Now  
*v\(Q\*\)* \~ *v\(Q\** U *QI\)*  
*k*  
*= V\(QI\)* + I *\(V\(QI* U \{j\), ... *,N\)* - *V\(QI* U \{j\), ... ,ji-l\}\)\)  
i\~l  
*k*  
\~ *V\(QI\)* + I *\(V\(QI* U \{jJ\) - *V\(QI»*  
i\~l  
\~ *V\(QI\)* + *PPI+\),*  
where the first inequality follows because v is nondecreasing, the second inequality follows  
by Proposition 3.4, and the last inequality follows since *p;\:;. k* and  
*PI+I* ;\:;. *V\(QI* U \{jJ\) - *V\(QI\)* by the definition of the greedy algorithm. • **404** 11.5. Special-Purpose Algorithms  
**Proposition** 3.6. *If ZG* = 1 - *«p* - *l\)/p* Y', *then Z* \~ 1.  
*Proof* By Proposition 3.5 we have  
z\~max11  
To show that *Z* \~ I  
11 \~ *L Pi* + *PPt+1* for *t* = 0, ... *,p* - 1  
i\~1  
*p \(P-1\)P*  
*LPi=l- -*  
i\~1 *P*  
*Pi* \~ ° for *i* = 1, ... *,p.*  
1, we consider the dual  
*p-I L Ui=* 1  
i=O  
I-I  
*- L Ui* - *PUt* + *up* \~ ° for  
i=O  
Ui\~ ° for  
*t* = 0, ... *,p* - 1  
*i* = 0, ... *,p* - 1.  
Now observe that a feasible solution to the dual is given by  
and *Ui* = *\(p* - *l/PY up* for *i* = 0, ... *,p* - l.  
*p*  
This is true since  
*p-I U* \( *\(P* - 1 *\)P\)*  
*L Ui* =..J!. 1 - -- *P*  
1=0 *P P*  
and  
I-I *U* \( \( 1 \)1\) \( 1\)1  
\~ *Ui* + *PUI* =; 1 - *P; P* + *P; up* = *up.*  
# •  
This proves the first part of Theorem 3.3\. Now we show that the bound is tight for  
*P* = 2, 3, ... *\(p* = 1 is trivial\).  
**Proposition** 3.7. *For the family ofp-facility location problems defined by cP* = *\(cfj\) with*  
*III =p\(p- l\)and INI* = *2p-* 1; *and with \(a\)forj=* 1, ... *,p-* 1  
*cfj* = *P*  
*\{ \(p* \_ 1\) *pP-2\(P* - 1 \)j-I  
*for i* = *\(j* - *l\)p* + 1, ... *,jp*  
# °  
*otherwise* 3\. Primal and Dual Heuristic Algorithms 405  
*\(b\) for j* = *p,* ... , *2p* - 1  
*P* \_ *\{pP-l for i* = 1 + *j* + \(l - *2\)p and I* = 1, ... , *p* - 1  
c *ij* - 0 *otherwise,*  
*we have*  
*Proof* An optimal solution is given by the last *p* columns, so  
*z* = *p\(p* - *1\)pP-l* = *\(p* - *1\)pp.*  
We now show that the greedy algorithm can select the first *p* columns. We have  
2\: Cfl = *p\(p* - *1\)pP-2* \~ 2\: *cIij* for *j* = 2, ... , *p* - 1  
*iEI iEI*  
and  
*2\: cIij* = *\(p* - *l\)pp-l iEI*  
for *j* = *p,* ... , *2p* - 1.  
Hence the greedy algorithm can choose the first column first. Now suppose that the greedy  
algorithm has chosen the first *t* - 1 columns and let QH = \{l, ... , *t* - 1\}. Then  
Table 3.4.  
*C2 =*  
6 0 9 0 0  
6 0 0 9 0  
C \~\),  
2 *C3 =*  
6 0 0 0 9  
0 0 4 9 0 0  
0 4 0 9 0  
0 4 0 0 9  
*C4 =*  
48 0 0 64 0 0 0  
48 0 0 0 64 0 0  
48 0 0 0 0 64 0  
48 0 0 0 0 0 64  
0 36 0 64 0 0 0  
0 36 0 0 64 0 0  
0 36 0 0 0 64 0  
0 36 0 0 0 0 64  
0 0 27 64 0 0 0  
0 0 27 0 64 0 0  
0 0 27 0 0 64 0  
0 0 27 0 0 0 64 406 11.5. Special-Purpose Algorithms  
*V\(QI-I* U \(t\}\) - *V\(QI-I\)* = *p\(p* - *l\)pP-2* p\~  
\( 1 *\)/-1*  
*\> V\(QI-I* U \(j» - *v\(Qt-l\),* for *j* = *t* + 1, ... , *p* - 1.  
But *forj* \> *p* - 1, we obtain  
*V\(QI-I* U \(j» - *V\(QI-I\)* = *\(p* - *l\)pP-'* - *\(p* - *l\)pP-2* I \~  
*1-2* \( 1\)1  
*= \(p* \_ *l\)pP-' \(P* \~ 1 r'  
I\~O *P*  
# .  
Hence the greedy heuristic can choose column *t* next. Thus  
*p-2* \( \_ *1\)1* \( \_ 1 *\)P-I*  
*ZG* = *\(p* - *l\)pP-'* I *p--* + *\(p* - *l\)pP-' p--*= *\(p* - *1\)\(PP* - *\(p* - 1\)P\)  
I\~O *P P*  
and  
*ZG* = *pP* - *\(p* - 1\)P = 1 \_ *\(P* - 1 *\)P.*  
*Z pP P* •  
Empirical evaluation of the greedy heuristic for the p-facility location problem shows  
that it performs reasonably well \(above 80% of the optimal value\) on most real and  
randomly generated instances. Moreover, the solution obtained by the greedy heuristic  
frequently can be improved by applying the interchange heuristic, which begins with a set  
of size *p* and recursively replaces an element in the set with one not in the set as long as the  
objective improves. However, there is a family of instances, like those given in Proposition  
3.7, where the greedy heuristic obtains a solution that achieves its worst-case performance  
and the solution cannot be improved by applying the interchange heuristic. In addition,  
when the interchange heuristic begins with an arbitrary set of size *p,* its worst-case  
performance is inferior to that of the greedy heuristic.  
Let *Z I* be the value of a solution produced by the interchange heuristic.  
Proposition 3.8 *ZI* \~ *\[P/\(2p* - *1\)\]z andfor each p there is an instancefor which the bound*  
*is tight, that is, rI* = *p/\(2p* - 1\).  
*Proof* Let *QI* C *N* be the set chosen by the interchange heuristic. Now apply the  
greedy heuristic to *QI* so that *ZI* = *V\(QI\)* = Lf\~1 *Pi,* where *Pi* = *V\(Qi\)* - *V\(Qi-l\)* for *i* = 2, ... ,  
*P,PI* = *V\(QI\),* and *QP* = *QI.* Let *Q\** be an optimal solution and *Q\** \\ *QP-I* = \{jl, ... *,h\}.* By  
the termination rule of the interchange heuristic, *V\(QP-I* U \(ji» - *V\(QP-I\)* \~ *PP'* By Propo-  
sition 3.4 we have  
*k*  
*v\(Q\*\)* \~ *V\(QP-l\)* + I *\(V\(QP-I* U \(j;\}\) - *V\(QP-I»*  
i\~1  
Now since *Pp* \~ *p;for i* \< *p,* we have *pp* \~ *\(1!P\)ZI* and *Z* \~ *\[\(2p* - *1\)/P\]ZI.*  
See exercise 13 for a family of instances which establishes that the bound is tight. • 3\. Primal and Dual Heuristic Algorithms 407  
Simulated Annealing  
The interchange heuristic stops when it finds a "locally optimal" solution relative to the  
chosen neighborhood structure. As combinatorial optimization problems may have many  
local optima, it is typical to run the interchange heuristic many times with randomly  
chosen starting points.  
A different approach for trying to obtain a global optimum using an interchange  
heuristic is called simulated annealing. Despite the fancy name, the idea is very simple.  
While the interchange heuristic produces a sequence of solutions with increasing objective  
value, here we allow the objective value to decrease occasionally to avoid getting stuck at a  
local optimum.  
Consider the problem  
\(3.5\) max *\{c\(Q\)\: Q* E \~\}.  
Q\~M  
Suppose *QO* is the current solution and we find a point *QI* in the neighborhood *N\(QO\)* of  
*QO*  
*.* If *C\(QI\)* \> *c\(QO\),* we proceed as before by replacing *QO* with *QI.* On the other hand if  
c\( *Q* I\) \~ c\( *QO\),* we replace *QO* with *Q* I with probability *p,* where *p* is a decreasing function  
of *c\(QO\)* - *C\(QI\).* The motivation for moving to a point with a smaller objective value is  
that if we are stuck in a shallow local optimum, there is a chance of escaping by moving to  
a neighbor having a lower objective value. The probability *p* can also be decreased as a  
function of the number of iterations. One reason for doing this is to obtain convergence;  
another reason is that as the global optimum is approached, making steps away from the  
optimum becomes less attractive.  
Simulated Annealing Algorithm for \(3.5\)  
*Initialization\:* Let ao \> 0, 0 \< *P* \< 1, *QO* E \~ and *i* = O.  
*Step* 1\: Given *Qi,* generate *Q'* \~ *N\(Qi\).*  
*Step* 2\: a\) If *c\(Q'\)* \> *C\(Qi\),* then *Qi+1* = *Q'.*  
b\) If *c\(Q'\)* \~ *C\(Qi\),* then *Qi+1* = *Q'* with probability *p* = *\{exp\[c\(Q'\)* - *c\(Qi\)\]la.\}*  
and *Qi+1* = *Qi* with probability 1-*p. I*  
*Step* 3\: *ai+1* = *ai\(1* - p\) and *i* ... *i* + 1.  
Now provided that  
i. it is possible to move from any set *Q* E \~ to any other *Q'* E \~ in a finite number of  
iterations,  
ii. each set in a neighborhood is chosen with equal probability, and  
iii. the neighborhoods are symmetric in the sense that *Q* E *N\(Q'\)* if and only if  
*Q' EN\(Q\),*  
it can be shown that the algorithm converges to the global optimum. However, the  
provable rate of convergence is exponential.  
The empirical efficiency of simulated annealing depends on the neighborhood struc-  
ture and the rate at which *a* is decreased. For some combinatorial optimization such as the  
traveling salesman problem and a variety of problems related to circuit design, simulated  
annealing has found much better solutions than those obtained by a random-start  
interchange algorithm.  
Probabilistic Analysis  
Experiments and statistical analysis can be done to draw conclusions about typical 408 11.5. Special-Purpose Algorithms  
behavior of heuristics. In some cases, it is even possible to do a probabilistic analysis to  
obtain a priori results about average behavior. With this approach, one must be careful to  
use a probability distribution of the instances that is both realistic and mathematically  
tractable.  
We mention three general types of stochastic models that are amenable to analysis. One  
such model deals with problems on graphs and uses random graphs as the underlying  
stochastic model. A *random graph* on *n* nodes is one in which the edges in the graph are  
selected at random. In the simplest of these models, the events of the graph containing any  
edge are identically and independently distributed random variables; that is, the probabil-  
ity that *\(i, j\)* E *E* is *q* for all *i, j* E *V.* When *q* = 1, all possible graphs on *n* nodes are equally  
likely. Then the probability of some property Q occuring on such a random graph with *n*  
nodes is simply the fraction of *n-nodes* graphs that possess property Q. We say that *almost*  
*all graphs possess property* Q if the probability approaches 1 as *n* .... 00. For our purposes,  
property Q could be that a certain heuristic finds an optimal solution to a given problem  
whose instance is specified by an *n-node* graph.  
To illustrate this idea, consider the p-facility maximization problem in which C is the  
edge-node incidence matrix of a graph. The problem is then to choose *p* nodes so that the  
number of edges incident to the chosen set is maximum. The greedy heuristic begins by  
choosing a node of maximum degree; then this node and all edges incident to it are  
deleted, and the process is repeated until *p* nodes have been chosen. It is a fact that for the  
random graph model given above, if *p* does not grow too fast as a function of *n,* then the  
greedy heuristic finds an optimal solution for almost all graphs. In addition, the greedy  
solution is optimal to the linear programming relaxation. We will not prove these results.  
But they are an easy consequence of an interesting theorem which says that in almost all  
graphs, if *p* does not grow too rapidly with *n,* then no two nodes in the set of *p* nodes of  
largest degree have the same degree.  
Another stochastic model deals with problems in which the data are points in the plane.  
For example, the p-median problem in the plane is the special case of the p-facility  
minimization problem in which C is an *n* x *n* matrix and C *ij* is the euclidean distance  
between points *i* and *j.* Here we assume that the points are placed randomly in a unit  
square using a two-dimensional uniform distribution.  
For this problem, a very sharp estimate has been obtained on the asymptotic value of  
ZIP. By this we mean that as *p* and *n* approach infinity in a well-defined way, ZIP *\(p, n\)*  
approaches *c/\(P, n\)* with a probability that goes to 1 \(almost surely\), where *c* is a constant.  
Here *c* = 0.377 and/\(p, *n\)* = *nip\}.* Results of this type are generally proved by comparing  
the asymptotic value of ZIP to the objective value of a continuous problem. These results  
can be used to analyze the asymptotic performance of heuristics since, as we have already  
shown, it is frequently not hard to analyze the behavior of the objective values produced  
by simple heuristics. For the p-median problem it has indeed been proved that there is a  
fast heuristic *\(H\)* that almost always finds a solution with *rH* \~ 1 - *t* for any *t* \> o. Similar  
analyses have been done for the linear programming relaxation of the p-median problem.  
Here it has been shown that ZLp\(P, *n\)* converges to 0.376 *nip\}* almost surely. Consequently,  
for this stochastic model of the p-median problem, the asymptotic value of the absolute  
value of the duality gap is very small.  
A third type of stochastic model deals with random objective or constraint coefficients.  
For example, in the p-median problem, we could assume that the *c;/s* are drawn randomly  
from a uniform distribution. Here it has been shown that \(ZIP - ZLP\) *I* ZLP converges to  
*\(p* - 1\) *I 2p* almost surely when *p* grows slowly with *n,* so a positive duality gap is to be  
expected. This has been confirmed by computational experiments as well.  
A final comment on these models and results concerns what is deducible from  
\(ZIP - ZLP\) *I* ZLP regarding the number of nodes *L* in a branch-and-bound algorithm that 4\. Decomposition Algorithms 409  
uses linear programming relaxation. For the euclidean problems it has been shown that  
\(ZIP - ZLP\) / ZIP converges to 0.00284 almost surely. Nevertheless, it has also been shown  
that a branch-and-bound algorithm will almost surely explore *nP/200* nodes.  
4. DECOMPOSITION ALGORITHMS  
In this section, we will consider algorithms based on Lagrangian duality \(Section 11.3.6\)  
and Benders' decomposition \(Section 11.3.7\).  
Solving the Lagrangian Dual by Subgradient Optimization  
Recall that to obtain the Lagrangian dual of an integer programming problem, we  
partitioned the constraints into a set oflinear constraints *\(A* I *x* \~ *b* I\) and a second set *Q* so  
that  
IP\(Q\) ZIP = max\{cx\: *A IX* \~ *b l , X* E *Q\}.*  
Then we obtained the Lagrangian dual with respect to the constraints *A IX* \~ *b* I given by  
\(LD\)  
where the Lagrangian relaxation for a given *A* is  
\(LR\(A»  
It is essential to choose *Q* so that for fixed *A,* LR\(A\) is easy to solve. As we have already  
shown in Example 6.2 of Section 11.3.6, there may be several ways of choosing *Q* and  
there generally is a tradeoff between the simplicity of solving LR\(A\) and the quality of the  
bound *Zw.* In this section, we will present a subgradient algorithm and a cut generation  
algorithm for solving the Lagrangian dual and use these algorithms to solve UFL.  
Applications to the traveling salesman problem will be presented in Section 11.6.3.  
It has been shown in Section 11.3.6 \(Corollary 6.4\) that ZLR\(A\) is a piecewise linear  
convex function of *A.* Furthermore, in Section I.2.4 we presented a subgradient algorithm  
for maximizing a piecewise linear concave function or, equivalently, minimizing a  
piecewise linear convex function. Here we use the subgradient algorithm to solve LD.  
Proposition 4.1. *dient ofzLR\(A\) at ..1=..1°.*  
*If XO is an optimal solution to* LR\(AO\), *then SO* = *bl*  
- *A IXO is a subgra-*  
*Proof* The result is a direct consequence of Proposition 4.2 of Section 1.2.4. •  
We now consider a Lagrangian dual for UFL. One option is to take the dual with respect  
to the constraints *LjEN Yu* = 1 for i E *I.* Then  
*Q* = *\{x* E *Bn, Y* E *R'\:'"\: Yu* - *Xj* \~ 0 for *i* E *I,j EN\},*  
*ZLR\(U\)* = max *\(L L \(cij* - *u;\)YiJ* - *L jjXj* + *LUi\)*  
*\(x,y\)EQ iEi JEN JEN iEi* **410** 11.5. Special-Purpose Algorithms  
and  
Recall \(Proposition 6.11 of Section 11.3.6\) that *u* is unconstrained since we have taken the  
dual with respect to equality constraints. Here we have used *u* rather than .Ie for the  
multipliers because of their connection with the dual variables introduced in the linear  
programming relaxation of UFL.  
With this formulation it is very easy to solve the Lagrangian relaxation for fixed *u.*  
**Proposition 4.2.** *For the Lagrangian relaxation of* UFL, *the following statements are*  
*true.*  
a. *ZLR\(U\)* = *LjEN \(LiE! \(Cij* - *u;\)+* - *jjt* + *LiE! Ui.*  
b. *Zw* = ZLP.  
c. *A subgradient of hR\(U\) at U* = *UO is given by Si* = 1 - *LjEN Yij\(UO\) for i* E *I, where*  
1 *if Cij* - *u?* \> 0 *and L \(cij* - *u?t* - *jj* \> 0  
\{  
*Yij\(UO\)* = 0 *iE!*  
*otherwise.*  
*Proof* otherwise. Hence  
Optimizing first over the *Y* variables, we set *Yij* = 0 if *Cii* - *Ui* \<S 0 and *Yij* = *Xj*  
Now, maximizing over *Xj,* we set *Xj* = 0 if *LiE! \(Cij* - *Uit* - *jj* \<S 0 and *Xj* = 1 otherwise.  
Hence statement a is true.  
Statement b follows from the observation that *ZLR\(U\)* = *w\(u\)* as defined in \(3.1\).  
Statement c follows from the optimal values of the variables given in the proof of a and  
Proposition 4.1. •  
Although the Lagrangian dual of UFL is not a stronger relaxation than the linear  
programming relaxation ofUFL, the Lagrangian dual is still of interest since subgradient  
optimization is reportedly a very efficient algorithm for minimizing *ZLR\(U\).*  
***Example* 1.1 *\(continued\).*** We apply the subgradient algorithm to LD.  
We start with *u'* = \(12 8 6 8 8 3\), where *u\}* is the second max in row *i.* We use  
the subgradient direction s given in Proposition 4.2 and a "geometric" sequence for  
determining step size. **In** particular, *ul+'* = *U'* - *e,s',* where *e,* = 2 and *e,* is halved every  
three iterations thereafter.  
The results of the first 11 iterations are shown in Table 4.1.  
We observe that even though *w\( u\)* has attained its minimum value of 411, the algorithm  
does not terminate since s \* O. To establish that s = 0 is a subgradient at *U* 10, we would  
need to generate multiple optimal solutions to LR\(UIO\) and then take a convex combina-  
tion of the corresponding subgradients. 4\. Decomposition Algorithms 411  
Table 4.1.  
Iteration *t*  
1 12 8 6 8 8 3 46 1 1 1 0 0 0 2  
2 10 6 4 8 8 3 43 0 0 -1 0 0 0 2  
3 10 6 6 8 8 3 42 1 1 1 0 0 0 2  
4 8 4 4 8 8 3 47 -1 -1 -1 0 0 0 1  
5 9 5 5 8 8 3 44 -1 -1 -1 0 0 0 1  
6 10 6 6 8 8 3 42 1 1 1 0 0 0 1  
7 9 5 5 8 8 3 44 -1 -1 -1 0 0 0 1  
2  
8 9\~ 5\~ 5\~ 8 8 3 421 -1 0 0 0 0 0 1  
2  
9 10 51 51 8 8 3 42 -1 0 0 0 0 0 1  
2  
10 101 51 51 8 8 3 411 1 1 1 0 0 0 1  
4  
11 1O± 5! 5! 8 8 3 42! 1 1 1 0 0 0 1  
4  
The example illustrates the difficulty of stopping the subgradient algorithm.  
However, since the Lagrangian relaxation is to be embedded within a branch-and-  
bound algorithm, subgradient optimization can be used to obtain good bounds easily and  
quickly without having to wait for the algorithm to "converge". In particular, we use three  
criteria for stopping the subgradient algorithm at iteration \(, namely\:  
a. *Sl* = 0;  
b. if the data are integral, then *hR\(UI \)* - \~ \< 1, where \~ is the value of the best available  
feasible solution; and  
c. after a specific number of sub gradient iterations has occurred, that is \{ \~ \(max.  
It is also important to use the multipliers to construct primal feasible solutions. For  
example, when solving UFL with a Lagrangian dual relaxation, for each *u* we can  
construct the solution *Q\(u\)* given by \(3.3\).  
Finally, the reduced prices and *z* can be used to fix variables at each node of the branch-  
and-bound tree \(see Proposition il\).  
*Example* 4.1. We consider a p-facility variant of Example 2.1 having the same matrix  
*\(cij\)* as before, with fixed-costsJi = 0 for all\} *EN* and where exactly four facilities must be  
opened. We solve this instance using a Lagrangian dual/subgradient optimization/branch-  
and-bound algorithm.  
Using the greedy-interchange heuristic described in Section 3, a feasible solution of  
value -12,509 is found. The subgradient algorithm is then initialized with *u\}* = second  
max *cij* for all *i.* The step size is halved every *n* = 33 iterations. After 102 iterations, an  
upper bound of -12,336 and a feasible solution of value -12,363 are found. By this stage,  
using reduced prices as in Proposition 2.1, two of the variables *Xj* can be fixed to 1, and 28  
of the *Xj* variables can be set to O. The remaining problem is to open facilities at two of the  
three remaining sites \(a problem that is easily solved\), and -12,363 is indeed shown to be  
the optimal value. The optimal solution is shown in Table 2.4.  
Solving the Lagrangian Dual by Constraint Generation  
It has been shown in Corollary 6.3 of Section 11.3.6 that if *\{xk* E R\~\: *k* E *K\}* and *\{rj* E R\~\:  
\} E J\} are the extreme points and extreme rays of conv\(Q\), then 412 11.5. Special-Purpose Algorithms  
*Zw* = min *r,*  
\~.A  
\(MLD\) *r,* + *A\(A* I *Xk* - *b* I\) \~ *exk* for *k* E *K*  
\~ *erj forj* E *J*  
Since MLD has a very large number of rows, it is a suitable candidate for the FCPA of  
Section 2. Assuming that all but the nonnegativity constraints are in g;;, we now describe  
the separation algorithm for g;;.  
Separation Algorithm for MLD. Given *\(r,\*, A\*\),* with A\* \~ 0, calculate  
*hR\(A\*\)* = max *ex* + *A\*\(b l \_AIX\).*  
xEconv\(Q\)  
If *Yf\** \~ ZLR\(A \*\), stop. *Yf* = ZLR\(A \*\), A = A \* is an optimal solution ofMLD.  
If *r,\** \< *ZLR\(A\*\),* an inequality in g;; is violated.  
a. If ZLR\(A\*\) .... 00, then there exists a ray *r j* for *j* E *J* such that \(e - *A\*A I\)rj* \> O. Hence  
the inequality *M* I *r j* \~ *erj* is violated.  
b. If ZLR\(A\*\) \< 00, then there exists an extreme point *Xk* for *k* E *K* such that ZLR\(A\*\) =  
*exk* + *A\*\(b l* - *A IXk\).* Since *r,\** \< ZLR\(A\*\), the inequality *r,* + *A\(A IXk* - *b l \)* \~ *exk* is  
violated.  
For UFL, with *Q* = *\{x* E *Bn, Y* E *R';zn\: Yij* - *Xj';;;;* 0 for i E *I,j* EN\},  
the extreme points *\{x\\ ykhEK* of conv\(Q\) are  
*\{xEBn,yEBmn\:yij-Xj';;;;O* for *iEI,jEN\}.*  
HenceMLDis  
*Zw* = min *r,*  
\~.u  
*r,* + I *Ui\(-l* + I *yt\)* \~ I I *eijyt* - I *jjxJ* for *k* E *K,*  
*iEI JEN iEI JEN JEN*  
where *yt* = 0 if *xJ* = 0 and *yt* E \{o, l\} if *xJ* = l.  
In the separation algorithm, the constraints are generated by solving for *ZLR\(U\)* as  
indicated in Proposition 4.2.  
Benders'Decomposition  
We have seen in Section 11.3.7 that the problem  
Z = max *ex* + *hy*  
\(MIP\) *Ax* + *Gy.;;;; b*  
*x EX* s; Z\~,  
yER\~ 4\. Decomposition Algorithms 413  
can be reformulated as  
z= max l1  
\(MIP'\)  
11 \~ *ex* + *uk\(b* - *Ax\)* for *k* E *K*  
*vj\(b -Ax\)* \~ 0 for\) E *J*  
*xEX, I1ERI,*  
where *\{Uk* E *R'J'\: k* E *K\}* are the extreme points of Q = *\{u* E *R'J'\: uG* \~ *h\},* and  
*\{v j* E *R'J'\:\)* E J\} is the set of extreme rays *of\{u* E *R'J'\: uG* \~ O\}.  
Though MIP' is not a linear program, the large number of constraints suggests the use  
of a cut generation algorithm. It suffices to adapt the FCPA for LP\(Bl'\) by replacing the  
linear programming relaxation Lpl with a mixed-integer programming relaxation *MIpt*  
and to describe the separation algorithm.  
Constraint Generation Algorithm for MIP'  
*Initialization\:* Find \(possible empty\) sets *KI* s\: *K, JI* s\: *J.* Let  
S1 = \{11 E *R* 1, *x EX\:* 11 \~ *ex* + *uk\(b* - *Ax\)* for *k* E *KI,*  
*vj\(b* - *Ax\)* \~ 0 for\) E *P\}.*  
*Sett* = 1.  
*Iteration t\:*  
*Step* 1\: Solve the relaxation ofMIP'\:  
Zl = max\{l1\: \(11, *x\)* E Sk, *x* EX\}  
a. IfMIpl is infeasible, stop. MIP' is infeasible.  
b. IfMIpl is unbounded, find a feasible solution pair \(I1t  
, *Xl\)* with 111 \> *w* for some large  
value *w.*  
c. Otherwise let the optimal solution be \(111  
, *Xl\).*  
*Step* 2\: *Separation.* Solve the linear program \(see Section 11.3.7\)  
*ZLP\(Xl\)* = max *hy*  
*Gy* \~b *-Axt*  
*yERf*  
or its dual.  
a. If *ZLP\(Xl\)* .... 00, stop. MIP' is unbounded.  
b. If *ZLP\(Xl\)* is finite, let the primal solution be *yt,* and the dual solution *ut•*  
c. If LP\(xt\) is infeasible, let *VI* be a dual ray with *vt\(b* - *Axt\)* \< O. \(Note that at the  
indication ofinfeasibility, we also get a dual extreme point *ut .\)*  
d. *Optimality test.* If *ext* + *hi* \~ I1t  
, stop. *\(xt*  
*, i\)* is an optimal solution ofMIP'.  
e. *Violation.* If *ext* + *hi* \< 111 or *LP\(xt\)* is infeasible, at least one constraint ofMIP' is  
violated. **414** 11.5. Special-Purpose Algorithms  
i. If *ZLP\(Xt\)* is finite, 11 \~ *ex* + *ut\(b* - *Ax\)* is violated. Set *K t+ 1* = *Kt* U \{f\}, that is,  
S\)71 = S\~ n \(\(11, *x\)\:* 11 \~ *ex* + *ut\(b* - *Ax\)\}.*  
ii. If *LP\(xt\)* is infeasible, *vt\(b* - *Ax\)* \~ 0 is violated. Set *p+l* = *Jt* U \{f\}, that is,  
\(Although it is not necessary, we can also update *Kt* by setting *Kt+l* = *Kt* U \{t\} so that  
*f.t\<-t+1.*  
There are several difficulties in implementing Benders' decomposition that concern  
solving the relaxation  
*zt* = max 11  
11 \~ *ex* + *uk\(b* - *Ax\)* for *k* E *Kt*  
*vi\(b* - *Ax\)* \~ 0 for\) E *P*  
11 E *R* I, *x* EX S Z\~,  
where *Kt* and *P* are the index sets of inequalities available after the first *t* iterations.  
One difficulty is that *MIpt* is a mixed-integer program with one continuous variable. A  
way of alleviating this difficulty is to replace 11 by a threshold value 11\* in MIP'. We then  
replace MIP' by the pure-integer feasibility problem for the constraint set  
*\{x EX\:* 11\* \~ *ex* + *uk\(b -Ax\)* for *k* E *K, vi\(b -Ax\)* \~ 0 for\) E J\}.  
Then if the resulting problem is feasible \(infeasible\), 11\* is increased \(decreased\) and the  
feasibility problem is solved again. If lower and upper bounds on *zt* are known, then  
binary search can be used to specify the sequence of values for the parameter 11\*.  
A second difficulty is that very often there is primal degeneracy in the problem *LP\(xt\),*  
so there is not a unique dual solution *ut •* The choice of the dual extreme point *ut* leading to  
a "good" violated constraint can be very important. One approach is to generate cuts that  
are not dominated by any other constraint.  
A third problem lies in the choice of the initial subset of constraints *Kl, P.* If care is not  
taken with this choice, very unstable behavior of the algorithm may be observed. One  
proposal is to solve the linear programming relaxation ofMIP' and to take *Kl* and *P* to be  
the index sets of the extreme points and extreme rays required to generate the optimal  
linear programming solution. A second alternative is to use a heuristic to generate a  
"good" solution *\(x\*, y\*\)* to MIP' and then to derive initial cuts from the solution of LP\(x\*\).  
*Example* 7.1 *of Section* 11.3.7 *\(continued\)*  
max *5Xl - 2X2* + *9X3 + 2Yl - 3Y2* + *4Y3*  
*5Xl - 3X2* + *7X3* + *2Yl + 3Y2* + *6Y3* \~ -2  
*4Xl + 2X2* + *4X3* + *3Yl - Y2 + 3Y3* \~ 10  
*Xi* \~ 5 for\) = 1,2,3  
*x* E zl, *y ERl.* 4\. Decomposition Algorithms  
*Initialization. KI* = *JI* = 0. *t* = 1.  
# 415  
*Iteration 1*  
*ZI* = max\{11\: 11 E R I  
, *X* E *ZI, Xj';;;* 5 forj = 1,2, 3\}.  
*Step* 2\: *Separation.* Solve the linear program  
hp\(XI\) = max 2YI - *3Y2* + *4Y3*  
2YI + *3Y2* + *6Y3* ,;;; -2  
3YI - *Y2* + *3Y3* ,;;; 10  
*YERI.*  
LP\(xI\) is infeasible since its dual is unbounded, which is verified by the dual extreme  
point *ul* = \(1 0\) and the extreme ray VI = \(1 0\) \(see Figure 7.1 of Section 11.3.7\).  
\~ = *KI* U \{l\}, *P* = *JI* U \{l\}.  
*Iteration 2*  
*Step 1\:*  
-2 - 5XI + *3X2* - *7X3* \~ 0  
*Xj* ,;;; 5 for j = 1, 2, 3  
11 E *R* I, *X* £; *Z;.*  
An optimal solution is *Z2* = 5, *X2* = \(0 5 1\).  
*Step 2\:* min *6UI* - *4U2*  
*2UI* + *3U2* \~ 2  
*3UI-* u2\~-3  
6UI+3u2\~ *4*  
*u* ER\~.  
The dual is unbounded, which is verified by the extreme point *u2* = \(0 extreme ray *v2* = \(1 3\). *K3* = *K2* U \{2\}, *J3* = *P* U \{2\}.  
3\) and the **416**  
11.5. Special-Purpose Algorithms  
*Iteration 3*  
*Step 1\:*  
Z3 = max *Yf*  
An optimal solution is Z3 = 3, *X3* = \(0 *Yf* \~ -2 + *1X2* + *2X3*  
*Yf* \~ 30 - 7Xl - *8X2* - *3X3*  
-2 - 5Xl + *3X2* - *7X3* \~ 0  
28 - 17xl - *3X2* - *19x3* \~ 0  
*Xj* \~ 5 for *j* = 1, 2, 3  
*Yf* E *R* 1, *x* E *Z!.*  
3 1\).  
*Step 2\: hp\(X3\)* = max *2Yl* - *3Y2* + *4Y3*  
*2Yl* + *3Y2* + *6Y3* \~ 0  
*3Yl* - *Y2* + *3Y3* \~ 0  
*yER!.*  
An optimal solution is *ZLP\(X3\)* = 0, *y3* = \(0 0 0\) and an optimal dual solution  
\}\). *CX3* + *hp\(X3\)* = 3 = *Yf3.* Hence *\(x3*  
*, y3\)* = \(0 3 1 0 0 0\) is an optimal  
is *u3* = \(0 solution.  
As we observed in Section II.3.7, Benders' decomposition is useful algorithmically  
when *LP\(x\)* has structure. There we used the structure ofUFL to obtain the reformulation  
\(7.2\) given by  
Z = max - I *jjXj* + I *Yfi*  
*jEN iE!*  
*Yfi* \~ *Cik* + I *\(cij* - *Cikt Xj* for *kEN* and *i* E *I*  
*jEN*  
I *Xj* \~ 1  
*jEN*  
We now illustrate the solution of this reformulation using the constraint generation  
algorithm.  
***Example* 1.1 *\(continued\)***  
*Initialization* 5\. Dynamic Programming  
417  
*Iteration 1*  
*Step* 1\: '11 = 13, *'1i* = 9, '1j = 6, '1i = 10, '1\~ = 10, '1A = 4, *Xl* = \(0  
LiE! '1i - *LjEN iJXj* = 49.  
*Step* 2\: *Separation* for each client *i. exl* + *ZLP\(XI\)* = 25\:  
*i* = 2\: '12 \~ 1 *+ 7XI + 3X2 + 8X3* + *X5*  
i = 4\: '14 \~ 2 + *Xl + 3X 2* + *8X4 + 6X5*  
*i* = 5\: '15 \~ 0 *+ 8XI + 5X3 + lOx4+ 8X5*  
i = 6\: '16 \~ 0 *+ 2XI + 3X3 + 4X4 + X5*  
o 0 0\). Zl  
is violated  
is violated  
is violated  
is violated  
*Iteration 2*  
*Step* 1\: *'1T* = 13, *'1i* = 9, '13 = 6, *'1l* = 10, '1\~ = 10, *'1l* = 4, *X2* = \(0 0  
*Step* 2\: *Separation. ex2* + ZLP\(X2\) = 37.  
'11 \~ 0 + *12xI* + *13x2* + *6X3* + *X5* is violated.  
0\). Z2 = 44.  
*Iteration 3*  
*Step* 1\: '1f = 13, '11 = 9, '1\} = 6, *'1J* = 10, '1\~ = 10, '1g = 4, *X3* = \(0 0\). Z3 = 41.  
*Step* 2\: *ex3* + ZLP\(X3\) = 41\. Since the upper and lower bounds are equal, the solution *X3* is  
optimal.  
5\. DYNAMIC PROGRAMMING  
Dynamic programming provides a framework for decomposing certain optimization  
problems into a nested family of subproblems. This nested structure suggests a recursive  
approach for solving the original problem from the solutions of the subproblems.  
Dynamic programming was originally developed for the optimization of sequential  
decision processes. In a *discrete-time sequential decision process,* there are *T* periods,  
*t* = 1, ... , *T.* At the beginning of period *t,* the process is in *state St-b* which depends on \(a\)  
the initial given state *so,* and \(b\) the decision variablesxb ... *,Xt-l* for periods 1, ... , *t* - 1.  
The significance of the state is that the contribution to the objective function in period *t*  
depends only on *St-l* and *Xt,* and the state in period *t* + 1 depends only on *Sl-I* and *Xl.*  
Formally we describe a sequential decision process by the model  
\(5.1\)  
*T*  
Z = max I *gt\(St-b Xl\)*  
*x\], ... , XT t\:\:\:\:l*  
*St* = *CPt \(St-" Xt\)* for *t* = 1, ... , *T* - 1  
*So* is given.  
The domains of the state and decision variables depend on the particular application  
being considered. **418** 11.5. Special-Purpose Algorithms  
We can consider the 0-1 knapsack problem  
\(5.2\)  
as a sequential decision process. An instance is specified by integers nand *b* and positive  
integral n-vectors c = *\(cl,* ... *,cn\)* and *a* = *\(ai,* ••. *,an\).*  
In period *k, k* = 1, ... , *n,* the decision is whether to putthe kth item into the knapsack.  
The state of the process in period *k* is the number of units of the knapsack that are  
available after we have made the decisions regarding items 1, ... , *k-l,* that is,  
k-l  
*Sk-l* = *b* - I *ajXj* = *Sk-2* - *ak-lXk-t-*  
j\~l  
Thus  
and So = *b.* For *k* = 1, ... *,n,* the feasible domain is given by ° \~ *Sk* \~ *b,* and *Xk* E \{O, n.  
Another problem that can be viewed as a sequential decision process is the uncapaci-  
tated lot-size problem \(ULS\), which has been formulated in Section 1.1.5 as  
*T*  
min I *\(PtYt* + *CtXt* + *htst \)*  
t\~l  
*St-l* + *Yt* = *dt* + *St* for *t* = 1, ... , *T*  
\(5.3\)  
for *t* = 1, ... , *T*  
So = 0, *ST* = °  
S E *RI+l, Y* E *RI, x* E *BT.*  
The data are the unit production costs \{pJ\~l, unit storage costs \{htr\{\~l, set-up costs  
\{clr\~l, and demands \{dlr\{\~l. All of the data are assumed to be nonnegative and integral. The  
variable *YI* is the production in period *t.* If *YI* \> 0, we must pay the set-up cost *Ct.* This is  
achieved by the constraint *YI* \~ *\{J\)Xt ,* where *\(J\)* is a suitably large positive number. The  
variable *St* is the inventory available at the end of period *t.* Since demand cannot be  
backlogged, we have *St* \~ 0.  
The formulation \(5.3\) is of the form \(5.1\) with  
and  
Here the decision variables are both *X t* and *Yt.*  
We now develop a recursive optimization scheme for the sequential decision process  
\(5.1\). For *k* = *T, T* - 1, ... , 1, let 5\. Dynamic Programming **419**  
*Zk\(Sk-l\)* = max I *gt\(St-b Xt\)*  
*\(5.4\) x" ...* ,XTt\~k  
*T*  
*St* = *¢Mt-b Xt\)* for *t* = *k,* ... , *T* - 1.  
Thus *Z* = ZI\(SO\) for the given value of *So.*  
**Proposition 5.1** *Zk\(Sk-d* = max\{gk\(sk-l, *Xk\)* + *Zk+l \(Sk\)\}*  
*Xk*  
*Proof* By the definition of *Zk+l* given in \(5.4\), the term on the right equals  
\~\~x *\{gk\(Sk-b Xk\)* + Xk'\~\~\\T t\~t *gMt-b X t\)\}*  
*St* = *¢Mt-b Xt\)* for *t* = *k,* ... , *T* - I  
= \~\~x Xk,\~a.\~XT *\{gk\(Sk-b Xk\)* + t\~t *gt\(St-b Xt\)\}*  
*St* = *¢t\(St-b Xt\)* for *t* = *k,* ... , *T* - I  
The first equality holds since *gk* is not a function of *Xk+b* ... , *XT,* and the second equality  
follows from \(5.4\). •  
The recursion given in Proposition 5.1 transforms the original optimization problem  
\(5.1\) with *T* decision variables, *T* - I state variables, and *T* - I state constraints into a  
sequence of *T* subproblems. The kth subproblem  
\(5.5\)  
has only one decision variable and one state constraint but must be solved for all possible  
values of *Sk+* Thus the efficiency of solving \(5.5\) depends on the number of values of *Sk-b*  
unless it is possible to determine *Xk* analytically as a function of *Sk+*  
The recursion expresses an intuitive *principle of optimality* for sequential decision  
processes; that is, once we have reached a particular state, a necessary condition for  
optimality is that the remaining decisions must be chosen optimally with respect to that  
state.  
The shortest-path problem between specified nodes provides a nice illustration of the  
principle of optimality. Suppose that *PO,T* is a path from node 0 to node *T* and that node *k,*  
*k* \* 0, *T,* is on *PO,T'* Hence *PO,T* decomposes into the two paths *PO,k* and *P k,T.* The principle of  
optimality says that a necessary condition for *PO,T* to be a shortest-path between nodes 0  
and *T* is that *Pk,T* be a shortest path between nodes *k* and *T.* We have used this fact in 420 n.s. Special-Purpose Algorithms  
developing the shortest-path algorithms of Section 1.3.2, which therefore may be consid-  
ered as dynamic programming algorithms.  
The principle of optimality is a means of excluding non optimal decisions by domina-  
tion. In the general sequential decision process given by \(5.1\), if *x?,* ... ,x\~ is a feasible set  
of decisions that yields *Sk* = *sZ,* then a necessary condition for its optimality is that  
*XZ+b* ... ,x\~ be an optimal set of decisions with respect to the problem that begins in  
period *k* + 1 with *Sk* = *s2.*  
A Dynamic Programming Algorithm for the **0-1** Knapsack Problem  
We now demonstrate the solution of a recursive formulation by developing a classical  
dynamic programming algorithm for the 0-1 knapsack problem \(5.2\). Although the  
recursion \(5.5\) can be applied to the 0-1 knapsack problem, it is easier to develop the  
standard dynamic programming algorithm using a slightly different approach that  
reverses the order of the recursion.  
For *k* = 1, ... , *n,* define *Nk* = \{I, ... , *k\}* and  
Thus *znCb\)* = *zCb\).*  
We will proceed recursively to calculate *zn\(b\)* from *Zn-l,* which in turn is calculated from  
Z *n-2,* and so on. The recursion is initialized with  
Now observe that if *Xk* = 1 in an optimal solution to \(5.6\) then *d* - *ak* \~ 0 and  
On the other hand, if *Xk* = 0 in an optimal solution to \(5.6\), then  
Hence for *k* = 2, ... , nand *d* = 0, ... *,b,* we obtain  
\(5.7\)  
Relation \(5.7\) is the basic recursion for determining *zn\(b\).* It also applies for *k* = 1 by  
defining *zo\(d\)* = 0 for *d* \~ O. To put it in a slightly more compact form, we define  
*zkCd\)* = - 00 for *d* \< O. Thus for *k* = 1, ... *,n* and *d* = 0, 1, ... *,b,* we obtain  
\(5.8\)  
where for all *k,* we have *zkCd\)* = -00 if *d* \< O. 5\. Dynamic Programming 421  
For fixed *k* and *d,* a constant number of calculations is needed to solve \(5.8\); hence  
O\( *nb* \) calculations are required to determine *Z* A *b* \).  
Given *Zk* for *k* = 0, 1, ... , *n,* a recursion in the opposite direction is used to determine  
an optimal solution *XO* = *\(x?,* ... ,x\~\). We have  
*XO* = \{O if *zn\(b\)* = *zn\_l\(b\)*  
*n* 1 otherwise.  
Now let *d2* = *b* - LJ\~k+1 *ajxJ.* Then, for *k* = *n* - 1, ... , 1, we obtain  
The amount of work required in the backward recursion to determine an optimal  
solution is dominated by the work in the forward recursion \(5.8\). Hence the overall  
running time of the algorithm is *O\(nb\).* Thus we have obtained a pseudopolynomial-time  
algorithm \(but not polynomial\) for the 0-1 knapsack problem.  
*Example* 5.1  
max *16xI* + *19x2* + *23x3* + *28x4*  
*2xI + 3X2* + *4X3 + 5X4* \~ 7  
*xEB4.*  
*zl\(d\) =* \{ ° for ° \~ *d* \~ 1  
16 for 2 \~ *d* \~ 7  
*z2\(d\) =*  
16 for *d=2*  
\{ ° forO \~ *d* \~ 1  
19 for 3 \~ *d* \~ 4 \[max\(16, 19 + 0\)\]  
35 for 5 \~ *d* \~ 7 \[max\(16, 19 + 16\)\]  
*z,cd\)* = \(  
*zz\(d\)* forO \~ d\~3  
23 for *d* = 4 \[max\(19, 23 + 0\)\]  
35 for *d* = 5 \[max\(35, 23 + 0\)\]  
39 for *d* = 6 \[max\(35, 23 + 16\)\]  
42 for *d* = 7 \[max\(35, 23 + 19\)\].  
Finally,  
*Z4\(7\)* = max\(42, 28 + *z3\(2»* = 44.  
Hence *xS* = 1, x\~ = *xg* = ° since *z3\(2\)* = *z2\(2\)* = zl\(2\), and *x?* = 1 since *Z* 1\(2\) \> 0.  
The recursion \(5.7\) can be interpreted as a method for solving a maximum-weight path  
formulation of the 0-1 knapsack problem. The directed graph has a node s and has nodes  
*\(k, d\)for k* = 0, 1, ... , nand *d* = 0, 1, ... *,b.* For 1 \~ *k* \~ *n* - 1 and all *d,* there is an arc  
of the form *«k* - 1, *d\), \(k, d»* of weight ° that corresponds to setting *Xk* = 0, given  
*LjENk\_1 ajxj* = *d.* For 1 \~ *k* \~ *n* - 1 and all *d* \~ ab there is an arc of the form *«k* - 1,  
*d* - *ak\), \(k, d»* of weight *Ck* that corresponds to setting *Xk* = 1, given *LjENk-I ajxj* = *d* - *ak.* 422 11.5. Special-Purpose Algorithms  
In addition, there are arcs *\(s,* \(0, *d»\)for* all *d* of weight ° \(see Figure 5.1\). The objective is to  
find a maximum-weight path that starts at node s and terminates at node *\(n, b\).* The  
recursion \(5.7\) chooses a maximum-weight path to node *\(k, d\),* given the weights of  
maximum-weight paths to nodes *\(k* - 1, *d\)* and *\(k* - 1, *d* - *ak\).*  
The algorithm given above generalizes straightforwardly to the *bounded variable*  
*knapsack problem*  
*\(5.9\) z\(b\)=max\{* I *CjXj\:* I ajXj'\:;;b,Xj,\:;;pjfOr\)EN,xEZ\~\},  
*JEN JEN*  
where the *a/s, c/s, P/s,* and *b* are positive integers. Note that \(5.2\) is the special case of  
\(5.9\) with *pj* = 1 for all\) *EN.*  
We simply replace \(5.8\) by the recursion  
for *k* = 1, ... *,n* and *d* = 0, 1, ... *,b,* where *zo\(d\)* = ° for all 0.\:;; *d* .\:;; *b.*  
The number of calculations needed to solve \(5.10\) for fixed *k* is *O\(b\(fJk* + 1\), which  
gives an overall running time of *O\(nb2\).* Finally, note that explicit bounds on the variables  
are not required since we can always take *Xj'\:;; lb/aj\]* for all\) *EN.* However, in Section  
II.6.1 we will give an *O\(nb\)* algorithm for the knapsack problem without explicit upper  
bounds on the variables.  
A Dynamic Programming Algorithm for the Uncapacitated Lot-Size Problem \(ULS\)  
For *k* = *T* - 1, ... , 1, the recursion \(5.4\) for ULS is  
\(5.11\)  
*Zk\(Sk-l\)* = min *\{PkYk* + *CkXk* + *hk\(Sk-l* + *Yk* - *dk\)* + *Zk+l\(Sk-l* + *Yk* - *dk\)\}*  
*xkE\{O.l\}*  
O\~Yk\~WXk  
and  
\(5.12\)  
mm  
X1E \{O,1\}  
O\~Yr\:OS;WXT  
Figure 5.1 5\. Dynamic Programming 423  
Figure 5.2  
Since the demands are integral-valued, it can be shown that the production and storage  
variables will also be integers. The difficulty is that since demand in period *k* can be met by  
production in any period *t* \~ *k,* it follows that *Sk-l* can be as large as "'L\~k *dt,* and it appears  
that a very large number of combinations of *\(Sk-b Yk\)* must be considered to solve \(5.11\).  
Fortunately, as the following theorem demonstrates, this is not the case.  
Theorem 5.2. *There is an optimal solution to* \(5.3\) *is which*  
a. *St-1Yt* = *0for t* = 1, ... , *T.*  
b. *IfYt* \> 0, *then Yt* = "'Lk\~t *dkforsome r, t* \~ *r* \~ *T.*  
c. *If St-l* \> 0, *then St-l* = "'L\~\~t *dkfor some q, t* \~ *q* \~ *T.*  
*Proof* We represent \(5.3\) as a fixed-charge flow problem on the network shown in  
Figure 5.2.  
Let *\(x\*, y\*, s\*\)* be an optimal solution to \(5.3\). *x\** specifies those arcs pointing out of  
node ° that are available for flow. Thus, given *x\*,* we can delete arc \(0, *j\)* if *xi* = ° and then  
determine *\(y\*, s\*\)* by solving a minimum-cost flow problem on the resulting network.  
By Proposition 6.2 of Section 1.3.6, the arcs with positive flow define a spanning forest  
rooted at node 0\. Suppose *\(j* - 1, *j\)* is in the forest. Then there is a path from ° to *j* - 1 in  
the forest. If \(0, *j\)* is also in the forest, we obtain a cycle. Hence it cannot be the case that  
both arcs *\(j* - 1, *j\)* and \(0, *j\)* are in the forest.  
Parts band c are simple consequences of a. •  
From Theorem 5.2, it follows that *2\(T* - *t\)* combinations of *\(St-l, Yt\)* must be consid-  
ered in solving \(5.11\) and \(5.12\). Thus the overall running time is *O\(T2\),* and recursive  
optimization yields a polynomial-time algorithm for ULS.  
*Example 5.2*  
*t* 1 2 3 4 5  
*p,* 3 3 4 5 5  
*h,* 1 1 1 2 2  
c, 30 30 30 30 30  
*dt* 32 41 48 36 20 424 11.5. Special-Purpose Algorithms  
*t* = 5\: S4 E \{O, 20\} and S4 + *Y5* = 20  
*Z5\(0\)* = *20P5* + C5 = 130, *Y5* = 20  
*Z5\(20\)* = 0, *Y5* = O.  
*t* = 4\: S3 E \{O, 36, 56\},  
*zi56\)* = *his3* - *d4 \)* + *z5\(20\)* = 2\(20\) = 40, *Y4* = 0  
*z4\(36\)* = *Z5\(0\)* = 130, *Y4* = 0  
*Z4\(0\)* = min\{36p4 + C4 + *Z5\(0\), 56p4* + *20h4* + C4 + *z5\(20\)\}*  
= min\{340, 350\} = 340, *Y4* = 36.  
*t* = 3\: S2 E \{O, 48, 84, 104\},  
*z3\(104\)* = *h3 \(S2* - *d3 \)* + *z4\(56\)* = 56 + 40 = 96, *Y3* = 0  
*z3\(84\)* = 36 + *z4\(36\)* = 36 + 130 = 166, *Y3* = 0  
*z3\(48\)* = 0 + *Z4\(0\)* = 340, *Y3* = 0  
*Z3\(0\)* = min\{48p3 + C3 + *Z4\(0\), 84p3* + C3 + *36h3* + *z4\(36\),*  
*104p3* + C3 + *56h3* + *z4\(56\)\}*  
= min\{562, 532, 542\} = 532, *Y3* = 84.  
*t* = 2\: Sl E \{O, 41, 89, 125, 145\}  
*zl145\)* = 104 + z3\(104\) = 200\}  
*z2\(125\)* = 84 + *z3\(84\)* = 250 \_ 0  
*z2\(89\)* = 48 + *z3\(48\)* = 388 *Y2 -*  
*z2\(41\)* = 0 + *Z3\(0\)* = 532  
*Z2\(0\)* = min\{41p2 + C2 + *Z3\(0\), 89p2* + C2 + *48h2* + *z3\(48\),*  
*125p2* + C2 + *84h2* + *z3\(84\), 145p2* + C2 + *104h2* + z3\(104\)\}  
= min\{685, 685, 655, 665\} = 655, *Y2* = 125.  
*t* = 1\: *ZI\(O\)* = min\{32pl + Cl + *Z2\(0\),* 73Pl + Cl + 41hl + *z2\(41\),*  
121Pl + Cl + 89h l + *z2\(89\),* 157Pl + Cl + 125h l + *z2\(125\),*  
177Pl + Cl + 145h l + *z2\(145\)\}*  
= min\{781, 822, 870, 876, 906\} = 781, Yl = 32.  
Hence. the optimal solution is Yl = 32, Xl = 1, *Y2* = 125, *X2* = 1, *Y3* = *Y4* = *X3* = *X4* = 0,  
*Y5* = 20, *X5* = 1, and the optimal cost is *z* = 781.  
6\. NOTES  
Section 11.5.1  
Efroymson and Ray \(1966\) have given a classical branch-and-bound algorithm for the  
uncapacitated facility location problem that uses bounds obtained from the weak formu-  
lation. Spielberg \(1969a,b\), among others, recognized the importance of the strong 6\. Notes 425  
formulation. Bilde and Krarup \(1977\) gave a family of UFLs for which the linear  
programming relaxation of the strong formulation always has an integral optimal solu-  
tion.  
Survey articles on the uncapacitated facility location problem are by Krarup and  
Pruzan \(1983\) and Cornuejols, Nemhauser and Wolsey \(1984\). An annotated bibliography  
of articles on location problems that were published from 1980-1985 appears in Wong  
\(1985\). Francis and Mirchandani \(1988\) is a collection of survey articles on various aspects  
of discrete location models.  
Section 11.5.2  
The strong cutting-plane algorithm described in this section was implemented by  
Morris \(1978\).  
Schrage \(1975\) showed how variable upper-bound constraints could be treated implic-  
itly within the simplex algorithm. Todd \(1982\) presented an alternative approach that  
circumvents degeneracy problems.  
The test problem data \(Table 2.1\) is from Kuehn and Hamburger \(1963\).  
Facets of the un polytope have been studied by Cornuejols, Fisher and Nemhauser  
\(1977b\), Guignard \(1980\), Cornuejols and Thizy \(1982b\), Cho et al. \(1983\), and Cho,  
Johnson et al. \(1983\).  
Strong cutting planes and FCPAs have been developed for a variety of other hard  
combinatorial problems. These include the capacitated plant location problem, Leung  
and Magnanti. \(1986\), the matching problem, Grotschel and Holland \(1985\); the assign-  
ment problem with side constraints, Aboudi and Nemhauser \(1987\); the three-index  
assignment problem Balas and Saltzman \(1986\); the max cut problem, Barahona,  
Grotschel, and Mahjoub \(1985\) and Barahona and Mahjoub \(1986\); the linear ordering  
problem, Grotschel, Junger, and Reinelt \(1984, 1985b\), and the acyclic subgraph problem,  
Grotschel, Junger, and Reinelt \(1985a\). Several others will be cited in the notes for  
Chapter II.6.  
Section 11.5.3  
Ball and Magazine \(1981\) and Rinnooy Kan \(1986\) gave general introductions on the  
design and analysis of heuristics for discrete optimization problems.  
A greedy algorithm for maximizing a \(constrained\) set function was used by Kruskal  
\(1956\) to solve the maximum-weight spanning tree problem exactly. This is one of the first  
formal uses of the greedy algorithm in combinatorial optimization. However, it must have  
been used for centuries as a common sense tool for problem solving. Kuehn and  
Hamburger \(1963\), Spielberg \(1969b\), and others have used greedy heuristics to obtain  
solutions to UFL.  
Much the same can be said for local search\\interchange heuristics. Kuehn and  
Hamburger \(1963\), Manne \(1964\), and many other researchers have used interchange  
heuristics to obtain solutions to UFL. Reiter and Sherman \(1965\) described an interchange  
scheme for a rather general class of combinatorial optimization problems and carried out  
a statistical analysis of the results based on random starting solutions; also see Reiter and  
Rice \(1966\). Many other uses of the greedy and interchange heuristics will be cited later in  
these notes and in the notes for Chapter II.6.  
Using primal and dual heuristics simultaneously is a more recent idea. The primal-  
dual heuristic described for UFL is essentially the DUALOC algorithm of Erlenkotter  
\(1978\). The projection algorithm of Conn and Cornuejols \(1987\) for UFL also can be given  
a primal-dual interpretation. A primal-dual vehicle routing algorithm has been given by  
Fisher and Jaikumar \(1981\). 426 11.5. Special-Purpose Algorithms  
The worst-case analysis of heuristics can be traced to a result of Graham \(1966\) on a  
scheduling problem. Another classical paper on worst-case analysis is the work of D.S.  
Johnson et al. \(1974\) on the bin-packing problem. Johnson \(1974\) also analyzed a variety  
of heuristics for several combinatorial optimization problems. Jenkins \(1976\), Korte and  
Hausmann \(1976\), and Hausmann et al. \(1980\) gave worst-case analyses of greedy-type  
algorithms for finding a maximum-weight subset in an independence system. Hausmann  
and Korte \(1978\) showed that in a certain well-defined way, no polynomial-time algorithm  
could improve on the performance of the greedy algorithm for this problem. Zemel \(1981\)  
discussed and evaluated various ways of measuring the quality of approximate solutions.  
Surveys on the worst-case analysis of heuristics for combinatorial optimization prob-  
lems are Sahni \(1977\), Chapter 6 of Garey and Johnson \(1979\), Korte \(1979\), and Fisher  
\(1980\). Wolsey \(1980\) explained how worst-case bounds are obtained from primal and  
dual feasible solutions and illustrated this idea with several examples.  
Golden and Stewart \(1985\) presented general techniques for the statistical analysis of  
the performance of heuristics. Proposition 3.2 appeared in Fisher \(1980\). The results on  
the greedy and interchange heuristics for the p-facility maximization problem appeared in  
Cornuejols, Fisher, and Nemhauser \(1977a\). Babayev \(1974\) and Frieze \(1974\) showed the  
submodularity of the set function objective of UFL. Extensions of the Cornuejols et al.  
results to submodular set functions will be presented in Section III.3.9.  
The term *simulated annealing* arises from an analogy with the physical process of  
cooling physical substances and how the state of the system depends on the rate at which  
the temperature is dropped. The method is described in Metropolis et al. \(1953\), Kirkpa-  
trick et al. \(1983\), and Kirkpatrick \(1984\). Hajek \(1985\) gave a survey on theory and  
applications of simulated annealing. Lundy and Mees \(1986\) studied the convergence of  
the algorithm. Applications of simulated annealing were given by Bonomi and Lutton  
\(1984\) and Vecchi and Kirkpatrick \(1983\).  
The probabilistic models for UFL come from Cornuejols, Nemhauser, and Wolsey  
\(1980b\), Fisher and Hochbaum \(1980\), Papadimitriou \(1981b\), and Ahn et al. \(1988\).  
Surveys of techniques and results in this field appeared in Karp \(1976\) and Karp and  
Steele \(1985\). An annotated bibliography was given by Karp, Lenstra et al. \(1985\).  
Algorithms iri which random or probabilistic choices are made are at an early stage of  
development in combinatorial optimization. Maffioli et al. \(1985\) gave an annotated  
bibliography on this subject, and Welsh \(1983\) and Maffioli \(1986\) gave surveys. Rabin  
\(1976\) published one of the first articles in this area.  
Section 11.5.4  
Subgradient optimization and the Lagrangian dual for UFL have been used by Cor-  
nuejols, Fisher, and Nemhauser \(1977a\) to solve a float maximization problem, by Mulvey  
and Crowder \(1979\) to solve a problem in cluster analysis, and by Neebe and Rao \(1983\) to  
solve a problem of assigning users to sources. Geoffrion and McBride \(1978\) have used a  
similar approach to solve capacitated location problems.  
The results of Example 4.1 were obtained by D. Peeters \(private communication\).  
Applications of Lagrangian duality to the group problem, to set covering and partition-  
ing, and to the traveling salesman problem will be presented or cited in Chapter II.6. Some  
other applications from the literature are\: combinatorial scheduling \[Fisher \(1973, 1976\),  
Fisher, Northup, and Shapiro \(1975\), and Potts \(1985\)\]; multiperiod scheduling of power  
generators \[Muckstadt and Koenig, 1977\); generalized assignment problem \[Ross and  
Soland \(1975\), Chalmet and Gelders \(1977\), and Fisher, Jaikumar, and Van Wassenhove  
\(1986\)\]; and hierarchical production planning \[Graves \(1982\)\]. 7\. Exercises 427  
Grinold \(1972\) gave an alternative approach to solving the Lagrangian dual.  
Other decomposition methods for solving UFL and the closely related p-median  
problem include\: Dantzig-Wolfe decomposition \[see Garfinkel, Neebe, and Rao \(1974\)\];  
Benders' reformulation \[see Magnanti and Wong \(1981\) and Nemhauser and Wolsey  
\(1981\)\]; primal subgradient optimization \[see Cornuejols and Thizy \(1982a\)\]; and a  
disaggregation scheme \[see Cornuejols, Nemhauser, and Wolsey \(1980a\)\].  
Benders' decomposition has been applied by Geoffrion and Graves \(1974\) to the design  
of a multicommodity distribution system. A combined Lagrangian/Benders' scheme has  
been used by Van Roy \(1986\) to solve a capacitated location problem.  
Section 11.5.5  
Richard Bellman coined the terms *dynamic programming* and *principle oj optimality,*  
pioneered the development of the theory and applications, and wrote the first book on this  
subject \[Bellman \(1957\)\].  
The recursion for the 0-1 knapsack problem appeared in Dantzig \(1957\) and Bellman  
\(1957\). Some computational improvements and generalizations to multiple constraints  
have been given by Weingartner and Ness \(1967\) and Nemhauser and Ullman \(1969\).  
Dynamic programming algorithms for the lot-size problem have been given by Wagner  
and Whitin \(1958\) and Zangwill \(1966\).  
Some general texts on dynamic programming are Bellman and Dreyfus \(1962\),  
Nemhauser \(1966\), White \(1969\), Dreyfus and Law \(1977\), and Denardo \(1982\).  
7. EXERCISES  
1. Solve the linear programming relaxation of WUFL for the problem instance with\:  
C\~\(; 3 5 0 6  
2 4 6 4  
i\) 9 7 7 7  
6 6 6 0  
and  
*J=* \(3 2 3 3 2 2\).  
2. Prove Proposition 1.2.  
3\. 4. Show that\: every noninteger extreme point *\(x, y\)* of the linear programming relaxa-  
tion ofUFL is of the form  
i\) *Xj* = maXi *Yij* for all\) E *Nb*  
ii\) there is, at most, one\) with 0 \< *Y ij* \< *Xj* for each *i* E *I,* and  
iii\) the rank of *A* equals IN I I ,  
whereNI = \{j *EN\:* 0 \< *Xj* \< l\},ll = \{i E *I\:Yij* = Oor xJorall\), andYij \$. Zl for some  
\)\}, and *A* is an 1111 x INI 10,1 matrix with *aij* = 1 *ifYij* \> O.  
Consider the problem instance of UFL in exercise 1. Which variable upper-bound  
constraints are violated by the solution found in exercise I? Use a linear program-  
ming system to solve the linear programming relaxation of UFL by adding such  
constraints. 428 11.5. Special-Purpose Algorithms  
5. i\) Show that if *i* \* *j* \* *k* and *r* \* *s\* t,* then  
*Yri* + *Yrj* + *Ysj* + *Ysk* + *Ytk* + *Yli* \~ 1 + *Xi* + *Xj* + *Xk*  
is a valid inequality for UFL.  
ii\) Find an inequality of this form that cuts off the fractional solution *\(y4, x4 \)* of  
Example 1.1 \(continued\) in Section 2.  
iii\) Generalize to show that if, for 1 \~ *t* \< *I, All* is a 0, 1 matrix with \<D rows and *1*  
columns whose rows are all the different 0, 1 vectors with *t* 1's and *1* - *to's* with  
*1* \~ *n,* CD \~ *m,* then for any l' \~ *I, N'* \~ *N* with If'I = CD, *IN'* I = *1* it follows that  
*L L a0Y ij* - *L Xj* \~ CD + *t* - *I -1*  
iEJ' *JEN' JEN'*  
is a valid inequality for UFL.  
iv\) Find an inequality cutting off the point *\(x\*, y\*\)* given in Table 2.3.  
6\. Consider the problem that arises when we solve IP over *N'* C *N* using valid  
inequalities *LjEN' njxj* \~ *no.* To solve the problem over *N* we need to lift the  
inequalities so that *LjEN' njxj* + *LjEN\\N' njxj* \~ *no* is valid for the IP over *N.* Suppose  
we have solved UFL with a subset *Xj* for *j EN'* and a subset of the *Yu* for *i* E 1',  
*j EN'.* Is it easy to lift the inequalities of exercise 5?  
7\. Apply the following heuristics to the instance ofUFL in exercise 1\:  
i\) greedy;  
ii\) reverse-greedy \(close one facility at a time\);  
iii\) I-interchange;  
iv\) I-interchange plus greedy;  
v\) design your own heuristic.  
8\. The k-enumeration plus greedy heuristic for maximizing a set function can be  
described as follows\:  
1. Enumerate all subsets S \~ *N* with I S I = *k.*  
2. For each such S, apply the greedy heuristic to the problem max\{vS\(Q\)\:  
*Q* \~ *N* \\ *S\},* where *VS\(Q\)* = *v\(S* U *Q\).* Let *QS* be the greedy solution.  
3. Let S\* U *Q\** = arg max\{v\(S U *QS\)\:* IS I = *k,* S \~ N\}.  
i\) Apply the I-enumeration plus greedy heuristic to the example of exercise 1.  
ii\) Show that the k-enumeration plus greedy heuristic for the p-facility maximiza-  
tion problem has worst-case behavior given by  
*z* - *ZH* \~ \(\~\) *\(p* -*k* - 1 *\)P-k.*  
*Z P p-k* 7. Exercises 429  
9. i. Given *cij* \~ 0 for *i* E *I* and\} *EN,* show that the set function *v\(Q\)* = *LiE\[*  
*maXjEQ* C *ij* for *Q* s\:\:\:; *N* can always be written in the canonical form  
*v\(Q\)* = *L rT* with *rT* \~ 0 for *T* s\:\:\:; *N.*  
*TnQ\*0*  
ii\) Write the set function arising from exercise 1 in canonical form.  
iii\) Write the resulting linear programming formulation and its dual.  
iv\) Which LP formulation would you choose and why?  
v\) Propose a branch-and-bound algorithm based on your choice.  
10. i\) Apply DUALOC to the instance ofUFL in exercise 1.  
ii\) Apply DUALOC to the instance written in canonical form \(see exercise 9\).  
11. Describe greedy and interchange heuristics for the capacitated facility location  
problem.  
12. i\) Formulate the problem of choosing *k* nodes to cover the maximum number of  
nodes in a graph G = *\(V, E\).* \(Note that i E *V* covers\) E *Vif\(i,\}\)* E *E.\)*  
ii\) State and interpret a greedy heuristic.  
iii\) Study the performance of this heuristic when I *V* I is large.  
13\. Show that for the family of instances of the p-facility maximization problem with  
*2p* - 1 clients and *2p* facilities, and with weights *CP* given by *Cj* = *ej* for\} = 1, ... ,  
*2p* - 1, C *i,2p* = 1 for i = 1, ... , *p,* and C *i,2p* = 0 otherwise, for example,  
1  
# o  
# o  
# o  
14\. 15. 16. the interchange heuristic satisfies *Z1* = *\[p/\(2p* - *1\)\]z* when it starts with  
S = \{I, 2, 3, 4\}.  
Described a simulated annealing algorithm for UFL.  
For the instance in exercise 1, solve the Lagrangian dual ofUFL by using each of the  
following\:  
i\) the subgradient algorithm;  
ii\) a constraint generation algorithm.  
Solve the instance of exercise 1 by Benders' decomposition. Investigate the choice of  
violated constraints at each iteration. 430 11.5. Special-Purpose Algorithms  
17\. Let  
*z* = mm I I *CijYij* + I *jjXj*  
*iEI jEN jEN*  
*D\:* I *Yij=* 1 for all *i* E *I*  
*jEN*  
c\: I *diYij* \~ *SjXj* foralljEN  
*iEI*  
*B\:* o \~Yij \~Xj for all iE *I,j* EN  
s\: I *SjXj* \~ I *d i*  
*jEN iEI*  
*I\: Xj* E \{O, 1\} forallj EN,  
where C \~ O,f\~ 0, *d* \~ 0, and *S* \~ 0 are given. Denote by *ZA* the bound obtained  
from this formulation by deleting constraint *A,* and denote by *z A* the bound given by  
the Lagrangian dual. For example,  
*zi* = max *zi\(u\),*  
*u*  
where  
z\~\(u\) = min I I *CijYij* + I *jjXj* + I *Ui* \(1 -I *Yij \)*  
*iEI jEN jEN iEI jEN*  
I *diYij* \~ *SjXj* for allj EN  
*iEI*  
*Xj* E \{O, 1\}  
for all *i* E *I,j* EN  
for allj EN.  
i\) Prove that *ZI* = *zi.*  
ii\) Prove that z\~ \~ z\~.  
iii\) Prove that z\~ = *Zsc.*  
iv\) Show that *z'i;* \< z\~ for the following data\:  
C= 0 1 0 ,  
\(1 0 0\)  
# 001  
*S* = \(3 3 3\), *f* = \(1 1\).  
18\. Let  
*w\(X\)* = max\{ I *jjXj* + I I *CijYij\:* I *Yij* = 1 for *i* E *I,*  
*jEN iEI jEN jEN*  
*Yij* \~ *Xj* for *i* E *I, j* E *N, Y* E R\~n\}.  
i\) Show that *w\(x\)* is concave. 7\. Exercises 431  
ii\) Show that maXO\<;x\<;1 *w\(x\)* = ZLP.  
iii\) Use the subgradient algorithm to solve *maXO\<;x\<;1 w\(x\)* for the instance of  
exercise 1.  
19\. Consider the Dantzig-Wolfe formulation ofUFL where *Lj Yij* = 1 for *i* E *I* are taken  
as the global constraints and where the feasible region of subproblem\} is  
20. 21. Interpret the columns and costs of the resulting master problem.  
Derive a dynamic programming algorithm to solve  
*max\{cx* + *hy\:* 2\: *Yj* \~ *b, Yj* \~ *ajxj* for\} *EN, x* E *Bn, Y* E R\~\}.  
*JEN*  
Derive an *O\(J'l\)* dynamic programming algorithm for the uncapacitated lot-sizing  
problem with backlogging\:  
22\. 23. *T*  
min 2\: *\(PtYt* + *CtXt* + *htst* + *gtft\)*  
t\~l  
*St-l* - *ft-l* + *Yt* = *dt* + *St* - *f t* for *t* = 1, ... , *T*  
*Yt* \~ *WXt* for *t* = 1, ... , *T*  
*So* = fo = *ST* = *fT* = 0  
*S,f* E *RI+I, Y* E *RI, x* E *BT.*  
See \(5.3\), where *f t* denotes the amount backlogged at the end of period *t.*  
Derive an O\(n 2cmax\) dynamic programming algorithm for the 0,1 knapsack problem  
\(5.2\) where Cmax = maxjcj.  
i\) Derive a dynamic programming recursion for the traveling salesman problem  
usingf\(S,\}\) = the length of the minimum-weight partial tour starting at node 1,  
traversing the nodes S \~ *N* \\ \{I,\}\), and terminating at\} EN \\ S.  
ii\) Use the recursion to solve the five-city problem with costs  
2 6 4 7  
1 3 8 5  
c= 9 2 4 12  
8 I 9 2  
3 2 9 4  
24.  
\(State Space Relaxation\). Let  
*g\(k,\}\)=* min \(f\(S,\}\)\:ISI=k\),  
S\~N\\\(I,j\)  
wheref\(S,\}\) is defined in exercise 23. What is *g\(k,\}\)?* Write a recursion for *g\(k,* i\),  
and show that *g\( n,* 1\) is a lower bound on the weight of an optimal tour. 432 11.5. Special-Purpose Algorithms  
25\. Apply the approach of exercise 24 to the multidimensional knapsack problem with  
*fk\(d\)* = maxL\~ *CjXj\:* j\~ *ajXj* \~ *d, x* E *Bk\}*  
*= max\{jk\_l\(d\), fk-l\(d* - *ak\)* + *Ck\}.*  
26\. Derive a dynamic programming algorithm for  
max\{ I *hjy/ Yi* + *Yi+l* \~ *Ui* for *i* = 1, ... , *n* - 1, *Y* E R\~\}.  
*JEN*  
27\. The amount of work to multiply together a *p* x *q* and a *q* x *r* matrix is *pqr.* Given *k*  
matricesMi of dimension *di* x *di+1* whose product *M 1M 2 •* • *• Mk* must be formed, use  
dynamic programming to derive the optimal way in which to form the product. 11.6  
# Applications of  
# Special-Purpose Algorithms  
1. KNAPSACK AND GROUP PROBLEMS  
The structure invoked in this section is that the problems have only one constraint other  
than bounds and integrality on the variables. We consider the integer knapsack problem,  
the group problem, and the 0-1 knapsack problem.  
Many of the algorithms developed in Chapter II.5 can be specialized when there is only  
one constraint, and some other more specific approaches are also applicable.  
The Integer Knapsack Problem  
The *integer knapsack problem* is  
\(1.1\)  
where *ch aj* E Z\~ for *j EN, bE* Z\~, *aj* \~ *b* for *j EN,* and there are no explicit upper  
bounds on the variables. In vector notation, \(1.1\) is stated as  
*z\(b\)* = max\{cx\: ax\~ *b, x* E Z\~\}.  
We suppose throughout this section that c1/ai \~ *c2/a2* \~ · · · \~ *Cn/an,* so the optimal  
solution of the linear programming relaxation is x 1 = *b I* a J. *Xj* = 0 otherwise.  
Dynamic Programming  
Since *Xj* \~ \[b/aj\] \~ b in any feasible solution to \(1.1\), we can use the recursion \(5.8\) of  
Section II.5.5 to obtain an algorithm with worst-case running time *O\(nb*2\). However, it is  
possible to do better. The recursion we now describe directly calculates the value function  
*z\(d\)* =max\{ I *cjxj\:* I *ajxj* \~ *d, x* E z\~\} jEN jEN  
*ford* E *D\(b\)* = \{0, 1, ... , b\}.  
We begin with *z\(d\)* = 0 for *d* = 0, ... , minjEN *aj-* 1, with corresponding optimal  
solution *x* 0 = 0. Given *z\(d'\)* for all *d'* \< *d,* we claim that  
\(1.2\) *z\(d\)* = max\{cj + *z\(d- aj\)\:j EN, d* \~ *aj\} ford* E *D\(b\), d* \~ !\]!\~ *aj.*  
433 434 11.6. Applications of Special-Purpose Algorithms  
To prove the validity of \(1.2\), we first observe that if x0 is an optimal solution to \(1.1\)  
with b = d- *ah* then *x* 0 + *ej* is a feasible solution to \(1.1\) with b = d. Hence  
On the other hand, if xis an optimal solution to \(1.1\) with b = d \~ minjEN *ah* then *xk* \> 0  
for some k with d \~ *ak\>* and x- *ek* is a feasible solution to \(1.1\) with b = d- *ak.* Hence  
*z\(d- ak\)* \~ z\(d\)- *ck\>* and \(1.2\) holds.  
The recursion \(1.2\)requires *O\(n\)* calculations for each *d,* minjEN *aj* \~ *d* \~b. Hence the  
overall running time is *O\(nb* \), which is better than the recursion \(5.8\) of Section II.5.5 by  
a factor of b.  
Examplel.l  
max 11xt + *7x2* + 5x3 + X4  
*6x1* + *4x2* + 3x3 + X4 \~ 25  
xEZ!.  
z\(O\) = 0  
z\(l\) = C4 = 1  
z\(2\) = C4 + z\(1\) = 2  
z\(3\) = max\(5 + z\(O\), 1 + z\(2\)\) = 5  
z\(4\) = max\(7 + z\(O\), 5 + z\(l\), 1 + z\(3\)\) = 7  
z\(5\) = max\(7 + z\(1\), 5 + z\(2\), 1 + z\(4\)\) = 8  
z\(6\) = max\(11 + z\(O\), 7 + z\(2\), 5 + z\(3\), 1 + z\(5\)\) = 11  
z\(7\) = max\(11 + 1, 7 + 5, 5 + 7, 1 + 11\) = 12  
z\(8\) = max\(11 + 2, 7 + 7, 5 + 8, 1 + 12\) = 14  
z\(9\) = 11 + z\(3\) = 16  
z\(lO\) = 11 + z\(4\) = 18  
z\(d\) = 11 + *z\(d-* 6\) ford\~ 11.  
Hence z\(25\) = 11 + z\(19\) = 22 + z\(13\) = 33 + z\(7\) = 44 + z\(1\) = 45, and an optimal solu-  
tionisx0=\(4 0 0 1\).  
As d increases, the recursion \(1.2\) has many redundant terms, since z\(d\) =  
*ck* + *z\(d- ak\)* for any k for which *xk* \> 0 in some optimal solution to \(1.1\) with b *=d.* Let  
*p\(d\)* = minU EN\: Xj is positive in some optimal solution to \(1.1\) with *b* = d\}. Then, since  
*p\(d* - *aj\)* \~ *p\(d\),* it follows that  
\(1.3\) z\(d\) = max\{cj + *z\(d-* aj\)\:j E *N,j* \~ *p\(d-* aj\)\}.  
*Ford* sufficiently large, no comparisons at all are needed. Let 7i = maxjEN\\\{1\) *aj.*  
Proposition 1.1. If *p\(d-* a\)= *p\(d- a+* 1\) = · · · = *p\(d-* 1\) = 1, *then* z\(b\) = *c*1 +  
z\(b- *a1* \)for *all* b \~d. 1. Knapsack and Group Problems 435  
*Proof* For *j* \> 1, we have *p\(d- aj\)* = 1 \<\). Hence by \(1.3\), we obtain *z\(d\)* = Ct +  
*z\(d-* at\). By induction we obtain the result. •  
In Example 1.1 it is readily checked that *p\(d\)* = 1 for 9 \~ *d* \~ 12. It then follows from  
Proposition 1.1 thatp\(d\) = 1 for all *d;;.* 9. It is important to observe that for any knapsack  
problem there exists a value of d for which the condition of Proposition 1.1 holds. The  
proof of the following proposition is a consequence of Proposition 5.6 of Section 11.3.5.  
Proposition 1.2. In *problem* \(1.1\), *we have p\(d\)* = *1/or all d;;.* \(at - 1 \)a.  
Note that in Example 1.1, we have \(at- 1\)a = 5 x 4 = 20 so that *a priori* we can  
conclude that *p\(d\)* = 1 for all *d* ;;. 20\. However, the computation establishes that *p\(d\)* = 1  
for all *d;;.* 9.  
It is interesting to observe that the recursions \(1.2\) and \(1.3\) are algorithms for the  
maximum-weight path formulation developed in Section Il.3.4. For eachj EN there is an  
arc ofweight *cj* from node *d- aj* to node *d,* and *z\(d\)* is the weight of a maximum-weight  
path from node 0 to node *d.* Thus \(1.2\) states that the weight of a maximum-weight path  
from node 0 to node *dis* the maximum over *j* E N of the weight of a variable *j* arc plus the  
weight of a maximum-weight path from node 0 to node *d- aj.*  
A Superadditive Dual Algorithm  
Here we give an algorithm that solves \(1.1\) and its superadditive dual  
\(1.4\) *min\{n\(b* \)\: *n\(aj\);;. cj* for *j* EN, n\(O\) = 0, *n;;.* 0, *n* superadditive\}.  
The idea of the algorithm is as follows. At each iteration, we have a dual feasible solution  
that also satisfies the complementarity slackness conditions  
*n\(d\)* + *n\(b-* d\)= *n\(b\)* for all dE \[0, b\]  
ofProposition 5.2 of Section 11.1.5.  
Let S = *\{x* E Z\~\: *I\:-jeN ajXj* \~ b\}, and for any dual feasible *n* define *H"* =  
\{xES\: *n\(ax\) =ex\}.* Then if there exists xt, *x* 2 E *H,* such that *x* = *xt* + *x* 2 E S and  
n\(axt\) + *n\(ax*2\) = *n\(b\),* it follows that *x* is an optimal solution to \(1.1\) and that *n* is an  
optimal solution to the superadditive dual. This result is a consequence of  
At the ith iteration of the algorithm we have a feasible *ni* that also satisfies complemen-  
tary slackness and an H; !;;; H"'· The initial solution is given by  
which is easily shown to be dual feasible, and *H* 0 = \{0, e\~o ... , \[b/adet\}.  
Suppose that *\(n;,* H;\) does not satisfy the optimality condition given above. Let  
D; =\{ax\: x E Hi\}. The dual solution ni+t is of the form 436 11.6. Applications of Special-Purpose Algorithms  
\{  
ni+1\(d\) = *ni\(d\)* if *dEDi*  
*ni\(d\)- 8* if *b -dEDi*  
*a\( d\)* otherwise, where *a\( d\)* is determined  
by linear interpolation between the points  
\{d, b - d\: d E Di\)  
An example is shown in Figure 1.1, where b = 10 and *Di* = \{0, 4, 8\}.  
To specify the numerical value of *8,* the algorithm works with a candidate set  
C =\{xES\: xis a minimal vector not in *Hi* and *ax\* d* for any *dE Dt*  
Since the optimality condition is not satisfied, *C \** 0. The algorithm sets 8 = Oi, where  
*ni+1 \(ax\)- ex-* ;;;;. 0 for all *x* E C  
and  
ni+1\(ax\)- *ex-* = 0 for some *x* E Ci.  
Let *yi+I* be any point in Ci such that ni+1\(ayi+I\)- e/+1 = 0\. The algorithm sets  
*Hi+* I = *Hi* U \{yi+I\} and then checks the points *x* + *yi+I, x* E *Hi,* for optimality with respect  
to the function *ni+I.* If optimality is not proved, then ci+l = *ci* u \(\{X + yi+I\:  
x E Hi+1\} \\ \{yi+1\)\). Note that we can augment *Hi* by all points y E *C* such that  
*ni+1 \(ay\)- ey* = 0.  
Although there is the possibility of a degenerate dual change \(i.e., Oi = 0\), by definition  
of *ci* we have *ayi+I* \$. *Di.* Hence *Di+I* =Diu \{ayi+l\) \:\:J *Di.*  
Now we claim that the algorithm stops after no more than *i\** = \[\( *b* + 1 \)/2J + 1 iterations.  
Otherwise, on iteration *i\** we obtain IDi'l \> *\(b* + 1\)/2, and hence there must exist values  
*d1*  
*, d2* E *Di\** with *d 1* = *b- d2•* Now ifx1  
, *x 2* are the associated points of *Hi\*,* the optimality  
criterion is satisfied for x = x 1 + x 2•  
0 2 4 6 8 10  
Figure 1.1 1. Knapsack and Group Problems 437  
Theorem 1.3. *The algorithm terminates with an optimal solution.*  
*Proof* We only need to show that ni+I is dual feasible and satisfies complementary  
slackness given that *ni* has these properties. We have already said that *n°* has these  
properties. Also, for all *i* we have that *ni\(aJ\)* \~ *eJ* for *j EN* is satisfied since, \(1\) for all  
j *EN,* either *aJ* E *Di* or *eJ* E Ci, and \(2\) *n\(aJ\)* = *eJ* if *aJ* E *Di,* and *n\(a1\)* \~ *e 1* if *e 1* E C.  
Now consider superadditivity. We will show that  
However, it suffices to consider the subset of \[0, *b\]* given by \{d, *b* - d\: *dEDi\}* since the  
result for all other points in \[0, *b\]* can be shown to follow from linear interpolation. There  
are two cases.  
*Case 1 \(b* - dr *orb* - *d*2 E *Di\).* Then  
ni+l\(dr\) + ni+l\(d2\),\:;; ni\(dr\) + ni\(d2\)- *ei*  
,\:;; *ni\(dr* + *d2\)- ei*  
,\:;; ni+1\(dr + *d2\).*  
The first and third inequalities follow from the construction of the dual solution, and the  
second inequality follows from the superadditivity of *ni.*  
*Case 2 \(dr, d2* E *Di\).* Then *ni+1 \(dJ\)* = *ni\(d\)* for j = 1, 2, and there exists x 1  
, *x 2* E *Hi* such  
that ax*1* = *dh ex1* = *ni\(d1\)* forj = 1, 2. Letx = x 1 + *x 2* and *d* = d 1 + *d2 .*  
If *dEDi,* then  
Also if x E Ci, then  
\(by the choice of *ei\)*  
and  
The final possibility is d \$. *Di* and x \$.C. We have  
\(by the dual change\)  
and, as above, *ex=* ni+1\(d1\) + ni+1\(d2 \). So it remains to show that *ex,\:;; ni\(d\)- ei.*  
Since *x* E S \\ *\(Hi* U Ci\), there exists *x'* \< *x* such that *x'* E *Ci* and *\(x* - *x'\)* E S. By the  
superadditivity *ofni,* we have  
Also *ex* = *ex'* + *e\(x* - *x'\).* Hence  
*ni\(d\)-* ex\~ *\[n;\(ax'\)- ex'\]+ \[ni\(a\(x- x'\)\)- e\(x- x'\)\].* 438 11.6. Applications of Special-Purpose Algorithms  
By the choice ei, we have n;\(a.x'\)- ex'\~ e;; and by the feasibility of n;, we obtain  
n;\(a\(x - x'\)\)- e\(x - x'\) \~ 0. Hence n;\(d\)- ex \~ *e;.* •  
Example1.2  
max 20xl + 9xz + 6x3  
10x1 + 5xz + 4x3 \~ 13  
xEZ\~.  
Initialization. n°\(d\) = 2d for 0 \~ d \~ 13, *H* 0 = \{\(0 0 0\), \(1 0 0\)\},  
C 0 = \{\(0 1 0\), \(0 0 1\)\}, and k = 0. The dual functions nk\(d\) are shown in Figure 1.2.  
Iteration *1.* n1\(13\) = 26- e, n1\(3\) = 6- e, n1\(10\) = 20, n1\(5\) \~ 9, and n1\(4\) \~ 6. Hence  
by linear interpolation we obtain \~\(6- e\)+ \~\(20\) \~ 9, so e \~ \~' and ¥\(6- e\)+ 7\(20\) \~ 6,  
so e \~ Jf-. Hence e = \~' y 1 = \(0 I 0\), and  
forO\~ *d* \~ 3  
for3\~d\~ 10  
for 10 \~ *d* \~ 13  
H 1 = *H* 0 U \{\(0 1 0\)\},  
C1 = \{\(0 0 1\), \(0 2 0\)\}.  
Iteration 2. n2\(13\) = 24i- e, n2\(3\) = 4i- e, n2\(8\) = lSi- e, n2\(4\) \~ 6, n2\(10\) \~ 18,  
n2\(10\) = 20, and n2\(5\) = 9. Hence \}\(4\~- e\)+ \}\(9\) = 6, so e =\~and y2 = \(0 0 1\).  
Now  
·\~d\)= \(  
d forO\~ d \~ 3  
3d - 6 for 3 \~ d \~ 5  
id + \~ for 5 \~ d \~ 8  
3d -10 for8 \~ d \~ 10  
10 +d for 10 \~ d \~ 13  
H 2 =H1 U \{0 0 1\}, C2 = \{\(0 2 0\), \(0 1 1\), \(0 0 2\)\}.  
Iteration 3. n3\(13\) = 23 - e, n\\3\) = 3 - e, n3\(8\) = 14- e, n3\(9\) = 17- e, n3\(10\) = 20,  
n3\(5\) = 9, n3\(4\) = 6, n\\10\) \~ 18, n3\(9\) \~ 15, and n3\(8\) \~ 12. Hence e = 14- 12 = 17-  
15 = 2, *y* 3 = \(0 1 1\) or \(0 0 2\), and *n\(b\)* = 21. Since \(0 1 0\) and \(0 0 1\) are in  
*H,* and \(0 1 0\) + \(0 0 2\) = \(0 0 1\) + \(0 1 1\) = \(0 1 2\) is feasible with value  
21, the algorithm stops with x = \(0 1 2\). The optimal dual function is  
*td*  
-14 + 5d  
- 6 +3d  
4 + d  
-12 +3d  
-30 + 5d  
\~ + *td*  
forO\~ *d* \~ 3  
for 3 \~ *d* \~ 4  
for4 \~ *d* \~ 5  
for 5 \~ *d* \~ 8  
for 8 \~ d \~ 9  
for 9 \~ *d* \~ 10  
for 10 \~ *d* \~ 13. 7T'  
26 26  
81  
24%  
82  
23  
83  
21  
3 4 5  
Figure 1.2  
439 440 11.6\. Applications of Special-Purpose Algorithms  
Heuristic Algorithms  
A very simple greedy heuristic for the knapsack problem \(1.1\) is obtained by considering  
the variables in order of decreasing Cjlaj and then making each variable as large as  
possible. Since we have assumed cdat;;;\:. · · · ;;;\:. cnlan, a greedy solution is Xj = \[bjlaj\] for  
*j* E *N,* where bt = *b* and *bj+t* = *bj- aj lbj/a.J* for *j* = 1, ... , *n* - 1. Its value is  
zn = r.jeN cj \[bjlaA. The running time is O\(n log n\), smce the time required to sort the  
cj/aj's in decreasing order is the most time-consuming step.  
Since an optimal solution to the linear programming relaxation of \(1.1\) is x t = *b I* at and  
Xj = 0 otherwise, we have that hP = hP\(b\) = Ctblat. Now let *zR* = ct\[blad be the value of  
the rounding heuristic with Xt = \[blad and Xj = 0 otherwise. We have *Zn;;;\:. zR* and  
ZLP;;;\:. *z\(b\)* = ZIP•  
We can use the optimal linear programming value to bound the worst-case relative  
errors of the greedy and rounding heuristics.  
Proposition 1.4  
a. ZR \> izLp,  
b. ZR \> ZLP- max C*1 ·•*  
jEN  
Proof  
a. Letf =blat -\[blad \< 1. Now  
ZR = \[b/ ad = 1 \_ \_j\_ ;;;\:. 1 \_ \_j\_ \> !  
ZLP blat blat 1 + f 2'  
Note that the first inequality holds since 1 + f ,;;; blat and that the second inequal-  
ity is true since *f* \< 1.  
b. zLP- *zR* = *ctf.;;;* max *c1 f* \< max *C 1 ·.*  
jEN jEN •  
To prepare for the presentation of a heuristic that always yields a relative error of no  
more than e and that runs in time that is a polynomial function of *n* and e-t, we introduce  
a scaling heuristic that uses dynamic programming to solve a formulation of the knapsack  
problem with the roles of the objective function and constraint reversed. Let  
\(1.5\)  
where tis a positive integer. Note that w\(t\) is a nondecreasing function oft.  
Analogous to \(1.2\), we have the recursion  
\(1.6\) w\(t\) = min\{a1 · + w\(t - c1 \)\}  
jEN  
fort \> 0, and w\(t\) = 0 fort ,;;; 0. The work required to solve \(1.6\) is O\(nt\).  
Now we show that with a suitable choice oft, an optimal solution to \(1.5\) yields an  
optimal solution to \(1.1\). 1. Knapsack and Group Problems 441  
Proposition 1.6. *Suppose* x0 *is an optimal solution to \(1.5\) with t* = t0  
. *Then* x0 *is an*  
*optimal solution to the knapsack problem*  
*for all* d *satisfying* w\(t0\) \~ d \< w\(t0 + 1 \).  
*Proof* Suppose w\(t0\) \~ *d* \< w\(t0 + 1\). Then x0 is feasible since *T.jeN ajxJ* = w\(t0\) \~d.  
Now suppose that x0 is not optimal; that is, there is a feasible *x\** =1= x 0 and  
*T.jeN cjxj* \~ *T.jeN cjxJ* + 1. Hence *T.jeN cjxj* \~ t0 + 1 and *T.jEN ajxj* \~ *d,* which contradicts  
the assumption that w\(t0 + 1\) *\>d.* •  
As a consequence of Proposition 1.6, we can take tin \(1.5\) equal to any known upper  
bound on z1 p; for example, t = \[zLpj. Then for some 1° \~ t, we will obtain  
w\(t0\) \~ b \< *w\(t*0 + 1\). Hence to solve \(1.1\) using \(1.6\), the running time is *O\(nZLp\).* This  
does not appear to be an improvement on the dynamic programming recursion \(1.2\)  
unless the c/s are small relative to the a/s.  
The *scaling* heuristic works by replacing the objective function coefficients *cj* by  
*pj* = \[cj Kj for some K \> 0. The resulting knapsack problem  
\(1.7\)  
is solved using the recursion \(1.6\). We denote an optimal solution to \(1. 7\) by *x\(K\)* and say  
that *x\(K\)* is a *scaling heuristic* solution. Its value is zs = *T.jeN cjxj\{K\).*  
Proposition 1.7. *IfK* \~ 8 minjeN *Cj, then* \~/z 1 p \> 1 - 8.  
*Proof* Since *pj* = *\[cj/* Kj, it follows that *pj* \~ *cj\( K* \< *pj* + 1. Hence  
zs = I *cjxj\(K\)* \~ *K* I *pjxj\(K\)* \~ *K* I *pjxJ,*  
*jEN jEN jEN*  
where the last inequality holds because *x\(K\)* is an optimal solution to \(1.7\). Also,  
Therefore  
The running time of the scaling heuristic is *O\(nZLP! K\).* Therefore it is of interest only if  
*K* is large-that is, if minjeN *cj* is much larger than 8-1• This, unfortunately, for any  
reasonable choice of 8 requires large profit coefficients.  
Observe that the greedy \(or rounding\) heuristic needs small profit coefficients to  
perform effectively and that the scaling heuristic requires large ones to run efficiently. By  
combining the two heuristics we are able to take advantage of the best features of each of  
them. The result is a heuristic that guarantees a relative error of no more than 8 for any  
8 \> 0 and whose running time is *O\(n/8*2\). 442 11.6. Applications of Special-Purpose Algorithms  
We partition the set N into \(N8  
, N \\ N 8\), where N 8 = *\{j* EN\: c *1* \> 8\} and 8 = \(E/4\) \[ZLpj.  
The rounding heuristic is applied to a knapsack problem that contains only the items in  
*N* \\ *N* 8  
, and the scaling heuristic with *K* = \(E/2\)8 is applied to a knapsack problem that  
contains only the items in *N* 8• The two solutions are then combined as explained below.  
The Scaling/Rounding \(SR\) Heuristic  
*p*1 = liJ for all\} EN.  
*Step* 1\: Solve the family of knapsack problems  
by the recursion \(1.6\) for all nonnegative integers *t* with *w\(t\)* \~ *b.* Let *x* 8*\(t\)* be the  
solution that yields w\{t\).  
*Step 2\:* Let *c,ja,* = *maxJEN\\No \(c1ja1\).* Define *x\(t\)* E Z\~ by *xj\(t\)* = *xJ\(t\)* for *j* E *N 8*  
*,*  
*x,\(t\)* = \[\(b - *w\(t\)\)ja,j,* and *xj\(t\)* = 0 otherwise.  
*Step 3\:* Suppose max1 *\{LJENo c1 x1\(t\)* + *c,x,\(t\)\}* is attained with *t* = t\~ Then *x\(t\*\)* is the SR  
heuristic solution of value *zsR* = *LJEN c1*x*1\(t\** \).  
The SR heuristic produces a feasible solution to the knapsack problem since all of the  
variables are nonnegative integers and, by definition of *x,\(t\), LJENo a1xj\(t\)* + *a,x,\(t\)* \~ *b*  
for all t.  
Proposition 1.8. *The running time of the* SR *heuristic is* O\(nE-2\).  
*Proof* To solve the family of knapsack problems in Step 1 by the recursion \(1.6\), we  
need to consider no more than \[zLP/KJ = 1 values *oft.* But  
Thus the running time of \(1.6\) is O\(nE-2\). Steps 2 and 3 take O\(E-2\) time, so the proof is  
complete. •  
Theorem 1.9. *zsR;;;.* \(1 - E\)ziP·  
*Proof* Suppose *x 0* is an optimal solution to \(1.1\) and *Lp,\:oNo c1xJ* = *t0*  
*.* Since  
Proposition 1. 7, restricted to the variables in *N* 8  
, yields 1\. Knapsack and Group Problems Hence  
443  
\(1.8\)  
since *N* 8 s; *N.*  
Now by b of Proposition 1.4 and bye\~ *maxjeN\\Ne cj,* the rounding heuristic yields  
\(1.9\)  
\(since j\~ *cjxJ* \~ \~ \[hP\(b \)j by a of Proposition 1.4\).  
Adding the inequalities \(1.8\) and \(1.9\) yields  
I *CjXj\(t0\)* \~ I *CjxJ-* E I *CjXJ,*  
*jEN jEN jEN*  
The proof is completed by observing that zsR \~ *LjEN cjxj\(t*0\). •  
There are some refinements of the SR heuristic that yield improvements on the relative  
error bound.  
Example 1.3. We apply the SR heuristic to  
ziP= max 592xi + 381x2 + 273x3 + 55x4 + 48x5 + 37x6 + 23x7  
3534xl + 2356x2 + 1767x3 + 589x4 + 528x5 + 451x6 + 304x7 \~ 119,567  
# xEz\:.  
Suppose we are given E = 0.2. Hence e = 0.05 \[zLP\] = 1001.45 and *K* = 100.15. Now  
observe that *cj* \~ e for allj E *N.* Hence *N* 8 = 0, and the SR heuristic is trivial to execute. It  
yields x 1 = \[119,567 /3534\] = 33, *Xj* = 0 otherwise, and zsR = 19,536. It is not hard to show  
that an optimal solution *isx?* = 33, *xg* = x\~ = 1, *andxj* = 0 otherwise and that z1 p = 19,972.  
Hence the actual relative error is 436/19,972 = 0.0218.  
Note that if we replace the rounding heuristic by the greedy heuristic we obtain the  
optimal solution.  
If E = 0.02, then *e* = 100.15 and *K* = 1.00. Hence *N* 8 = \{1 2 3\}. Now in Step 1, with  
*t* = 19,917 we obtain x 8*\(t\)* = \(33 1 0\) and *w\(t\)* = 118,978. Since b - *w\(t\)* = 589 = a4,  
this solution is completed to x = \(33 1 0 1 0 0 0\), which we have already indi-  
cated is optimal. 444 11.6. Applications of Special-Purpose Algorithms  
The Group Problem  
In Section 11.3.5 we have shown how the problem  
\(IP\) z1 p = max\{cx\: Ax= b, x E Z\~\}  
can be relaxed by choosing u E R m, an appropriate m x m unimodular matrix R, and a  
nonsingular diagonal matrix L\\ with positive integer entries 6; for *i* = 1, ... , m to give the  
problem  
z\(u, L\\\) = ub + max\{\(c- uA\)x\: x E S11\(b\)\},  
where *S*11\(b\) = \{x E Z\~\: RAx = Rb + L\\w for some wE zm\}.  
In this section we consider what to do when the group problem for the given choices of  
u, *R,* and L\\ does not yield a feasible solution to IP. We assume here that z1 p \< oo, and we  
have chosen a *u* such that *p* = *uA* - *c* \~ 0. In this case the group problem is a minimum-  
weight path problem on a digraph *9iJ*11 = \( \~, *.s!l* 11 \) having 1 det L\\ 1 = I17!1 J; nodes. We state  
the minimum-weight path problem as  
SP\(d\) *lf/\(d\)* =min I *p1*x*1*  
jEN  
jEN  
I \(Ra*1\)x1* = *d* \(modL\\\)  
xEZ\~,  
where z\(u, L\\ = ub -lji\(Rb\). Here \~=\{dE *Z';'\:* d; \< J; fori= 1, ... , m\}, *.s!l* 11 = \{\(d, d +  
Ra*1* \(mod L\\\)\)\: dE\~ for j = 1, ... , n\}, the weight of the arc \(d, d + Ra*1* \(mod Ll\)\) is *Pi\>*  
and we seek a minimum-weight path from node 0 to node Rb \(mod L\\\).  
The reader should recall that the relaxation simply replaces the ith equation a; x = b; by  
the modular equation *aix* = b; \(mod J;\) and that *u* has been chosen so that the objective  
function of the relaxation is bounded from above. In particular, any feasible solution to IP  
corresponds to some path from node 0 to node Rb \(mod L\\\).  
The connection between knapsack and group problems is motivated by taking  
u = cBAi\} and L\\ to be the Smith normal form of AB, where AB is an optimal basis for the  
linear programming relaxation of IP \[see \(5.3\) of Section 11.3.5\]. In this case, *p*1 = 0 and  
Ra*1* = 0 \(mod Ll\) if x*1* is a basic variable. Thus *SP\(d\)* only involves the nonbasic variables.  
Moreover, L\\ = I det AB I, and it is frequently the case that 61 = · · · = Jm-t = 1 and  
Jm = I det *AB* 1. \(This must be the case if I det *AB* I is a prime number.\) Since the ith  
equation ofSP\(d\) is trivially satisfied when J; = 1, this choice of u and L\\ frequently leads  
to a single-constraint problem in nonnegative integer variables, which is an integer  
knapsack problem with ordinary arithmetic replaced by modular arithmetic.  
When 1 det L\\ 1 is not too large, the minimum-weight path problem SP\(d\) is easily solved  
by Dijkstra's algorithm \(see Section 1.3.2\). Here we consider how algorithms can be  
constructed for IP that make use of this shortest-path viewpoint.  
The following proposition motivates the construction of an implicit path enumeration  
algorithm.  
Proposition 1.10. If IP is feasible and p*1* \> 0 for all j E *N,* then there exists a positive  
integer k such that an optimal solution to IP corresponds to a kth best minimum-weight  
path in *9iJ* t.· 1. Knapsack and Group Problems 445  
To enumerate we need to specify how to branch. One way is to subdivide the set of all  
solutions \(paths\) into sets in which each variable x*1* \(arc type\) occurs at least y*1* times. The  
following proposition tells us how to calculate an optimal solution at a node of the tree. Let  
x\(d\) be an optimal solution to SP\(d\) such that the corresponding path in 0h is acyclic.  
Proposition 1.11. Given y E zr\:\_, an optimal solution to SP\(Rb\) satisfying x \~ y is  
x\* = y + x \(R\(b- Ay\)\), with weight py + 1/f\(R\(b- Ay\)\).  
Proof Setting x = y + x', x' E Z\~, and substituting in SP\(Rb\) gives  
I P;Y; + min I P;X\}  
*\}EN \}EN*  
*jEN*  
I \(Ra;\)xJ = Rb -RAy \(modLl\)  
x'EZ\~.  
Hence to find an optimal solution x\* to SP\(Rb\) that satisfies x \~ y, we find a  
minimum-weight path from node 0 to node R\(b- Ay\) mod Ll. •  
The next proposition tells us how to define the new nodes when we branch; and it uses  
the fact that if x\* is defined as in Proposition 1.11, then there is no vector x satisfying  
y \~ x \~ x\* that is feasible in SP\(Rb \).  
Proposition 1.12. Ifx\* = y+x\(R\(b- Ay\)\), then anyx =t= x\* that corresponds to apathfrom  
0 to Rb \(mod Ll\) subject to the restriction x \~ y must satisfy  
*fork* =t= *j*  
for some\}.  
Now we describe a straightforward path enumeration algorithm. We start from the  
group problem \(5.3\) of Section 11.3.5 mentioned above. Hence we let As denote the basis  
of an optimal solution to the linear programming relaxation and let *AN* denote the  
columns of the nonbasic variables. Now SP\(Rb\) only involves the non basic variables,  
which we suppose are numbered 1, 2, ... , n - m, and the enumeration is carried out only  
over these variables. Also at this point, x, y, and *p* are dimensioned appropriately.  
A Shortest-Path Enumeration Algorithm for IP  
We begin by solving the shortest-path problem from node 0 to node *d* for all *d* E Vt- and let  
x\(d\) be an acyclic optimal solution to SP\(d\) of cost *I/!\(* d\). If there is no path from 0 to d, let  
1/f\(d\) = oo. We then construct a branch-and-bound tree where the node labeled *y*  
corresponds to the feasible set x \~ y.  
Initialization\: x\( d\) and 1/1\( d\) are given for all d E *vt-.* If 1/!\(Rb\) = oo, stop; IP is infeasible.  
Otherwise put 0 E z\~-m with lower bound \~\(0\) = 1/f\(Rb\) on the node list. z = +oo.  
Iteration t  
Step *1\:* If the node list is empty, stop. Then \(a\) ifz = +oo, IP is infeasible and \(b\) ifz is  
finite, xis an optimal solution. Otherwise choose a vector yon the node list and remove  
it from the list. Go to Step 2. 446 II.6. Applications of Special-Purpose Algorithms  
*Step* 2 *\(Feasibility check\)\:* If *lfi\(R\(b* - *Ay\)\)* = oo, return to Step 1. Otherwise let  
*x\** = *y* + *x\(R\(b* - *Ay\)\).* If *Ai/\(b* - *ANx\*\);;.* 0, go to Step 3. Otherwise go to Step 4.  
*Step* 3 *\(Pruning by optimality\)\:* Set *.X\<-- x\*,* z \<-- *py* + lf/\(R\(b- Ay\)\). Delete from the node  
list any node y with \~\(y\) ;;. *z.* Return to Step 1.  
*Step* 4 *\(Branching\)\: Fori=* 1, ... , *n- m,* define *yi* by  
. \{Yi  
*yj* = x; + 1  
*forj \* i*  
*for\}= i*  
and let \~\(y;\) Step 1.  
= *py;* + *lfi\(R\( b* - Ay;\)\). If \~\(y;\) \< z, add *\(y;,* \~\(i\)\) to the node list. Return to  
When the algorithm terminates, it solves IP. One way to guarantee finiteness is to  
impose upper bounds on the variables and to modify Step 4 accordingly.  
Example 1.4 \(Example 5.2 of Section I/.3.5 continued\)  
max 7xt + *2x2*  
-Xt+ *2x2* + *X3*  
= 2  
5Xt+ *X2*  
*+X4*  
= 19  
-2Xt- *2x2*  
*+Xs*  
= -5  
xEZ\~  
leads to the relaxation SP\(7\)\:  
lf/\(7\) =min *-frx3* + *-l¥x4*  
*Sx3* + *X4* = 7 *\(x3, X4\)* E Z\~  
\(mod 11\)  
with *u* = *cBAi/* = \(rr -1¥ 0\), and *z\(u,* .1\) = ¥f- lf/\(7\).  
The shortest paths and corresponding solution values found by Dijkstra's shortest-path  
algorithm are given in Table 1.1.  
To test the feasibility of proposed paths involving the values of the non basic variables  
*\(x3,* x4\), we use *XB* = *Ai/b- A!/ANXN* given by  
*Initialization\: y* = 0, \~\(0\) = W, z =+co.  
Table 1.1.  
d\:  
*x\(d\)* = \(XJ X4\)\:  
II If/\( d\)\:  
0 I 2 3 4 5 6 7 8 9 10  
\(0 0\) \(0 I\) \(7 0\) \(5 0\) \(3 0\) \(I 0\) \(I I\) \(8 0\) \(6 0\) \(4 0\) \(2 0\)  
0 16 21 15 9 3 19 24 18 12 6 I. Knapsack and Group Problems 447  
Iteration 1\:  
1. Picky = \(0 0\) from the node list.  
2. x\*= \(x3 x4\) = \(8 0\).  
XB=\(XI X 2 X 5\)=\(4 -1 1\);t0.  
4. Add the nodes \(9 0\) and \(0 tively.  
1\) to the list, with lower bounds \~\(y\) = 'tf-, ti-, respec-  
Iteration 2\:  
1. Picky= \(9 0\) from the node list. \[Note that \(0 1\) would be chosen if we followed  
the rule of smallest lower bound.\]  
2. x\* = \(9 0\) + X\(6\) = \(10 1\). XB =\(XI X 2 X5\) = \(4 -2 -1\) ;t 0.  
4. Add the nodes \(11 0\), \(9 2\) to the list with bounds \~\(y\) = fl-, ¥t-, respectively.  
Iteration 3\:  
1. Picky = \(0 1\) from the list.  
2. x\*= \(0 1\) + \(1 I\)= \(1 2\). xB = \(3 2 5\).  
3. *z ....* ti-, *x* = \(1 2\). Delete the nodes \(11 bounds exceed *z.*  
0\) and \(9 2\) from the list since their lower  
Iteration 4\:  
I. The node list is empty. Stop. \(xi Xz Xs\) = XB = \(3 2 5\), XN = \(x3 X4\) = \(1 2\)  
is an optimal solution.  
The enumeration tree is shown in Figure 1.3.  
The relaxation SP\(d\) can also be used as a basis for several other algorithms. One  
obvious approach is to consider the dual problem minu,A *z\(u,* A\). For fixed Rand *Ll,* we  
can consider the standard Lagrangian dual minuERm *z\(u,* Ll\). Based on Theorem 6.2 of  
Section 11.3.6, we have the following result on the Lagrangian dual.  
y=\(O, 1\)  
\~ = '7b feasible  
y=\(ll,O\)  
\~=5\~1  
Figure 1.3 448 11.6. Applications of Special-Purpose Algorithms  
Proposition 1.13. *minu\<=Rm* z\(u, Ll\) = max\{cx\: Ax= b, x E conv\(Sd\(b\) \)\}.  
A different dual is obtained by fixing *u* and allowing Ll to vary over the set of *m* x *m*  
integer nonsingular diagonal matrices, giving the dual problem mind z\(u, Ll\).  
The algorithm described below for IP involves the solution of minimum-weight path  
problems over a series of digraph 7iJ d that increase in size from one iteration to the next but  
remain finite. The algorithm solves minimum-weight path problems of the form  
ID\(Ll\)  
lf/d =min I pjXj  
jEN  
jEN  
.L \(Raj\)xj = \(Rb\)  
\(mod Ll\)  
xEZ\~,  
where R and p = uA - c \> 0 are fixed, Ll varies, and z\( u, Ll\) = ub - 1f1 d· Also we no longer  
require J; I Ji+I for *i* = 1, ... , m - 1.  
The Increasing Group Algorithm  
Initialization\: Choose  
with J\) E Zl \\ \{0\} for all i. Set t = 1. \(A reasonable choice is Ll1 equal to the Smith  
normal form of *A*8 , but this is not necessary.\)  
Iterption t\:  
Step 1\: Solve the minimum-weight path problem ID\(Ll1\). Let x *1* be the resulting solution.  
Step 2\: If RAx*1* = Rb, stop. x *1* is an optimal solution ofiP.  
Step 3\: If RAx*1 \** Rb, calculate  
Step 4\: Set k; = 1 if wl = 0, and otherwise let k; be the smallest integer greater than 1 such  
that gcd\{ I wll, k;\} = 1. Set  
and t .... t + 1.  
Theorem 1.14. IfiP has a finite optimal value and p \> 0, the increasing group algorithm  
terminates after a finite number of iterations with an optimal solution to IP.  
Proof Let Sd,\(b\) = \{x E R\~\: RAx = Rb \(mod Ll1\)\}. As in the generic relaxation algo-  
rithm of Section 11.4.1, we show that if x *1* is not feasible for IP, then SN+'\(b\) s Sd,\(b\) \\ \{x1\}. 1. Knapsack and Group Problems 449  
Since  
with k; E Zl \\ \{0\} for all *i,* RAx = Rb \(mod Ll1+1\) implies RAx = Rb \(mod Ll1\), and hence  
S11,,\(b\) s S11,\(b\). Now X *1* E S11,+,\(b\) only if\(L11+1t 1\(Rb- RAx*1\)* E zm. But  
Since wl *\** 0 for some *i,* and k; is chosen such that wl/k; \$. Z 1  
, x*1* \$. S11,,\(b \).  
Finally asp \> 0, we know from Proposition 1.10 that the optimal solution to IP is a qth  
best solution to S11 \{b\). Hence the algorithm must terminate after no more than q  
iterations. •  
Corollary 1.15. Given p \> 0, there exists a diagonal integer matrix Ll such that an optimal  
solution to ID\(il\) is an optimal solution to IP.  
Example 1.4 \(continued\)  
max ?x1 + 2xz  
-x\~ + 2xz+X3  
Sx1 + Xz  
-2xl- 2xz  
xEZ\~.  
= 2  
+ X4 = 19  
+ Xs = -5  
Taking *u* = \(1 2 \~\),As, and R = \(=\~ \~ n as previously, we obtain ID\(Ll\), namely  
lf/11=min X1 + Xz + X3 + 2x4 + \~Xs  
X1 - 2xz - X3 - 6xz - 2x3 + Xs = -9  
11xz + 5x3 + X4 = -2  
= 29  
xEZ\~,  
\(mod J1\)  
\(mod Jz\)  
\(mod 63\)  
and *z\(u,* Ll\) = 37\~- lf/11.  
Now we apply the increasing group algorithm.  
Initializatian\: C.'\~ \(  
1  
Iteration 1\:  
Step 1\: x 1 = \(0 0 1 2 0\), If/""' = px1 = 5. 450 11.6. Applications of Special-Purpose Algorithms  
Step 3\: Rb- RAx*1* = \(;;\) = \~ 1 w 1 with w1 = \(=;\)·  
Step 4\: k1 = k2 = 2, k3 = 3, \~ 2 = \( 2 33 \)·  
Iteration 2\:  
Step 1\: x2 = \(1 2 2 1\), !f/c,.' = px2 = 8\:!\:.  
Step 3\: Rb- RAx2 = 0\) = \~ 2 w 2 with w2 = 0\)-  
Step 4\: k 1 = 2, kz = 3, k3 = 1, \~ 3 = \( 6 33 \)·  
Iteration 3\:  
Step 1\: x 3 = \(3 2 2 5\), !f/c,.' = px3 = 12\~.  
Step 2\: RAx3 = Rb. x 3 solves IP with cx3 = 37\~- 12\~ = 25.  
The 0-1 Knapsack Problem  
In many cases, 0-1 knapsack problems have to be solved repeatedly and quickly. For  
instance, in Example 6.2 of Section 11.3.6, one of the Lagrangian relaxations resulted in a  
knapsack problem. In the next section, we will use the 0-1 knapsack problem as a  
subroutine in a fractional cutting-plane algorithm for 0-1 integer programs.  
When the constraint coefficients are small integers, the dynamic programming recur-  
sion of Section 11.5.5 is an efficient algorithm; and when the objective function coeffi-  
cients are small integers, an efficient recursion is obtained by reversing the roles of  
the objective and constraint as in \(1.5\). In addition, there is a scaling/rounding heur-  
istic similar to the one we have given for the integer knapsack problem with running time  
*O\(nje*3\) that guarantees a solution with a relative error of no more than *e* for any *e* \> 0.  
Nevertheless, a linear-programming-based branch-and-bound algorithm is still used to  
solve 0-1 knapsack problems. Here we examine the simple techniques that make such an  
algorithm effective.  
Given the 0-1 knapsack problem  
\(1.10\)  
without loss of generality we suppose that ah cj \> 0 for all\) and LjEN aj \> b. We note that if  
the variables are ordered so that c*1*/a *1* \~ • • • \~ Cn/an, an optimal solution of the linear  
programming relaxation is  
for *j* = 1, ... , r - 1,  
b ..\_,r-1  
\_ - .\:..j=I aj \_ 0  
*Xr-* , *Xj-*  
ar  
for *j* = r + 1, ... , n,  
where r is such that Lj;;! aj \~band L\}=1 aj \> b. Hence the solution is essentially character-  
ized by r or, more definitely, by A,\*= Cr/ar.  
The optimal value function zLP\(b\) of the linear programming relaxation is shown in  
Figure 1.4. Note that ..1\* is the slope of the function zLP at the point b.  
Since sorting the \{cja\)jEN into nondecreasing order can be done in O\(n log n\) time,  
there is an obvious O\(n log n\) algorithm for solving the linear programming relaxation. 1. Knapsack and Group Problems 451  
c,  
# --  
# --  
a3 l  
I c2 I  
ai1 I  
I I  
I I  
I I  
# I I  
# I I  
al b  
Figure 1.4  
However, if *It\** is known, the linear programming relaxation can be solved in linear time  
since *X;=* 1 if *c;/a;* \>It\* and *X;=* 0 if *c;/a;* \<It\*. We now give an algorithm that solves the  
linear programming relaxation in *O\(n\)* time.  
An Algorithm for the Linear Programming Relaxation  
Let N 1 and *N°* denote the variables fixed to 1 and 0, respectively, and let *Nf* be the free  
variables. Given a candidate value *It,* let  
*Initialization\:* N 1 = *N°* = 0, *Nf* = *N.*  
*Step 1\:* Let *It* be the median of *\{c1ja1\: j* E *Nf\).*  
*Step 2\:* Construct the sets *N\>,* N\~, *N\<* and calculate S1 \(/t\) and Sz\(lt\).  
i. S 1\(/t\) \> b implies that *It* is too small. Let *N° .... N°* UN\~ UN\<, *Nf* = *N\>.* Return to  
Step 1.  
ii. S 2\(/t\) \< b implies that *It* is too big. Let N 1  
... N 1 UN\> UN\~, b ... *b* - I\:JEWuN- *a;,*  
*Nf* = *N\<.* Return to Step 1.  
iii. Otherwise, S1 \(/t\) \< *b* \< Sz\(lt\). If S1 \(1t\) or S2\(/t\) = *b,* we immediately obtain an  
optimal integer solution. Otherwise, take the elements ofN\~ in arbitrary order. If  
N\~ = \{\}\), ... *,jp\},* find q such that S1\(/t\) = I\:t\:l *a*1 , \< b and S1\(lt\) = 1\:1\~ 1 *a;,\>* b. Set  
N 1 \<- N 1 U \{j b ••• ,jq\_1 \}, *r* = \)q, and *N°* \<- *N°* U \{\}q+\~\> ... ,\)p\}. Stop.  
The algorithm terminates with an optimal solution to the linear program with *X;* = 1  
for *j* E Ni, *X;=* 0 for *j* E N°, and x, = \(b - LJEN' a;\)/ a,. To verify that the algorithm has 452 11.6. Applications of Special-Purpose Algorithms  
O\(n\) running time, we use the result that the median of k numbers can be found in O\(k\)  
time. Because A. is chosen as the median, we have that *INfl* is at least halved at each  
iteration since IN\< I -'S; \~ IN I and IN\> I -'S; \~ IN 1. Hence the total running time is  
O\(n\) + O\(n/2\) + · · · + O\(n/2*1\)* • • • = O\(n\).  
Once N *1*  
*,* N°, and A.\* are determined, a natural greedy heuristic yields a solution to  
\(1.10\).  
Primal Heuristic Algorithm  
Step 1\: Set xj= 1 for all\) E N *1*  
*,* and x\:= 0. Let N° = \{r + 1, ... , n\}.  
Step 2\: Set b ...\_ b *-LiEN'* a1•  
Step *3\: For\)* E N°, if a*1* \> b, set xj= 0; otherwise, set xj= 1 and b...\_ b- a*1.* Return.  
An obvious improvement of the heuristic is to order the elements of N° so that  
Cr+l/ar+l? Cr+2/ar+2? · · · ? Cn/an.  
Given a lower bound z equal to the value of the best feasible solution found so far to  
\(1.10\) and zLP, we now present two tests that may allow us to fix some variables. The first is  
just a restatement of Proposition 2.1 of Section 11.5.2.  
Variable Elimination Test 1. If k E N *1* and zLP- \(ck- A.\*ak\) -'S; z, then Xk = 1. Similarly if  
k E N° and zLP + \(ck- A.\*ak\) -'S; z, then xk = 0. -  
Note that ck- A.\*ak is just the reduced price of non basic variable xk at either its upper or  
lower bound.  
If k E N *1* and we impose the condition xk = 0, the new linear programming relaxation  
IS  
ztp = I c*1* + max I c*1*x*1*  
*jEN1\\\(k\)* \)EN°U\{r\)  
I *aixi* -'S; bk  
\)EN°U\(r\)  
0-'S;XJ-'S; 1 forjEN°U\{r\},  
where bk = b - LJEN'\\\(kJ a*1.* Thus we have the following test.  
Variable Elimination Test *2.* If k E N *1* and ztp -'S; z, xk can be fixed at 1. A similar test  
exists for fixing xk = 0 *fork* E N°. -  
A weakened version of Test 2 uses an upper bound on ztp. Since  
ztp -'S; I c*1* + A.\*bk = zLP- ck + A.\*ak\>  
*\)EN1\\\(k\)*  
and equality holds only if *L1* \{a1\: j E N° U \{r\}, cjai =A.\*\}? bk. This validates Test 1 and  
shows that Test 2 dominates Test 1.  
To obtain a better upper bound on ztp, it suffices to find a set of variables in N° with  
the largest values of c1jai-that is, a set \{r + 1, ... , q\} \~ N° such that A.\*= c,/a,?  
Cr+t/a,+t? · · · ? Cq/aq and Cq/aq? cja*1* for *j* E N°\\\{r + 1, ... , q\}. Then I. Knapsack and Group Problems 453  
if aq"" bk- a,- · · · - aq-! \> 0  
and  
These values can be used in Test 2.  
The problem remaining after all the elimination tests have been carried out is called the  
reduced problem. Note that A.\* for the reduced problem is the same as for the original  
problem.  
Example1.5  
max 16xl + 12x2 + 14x3 + 17x4 + 20x5 + 27x6 + 4x7 + 6x8 + 8x9 + 20x 10 + llx11 +  
10x12 + 7x!3  
7xl + 6xz + 5x3 + 6x4 + 7xs + 10x6 + 2x7 + 3xs + 3x9 + 9x10 + 3xu +  
5x12 + 5x13 \~ 48  
X E *B* 13 •  
First we solve the linear programming relaxation.  
Initialization\: N*1* = \{1, ... , 13\}, N1 = N° = 0, b = 48.  
Step 1\: A. = .\\f  
Step 2\: N\> = \{3, 4, 5, 6, 9, 11\}, N= = \{1\}, N\< = \{2, 7, 8, 10, 12, 13\}.  
i. S1 \(A.\)=34.  
ii. S2\(A.\) = 41 *\<b.* A. is too big.  
N 1 = \{1, 3, 4, 5, 6, 9, 11\}, b = 7.  
Step 1\: Nf = \{2, 7, 8, 10, 12, 13\}, A.= 2.  
Step 2\: N\> = \{10\}, N= = \{2, 7, 8, 12\}, N\< = \{13\}.  
i. S 1\(A.\) = 9 *\>b.* A. is too small.  
N° = \{2, 7, 8, 12, 13\}. 454 Step 1\: N*1* = \{10\}, A-= f.  
Step 2\: N\> = 0, N\~ = \{10\}, N\< = 0.  
11.6. Applications of Special-Purpose Algorithms  
i. S 1 \(A-\)=0\<b.  
ii. S2\(A-\) = 9 *\>b.*  
111. r = 10.  
Hence the linear programming solution is x1 = 1 for *j* = 1, 3, 4, 5, 6, 9, 11, x 10 = \~ and  
x1 = 0 otherwise, with zLP = 128\~ and A,\*= f.  
Applying the primal heuristic algorithm, we first set x*1* = 1 for *j* = 1, 3, 4, 5, 6, 9, 11,  
x w = 0, and then fill the remaining 7 units in greedy fashion. This gives x 2 = 1, with x1 = 0  
otherwise. The solution has value 125. Hence 125 \~ z1 p \~ 128\~.  
We calculate the reduced prices c1 - A-\*a1 for *j* EN\:  
1  
9  
2 3  
22  
9  
4  
11  
9  
5  
40  
9  
6  
,u  
9  
7 8 9  
12  
9  
10  
0  
11  
.12\_  
9  
12  
10 -9  
13  
Applying Variable Elimination Test 1, we can fix x 4 = x 5 = x 6 = x 11 = 1, and x 13 = 0 as  
lc1 - A-\*a*1* I \~ zLP- z = ¥.  
To apply Variable Elimination Test 2, we observe that cja1 \~ 2 for allj E *N°.* Thus we  
can take q = r + 1 with c,+Ja,+1 = 2. For k = 3, we have *bk* = 12; and we obtain  
z\[p \~ 99 + 20 + 2\(12- 9\) = 125 \~ \~' and hence we can fix x 3 = 1. None of the other  
variables can be fixed. Hence we are left with the reduced problem  
z = 89 +max 16x1 + 12x2 + 4x7 + 6xs + 8x9 + 20xw + 10x12  
7x1 + 6x2 + 2x7 + 3xs + 3x9 + 9xw + 5x12 \~ 17  
Branch-and-Bound  
We suppose that \(1.10\) is a reduced problem in which as many variables as possible have  
been fixed. The variables are now ordered so that c1/a 1 \~ • • • \~en/an. The order of  
branching is fixed to be x \~\> x2 , ... , Xn. Each variable is first set to 1 and then to 0.  
A node *t* is completely specified by its level k, and a set N*1* \~ \{1, ... , k\}. Node *t*  
represents the set of x E Bn for which x1 = 1 for *j* EN and x1 = 0 for *j* E \{1, ... , k\} \\ N.  
We let z*1* = L.JEN' c1 and let b*1* = b- LJEN' a1• Note that node *t* corresponds to a nonempty  
set of feasible solutions if and only if b*1* \~ 0, and when this holds Zt is a lower bound on the  
optimal value of the solutions in this set.  
An upper bound is given by 1. Knapsack and Group Problems 455  
z=36  
Figure 1.5  
Since the variables are appropriately ordered, zLP can be determined by a greedy algo-  
rithm. The node is pruned by bound *ifz1* \~\~and is pruned by optimality ifz1 = *z1•* If the  
node is not pruned by bound, there are three cases\:  
1. *ak+l* \< bt. If k + 1 \< *n,* we branch on *Xk+l* = 1. If k + 1 = *n,* an optimal solution for  
node tis Xn = 1. We set\~\<- Z1 and prune node *t* by optimality.  
ii. *ak+I* = b*1•* An optimal solution for node tis obtained by setting *xk+l* = 1 and *xj* = 0  
for *j* \> *k* + 1. We set\~ .... z*1,* and we prune node t by optimality.  
111. *ak+l* \> b*1•* We prune the node with *Xk+l* = 1 by infeasibility, and we branch on  
*Xk+1* = 0.  
Backtracking from t. N =\{\}I. ... ,\)r\} c\:; \{1, ... , k\} with\)1 *\<h* \< · · · *\<\),.* 456 11.6. Applications of Special-Purpose Algorithms  
Case 1. k \$. N; that is, the last branch is *xk* = 0. We move back up to level\}, and set  
*Xj,* = 0. Hence node *t* + 1 is at level\}, with N+1 = \{\}1, ... *,J,\_a.*  
Case 2. k EN'; that is, the last branch is *xk* = 1. Here we move back up to level\} ,\_1 and  
set *Xj,\_,* = 0. Hence node t + 1 is at levelJr-1 with N1+1 = \{jb ... ,\),\_2\}.  
Note that in Case 2 we do not branch on *xk* = 0. To show that it is unnecessary to do so,  
first observe that because of the ordering of the variables the upper bound on the branch  
with xk = 1 is at least as great as the upper bound on the branch with xk = 0. Hence if node  
tis pruned by bound, the branch with *xk* = 0 would have been as well. Alternatively, if  
node *t* is pruned by finding a feasible solution in i or ii, no better solution can be found on  
the branch with xk = 0 because of the ordering of the variables.  
The algorithm terminates when a node t with N' = 0 is pruned. We also repeat the  
variable elimination tests each time the value z of the best feasible solution found  
increases.  
Example 1.5 \(continued\). problem is  
After reordering and renaming the variables, the reduced  
max 8x1 + 16xz + 20x3 + 12x4 + 6xs + 10x6 + 4x7  
3x1 + 7x2 + 9x3 + 6x4 + 3x5 + 5x6 + 2x7 \~ 17  
X EB7•  
The optimal solution to the linear programming relaxation is x 1 = x 2 = 1, x 3 = \~' *Xj* = 0  
otherwise, zLP = 39\~, N1 = \{1, 2\}, r = 3, N° = \{4 5 6 7\}, ..1.\*= \~as observed earlier.  
The enumeration algorithm for the reduced problem leads to the tree shown in Figure  
1.5. At each node *t* we give the values of *b,,* z, and z,.  
The first feasible solution found at node 7 is precisely the primal heuristic solution.  
Node 10 is fathomed by bound. A feasible solution of value 38 is found at node 12 \(with  
x *7* = 1\). Nodes 13 and 14 are fathomed by bound. Hence *Xj* = 1 for *j* = 1, 2, 6, 7 and *Xj* = 0  
otherwise is an optimal solution to the reduced problem of value 38.  
2. 0-1 INTEGER PROGRAMMING PROBLEMS  
The general 0-1 integer programming problem  
\(BIP\) max\{cx\: Ax\~ *b, x* E Bn\},  
where A is an m x n integral matrix and b E zm, typically is solved by a general branch-  
and-bound algorithm with linear programming relaxations \(see Section 11.4.2\). However,  
·BIP possesses a few properties that can be used to refine a general algorithm and make it  
more efficient.  
As we have already noted in Section 1.1.6, preprocessing can be quite useful for BIPs to  
reduce the number of variables and constraints. We assume here that preprocessing  
operations have already been done. But it is important to remember that they can be  
applied recursively and, perhaps, should be considered at each node of a branch-and-  
bound tree.  
Linear programming relaxations can yield more information for BIPs than for general  
integer programs because of the following proposition. 2\. 0-1 Integer Programming Problems 457  
Proposition 2.1. Every feasible solution to BIP is an extreme point of P = \{x E R\~\:  
Ax\~ b, X\~ 1\}.  
Proof If xis not extreme, then x = !x1 + !x2  
, x 1  
, x 2 E P with x 1 *=1=* x 2  
, which implies  
0 \< *Xj* \< 1 for somej EN; that is, *x* \$. Bn. •  
This result motivates a heuristic that systematically searches the integral extreme points  
of *P* in the neighborhood of an optimal solution to the linear programming relaxation for  
good feasible solutions to BIP.  
Another useful fact is that by complementing variables, the individual constraints of  
BIP can be written as the constraint sets of 0-1 knapsack problems. Specifically, the ith  
constraint can be restated as  
\(2.1\)  
where *xj* = *Xj* if *aij* \> 0 and *Xj* = 1 - *xj* if *aij* \< 0. This transformation enables us to use  
strong valid inequalities for the 0-1 knapsack constraint set \(see Section II.2.2\) as valid  
inequalities for BIP in an FCP \\ branch-and-bound algorithm.  
After developing these ideas, we will invoke a bit more structure and consider  
set-covering and -packing problems in which A is a 0-1 matrix and b; = 1 for *i* E M =  
\{1, ... 'm\}.  
A Simplex-Based Heuristic for BIP  
Suppose we solve the linear programming relaxation of BIP by a simplex algorithm that  
treats the upper bounds *Xj* \~ 1 for *j* E N as implicit constraints. If, in an optimal solution,  
*Xj* is nonbasic for allj EN or, equivalently, the slack variables *Xn+i* are basic for all *i* EM,  
then the solution is integral.  
This suggests the idea of finding good integral solutions by pivoting out of the basis the  
regular \(non-slack\) variables and replacing them by slack variables. These pivots, other  
related ones, and the rounding of the values of the fractional basic variables are attempted,  
with the objective of finding a feasible integral solution. If a feasible integral solution is  
found, then we try to improve it by local search. This is done by complementing non basic  
regular variables \(switching their values from 0 to 1 and vice versa\).  
Algorithm  
Phase 0. Solve the linear programming relaxation. If the solution is integral or there is no  
feasible solution, stop. Otherwise go to Phase I. Let X;= aw be the value of the ith basic  
regular Variable; then let Q = r\_iEN' min\(a;o, 1 - a;o\) be the ValUe Of integer infeasibility,  
where *N'* = \{i\: X; is basic and a regular variable\}.  
Phase I \(Feasibility Search\)  
Step *1\:* If there is at least one pivot that maintains primal feasibility and reduces the  
number of basic regular variables, then do that pivot which yields the largest value of  
the objective function. If the resulting solution is integral, go to Phase II; otherwise  
return. If no such pivot exists, go to Step 2.  
Step 2\: If there is at least one pivot that maintains primal feasibility, leaves unchanged the  
number of basic regular variables, and reduces q, then do the first one found. If the  
resulting solution is integral, then go to Phase II and otherwise return to Step 1. If no  
such pivot exists, go to Step 3. 458 11.6. Applications of Special-Purpose Algorithms  
Step 3\: Round each basic regular variable to the nearest integer. If the solution is feasible,  
go to Phase II; otherwise reduce each fractional regular variable to zero. If the solution  
is integral, go to Phase II; otherwise go to Step 4.  
Step 4\: Among those pivots that make a slack variable basic and positive and that make a  
regular variable nonbasic, do the one that minimizes the resulting primal infeasibility  
given by h *=LiEN'* max\(O, -a;o, aiO- 1\). Go to Step 5.  
Step 5\: If there is a nonbasic regular variable that can be complemented to reduce h,  
complement the one that yields the largest reduction in infeasibility. Then if h = 0, go to  
Step 3; otherwise return. If no such variable exists, go to Step 6.  
Step 6\: If there is a pair of nonbasic regular variables that can be simultaneously  
complemented to reduce *h,* then do the first such complementation that is found. Then  
if h = 0, go to Step 3; otherwise go to Step 5. If no pair exists, the feasibility search has  
failed.  
Phase I either produces a feasible solution and we go to Phase II, or it ends in failure and  
the heuristic terminates.  
Phase II \(Local Search for Improvement\)  
Step 1\: Fix variables using the reduced-profit criterion ofProposition 2.1 ofSection II.5.2.  
Go to Step 2.  
Step 2\: If a better feasible solution can be found by complementing one non basic regular  
variable, do the complementation that yields the largest improvement and go to Step I.  
Otherwise go to Step 3.  
Step 3\: For *i* = 2, 3, if a better feasible solution can be found by complementing *i*  
non basic regular variables, do the first such complementation found and go to Step I.  
Otherwise terminate.  
This heuristic has performed well in practice on a variety of types and sizes of binary  
integer programs. It is typical for such heuristics to work reasonably well for the larger,  
more complicated instances where other alternatives are not available; however, for small  
instances and restricted problem classes, such heuristics usually fail or do what much  
simpler heuristics are capable of doing.  
For example, in a 0-1 knapsack problem, Step 1 of Phase I immediately pivots out the  
fractional variable and pivots in the slack variable, yielding the solution that would have  
been obtained by the greedy algorithm if it were stopped upon first encountering an item  
that did not fit into the knapsack. Such a solution would be completed to a greedy solution  
in Step 1 of Phase II.  
Example2.1  
max z = 9x*1* + 10x2  
3x1+3X2+X3  
4xl + 5x2  
=4  
+ X4 = 6  
where x 3 and x4 are slack variables. 2. 0-1 Integer Programming Problems  
The optimal solution to the linear programming relaxation is  
459  
X1=!-!x3+X4  
X2=!+h3-X4  
In Step 1 if x3 becomes basic, the pivot yields x2 = 1, x1 =\~'and z = \~· If x4 becomes  
basic, the pivot yields x1 = 1, x2 =\~,and z = Jf. Hence we choose to make x4 basic. The  
resulting basic solution is  
X4 = 1 + hJ- \(l - Xt\)  
x2 = 1 -hJ + \(1 - Xt\)  
Xt=l,xJ=O.  
Step I is repeated, and the next pivot yields the integral solution x 1 = 1, x 2 = 0\. Hence  
Phase I terminates.  
An FCP/Branch-and-Bound Algorithm  
We have observed that the individual constraints ofBIP can be stated in the form \(2.1\) and  
in Section 11.2.2 we studied strong valid inequalities for S = \{x E Bn\: *LJEN* a*1*xi \< b \},  
where a*1* E Z\~ for *j* E Nand b E Z\~. In particular, we gave the class \:JP of cover inequalities  
I *Xj* \< ICI- 1,  
jEC  
where C s; N is a cover if *LJEC* a *J* \> b.  
Now to be able to apply the FCPA of Section 11.5.2, we formalize the separation  
problem for the class\:¥. Here Cis an unknown subset *ofN,* and given apointx\*E R\~ \\ Bn  
we want to find a C \(assuming that one exists\) with *LJEC ai* \> b and *LJEC* xj \> I C I - 1.  
Introducing a vector z E Bn to represent the unknown set C, we attempt to choose z such  
that *Ljr=N* a*1*zi \> b and *LJEN* xjz*1* \> *LJEN* z*1* - I. The second inequality is equivalent to  
*LJr=N* \(1 - xj\)z*1* \< 1.  
Thus we obtain the Separation Problem for Cover Inequalities\:  
\(2.2\)  
Note that, since the constraint coefficients are integral, *Ljr=N aizJ* \> b is equivalent to  
*LJr=N aizJ* \~ b + 1. Let *zc* be the characteristic vector of C s; *N.*  
Proposition 2.2. Let \( \(, zc\) be an optimal solution to \(2.2\). Then\:  
a. *if* \( \~ 1, then x\* satisfies all the cover inequalities for S; and 460 11.6. Applications of Special-Purpose Algorithms  
b. if *C* \< 1, then L*1Ec* x*1* \~ 1 C 1 - 1 is a most violated cover inequality for S, and it is  
violated by the amount 1 - *C.*  
Proof If *C* \~ 1, then all z E Bn satisfying LJEN a1z1 \> b also satisfy LJEN xjz1 \~  
LJEN z1 - 1; that is, for all covers C, the corresponding cover inequality is satisfied by x\~  
If *C* \< 1, then LJEN \(1 - xj\)zf = *C* \< 1; hence  
I xj = I Ci - C \> I c I - 1.  
jEC  
Since zc is optimal in \(2.2\), the maximum violation is by the amount 1 - *C.* •  
Example 2.2 S = \{x E B5\: 47xi + 45x2 + 79x3 + 53x4 + 53x5 \~ 178\} and x\*= \(0 0 1  
1 \~\). To check whether there is a cover inequality for *S* violated by x\~ we solve\:  
C =min 1z1 + 1z2 + Oz3 + Oz4 + nzs  
47z 1 + 45z2 + 79z3 + 53z4 + 53z5 \~ 179  
*z EB*5  
,  
having optimal solution C = -0, zc = \(0 0  
*x* 3 + *X4* + *Xs* \~ 2 is violated by x\~  
1\). As *C* \< 1, the cover inequality  
It is now straightforward to implement the FCPA with separation for BIP. As the initial  
relaxation we take S1 = \{x E R\~\: Ax\~ *b, x* \~ 1\}. The separation algorithm for BIP  
involves the solution of the knapsack separation problem \(2.2\) for each constraint  
LJEN aux1 \~ b;, restated as the knapsack set \(2.1\). Note that if we find a violated cover  
inequality specified by C, we can easily strengthen it to LJEE\(C\) x*1* \~ ICI- 1, where  
*E\(C\)* = \{j \$. C\: *a1* \~ *ak* for all *R* E C\}, \(see Section 11.2.2\). Thus when *A* and *b* are  
nonnegative, the algorithm will terminate with a solution satisfying L*1Ec x1* \~ I C I - 1 for  
all C with L1*Ec a*1 'f b, *x* E R\~, where *a*1 is the jth column of A, and the original con-  
straints.  
Example 2.3. We apply the FCPA of Section 11.5.2 to the BIP  
max 77xi + 6x2 + 3x3 + 6x4 + 33xs + 13x6 + llOx1 + 21xs + 47x9  
774x*1* + 76x2 + 22x*3* + 42x4 + 21x*5* + 760x6 + 818x7 + 62xs + 785x9 \~ 1500  
67x *1* + 27x2 + 794x*3* + 53x4 + 234x*5* + 32x6 + 797x7 + 97xs + 435x9 \~ 1500  
*xEB*9•  
Iteration I. Solution of the linear programming relaxation LP1 of BIP yields  
xi= x3 = x\~ = x\~ = 1, xl = 0.71, xj = 0.35, x\) = 0 otherwise, and zLP = 225.7. 2. 0-1 Integer Programming Problems 461  
Solution of the separation problem \(2.2\)forrow 1 yields'= 0.29, z *1* = z7 = 1, and z*1* = 0  
otherwise, giving the violated cover inequality x 1 + x1 \~ 1. Here *E\(* C\)= C.  
Solution of the separation problem \(2.2\) for row 2 yields'= 0.65, z3 = z7 = 1, and z1 = 0  
otherwise, giving the violated cover inequality x 3 + x1 \~ 1. Again E\(C\) =C.  
Iteration 2. Solution of the linear programming relaxation LP2 of BIP with the two  
additional constraints x 1 + x7 \~ 1 and x 3 + x7 \~ 1 yields xi= x\~ = x\~ = x\~ = x\~ = 1,  
*x§* = 0.61, *xJ* = 0 otherwise, and z\[p = 204.8.  
Solution of the separation problem \(2.2\) gives the violated cover inequalities x7 + x9  
\~ 1 for row 1 and x4 + x5 + x7 + x9 \~ 3 for row 2.  
Iteration 3. x3 = \(0.63, 1, 0.60, 1, 1, 0, 0.37, 1, 0.63\). The separation routines give C =  
\{ 1, 9\} and the extended cover inequality x *1* + x7 + x*9* \~ 1 for row 1, and C = \{3, 5, 8, 9\}  
and the extended cover inequality x3 + x5 + x7 + x8 + x9 \~ 3 for row 2.  
Iteration 4. x4 = \(0, 1, 0, 1, 1, 0.63, 1, 1, 0\). The cover inequality x6 + x7 \~ 1 is added.  
Iteration 5. x5 = \(0, 1, 0, 1, 1, 0, 1, 1, 0\) is integer and thus solves BIP.  
Example 2.3 raises two issues. Given that the separation problem \(2.2\) is a knapsack  
problem, which is an JV'g\}l-hard problem, should we solve \(2.2\) exactly or use a fast heuristic  
algorithm? In practice, heuristics have been used very effectively. But this, of course,  
means that some cover inequalities may be missed by the separation procedure.  
The second issue stems from the observation that the first two cuts generated from  
row 1 in the course of the algorithm, namely *x* 1 + *x*7 \~ 1, *x*7 + *x* 9 \~ 1, are dominated by  
the third cut x 1 + X7 + x 9 \~ 1. Hence, we could speed up the algorithm if we could obtain  
this stronger cut from row 1 on the first iteration.  
To obtain the stronger cuts, remember from Proposition 2.5 of Section 11.2.2 that every  
cover inequality generated from a minimal cover C gives rise to a lifted cover inequality of  
the form  
\(2.3\)  
where C 1 n C2 = 0 and C 1 U C2 = C. Moreover, *\{a\)* and \{y\) can be chosen so that \(2.3\)  
defines a facet of the knapsack convex hull.  
The coefficients in \(2.3\) are obtained by sequential lifting. Unfortunately we know of  
no efficient way to consider all possible ordering of the elements of *N* \\ C that can be used  
in sequential lifting. From a practical point of view, we avoid this difficulty by choosing an  
ordering of the elements of *N* \\ C in a greedy fashion.  
A Lifting Heuristic to obtain a lifted cover inequality of the form \(2.3\) with C2 = 0  
Initialization\: Given x\~ solve the knapsack problem \(2.2\) to obtain a cover C. \(Note that  
the cover inequality may not be violated.\) Let L 1 = N \\ C and let k = 1. Set a1 = 1 for all  
jEC. 462 11.6. Applications of Special-Purpose Algorithms  
Iteration k\: For all *j* E L *k* find *ph* which is the maximum value of *n 1* such that  
*n1x1* + *'L;EN\\Lk a;x;* \~ I C I - 1 is valid. Let *j\** = arg max1Eu *P 1 xj.* Set L *k+l* = L *k* \\ \{j\*\}  
and *a1• =Pi'·* If L k+l = 0, test whether *r.JEN a1 xj* \> 1 C 1 - 1. If so, add the cut  
*r.JEN* a*1*x*1* \~ I C I - 1. If L k+l *\** 0, k ..\_ k + 1. Return.  
As shown in Section 11.2.2, we have P 1 = I C I - 1 - 'h where  
\(2.4\)  
Note that because of the small size of the coefficients a*1* \~ I C I - 1, the knapsack problem  
\(2.4\) can be solved efficiently by dynamic programming \(see Proposition 1.6\).  
A simple extension of the lifting heuristic suggests how we can also search for extended  
cover inequalities of the form \(2.3\) with C2 *\** 0.  
Separation Algorithm to obtain lifted cover inequalities \(2.3\)  
Step *1\:* Apply the lifting heuristic described above. If a violated inequality is found, stop.  
Step 2\: If not, choose k = arg\(max1 Ec a*1 xj\).* Set C2 = \{k\}, and use the lifting heuristic to  
generate a facet-defining inequality for conv\(Sk\) from the cover *C* \\ \{k\}, where  
*sk* = \{x E Bn-l\: *LjEN\\\{k\) ajXj* \~ b - *ak\}.*  
Step 3\: Convert this inequality into a facet-defining inequality of the form \(2.3\) for S by  
lifting back in the variable *Xk·* \(See Example 2.2 of Section II.2.2\).  
Step 4\: Check the resulting inequality for violation. Stop.  
Example 2.2 \(continued\)  
and *x\** = \(\~ \~ 1 \~ 1\). The knapsack problem \(2.1\) gives the cover inequality  
*x* 3 + *x* 4 + *x* 5 \~ 2, which is not violated by x\~ The separation algorithm starts with  
*c* = \{3, 4, 5\}.  
Step 1\: The lifting heuristic leads to the same inequality.  
Step 2\: C2 = \{3\} is chosen, and the lifting heuristic is called, starting with the cover  
inequality X4 + Xs \~ 1 for S 3 = \{x E B 4\: 47xt + 45x2 + 53x4 + 53xs \~ 99\}.  
Iteration 1\: L 1 = \{1, 2\} and P1 = 1, *P*2 = 0\. Hence *xi'= x* 1 is lifted with coefficient a1 = 1.  
Iteration 2\: L 2 = \{2\}, Pz = 0. The resulting inequality for S 3 is X l + x4 + X 5 \~ 1.  
Step 3\: Variable *x* 3 is lifted in giving the inequality *x* 1 + *2x*3 + X4 + *x* 5 \~ 3, which defines a  
facet of conv\(S\) that is violated by x\~  
Given the heuristic nature of the above separation algorithm, we can no longer  
determine a priori what problem will be solved at the termination of the FCPA with  
separation. We can only assert that the cuts generated at least include all the cover  
inequalities. Remember that even this assertion may be false if we use a heuristic 2\. 0-1 Integer Programming Problems 463  
algorithm for the knapsack problem \(2.2\). However, as the example below suggests, and as  
computational experience shows, the use of the lifted cover inequalities \(2.3\) in place of  
the cover inequalities leads to significant improvements in performance.  
Example 2.3 \(continued\). We apply the FCP/branch-and-bound algorithm, where the  
separation algorithm for extended cover inequalities is applied to each row ofBIP.  
Phase 1 \(FCPA\)  
Iteration 1\: Solution of the relaxation LP1 ofBIP yields xl = x\~ = x\~ = x\~ = 1, xl = 0.71,  
x\~ = 0.35, *x\}* = 0 otherwise, and *zf..p* = 225.7.  
Row 1. Cut x 1 + x6 + x7 + x9 \:\:s\:; 1 is generated.  
Row 2. Cut *x3* + *x1* .,;;;; 1 is generated.  
Iteration 2\: Solution of the relaxation LP2 yields x\~ = x\~ = x\~ = x\~ = x\~ = 1, xJ = 0 other-  
wise, and *z\[p* = 176. Because *x* 2 is integer, it is an optimal solution ofBIP.  
An alternative or complement to the use of the heuristic lifting algorithms is to use  
Proposition 2.6 of Section II.2.2, which provides upper and lower bounds on the values  
taken by *aj* for *j* E N \\ C in the lifting heuristic. In particular, we obtain conditions for the  
existence of a violated inequality \(2.3\) when Cis a minimal cover and C2 = 0.  
Proposition 2.3. Let C = U \~\> ... , j,\} be a minimal cover with aj, \~ ah \~ · · · \~ aj,• and  
for h = 0, ... , r let  
where J.lh = I\:ti aj •• J.lo = 0, and A.= J.lr- b \> 0.  
l. *If* I\:jec xj + I\:h I\:jeQ, hxj + I\:h I\:jeR, \(h + 1\)xj.,;;;; I C I - 1, there is no violated lifted  
inequalityforCwith C2 = 0.  
2. Ifl\:jec xj + I\:h I\:jeQ, hxj + I\:h I\:jeR, hxj + maxjeu,R, xj \> I C I - 1, then  
*L,* Xj + *L, L,* hxj + *L. L.* hxj + Xr .,;;;; I C I - 1,  
jEC h jEQ, h jER,  
where t = arg\(maxjeu,R, xj\), is a valid inequality violated by x\*.  
The proof is an immediate application of Proposition 2.6 of Section II.2.2. This  
proposition can be used to speed up the lifting heuristic by stopping the algorithm if  
condition 1 is satisfied, or otherwise fixing the values of *a.j* for *j* E UhQh. Alternatively, we  
can simply use the valid inequality given in condition 2.  
Example 2.3 \(continued\). At iteration 1 we have xi= x! = x\~ = x\~ = 1, xl = 0.71,  
x\~ = 0.35, Xj = 0 otherwise. For row 1 the knapsack problem \(1.1\) gives the cover C = \{1, 7\}.  
From Proposition 2.3 we have Q0 = \{2, 3, 4, 5, 8\} and r0 = \{6, 9\}. It follows without 464 11.6. Applications of Special-Purpose Algorithms  
further calculations that both *x 1* + *x6* + *x7* \~ 1 and *x 1* + *x7* + *x9* \~ 1 are valid inequalities.  
To establish the validity *ofx1* + *x6* + *x7* + *x9* \~ 1, we must lift one of the above inequalities.  
Example 2.4. This is a 0-1 minimization problem with 15 constraints and 33 variables.  
The data, as well as the 20 cuts added in seven iterations \(six sets of cuts\) of the FCPA, are  
given in Table 2.1. Note that the value of the initial LP relaxation is *zLP* = 2520.7, and after  
adding the cuts the lower bound given by the LP relaxation of the reformulation  
max\{cx\: *x* E S\[p\} is *zLP* = 2962.2.  
The corresponding solution x 7 for which no cuts are found is  
\(0 1 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  
0 0 0 .83 .17 .83 .83 0 .83 0 .17 .66 0 1 0  
18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  
1 0 0 0 0 1 0 1 .67 1 .50 1 1 0 0 .83\).  
Applying branch-and-bound to the reformulated problem, a solution of value 3095 is  
found at node 17, and an optimal solution of value 3089 is found at node 65. Optimality is  
proved \(i.e., the search is completed\) at node 77. The optimal solution isx1 = x*7* = x *8* = x *10*  
= X14 = X1s = X21 = X23 = Xz5 = Xz6 = Xn = Xzs = Xz9 = X3o = 1, and x *1* = 0 otherwise.  
If branch-and-bound is applied without adding cuts, the best solution found after 1000  
nodes has value 3095, and the tree still contains 163 active nodes.  
Set Covering and Packing  
When *\(A, b\)* is a 0-1 matrix, each individual constraint is already in the form of a covering  
or packing inequality, and no mileage can be gained from the cutting-plane approach  
developed above. Some simple combinatorial ideas can yield cuts. For example, in a  
packing problem the constraints  
imply the inequality x 1 + x 2 + x 3 \~ 1. And in a covering problem, the constraints  
imply the inequality x 1 + x 2 + x 3 "" 2. More generally, if we have constraints for all sets of  
size *k* from *k* + 1 variables, then we can derive a nontrivial valid inequality involving all  
k + 1 variables.  
The disjunctive approach can also be used to derive valid inequalities for covering and  
packing problems. Here we leave the cutting-plane approach and consider some other  
features of covering and packing problems. Table 2.1.  
I 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 j  
171 171 171 171 163 162 163 69 69 183 .183 183 183 49 183 258 517 250 500 250 500 159 318 159 318 159 318 159 318 114 228 159 318 Cj  
I I  
2  
3  
4  
5  
6  
7  
8  
9 300  
10 300  
11  
12  
13  
14  
15  
I  
230  
200 400  
300 300 285 285  
300 300 285 285  
300  
285  
300 285  
285  
285  
265 265  
265 265  
265  
265  
265  
265  
230 230 190  
230 230 190  
190  
230 190  
200 400 200 400 200 400 200 400 200 400  
200 400 200 400 200 400 200 400 200 400  
200 400  
200 400  
200 400 200 400  
230  
230 230  
200 400  
200 400 200 400  
300  
300  
200 400  
200 400 200 400  
285  
\~I  
\~ I  
\~I  
\~ I  
;;.,5  
\~ 2700  
;;., 2600  
;;., 100  
;;., 900  
;;., 1656  
;;., 335  
;;., 1026  
;;.,5  
;;., 500  
200 400;;.. 270  
2  
\~ **1**  
\~8  
;;., I  
;;;.4  
;;;.3  
;;;.2  
;;;.1  
\~9  
;;;.3  
;;;.3  
\~9  
;;;.4  
;;;.3  
;;;.3  
;;;.2  
;;., I  
2 I 2  
;;;.5  
1  
2  
\~8  
;;.,7  
;;;.5  
\~  
Row 5  
6  
8  
10  
12  
14  
15  
Row 6  
10  
12  
Row 6  
7  
9  
10  
12  
14  
Row12  
Row 6  
7  
RowiO 466 11.6. Applications of Special-Purpose Algorithms  
The greedy heuristic has a natural realization for the set-covering problem  
\(SC\) Zsc =min\{ I cixi\: I auxi \~ 1 fori EM, x E Bn\},  
jEN jEN  
where au E \{0, 1\} for all i andj. We assume that I\:.jeN au\~ and sufficient for a feasible solution. Let Mi = \{i\: au= 1\}.  
1 fori EM, which is necessary  
Greedy Heuristic for Set Covering  
Initialization\: M 1 = M, N 1 = *N,* t = 1.  
Iteration t\: Select/ E *N* 1 to min\{cj IMi n M 1 1\}. LetN+1 = *N*1*\\UZ\}* andM1+ 1 = M1\\Mp. If  
M 1+ 1 = 0, the greedy solution is given by xi = 1 for *j* \~ Nl+1 and by xi = 0 otherwise. Its  
cost is ZG = r.iff-NI+' ci. If M 1+ 1 *\** 0, then lett .... t + 1 and return.  
We see that at each step the greedy heuristic selects the column that meets the largest  
number of uncovered rows per unit cost and then stops when a feasible solution has been  
found.  
Although we cannot give a positive, data-independent performance guarantee for the  
greedy heuristic, we will show that it has a performance guarantee that is independent of n  
and the objective coefficients, and that decreases only logarithmically with 1M 1.  
For any positive integer k, let H\(k\) = 1 +! + · · · + t and let d = maxjeN r.iEM au. We  
will use the following elementary result.  
Proposition 2.4. Let u = \(ub ... , Un\) E R\~ and v = \(vb ... , Vn\) E Z\~. *If* 0 \< Ut \~ U2  
\~ · · · \~ Un and V1 \~ V2 \~ · · · \~ Vn then  
n-1  
I ui\(vi- vi+I\) + UnVn \~ max\(uivi\)H\(vt\).  
i\~l I  
Theorem 2.5. zsdzG \~ 1/H\(d\).  
Proof We use the approach presented in Section II.5.3 for worst-case analysis of  
heuristics. In particular, we construct a feasible solution u\* to the dual of the linear  
programming relaxation ofSC. Then, by duality, we obtain zsc \~ I\:.7,!1 uj. The result then  
follows by showing that I\:.7,!1 ui= zG!H\(d\).  
Suppose that the greedy heuristic terminates on iteration *T,* and let *\(\)I* =  
mini \{ci/ IMi n M *1* 1\}. The dual vector u\*is defined by ui= \(\)ljH\(d\) fori E M *1*\\Mt+*1•*  
We will show dual feasibility for eachj E *N* by using Proposition 2.4. Since *M 1* \:\:J M 1+ 1  
,  
it follows that IMj n M *1* I \~ IMj n Ml+l I for all t. Hence if VI= IMj n M *1* I, then  
v1 \~ v2 \~ • • • \~ Vr \~ 0. We also have from its definition that 0 \< 81 \~ • • • \~ \(JT.  
Now  
I u\~a-·=I- I a·  
*T \(\}I* \( \)  
iEM I *I\)* 1\~1 H\(d\) *iEM1 \\MI+i I\}*  
= - 1  
- f 8*1* \(IM1 nMjl- IMI+l nMjl\)  
H\(d\) 1\~1  
1 T  
= H\(d\) \~ 8*1* \(vi- vi+I\), 2. 0-1 Integer Programming Problems and applying Proposition 2.4 to the last term gives  
467  
Since IM1 n *Mj* I = Li\:t *aij.\:;; d,* and *81\( IM1* n *Mj* I\).\:;; *Cj* for all t by definition of *81*  
*,* we  
obtain L;EM *u7aij.\:;; Cj,* and u\*is dual feasible.  
Finally, the dual objective value is  
# •  
We leave it as an exercise to show that the bound of Theorem 2.5 can be asymptotically  
achieved.  
We now turn to set-packing problems and, in particular, to the node-packing problem.  
An instance of the node-packing problem is given by a graph *G* = \( *V, E\)* and a weight  
function c\: *V .... R* 1• A feasible solution is any subset of nodes such that no pair in the subset  
is joined by an edge. The weight of a solution *U* £ Vis c\( *U\)* = L;Eu c ;, and the objective is  
to find a solution of maximum weight. Node packing is K9P-hard \(see Section 1.5.6\).  
Moreover, any set-packing problem is easily transformed to a node-packing problem on  
the intersection graph ofthe family of sets.  
Here we are going to present a rather unusual property of node packing that does not  
appear to be shared by any other K9ll-hard problem and that may yield a substantial  
reduction in the size of an instance once the linear programming relaxation has been  
solved. If the linear programming relaxation of an integer program has an optimal integral  
solution, that solution is, of course, also optimal to the integer program. But if just one  
variable is fractional in the optimal linear programming solution, we can no longer deduce  
anything about the variables in the integer program. On the other hand, in node packing,  
all of the variables that are integral in the solution of the linear programming relaxation \(if  
any\) keep these same integer values in some optimal solution to the integer program.  
Hence, having solved the linear programming relaxation, we can fix the values of the  
integral variables and then eliminate them from the problem.  
A binary integer programming formulation of the node-packing problem on  
G = *\(V,* E\) is  
\(NP\) X;+Xj.\:;; 1 *for\(i,j\)EE*  
xEBn,  
where n = I *vI.* Its linear programming relaxation \(LNP\) is obtained by replacing X E sn  
*byx* ER\~.  
We need the following proposition that relates local and global optimality. For *U* C *V,*  
the neighbors of U\_are the set *N\(U\)* = \{i E *V\: i* \$ *U, \(i,j\)* E *E* for some *j* E U\}. Let  
S\(U\) = *U* U *N\(U\),* S\(U\) = *V\\\(U* U *N\(U\)\),* and let G\(S\(U\)\) be the subgraph induced by  
S\( U\). A property that we use several times is the following\:  
\(P.l\) There are no edges joining a node of U and S\( U\). 468 11.6. Applications of Special-Purpose Algorithms  
Proposition 2.6. *If* U is an optimum packing on G\(S\(U\)\), then there is an optimum  
packing VO on G with VO ;2 U.  
Proof Let *V\** = *VT* U *V!* be an optimum packing on *G,* where *vr* = *V\** n S\( U\). By  
\(P.1\), *U* U Ji1isapacking on *G.* By hypothesis, c\(U\);;;;. *c\(VT\);* hence c\(U\) + *c\(Vi\);;;;.* c\(V\*\).  
Theorem 2. 7. If x0 is an optimal solution to LNP, then there is an optimal solution x\* to  
NP with xj\*= *xJ* for all *j* such that xJ is integral.  
Proof The result is trivial if x0 is integral, so we suppose that it is not. We first show  
thatifU = U\: xJ = 1\}, there exists an optimal solutionx\* toNPwithxj= 1 forallj E *U.* By  
Proposition 2.6, we need to show that *U* is an optimal packing on G\(S\(U\)\). Note that  
xJ = 0 for *j* E *N\(* U\). Also, for some k E S\( U\), x2 \> 0; otherwise x0 is integral.  
Suppose that 0 *\* U* is an optimal packing on G\(S\(U\)\) and that c\(0\) \> c\(U\). We will  
show that this contradicts the optimality of x 0• Let  
forj E 0  
forj E *S\(U\)\\0*  
forj E S\(U\)  
andx = A.x0 + \(1- .A.\).X, where .A.= max\{xJ\:j E S\(U\)\}. SinceO \~ xJ \< 1 forallj E S\(U\), we  
obtain 0 \< A. \< 1. We claim that x is a feasible solution to LNP; that is, X; + X".i\_ \~ l for  
\(i,j\) E *E.* This is clear ifi,j E *S* \(U\) \(since *U* and 0 are packings\) and ifi,j E S\(U\). By  
\(P.l\), the remaining case is i E *N\(U\)* andj E *S\(U\).* Thenx; = A.x? + \(1- A.\).X; \~ 1- A. since  
x? = 0, and Xj = xJ since xJ = Xj. Hence X; + Xj \~ \(1 - A.\) + xJ \~ 1 by the definition of A..  
Now  
I CjXj = A.c\( *U\)* + \(1 - A.\)c\( *0\)* + I cjxJ  
jEV jES\(U\)  
\> c\(U\) + I cjxJ = I CjxJ,  
jES\(U\) jEV  
which contradicts the hypothesis that x0 is an optimal solution to LNP.  
Finally, xj = 1 for *j* E *U* implies xj = xJ = 0 for *j* E *N\( U\),* and if xJ = 0 for *j* E S\( *U\),*  
then cj \~ 0 so that xj = 0 as well. •  
The use of Theorem 2.7 is enhanced by the fact that LNP can be solved in polynomial  
time, essentially as an assignment problem on a graph with 2n nodes. On the other hand,  
the theorem will be useful only if an optimal solution to LNP contains a significant  
number of integer-valued variables. It is also important to observe that the bound obtained  
from the linear programming relaxation of a set-packing problem is stronger than the  
bound obtained from LNP when the set-packing problem is transformed to a node-  
packing problem.  
These advantages and disadvantages must be balanced, but if we decide to use LNP as a  
relaxation to NP in a branch-and-bound algorithm, then Theorem 2. 7 should be applied  
at every node of the branch-and-bound tree.  
Example 2.5. Consider the node-packing problem on the graph of Figure 2.1 with  
*c* = \(3 1 1 2 2 3\). An optimal solution to LNP is *x* = \(l 0 0 i ! i\). Hence  
there is an optimal solution to NP with x *1* = 1 andx2 = x *3* = 0. Now it is trivial to solve NP 3. The Symmetric Traveling Salesman Problem 469  
6  
Figure 2.1  
on the subgraph induced by nodes \{4, 5, 6\}; that is, the solution isx4 = x 5 = 0 and x6 = 1.  
Hence an optimal solution to NP on the whole graph is x = \(1 0 0 0 0 1\).  
3. THE SYMMETRIC TRAVELING SALESMAN PROBLEM  
An instance of the symmetric traveling salesman problem is given by a graph *G* = \( *V, E\)*  
and a weight vector c E RIEl. A tour *T* of *G* is a subgraph of *G* that is a cycle on *V;* that is, if  
*T* = \( *V,* Er \), then each node of *T* is of degree 2, *T* is connected and I ET 1 = I *V* 1. The  
feasible solutions are all of the tours of G \(if any\), and assuming that G contains at least  
one tour, the objective is to find a tour of minimum weight. The weight of a tour *T* with  
edge set Er C E is LeEET Ce. Let  
ZTs =min\{ I Ce\: *T* = *\(V,* Er\) is a tour of G\}.  
eEET  
Several special-purpose algorithms originally were developed to solve the traveling  
salesman problem, which has become a prototype problem for illustrating, testing, and  
comparing algorithms. We begin this section by describing and comparing various  
relaxations. We then present and analyze some heuristics for obtaining good feasible  
solutions. Finally, we give some algorithms that use a heuristic for finding feasible  
solutions and upper bounds, a relaxation or dual problem for finding lower bounds, and, if  
necessary, a branch-and-bound phase for finding an optimal solution and proving  
optimality.  
Relaxations  
We first consider two relaxations that can be obtained by considering families of subgraphs  
that contain all of the tours of G. First we drop connectedness and consider the family of  
subgraphs of G that contain I VI edges and in which each node is of degree 2. These  
subgraphs are called 2-matchings \(see Figure 3.2 of Section 11.2.3\).  
Let  
zM =min\{ I Ce\: *M* = *\(V, EM\)* is a 2-matching ofG\}.  
eEEM  
Since every tour is a 2-matching, we have  
\(3.1\) 470 11.6. Applications of Special-Purpose Algorithms  
Next we drop the degree-2 requirement on all nodes except node 1, but we keep  
connectedness and the requirement that the subgraph contains I VI edges. This means  
that the subgraph on nodes V \\ \{1\} is connected and contains I VI - 2 edges. By Proposi-  
tion 1.2 of Section 1.3.1, it is a tree. Hence the subgraph on *G* is a spanning tree on *V* \\ \{1\},  
together with 2 edges incident to node 1. These subgraphs are called 1-trees \(see Fig-  
ure 3.1\). Let  
*Zn* =min\{ 2\: Ce\: T = *\(V, ET\)* is a 1-tree of G\}.  
*eEET*  
Note that a 1-tree is a tour if and only if each node of the 1-tree is of degree 2. Since every  
tour is a 1-tree, we have  
\(3.2\) ZTS \~ ZtT  
The above discussion implies the following proposition.  
Proposition 3.1. 1-tree.  
T = \( V, *ET\)* is a tour of G if and only if Tis both a 2-matching and a  
We now consider integer programming formulations of these relaxations. For *F* C *E,*  
let *xF* E B1 EI be the characteristic vector *ofF;* that is, *x\:* = 1 if *e* E *F,* and *x\:* = 0 if *e \$.F.*  
The characteristic vectors of2-matchings are simply described by  
\(3.3\)  
*xEBIEI*  
\(3.4\)  
2\: *X* e = 2 for v E V  
\(degree constraints\),  
eEO\{\(v\}\)  
where for any *U* C *V,* 6\( *U\)* is the set of edges with one end in *U.*  
Let£\( *U\)* be the set of edges with both ends in *U.* The characteristic vectors of1-trees are  
described by \(3.3\), \(3.4\) for v = 1,  
\(3.5\) 2\: Xe \~ I *U* I - 1 for all *U* \~ *V* \\ \{1\} with 3 \~ I *U* I  
*eEE\(U\)*  
\(subtour elimination constraints\)  
and  
\(3.6\) *L* Xe = I VI.  
*eEE*  
2 6  
4 5 8  
3 7  
Figure 3.1 3. The Symmetric Traveling Salesman Problem 471  
Thus by applying Proposition 3.1, we get that tours are described by \(3.3\)-\(3.6\).  
However, there are redundancies that can be eliminated. First observe that \(3.4\) implies  
\(3.6\). Then, as we observed in Section 11.2.3, the subtour elimination constraint for *V* \\ *U*  
is implied by the subtour elimination constraint for *U.* Hence the characteristic vectors of  
tours are given by \(3.3\), \(3.4\), and \(3.5\) for *U* C Vwith 3 \~ I VI\~ \[!I VI\].  
Two relaxations that are themselves relaxations of 2-matchings are fractional 2-  
matchings and integer 2-matchings. In fractional 2-matchings, the variables are not  
required to be integral, so \(3.3\) is replaced by  
\(3.7\) *X* E Rif1  
and  
\(3.8\) Xe \~ 1 fore E E.  
In integer 2-matchings, the variables are not required to be binary, so \(3.3\) is replaced by  
\(3.9\) xEZ\~.  
We have  
zFM =min\{ I CeXe\: X satisfies \(3.4\), \(3.7\) and \(3.8\)\} \~ *zM*  
eEE  
and  
z1 M =min\{ I CeXe\: *x* satisfies \(3.4\) and \(3.9\)\} \~ *zM.*  
eEE  
Furthermore, we will prove in Chapter III.l that all of the extreme points of the polytope  
\{x E R\~\: x satisfies \(3.4\)\} are integral. Hence  
z,M =min\{ I CeXe\: X satisfies \(3.4\) and \(3.7\)\} \~ *zFM·*  
eEE  
Example 3.1 Consider the graph shown in Figure 3.2. The numbers on the edges are  
their weights. Figure 3.3 shows an optimal tour, an optimal 2-matching, an optimal  
fractional2-matching, an optimal integer 2-matching, and an optimall-tree.  
2  
5  
6  
4  
5  
8  
3  
4  
Figure 3.2 472 11.6. Applications of Special-Purpose Algorithms  
2 6  
2 2  
4 1 5 Optimal tour  
8  
ZTs = 13  
3 4 7  
2 6  
5 8 Optimal2-matching  
4 zu = 12  
3 7  
2  
2 2  
4 5 8  
Optimall-tree  
ZIT= J0  
3  
2 6  
4 5 8  
Optimal fractional  
2-matching; wavy  
lines indicate edges  
withXe=\~  
*ZFM* = J0  
3 7  
2 6  
4 5 Optimal integer  
2-matching;  
8 the thick line  
indicates an edge with  
Xe = 2  
ZIM = 8  
3 7  
Figure 3.3  
We now consider two more powerful relaxations that combine 2-matchings and 1-trees.  
In the first of these, we seek a minimum-weight convex combination of 1-trees that  
satisfies the degree constraints. To formulate this problem, let *xi* E *B* IE 1 be the character-  
istic vector of the ith 1-tree *fori=* 1, ... , *p,* where pis the number ofl-trees of *G,* and let  
*ci* = LeEE CeX\~ be the weight of the ith 1-tree. The problem is 3\. The Symmetric Traveling Salesman Problem  
473  
*p*  
zMn = min *2.\:* A;C;  
i=l  
\(3.10\)  
I A;\( *2.\:* x\~\) = 2 for v E *V*  
i=I eEo\(\{v\)\)  
p  
*2.\:* A;= 1  
i=l  
AER\~.  
The linear program \(3.10\) is a relaxation of the traveling salesman problem because if *xi*  
is the characteristic vector of a tour, then A; = 1 and *Ak* = 0 for *k \* i* is a feasible solution.  
The problem contains an enormous number of variables, since p is generally exponential  
in the size of the graph.  
Figure 3.4 shows a feasible solution to \(3.10\) that is not a tour. Note, however, that it is a  
fractional2-matching.  
To see the relationship between \(3.10\) and the previous relaxations, we substitute  
*x* = I\:f=1 A;X; and use the fact that for all *i, x;* satisfies \(3.5\). Thus *x* satisfies \(3.4\), \(3.5\),  
\(3.7\), and \(3.8\), so  
zMn \~min\{ *2.\:* CeXe\: x satisfies \(3.4\), \(3.5\), \(3.7\), and \(3.8\)\}.  
eEE  
2 6  
4 5 One-tree with three edges incident  
to node 4, one edge incident to node 7,  
8 and all other degree constraints  
satisfied  
3 7  
2 6  
4 5 1  
One-tree with three edges incident to  
8 node 7, one edge incident to node 4,  
and all other degree constraints  
satisfied  
3 7  
2 6  
1  
Feasible solution to \(3.10\) obtained  
5 by weighting each of the above  
8 !-trees by! \(i.e., At = A.2 = !\);  
wavy lines indicate edges with  
x, =!  
3 7  
Figure 3.4 474 11.6. Applications of Special-Purpose Algorithms  
Moreover, we will prove in Section III.3.3 that the convex hull of 1-trees is given by the  
polytope \{x E *RJE* I\: x satisfies \(3.4\) for v = 1, \(3.5\) and \(3.8\)\}. Hence if x satisfies \(3.4\),  
\(3.5\), \(3. 7\), and \(3.8\), there is a A, that satisfies the constraints of \(3.10\) such that  
x = !\:f\~1 A-ixi. Hence  
\(3.11\) ZMn =min\{ I CeXe\: x satisfies \(3.4\), \(3.5\), \(3.7\), and \(3.8\)\},  
eEE  
and we obtain the result that the linear program \(3.10\) is equivalent to the linear  
programming relaxation of the integer programming formulation with the degree con-  
straints and subtour elimination inequalities.  
Also note that  
*ZM!T* \~ min *Ci* = ZIT and *ZM!T* \~ *ZFM*  
i\~l, *... ,p*  
since fractional 2-matching is a relaxation of \(3.11\) with the constraints \(3.5\) omitted.  
Example 3.2. provides an example with zMIT \< zTs·  
In Example 3.1, it can be shown that zMIT = zTs· The graph of Figure 3.5  
Analogous to the previous relaxation, we can consider the problem of finding a  
minimum-weight convex combination of 2-matchings that satisfies the constraints \(3.5\).  
This relaxation, as we will see, yields a bound that dominates all of the ones given above.  
Let *yi* E *B* 1 *E* I be the characteristic vector of the ith 2-matching fori = 1, ... , s, where sis  
the number of2-matchings *ofG,* and let *di* = LeEE CeY\~ be the weight of the ith 2-matching.  
5  
5  
2  
6 2  
6  
2  
2  
3 4  
2  
2  
2  
2  
Optimal tour,  
ZTS = 10  
2  
Optima11-tree  
zn =9  
Another optimal  
1-tree  
Figure 3.5  
Optimal solution to \(3.10\);  
wavy edges have Xe = V2 ,  
ZMn=9 3\. The Symmetric Traveling Salesman Problem  
The problem is  
475  
\(3.12\)  
s  
zTM =min \:2\: cx.;d;  
i\~l  
± ex.; \( \:2\: y\~\) \~ I u I - 1 for 3 \~ I u I \~ l1 v 2 I J  
i\~l *eEE\(U\)*  
and U s V\\\{1\},  
The linear program \(3.12\) is a relaxation of the traveling salesman problem because ifyi  
is the characteristic vector of a tour, then CX.; = 1 and cx.k = 0 for k *\** i is a feasible solution.  
To see the relationship between \(3.12\) and the previous relaxations, we first substitute  
y = I\:f\~ 1 cx.;y; and use the fact that for all i, yi satisfies the degree constraints. This yields  
ZTM \~ ZMJT \[See \(3.11\)\].  
Moreover, additional valid inequalities for the convex hull of 2-matchings are the  
2-matching inequalities \(3.6\) of Section 11.2.3. It can be shown that the 2-matching  
inequalities, together with \(3.4\), \(3.7\), and \(3.8\), define the convex hull of 2-matchings.  
Hence  
\(3.13\)  
zTM =min\{ \:2\: CeXe\: *X* satisfies \(3.4\), \(3.5\), \(3.7\), \(3.8\),  
*eEE*  
and the 2-matching inequalities\}.  
Example 3.3. In Example 3.2, we have zTM = *zTs* \> zMm which shows that \(3.13\) may  
give a strictly better bound than \(3.11\). The graph of Figure 3.6 shows that it is possible to  
have *zTs* \> zTM· An optimal solution to \(3.13\) is obtained by taking \~ of each of the  
2-matchings in Figure 3.6. Wavy edges have value of\~, and z™ = 21.  
Figure 3.7 summarizes the bound information from the various relaxations.  
The two relaxations that are most interesting are \(3.11\) and \(3.13\) since they alone use  
both the degree constraints and the subtour elimination constraints. In fact, we will see  
later in this section that both of these relaxations can be solved in polynomial time.  
Unfortunately, the only polynomial-time algorithms known for solving them require  
combining a cutting-plane or separation algorithm with an ellipsoid linear programming  
algorithm. Although this is not practical, a good \(but not polynomial\) approach is to use an  
FCPA for the subtour elimination constraints and to solve the resulting linear programs by  
a simplex algorithm. The other four relaxations can be solved efficiently by combinatorial  
polynomial-time algorithms.  
Primal Heuristics  
The general heuristic approaches proposed in Section 11.5.3 are applicable to the sym-  
metric traveling salesman problem. Several greedy-type algorithms can be constructed.  
1\. Nearest Neighbor. Start at an arbitrary node i 1 and construct a path i \~\> *i*2, .•• , ih  
*ij+h ... , in,* where *ij+I* = arg\(min\{c;jk\: k E V \\ \{i\~\> *i2, •.. , ij\}\),* with ties broken arbitrarily. 476  
4 3  
11.6. Applications of Special-Purpose Algorithms  
4 3  
5 2  
5 2  
6 6  
6 Optimal tour,  
zrs=23  
4 3  
4 1 3  
# v  
9  
5 2 7 8 2 2  
# v v  
6  
1  
5  
9  
\~ 2  
7 2 8  
Optimal 2-matching,  
ZM = 15  
6 6  
Feasible 2-matching  
of weight 27  
4 3  
Optimal solution to \(3.12\),  
ZTM = 21  
6 6  
Figure 3.6  
Complete the path to a tour by adding the edge \(i\~\> *in\).* Note that unless the graph is  
complete, the procedure may fail to find a tour even if one exists. Moreover, even on  
complete graphs it can perform very badly by being forced to choose edges of very large  
weight in the last steps. In Example 3.1, nearest neighbor, starting at node 4, can choose the  
optimaltour\(4 5 6 8 7 3 1 2 4\),butitcanalsogetstuckat\(4 5 6 7 8\).  
2. Greedy Feasible. Start with *E*0 = 0. Given a set *E1* at step t \< *n* - 1 such that  
\(i\) \( V, *E1\)* is acyclic and \(ii\) each node is of degree equal to or less than 2, add a minimum-  
weight edge e E *E* \\ *E1* \(if one exists\) sothat\(V, *E1* U \{e\}\)has propertiesiandii. Complete  
\( V, *En-I\)* to a tour \(if possible\) by joining the two nodes of degree 1. The remarks we made  
about nearest neighbor also apply to greedy feasible. In particular, in Example 1.1, greedy  
feasible can find the optimal tour by taking edges in the order \(1 2\), \(1 3\), \(6 8\),  
\(7 8\), \(4 5\), \(2 4\), \(5 6\), \(3 7\), but it can also fail to find a tour by beginning with  
the edges \(6 7\), \(6 8\). 3\. The Symmetric Traveling Salesman Problem  
\(3.10\) or \(3.11\)  
ZMlT  
477  
\(1-tree\)  
zn  
zrs  
\(Traveling  
salesman\)  
ZM  
\(2-matching\)  
ZfM  
\(Fractional  
2-matching\)  
ZJM  
\(Integer  
2-matching\)  
Figure 3.7  
3\. Nearest Insertion. \(Here we suppose that G is a complete graph.\) Given a subtour T  
and a node i E V \\ T, let d\(i, T\) = milljET ciJ, and let i\*= arg\(min\{d\(i, T\)\: i E V \\ T\}\).  
Suppose\)\*= arg\(min\{c;•J\:\) E T\}\). Thus i\* is the "closest" node toT, and\)\* is the node in  
*T* that is closest to i\~ Now construct a subtour on *T* U \{i\*\} by inserting *i\** between\)\* and  
one of its neighbors in *T;* that is, if\(j1, \)\*\)and \(\)2, *\)\*\)* are edges *ofT* and c;,, \~ c;,,, insert i\*  
between *j* 1 and *j\*.* This process terminates with a tour, but again we cannot guarantee that  
it will be a good tour.  
4\. k-Interchange. Local search heuristics are also useful for the traveling salesman  
problem. Given a tour, the k-interchange heuristic replaces k edges in the tour by k edges  
that are not in the tour if such a change yields a tour oflower weight. When k = 2, the two  
edges to be replaced cannot be adjacent, and there is a unique pair of replacement edges \(if  
they exist\) \(see Figure 3.8\) where the edges *\(i,j\)* and *\(i* + *I,j* +I\) replace *\(i, i* +I\) and  
\(j, *j* + I\). Unfortunately, it is possible for a locally optimal tour to be poor for any k that is  
small relative to I *V* 1.  
The negative remarks we have made about each of the heuristics is to be expected. In  
fact, for complete graphs and arbitrary edge weights, we cannot expect any fast heuristic to  
provide a good performance guarantee. The proof of the following proposition, which is  
similar to the proof of Proposition 3.2 of Section 11.5.3, is left as an exercise.  
Proposition 3.2. The traveling salesman problem with performance guarantee rH \~ r for  
any r \> 0 is .NrP-hard.  
Example 3.4. We apply the four heuristics given above to the traveling salesman  
problem on the 10-city distance matrix given in Table 2.1 of Section 1.3.2.  
I. Nearest neighbor starting at city 1. This yields the tour \(1 8 9 4 7 10  
6 2 3 5 1\) of weight or distance 349.  
i+ 1  
j+ 1 j  
Figure 3.8 478 11.6. Applications of Special-Purpose Algorithms  
2. Greedy feasible. This yields the edge set \(6 10\), \(4 9\), \(4 10\), \(2 6\), \(8 9\),  
\(1 8\), \(5 7\), \(1 5\), \(3 7\), \(2 3\) and the tour \(1 8 9 4 10  
6 2 3 7 5 1\) of distance 323.  
3. Nearest insertion beginning with the triangle. \(4 9 10 4\). The successive sub-  
tours are \(4 9 6 10 4\), \(4 9 6 10 7 4\), \(4 9 6 2 10 7 4\),  
\(4 8 9 6 2 10 7 4\), \(4 1 8 9 6 2 10 7 4\). The resulting tour  
\(1 8 9 6 2 10 3 7 5 4 1\) has weight 372.  
4. 2-Interchange beginning with the tour produced by nearest insertion. We find the  
following sequence of improving tours \(l 8 9 2 6 10 3 7 5 4 1\) of  
weight 353, \(1 8 9 2 6 10 3 7 4 5 1\) of weight 328, and  
\(1 8 9 2 6 10 3 4 7 5 1\) of weight 325.  
To obtain performance guarantees on the performance of the heuristics, the weight  
matrix must have structure. A natural structure to impose is nonnegativity and the  
triangle inequality, that is,  
*Cu* + *Cjk* \~ *C;k* for all *i,j,* k E V.  
The triangle inequality is, for example, satisfied by euclidean and rectilinear distances. We  
use the following property implied by the triangle inequality which is easily proved by  
induction.  
Proposition 3.3. If the triangle inequality is satisfied, then L.\~\:J *C;,;,+l* \~ *C;*0;k.  
When the triangle inequality is satisfied, performance guarantees can be established for  
several heuristics. Some of these results are given as exercises. Here we present the  
polynomial-time heuristic, called spanning tree-matching, that has a performance guaran-  
tee of two-thirds. No other polynomial-time heuristic is known that has a performance  
guarantee that is as good. Moreover, it is not known if a polynomial-time heuristic with a  
better performance guarantee exists.  
Before describing and analyzing this heuristic and a related one, we need to present a  
few additional definitions and results from graph theory. A graph *G* = \( *V, E\)* in which  
there may be more than one edge joining a pair of nodes is called a multigraph. A eulerian  
cycle of a multigraph is a walk with the same beginning and end points that contains each  
edge of the graph exactly once. The graph of Figure 3.9 contains the eulerian cycle with  
node sequence \(l 2 3 4 2 3 1\) and edge sequence \(e1 *e3 e6 es e4 e2\).*  
The following classic result from graph theory will be used to establish the performance  
bounds.  
3  
4  
2  
Figure 3.9 3\. The Symmetric Traveling Salesman Problem Proposition 3.4. degree.  
479  
A multigraph contains a eulerian cycle *if* and only *if* each node is of even  
Moreover, there is a simple and fast procedure \(linear in the number of edges\) for  
finding a eulerian cycle when one exists.  
Now suppose we are given a complete graph *G* = *\(V, E\)* and a spanning tree *G'* = *\(V,*  
*E'\)* of *G.* Here is a procedure for constructing a tour on *G.* Construct the multigraph *G*  
from *G'* by duplicating each e E *E'.* Since each node of G is of even degree, G contains a  
eulerian cycle *Q.* Delete all node repetitions from *Q* except for the final return to the first  
node. The resulting node sequence *T* is a tour on *G.*  
The procedure is illustrated in Figure 3.10. The node sequence of a eulerian cycle on *G*  
is *Q* = \(1 2 3 4 5 4 6 7 6 8 6 4 3 2 9 2 1\). Hence *T* = \(1 2  
3 4 5 6 7 8 9 1\).  
Double Spanning-Tree Heuristic. Find a minimum-weight spanning tree *G'* = *\(V, E'\)* of  
*G.* Duplicate each e E *E* and find a eulerian cycle *Q* on the resulting graph. Extract a tour  
T on G from *Q* by deleting node repetitions.  
Theorem 3.5. If the edge weights are nonnegative and satisfy the triangle inequality, then  
any tour produced by the double spanning-tree heuristic is of weight not greater than twice  
the weight of an optimal tour.  
5 5  
4 3 2  
9 9  
7 7  
8  
G' *T*  
5  
7  
A  
G  
Figure 3.10 480 11.6. Applications of Special-Purpose Algorithms  
Proof Let T0 be an optimal tour with edge set Ero. Let Er be the edge set formed by  
the heuristic, let *E Q* be the edge set of the eulerian cycle, and let *E'* be the edge set of a  
minimum-weight spanning tree. Then  
I Ce \~ I Ce = 2 I Ce \~ 2 I c\_,  
eEET eEEQ eEE' eEETo  
where the first inequality follows from the triangle inequality, and the second one follows  
from nonnegativity because if an edge is deleted from a tour, the resulting subgraph is a  
spanning tree. •  
To produce a heuristic of this type that has a better performance guarantee, we need to  
find a smaller-weight set of edges to add to the minimum-weight spanning tree while  
maintaining the property that the resulting subgraph is eulerian.  
Consider the nodes *U* \~ *V* of G' = \( *V, E* '\) that are of odd degree. Since the sum of the  
nodedegreesforanygraphiseven, I *U* I is even. Hence if we add I *U 112* edges toG', each of  
which is incident to two nodes of *U,* the resulting graph is eulerian. To find a minimum-  
weight set of such edges, we find a minimum-weight perfect matching *M* on the induced  
subgraph *G\(U\)* = *\(U, E\(U\)\)* of G. \(In a perfect matching, each node is of degree 1.\) This  
can be done in polynomial time \(see Section III.2.3\).  
Now observe that a tour Tis a sequence ofpathsP1 U P2 U · · · UP 1 u 1 , whereP;joins  
the ith and *\(i* + l\)st nodes\}; andj;+1 of U on the tour T \(see Figure 3.11\). By the triangle  
inequality, the length of path *Pk* is greater than or equal to c*1*Jk+l' Moreover, edge sets  
M1 = \{\(\}\~\>\}2\), *\(\}3,\}4\), ... ,* \(j IUI-\~\>i 1u1\} and M2 = \{\(\}2,\}3\), ... , \(j IUI,\}I\)\} are both per-  
fect matchings on G\( *U\).* Hence  
2 I Ce \~ I Ce + I Ce \~ I Ce•  
eEM eEM*1* eEM, eEET  
Spanning-Tree/Perfect-Matching Heuristic. Find a minimum-weight spanning tree G' =  
\( *V, E'\)* of G. Find a minimum-weight perfect matching on the induced subgraph G\( *U\)* of  
G, where U \~ Vis the set of nodes of V that are of odd degree in G '. Let M be the edge set  
of the perfect matching. Find a eulerian cycle *Q* on the multigraph G = *\(V, E'* U *M\).*  
Extract a tour *T* on G from *Q* by deleting node repetitions.  
The heuristic is illustrated in Figure 3.12.  
jfUI-1 · ..............  
·. '  
.. '  
# ..  
# ....  
l4 p3  
Figure 3.11 3. The Symmetric Traveling Salesman Problem  
481  
5  
3  
5  
# \\  
1  
4  
# /  
2  
6  
9  
9  
7  
# •  
8  
# •  
Perfect matching on *G\(U\)*  
G'\: 0 indicate nodes of odd degree  
5  
5  
-  
Eulerian graph G  
Tour *T*  
Figure 3.12  
We have sketched a proof of the following theorem\:  
Theorem 3.6. If the edge weights are nonnegative and satisfy the triangle inequality, then  
any tour produced by the spanning-treejperfect-matching heuristic is of weight not greater  
than three-halves the weight of an optimal tour.  
In fact there are families of graphs for which the bound is asymptotically achieved.  
Example 3.4. \(continued\). A minimum-weight spanning tree and a minimum-weight  
perfect matching on the nodes of odd degree in the tree are shown in Figure 3.13.  
A eulerian cycle obtained from the double spanning-tree heuristic is  
\(1 8 9 4 10 6 2 6 10 3 10 4 7 5 7 4 9 8 1\), yielding the tour  
\(1 8 9 4 10 6 2 3 7 5 1\) of distance 323.  
3  
3  
5  
4  
4  
# •  
5  
# \\  
6  
2 8  
Minimum-weight spanning tree  
Minimum-weight perfect matching  
Figure 3.13 482  
11.6. Applications of Special-Purpose Algorithms  
2  
0  
4 2 5  
8  
0  
3  
7  
G and an integer 2-matching  
9  
2  
3  
4  
11  
*Gs* and the corresponding perfect  
matching; wavy edges denote matching  
13 edges  
5  
6  
7  
8  
Figure 3.14  
A eulerian cycle obtained from the spanning-tree/perfect-matching heuristic is  
\(1 8 9 4 10 6 2 3 10 4 7 5 1\), which yields the same tour. Note that  
each of these heuristics could have produced several other tours, depending on the  
eulerian cycle chosen.  
Relaxation/Branch-and-Bound Algorithms  
Here we use the relaxations developed earlier in the section, together with primal  
heuristics and branch-and-bound, to develop algorithms for the traveling salesman  
problem that are capable of finding an optimal solution and proving optimality.  
An Assignment Problem/Branch-and-Bound Algorithm. One of the earliest approaches  
for solving the traveling salesman problem used the integer 2-matching relaxation. In fact,  
the integer 2-matching relaxation can be solved as a I *V* 1 x 1 *V* 1 assignment problem or,  
equivalently, as a perfect-matching problem on a bipartite graph.  
Figure 3.15 3\. The Symmetric Traveling Salesman Problem 483  
X = 1  
x =1De1  
**e4 *e2***  
*X* =1  
X = 1  
e3  
Figure 3.16  
The bipartite graph Gs = \( *VL* U *VR,* E\*\) is constructed from G as follows. Given  
*V* = \{1, 2, ... , m\}, then *VL* = *V* and *VR* = \{m + 1, *m* + 2, ... , 2m\}. Corresponding to  
each edge *e* = *\(i,j\)* E *E,* Gs contains two edges, *eL* = *\(i, m* +\)\)and *eR* = \(\), *m* + *i\).* Also  
CeL = Ce• = Ce for all e E E. The construction is illustrated in Figure 3.14. It is easy to see  
that if y0 E *B* 2 IE I is the characteristic vector of an optimal perfect matching in Gs, then x0  
with x\~ = y\~\~. + y\~. is an optimal integer 2-matching on G. Figure 3.14 also shows an  
integer 2-matching on G and a corresponding matching on Gs.  
If we want to consider an integer 2-matching on G with Xe = 0, where e = *\(i,j\),* then in  
Gs we delete *eLand eR.* Similarly, to obtain an integer 2-matching on G with Xe = 1, we  
delete nodes *i* and\)+ m and all of the edges adjacent to them.  
Now suppose we have solved the integer 2-matching problem and it is not a tour. To  
eliminate a solution with Xe = 2, we branch as shown in Figure 3.15.  
To eliminate a subtour, we branch as shown in Figure 3.16. Multibranching is necessary  
in the case of a subtour in order to produce a tree in which the current infeasible solution is  
violated along every branch. Note that in the kth branch with Xek = 0, k \~ 2, we set  
Xe, = Xe, = · · · = Xe,\_, = 1 since any tour that does not contain all of the edges e,, ... , *ek-I*  
is contained in one of the branches 1, ... , k - 1. To avoid creating many branches, it is  
desirable to choose a subtour containing the fewest number of edges.  
Example 3.1 \(continued\). We solve this problem using the integer 2-matching relaxa-  
tion. The initial solution of weight 8 is shown in Figure 3.3. We choose to branch on the  
edge \(4, 5\) since X4s = 2 \(see Figure 3.17\).  
The node 1 and 2 solutions are shown in Figure 3.18. Branching from node 1, as shown  
in Figure 3.19, we find that none of the remaining nodes are feasible. Hence the solution at  
node 2 is optimal.  
Weight 12 Weight 13  
Figure 3.17 484 II.6. Applications of Special-Purpose Algorithms  
2 2 6  
5 8 8  
3 3 4 7  
Node 1 solution Node 2 solution  
Figure 3.18  
A 1-Tree, Subgradient Optimization, Branch-and-Bound Algorithm. Now we consider  
a branch-and-bound algorithm that uses a Lagrangian dual relaxation. For A = \(A1 = 0, A 2,  
..• , \}yVI\) E R*1*Vi, let  
\(3.14\)  
where x satisfies \(3.5\) and, for node I, also satisfies \(3.4\); that is, x is the characteristic  
vector of a 1-tree. Let  
zw =max z 1 T\(A\).  
j\:ERIVl  
At\~O  
As noted above, the vertices of the polytope *\{x* E *R'.;1*  
*\: x* satisfies \(3.5\), \(3.4\) for node 1,  
and \(3.8\)\} are precisely the 1-trees. Hence from Corollary 6.6 of Section II.3.6, we have  
\(3.15\)  
For a given *A,* problem \(3.14\) is to find a minimum-weight 1-tree with respect to the  
weights *ciJ-* Ai- A *1.* In Section 1.3.3, we gave an efficient "greedy" algorithm for finding a  
minimum-weight spanning tree of a graph. To find a minimum-weight 1-tree, we first find  
a minimum-weight spanning tree for the subgraph induced by nodes V \\ \{1\} and then we  
add the two smallest-weight edges incident to node 1.  
If the resulting 1-tree is a tour, then by Corollary 6.8 of Section II.3.6, zIT\( *A\)* = zw = zTs·  
If the resulting 1-tree is not a tour, we can iterate on the A's. An intuitive scheme, suggested  
by the objective function in \(3.14\), is to increase Ai when the degree of node *i* in the 1-tree is  
equal to 1 and to decrease Ai when the degree of node i in the 1-tree is greater than 2. In fact,  
Figure 3.19 3. The Symmetric Traveling Salesman Problem 485  
2 6  
2  
4 5  
7  
3 8  
Figure 3.20  
for a given A.\*, the vector J\(A.\*\) with O;\(A.\*\) = \(2- degree of node *i* in an optimal1-tree\) is a  
subgradient to the objective function z n\(A.\) at A.= A.\*. Hence we only need to specify a step  
size to solve the Lagrangian dual by subgradient optimization \(see Section 1.2.4\). An  
intuitive explanation of the Lagrangian relaxation is that by transforming the edge weights  
to cij = cij- A.;- A.h the weight of all tours decreases by 2 LA.;. Thus we get an equivalent  
problem with weight vector c'. However, minimum-weight 1-trees are a function of A, so  
the objective is to find a A such that the minimum-weight 1-tree is a tour.  
It may be difficult to solve the Lagrangian dual to optimality, particularly when  
zw \< zTs· Corollary 6.9 of Section 11.3.6 can be used to find a nearly optimal A; alterna-  
tively, we can stop with z n\(A\*\) if I z n\(A\* \) - \:ZTs I \< e, where e \> 0 is a prescribed tolerance  
and \:ZTs is the weight of some feasible tour.  
When we terminate without having found an optimal tour, the calculations can be  
continued using branch-and-bound. Suppose Zn\(A\*\) is the largest known value of zn\(A\),  
and *letx\** be the characteristic vector of the 1-tree obtained from solving \(3.14\) with A= *A.\*.*  
This 1-tree contains a subtour. Thus we can proceed as we did with the integer matching  
relaxation algorithm to develop a branch-and-bound tree.  
Example 3.1 \(continued\). With A 0 = 0, an optimal1-tree is shown in Figure 3.3 and we  
obtain z n\(A.0\) = 10. Since node 2 is of degree 3 and node 8 is of degree 1, we decrease A 2  
and increase A 8• Let A 1 = \(0 -1 0 0 0 0 0 1\). An optimal 1-tree is shown in  
Figure 3.20 and we obtain zn\(A1\) = 10.  
Continuing in this manner, after several iterations, we find A\*= \(0 -2 -2 -1  
0 2 1 2\). The weights cij- A7- Ajand an optimal1-tree are shown in Figure 3.21. Thus  
we have found an optimal tour.  
An FCP/Branch-and-Bound Algorithm. Here we consider an FCP/branch-and-bound  
algorithm of the type described in Section 11.5.2. As shown above, the characteristic  
vectors of tours are given by \(3.3\), \(3.4\), and \(3.5\) for *U* C *V* with 3 .;;; I *U* I .;;; \[11 *V* 1\].  
Hence the formulation we work with is  
I Xe=2 for v E V \(3.4\)  
\(STSP\) eEJ\(v\)  
I Xe .;;; I U I - 1 for U C V, 131 .;;; I U I .;;; l12 V I J  
*eEE\(U\)*  
\(3.5\)  
\(3.3\) 486 11.6. Applications of Special-Purpose Algorithms  
5 5  
Figure 3.21  
In Section 11.2.3 we derived some classes of facets for the convex hull of solutions to  
STSP, so we now investigate the separation problems for these classes. First we examine  
the separation problem for the subtour elimination inequalities \(3.5\). Though these  
appear in our formulation ofSTSP, the exponential number of these inequalities makes it  
impossible to consider all of them as part of the initial LP relaxation. Therefore we  
typically start with the relaxation LP1 involving just the degree constraints \(3.4\),' nonnega-  
tivity, and the upper bounds \(3.8\), namely,  
Sk = \{x E R';1 \: x satisfies \(3.4\) and \(3.8\)\}.  
Proposition 3.7. If *x\** E Sk, then *LeEE\(WJ x;* = I WI - 1 + E *if* and only *if* LeEJ\(WJ *x;* =  
2- 2E.  
Proof From \(3.4\), we obtain  
21 w I = 2\( I x;\) + I x;;  
*eEE\( W\)* eEJ\( *W\)*  
or in other words,  
2- I *x;=2* \( I x;-\(IWI-1\)\).  
eEJ\(W\) *eEE\(W\)* •  
It follows that a subtour inequality \(3.5\) is violated by *x\** if and only if some cut-set  
inequality  
L Xe \~ 2  
eEJ\(W\)  
is violated by x\~ Hence to determine whether there exists W C V with I WI \~ 3 for which  
LeEJ\(WJ x; \< 2, it suffices to solve  
\(3.16\)  
and check whether \( \< 2 or not.  
Now if we imposes E *U,* and *t* E *U,* then  
min\{ L x;\: *u* c *v,* s E *U,* t E u\}  
*eEJ\(U\)* 3\. The Symmetric Traveling Salesman Problem 487  
is a minimum s - t cut problem and can be solved by the maximum s - t flow algorithm  
\(see Section 1.3.4\). It follows that \(3.16\) can be solved efficiently by solving a set of  
maximum s - t flow problems.  
Based on the symmetry *LeEb\(U\)* x; = LeEJ\(UJ x;, an alternative to \(3.16\) is  
\(3.17\) '=min\{ I x;\: 3 \< I *u* I \< I *VI* - 3, 1 E u\}.  
*eEb\(U\)*  
Note that the choice of the node fixed in *U* is arbitrary. To solve \(3.17\), let  
\(3.18\) \(j=min\{ I *x;\:\{1,2, ... ,j-l\}CU,jEU,3\< lUI\<* IVI-3\}  
*eEb\(U\)*  
for *j* = 2, ... , I *VI* - 2.  
Then \( = minj=2, ... ,1 vr-2 \(h since the minimum cut is a I - *j* cut for some *j.* Imposing the  
condition \{2, ... , *j* - 1\} C U in the 1 - *j* cut problem is easily carried out by replacing the  
capacities *xfk* by oo for k = 2, ... ,\} - 1. Thus the separation algorithm is to solve the  
maximum 1 - *j* flow problem for *j* = 2, ... , I V 1 - 2.  
Proposition 3.8. Let \( \(, U\) be an optimal solution resulting from the separation algo-  
rithm\:  
a. If \(;;;. 2, no subtour elimination constraint is violated.  
b. If \( \< 2, the subtour elimination inequality \(3.5\) with W = U is a most violated  
inequality.  
It is very often possible to reduce the size of the separation problem for subtour  
elimination constraints. Let x\* be a feasible solution of S1 and let G\(x\*\) = *\(V,* E\(x\*\)\),  
where e E E\(x\*\) only if x; \> 0. The simplest case is when G\(x\*\) is not connected \(see, e.g.,  
Figure 3.22\). For each component with node set *U,* we obtain *LeEE\(U\) Xe* = I *U* I because of  
the degree constraints, and hence the violated inequalities are found by testing G\(x\*\) for  
connectedness.  
The second case is where G\(x\*\) is connected, but x\: = 1 for some e E E. All the edges  
with x; = 1 can be shrunk by the following procedure.  
Shrinking an Edge *e* = \(i,j\) of *G\(x\*\)* with *x\:=* 1  
Step 1\: Replace nodes i and\} by a single node!.  
Step 2\: Every pair of edges e*1* = \{i, k\}, e*2* = \{\}, k\} is replaced by a single edge e\* = *\(!,* k\)  
with edge weight x;. = x;, + x;,.  
Step 3\: All other edges \(i,p\) and\(\}, q\) are replaced by the edges \(l,p\) and *\(1,* q\),  
respectively, with the same weight as before.  
Figure 3.22. Wavy lines indicate Xe = !. 488 11.6. Applications of Special-Purpose Algorithms  
Let G\(x'\) = *\(V',* E\(x'\)\) be the new graph obtained after shrinking.  
Proposition 3.9. There exists a W C V, W *\** \{i, j\}, such that LeEE\(WJ x; \> I WI - 1 *if* and  
only *if* there exists a W' C V' such that LeEE\(W\) x; \> I *W'* I - 1 in the reduced graph.  
Proof If \{i, j\} C W, then it suffices to take W' = \( W \\ \{i, j\}\) U /.  
If *i* E W, *j* \$. Wand the subtour inequality constraint for *W* is violated, then the one for  
*W* = *W* U \{j\} is violated by at least as much. Now\{i,j\} C W, and the argument is as above.  
If i \$. W, *j* \$. W, then take *W'* = *W.* •  
\(a\)  
*\(b\)*  
\(c\)  
Figure 3.23. \(a\) Shrinking edge \(1, 2\) leads to the graph in \(b\). \(b\) Shrinking edge \(5, 6\) leads to the graph in \(c\). 3\. The Symmetric Traveling Salesman Problem 489  
Obviously this procedure can be applied iteratively, so the separation algorithm need  
only be applied to the reduced graph in which all the initial edges with x; = 1 \(and possibly  
others created during the procedure\) have been shrunk \(see Figure 3.23\).  
There is also no doubt that the human eye is very good at detecting anomalies in tours,  
routes, and so on, and several researchers have very successfully found violated inequali-  
ties in this way. The reader should therefore have no difficulty in finding a violated subtour  
inequality in the last graph of Figure 3.23, which can then be converted into a violated  
inequality for the initial graph.  
Now we describe a modification of the FCP/branch-and-bound algorithm of Sec-  
tion 11.5.2 with *fJf* equal to the set of subtour elimination inequalities. A modification is  
required because all the subtour elimination constraints are needed to correctly describe  
the integer programming formulation of STSP, and the branch-and-bound algorithm is  
applied to a formulation involving only a subset of these constraints. Thus the linear  
programming relaxation at any node other than the initial node may yield an integer  
solution that is a 2-matching but not a tour. \(At the initial node 2-matchings are always cut  
offby subtour elimination inequalities.\)  
We describe three options that differ only in their treatment of the problem at nodes of  
the tree other than the initial node. In option 1, the remaining nodes are pruned when an  
integer solution is found. Hence the branch-and-bound phase terminates with an integer  
solution that may be a tour or a 2-matching. If it is a tour, it is an optimal solution of  
STSP. Otherwise we add the subtour elimination inequalities that are violated by the  
2-matchings that have been found and not pruned by bounding, and we restart the  
branch-and-bound algorithm from the beginning.  
Option 2 is to apply the separation algorithm at each node of the tree. Then the linear  
programming relaxation of STSP is solved exactly, and the lower bound obtained at each  
node is identical with that obtained by Lagrangian duality.  
A third option, which is a compromise between the first two, is to apply the separation  
algorithm only at those nodes of the tree that yield an integer solution that is not a tour. A  
justification for this option is that the separation routine for integral solutions only  
involves a test for connectedness and allows us to exclude infeasible integral solutions.  
Example 3.3 \(continued\). We apply the modified FCP/branch-and-bound algorithm.  
Phase 1  
Iteration 1\: The solution is the optimal 2-matching given in Figure 3.6 and we obtain  
zLr = 15. Because G\(x1\) is not connected, the connected components immediately give  
the cuts  
X12 +XIs+ Xzg \~ 2,  
Xs6 +X 57+ X67 \~ 2,  
X34 + X39 + X49 \~ 2.  
Iteration 2\: z\[r = 21. Applying the separation algorithm, no violated subtour elimination  
inequalities are found. The solution x2 is the fractional solution shown in Figure 3.6.  
Phase 2. The branch-and-bound tree has three nodes \(see Figure 3.24\). With x *16* = 0, the  
relaxation has an integer optimal solution that is a tour of weight 23, given in Figure 3.6.  
With x 1 6 = 1, the branch is pruned by bounding. 490 11.6. Applications of Special-Purpose Algorithms  
Figure 3.24  
The second class of facets of interest for STSP are the 2-matching inequalities \[see \(3.6\)  
of Section 11.2.3\]. There is a polynomial algorithm, again involving the solution of  
maximum-flow problems, to detect whether a point *x\** feasible in S1 violates a 2-matching  
inequality. This separation algorithm is based on the fact that the 2-matching inequalities  
are valid inequalities for the set  
s2M = \{x E *BIEI\:* I Xe = 2 for all v E v\}.  
eE\<i\(v\)  
Note that ifFCPA is applied with both subtour elimination and 2-matching constraints, it  
terminates with an optimal solution to \(3.13\) with value zTM· The same modifications as  
before must be made in the branch-and-bound phase.  
For more general comb inequalities and clique tree inequalities \(see Section 11.2.3\), no  
polynomial-time separation algorithm is known. However, using heuristics to reduce the  
size of the problem and inspection is sometimes a viable way of finding violated comb  
inequalities.  
Example 3.3 \(continued\). We apply the FCPA with separation where subtour elimina-  
tion, 2-matching, and comb inequalities are added.  
Iteration 1\: As before, zh = z2M = 15.  
Iteration 2\: After adding subtour elimination inequalities, we obtain ziP = ZMtT = 21.  
Applying a separation algorithm for 2-matching inequalities to the solution x 2 in  
Figure 3.6, no violated inequalities are found. Hence ziP = zTM· However, the comb  
inequality \(3. 7\) of Section 11.2.3 with *H* = \{7 8 9\}, *WI* = \{1 2 8\}, w2 = \{3 4 9\},  
w3 = \{5 6 7\},  
is violated by x2•  
Iteration 3\: After adding this constraint, we obtain *ztp* = 23; the resulting solution is the  
optimal tour shown in Figure 3.6.  
Solution of a Large Problem  
Example 3.5. The problem is to find the shortest tour through 67 cities in Belgium. The  
intercity distances, in kilometers, are shown in Table 3.1.  
The nearest-neighbor heuristic, starting at city 1, leads to a tour oflength 2045 km. The  
greedy heuristic gives a tour oflength 1805; and when the 2-interchange heuristic is applied  
to the greedy tour, a solution oflength 1691 is found \(see Figure 3.25\). Table 3.1.  
AA.lST  
;\~ \~S\~N\~\~ERPEN  
222\~\~ARLON  
52 92 97 219 ATH  
182 163 199 40 179 BASTOGNf  
99 116 121 156 57 130 BEAUMONT  
83 143 115 285 123 265 168 BLANKENBERGE  
Ill 181 203 63 159 65 91 M9 BOUilLON  
72 141 101 299 TOO 251 !54 14 255 BRUGGE  
\:1\:1 \:1\:\~ 50152,.113156  
61 135 27115104  
118140145150 81 139 24 20& 87  
\~SSEL  
' I CHARLEROI  
\~\~  
!18 T5il CHIMAY  
46 118 71 283 60 2\<13 114 54 232 40 16 108 138 OEINZE  
13 61 38 221 85 181 107 92 185 78 29 78 131 4 DENDERMONOE  
85 18 50 187 105 141117 168 182 154 ';5 80 131 131 77 OIEST  
96 Ul6 121 !!16 100 256 145 47 2\<47 41 114 143 188 50 98 179 OIKSMUIOE  
120 95 137 110 117 92 53 203 68 189 to 55 62 186 119 96 198 OINANT  
\~ ttn 172 9!1 161 59 103 243 81 229 110 107 102 198 168 97 231 50 OURBUY  
47 112 74 2811 98 22S 1•1 39 233 25 n 124 165 37 51 132 66 167 202 tt\:I\\LU  
32 72 77 228 20 171 62 120 182 106 SO 44 88 60 45 85 110 118 !53 *19* ENGHJEN  
173 122 165 1..- 200 11\>4 166 2151 169 247 I 139 176 219 172 106 269 114 74 220 173 EUPEN  
27 83 78 235 25 1115 82 97 184 83 4i 61 106 43 40 99 93 116 168 73 17 184 GERAAROSBERGEN  
21 92 54 249 i 10 202 121 Sl 213 52 IS 104 14S 17 31 liD 67 147 182 20 S9 200 41 GENT  
38 78 165\: 130 I 195 120 193 1159 17!i r7 gJ !\<M\> 1sJ 99 22 206 95 92 152 107 84 118 132 HASSELT  
124 77 101 128 121 88 103 207 114 193 7 76 108 163 123 67 198 46 33 171 120 68 137 144 57 HUY  
184 148 201 57' 181 17 132 267 811 253 1"' 136 141 245 183 130 258 79 51 231 180 87 197 211 108 84 HOUFFAUZE  
101 164 126 309 92\:270 132 70 239 56 ,fa !35 156 55 103 180 23 190 242 71 102 269 85 68 202 211 280 lEPER  
68 1\<10 93 272 63 · 242 103 57 228 43 t7 107 127 22 70 156 42 162 214 59 73 246 58 39 173 183 243 29 KORTRIJK  
59 136 176 69 158 29!107 242 72 218 119 111 116 220 158 126 233 54 26 206 155 112 172 184 113 59 25 246 218 LAROCHE  
56 16 •1 201 78 14!!1..!00 139 145 125 16 79 124 102 55 29 162 79 94 103 56 117 67 81 54 61 145 152 122 120 LEUVEN  
64 '104 109 231 12 191 66 102 168 88 121 70 90 48 77 117 86 129 175 87 32 212 37 58 142 133 193 80 51 168 88 LEUZE  
128 n \\29 126 155 861136 211 129 !97 te !09 144 174 127 61 222 79 48 175 128 45 139 153 39 33 69 224 195 74 72 161 LIEI\>E  
51 26' 17 229' 89•1891113'!32 195 118 l9 86137 94 46 42 138 121 132 94 69 148 84 71 64 109 172 149 116 162 42 101 !OJ'LIER  
227 269 26 i 2461 70 !91 335 89 321 222 204 176 313 251 217 322 136 125 299 2\<t8 141 265 277 195 !56 87 339 298 99 186 271 156 1 259 lUXEMBURG  
88 118 184 Tn\: 144,182 235 187 220 ll7 !52 188 203 149 72 256 136 120 193 !57 103 168 172 41 87 127 253 223 132 101 189 5! 116 214 MAASEIK  
130 173 115 192 75 152 258 128 244 145 156 lSI 221 174 114 299 97 60 222 186 29 2\<13 200 86 91 58 291 274 61 119 224 47 156 112 105 MAlMED'I  
114 • 156 63 !36 43 87 222 85 208 109 91 98 200 138 1\\5 213 34 !8 166 135 92 152 159 103 49 45 224 198 20 98 148 64 148 113 122 6J MARCHE  
42 30' 24 224 74 193 98 113 180 109 24 71 122 88 31 46 129 102 117 82 54 140 65 66 68 84 17\$ 134 101 143 23 86 107 IS 246 118 149 121 MECHELEN  
62 96, 101 210 25 163 32 136 134 122 54 36 56 82 75 109 113 91 143 109 30 175 so 95 129 112 172 114 71 145 80 37 145 93 227 181 203 125 78 MONS  
92 67, 109' 130 89 90 71 175 94 161 •2 44 79 138 91 68 166 28 63 139 88 100 IDS 119 67 32 92 170 151 67 Sl 101 \~· 89 160 108 123 47 74 80 NAMUR  
94 169 217 36 192 28 120 249 37 263 1115 129 124 258 193 175 262 74 72 242 190 132 184 220 !59 105 45 258 238 50 153 204 120· 217 62 178 100 58 176 165 102 NEUFCHATEAU  
\$4 74 79 180 47 133 42 145 119 136 12 \\5 66 93 61 87 1\<13 7fl 106 109 33 \\53 50 89 109 75 135 127 9fl 110 54 59 108 71 203 159 158 90 56 38 43 \\44 NIVELLES  
98 1531126 29S 113 275 160 21 279 24 123 !58 184 \~ 92 169 26 213 243 49 125 268 108 69 203 217 277 49  
57 252149101 221'143 345 256268 249  
128 185 259 159 I OOSTENOE  
40 112 82 260 42 221 96 72 190 58 86 86 120 18 53 128 69 1-iO 190 48 42 204 28 28 ISO 125 222 61  
32 178 92 30 164· 97 290 197 211 158  
75 83 OUOENAAROE  
03' 115 120 139 61 121 24 183 78 172 73 26 33 138 102 116 169 29 79 156 70 143 106 128 115 75 108  
133 83 99 90 Ill 112 165 156 126 63  
75 99 114 208 38 194 27 149 1.7 135 17 49 69 95 88 122 126 98 154 112 43 182 57 117 147 125 1771113  
471158 1081 236119412001132  
91 165 250 122 37 47 147 104 ROESELARE  
52 117 202 65 85 12 108 85 48 RONSE  
16 65 159 22 139 56 56 29 91 43 SOIGNIES  
149 86 25 138 2113 208 87 156 234 18fl 143 ST. HUBERT  
207 129 82 172 289 247 92 111 213 78 105 61 Til!\:t 119 22U 105 294 242 194 *19* \:\:;1. vnn  
82 90 86 189 203 ST NIKLAAS  
112 SO 147 83 188 133 96 125 1'11 129 101 122 128 93 ST.TRU!D\~fl.  
182 93 122 143 271 231 120 195 2118 229 178 87 40 183 83 SPA  
78141 103 44 46 106 94 80 85 198 JO 93 29S 238 195 76 30 25 256123 220 84 177 48 47 225 33 90 188 7fl 127 151 127 Ttl 121147 32 80157 2217\$228 48 89 256 72 49185197 249 23  
64 73 108 30 63 119 70 128 178 60 40 209 23 40 141 ,., 207 62  
38 30 72 74 59 93 113 85 130 93 14 189 31 73 115 97 1715 105  
2D 232139 64 211,113 33182 90 18164,103 76 139 64 37 130 n •••wMm•••\~rn\~rn•m-••mm\~m•\~\~·-  
95,175  
\\51 63 212 21 94 241 232 82 54 2DI 149 124 296 119 282 95 20!i 81 1115 171 178 242 220 128 314 98 75 241 203 SO 220 221 107 108 37 306 278 62 165 231 49 93 147 50 .17 79 100 139 150 56 62 173 57 33 99 117 203 IDS 72 178 56 94 68 177 157' 38 41 83 25 187 88 75 140 93 80 104 12D 17 42 108 189 HiO 101 37 125 95 281 91 41 212 189 33 206 192 67 61 62 291 262 62 107 218 35 67 3S, 137 325 234 2«1258 264191225162  
165 166 212  
119  
91 267 195 131 133 151 58 86 21 175 82 19 82 158 91 57 " " 92 62 170 33 60 130 193 96 61 75 113 80 81 93 128 179 12t 213 66 135 281 151 267 108 103 184 59 163 18 175 120 161 124 86 283 75 2\~3 129 44 247 30 35 66 17D 95 130 95 148 140 144 58 104 145 133 105 140 1116 1.ta 182 145 107 315 98 281 150 34 252 20 121 126 245 29 227 78 84 201 10 44 42 227 1211 187 159 157 204 143 ISO 152 144 239 192 112 289 82 43 240 184 44 201 220 91 76 49 278 249 51 137 222 52 162 12i 1-1-D 15 48 16o 188 108 94 138 299 226 111 \:201 273 210 173 7i 32 193 87 24 SlA.VELOT  
68 19 •2 121 95 99 147 74 124 158 64 158 81 126 112 95 153 141 105 128 92 68 128 105 223 !53 171 108 90 34 83 l.tl 34 162 !OJ •2 n Tl9 86 40 132 170 102 95 165 171 THUIN  
\)1 133 153 15 63 140 35 \\81 216 31 75 234 58 32 162 165 245 40 21 220 117 63 189 96 313 212 236 200 M 97 153 250 110 50 33 159 110 17 45 88 231 257 ISS 154 217 24' 131 TIElT  
45 58 109 121 74 22 180 74 75 131 75 98 86 105 35 42 126 171 142 101 19 107 53• 61 206 94 107 93 42 94 46 147 65 168 111 94 107 158 109 83 122 144 75 18 81 lid 77 136 TIENEN  
83 96 145 159 112 42 209 83 67 160 113 64 124 140 20 37 88 209 180 93 57 145 19 84 175 50 8fl 83 88 149 69 139 112 208 !53 112 146 196 147 121 114 94 Ill 20 •1 11 132 174 38 TONGEREN  
110 154 17. 36 84 161 21 209 241 45 106 253 89 53 183 197 264 31 33 283 136 84 208 124 341 233 2i56 245 115 118 178 283 135 24 80 17. 131 13 68 108 276 276 8fl 175 247 275 152 21 157 195 TORHOUT  
79 80 100 81 94 134 69 !o35 192 98 49 219 54 71 156 156 214 56 27 189 105 17 189 116 271 206 236 1159 103 44 124 211 76 84 43 106 57 \~7 31 54 202 253 104 158 226 232 78 48 12\<4 178 60 TOURNAI  
79 122 174 113 80 42 163 138 \\42 119 109 148 12\~ 96 65 109 190 188 135 168 80 141 1D3 40 259 76 158 157 55 140 110 214 Ill 168 137 158 153 145 147 117 189 172 63 87 137 163 141 128 64 84 148 158 TURNHOlJT  
112 233 \~  
# ...  
•••••\~---m\~\~-m••\~\~-mg-•••n••••m\~w\~•••\~\~-\~\~\~•mmw•••••••••-\~·---\~\~  
188 l\<t& 312 118 272 183 ., 283 50 142 167 187 66 114 197 16 214 2"7 71 126 285 109 83 219 214 274 31 60 249 188 104 240 154 338 269 305 229 154 131 182 288 158 26 38  
208 245 28 211 88 154 317 52 303 203 156 139 284 232 211 299 113 111 280 229 172 236 258 198 144 84 291 275 89 192 220 159 242 51 217 1•5 95 231 HI& 1•1 39 183 262  
127 144 156 104 154 219 137 205 106 127 182 162 135 84 230 97 66 183 136 47 1•7 163 42 51 87 232 203 92 80 168 18 106 170 •7 55 62 110 163 83 132 137 J229 172 128 17\$ 212 140 144 112 88 138 43 48 7\:1 146 195 151 23 216 207 108 29 172 151 VISE Figure 3.25. Greedy/2-interchange tour oflength 1691 km.  
Table 3.2.  
Subtour 1. *W* = \{1 2 3 11 15 16 25 31 33 34 36 37 39 52 53 54 56  
59 60 63 67\}  
2. \{4 22 35 55 64 66\}  
3. \{4 35 66\}  
4. \{22 55 64\}  
5. \{1 2 3 11 15 16 25 31 33 34 36 39 53 54 59 60 63 67\}  
6. \{4 47 50\}  
7. \{2 16 25 31 33 36 54 59 60 67\}  
8. \{2 16 63\}  
9. \{5 14 21 23 29 32 40 45 47 48 49 50 58 61 62\}  
10. \{5 14 21 23 29 32 40 45 47 49 50 58 62\}  
11. \{2 16 25 31 33 36 54 59 60 63 67\}  
12. \{12 43 57\}  
13. \{17 28 65\}  
14. \{25 33 36 54 59 60 67\}  
15. \{5 8 10 14 17 20 21 23 24 28 29 32 44 45 48 49 58  
61 62 65\}  
16. \{5 8 10 14 17 20 21 23 24 28 29 32 40 44 45 47 48  
49 50 58 61 62 65\}  
17. 18\. 19. \{25 33 36 54 60 67\}  
\{22 37 55 56 64\}  
\{22 37 52 55 56 64\}  
492 3\. The Symmetric Traveling Salesman Problem 493  
To find the optimal tour, we apply the FCP /branch-and-bound algorithm using subtour  
elimination inequalities and the first option described above so that subtour elimination  
constraints are only added at the initial node.  
Phase 0. The initial LP problem with the degree constraints \(3.4\) and the upper-bound  
constraints \(3.8\) has value *zFM* = 1571.5.  
Phase 1. LP\(81'\) is solved after adding 19 subtour elimination inequalities \(3.5\), with the  
sets W given in Table 3.2. zLP = *zMn* = 1606.75.  
Phase 2. t = 1. Branch-and-bound applied to LP\(81'\) finds a tour of length 1615 at  
node 22, a 2-matching oflength 1614 at node 37, and a 2-matching oflength 1613 at node  
55, and then it terminates at node 78.  
t = 2. Two subtour elimination constraints are added, with *W* = \{1 15 53\} and  
*w* = \{8 10 14 17 20 24 28 29 32 44 45 48 49 58 61 62 65\}  
eliminating the 2-matching solutions of length 1613 and 1614, respectively. Branch-and-  
bound is now applied with a cutoff of 1615; and the search terminates after 73 nodes, with  
no solution of value less than 1615. Hence an optimal tour is of length 1615 \(see  
Figure 3.26\).  
Figure 3.26. Optimal tour oflength 1615 km. 494  
11.6. Applications of Special-Purpose Algorithms  
Table 3.3.  
1-Tree relaxation  
Fractional matching  
Linear programming \(Lagrangian 1-tree\)  
Optimal tour  
Greedy + 2-interchange heuristic  
Greedy heuristic  
Nearest neighbor  
1401  
1571.5  
1606.75  
1615  
1691  
1805  
2045  
A smaller branch-and-bound tree would be obtained if the 1-tree Lagrangian relaxation  
was used to obtain the bounds at each node. The subgradient algorithm was used to solve  
the Lagrangian dual having value zMIT = 1606.75 at node 1. The length of the initial1-tree  
with the multipliers at zero is zIT= 1401. Using an initial step-size of 5 and decreasing by a  
factor of2 every *N* = 67 iterations, a bound exceeding 1600 was first obtained on iteration  
197, a bound exceeding 1605 was obtained on iteration 276, and a bound of 1606.23 was  
obtained on iteration 399. A summary of the bounds obtained is given in Table 3.3.  
20 h--/  
# --  
2\~ \~15I ,4  
\:39  
1 ........... 31  
45 23 •'. 11  
21 •• •• \:  
.. · \:  
---- Xe=l  
-·- Xe='lil  
--- Xe='l2  
............ Xe=V•  
Figure 3.27. *zLr* = zMIT = 1606.75. 4\. Fixed-Charge Network Flow Problems  
495  
63  
3 \~--\~ \:··y' \\  
15 !'. 31 \~16  
11 \_\( /  
---Xe=l  
-·-Xe=\:Y..  
---Xe=V2  
•••••••••••• Xe= '14  
Figure 3.28. Length= 1609.75.  
Finally we consider briefly the addition of2-matching and comb inequalities which are  
found by inspection. The fractional solution to LP\(\$\), obtained by the addition of the  
subtour elimination constraints, is shown in Figure 3.27. Inspection of the figure readily  
reveals that at least four 2-matching inequalities are violated. Adding the inequalities with  
1. *H* = \{3 34 63\}  
2. *H* = \{28 29 48\}  
3. *H* = \{25 54 60\}  
4. *H* = \{22 33 55 64 67\}  
£ = \{\(3 53\), \(34 39\) , \(16 63\)\}  
£ = \{\(17 28\), \(29 62\), \(48 61\)\}  
£ = \{\(25 36\), \(54 59\), \(33 60\)\}  
£ = \{\(33 60\), \(55 56\), \(36 67\)\}  
leads to the solution of value 1609.75 shown in Figure 3.28. It is left to the reader to find  
further violated inequalities.  
4\. FIXED-CHARGE NETWORK FLOW PROBLEMS  
So far in this chapter, we have considered classes of pure-integer programming problems.  
Here we consider an important class of mixed-integer problems. The fixed-charge network  
flow problem \(FN\) was formulated in Section 1.1.3 as 496 11.6. Applications of Special-Purpose Algorithms  
\(4.1a\) \(4.1\)  
min I *cuxiJ* + I *hiJYiJ*  
\(i,j\)E.s4 \(i,j\)E.s4  
*L YiJ- L YJi* = b; *fori* E *V*  
JEo•\(i\) JE&-\(i\)  
\(4.1b\) Yv\~UiJXiJ for\(i,j\)E.stl  
y ER\~ 1  
, x EB',st/1,  
where *g;* = *\(V,* .stl\) is a digraph, J+\(i\) = *U* E V\: \(i,j\) E .stl\}, J-\(i\) = *\(j* E V\: *U,* i\) E .stl\}, b; is  
the supply at node *i, cu* is the fixed cost of having flow on arc \(i,j\), *hiJ* is the variable cost  
per unit of flow on arc \( *i,* j\), and u *iJ* is the capacity of arc \( *i,* j\). Recall that the difference  
between FN and the linear minimum-cost flow problem is that in FN *ifyiJ* \> 0, then the  
cost of the flow is c *iJ* + *hiJy iJ·* This is achieved by the capacity constraints \( 4.1b \), which force  
*Xu=* 1 whenyiJ \> 0.  
A necessary condition for feasibility, assumed throughout this section, is \:E;Ev b; = 0.  
We also assume that *ciJ;?;* 0 for all \(i,j\) E .stl since if *ciJ* \< 0, we can set *xu=* 1 and·eliminate  
*xu* from the problem. Similarly, we assume that with respect to the *hiJ* there are no  
negative-cost directed cycles. This assures that the objective function is bounded from  
below.  
Besides being an important model in its own right for a variety of network design  
problems, several special cases of FN are of substantial interest. A simple way to obtain  
special cases is to restrict the network structure \(e.g., as in the transportation problem\).  
Another simplification concerns the capacity constraints. When *uiJ* is sufficiently  
large-for instance, *UiJ;?;-!* \:E;Ev 1 b; 1-there is no feasible flow with *Yu* \> *uiJ,* and the  
capacity constraint only serves to force the fixed cost to be included in the objective  
function when the flow is positive. We call such problems uncapacitated and denote them  
by\(UFN\).  
Yet another important subclass of FN s are those in which 1 *i* E V\: b; \> 0 I = 1. We call  
these problems single source \(SFN\) and use the notation \(SUFN\) for single-source  
uncapacitated problems.  
Several interesting problems can be modeled as SUFNs-for instance, the uncapaci-  
tated facility location problem \(UFL\) considered in Chapter II.5. Figure 4.1a gives the  
digraph for a UFL with *m* = 2 and *n* = 3. The arcs joining the dummy node to the facility  
nodes are uncapacitated and have only the fixed cost of opening the jth facility. The arcs  
that join facility *ito* customerj are also uncapacitated and have the variable cost *hu.* Note  
that since UFL is .N9Jl-hard, SUFN is .N9Jl-hard.  
Another interesting SUFN is the Steiner r-branching problem. Given a subset *D* £ *V,* a  
root r E D, and weights on the arcs, a Steiner r-branching is a minimum-weight branching  
that spans *D.* Here fortherootnode *rED* weletb, = ID 1 - 1, b; = -1 *fori ED\\* \{r\}, and  
*b;* = 0 *fori* E *V \\D.* The objective function is accommodated by letting *ciJ* be the weight  
of arc \(i,j\) and letting *h;J* = 0 for all \(i,j\). It then follows that feasible solutions without  
directed cycles are branchings that span *D* \(see Figure 4.1b\). Note that when *D* = *V* we  
obtain the minimum-weight directed r-branching problem \(see Section III.3.5\), and when  
ID 1 = 2 we obtain the shortest-path problem \(see Section 1.3.2\). Although both of these  
problems can be solved in polynomial time, the general Steiner branching problem is  
.N9P-hard.  
In several practical models, FN or special cases arise as subproblems. For example,  
production planning problems frequently contain the uncapacitated lot-size problem,  
which is an SUFN \(see Figure 5.2 of Section 11.5.5\). Thus algorithms based on Lagrangian  
relaxations and techniques for solving FNs can be used to solve practical problems that are  
FNs with additional constraints. This hierarchy ofFNs is displayed in Figure 4.2. **4.** Fixed-Charge Network Flow Problems 497  
Dummy D=\{l,3,4\}  
*r=* 1  
Facilities  
Customers  
\(a\) \(b\)  
Figure 4.1  
In this section, we begin by mentioning briefly a standard branch-and-bound algorithm  
for FN primarily to point out its advantages and limitations. We then propose an FCPA for  
FN and apply it to the fixed-cost uncapacitated transportation problem.  
Next we given another IP formulation for SFN and show that its linear programming  
relaxation is stronger than the linear programming relaxation of \( 4.1\). This formulation  
simplifies for SUFNs, and we illustrate it with the Steiner branching problem and the  
uncapacitated lot-size problem. In the case of the uncapacitated lot-size problem, this  
serves as the basis for other reformulations, one of which is a shortest-path problem. The  
shortest-path reformulation has been used in a linear programming relaxation of multi-  
item lot-size problems and appears to have the capability of solving quite large instances.  
A Branch-and-Bound Algorithm for FN  
An obvious way to solve \(4.1\) is by a branch-and-bound algorithm that uses linear  
programming relaxations. An advantage of this approach is that the linear programming  
Figure 4.2 498 **11.6.** Applications of Special-Purpose Algorithms  
relaxation of\(4.1\) is a network flow problem. Network flow problems can be solved very  
efficiently by, for instance, the network simplex algorithm \(see Section 1.3.6\). In addition,  
several other parameters used in a branch-and-bound algorithm \(e.g., penalties\) are easy  
to obtain from optimal basic solutions to network flow problems.  
Proposition 4.1. The linear programming relaxation of FN is the network flow problem  
\(4.2\) min\{ 2\: \(hu + cu\)Yu= \(4.la\), Yu \~ uu for *\(i,j\)* Ed, y E R\~ 1 \}.  
\(i,j\)Ed Uij  
Proof Replacing x E *Blsfi1* by xu\~ 1, the only constraints on xu are Yuluu \~xu\~ 1.  
Because cu;?; 0, there exists an optimal solution with xu= Yuluu. This substitution gives  
the network flow problem \(4.2\). •  
Unfortunately, the bounds obtained from these relaxations are frequently .very poor  
primarily because they do not accurately represent the fixed costs. This is true, as we have  
noted earlier, because if the optimal solution *y* has 0 \< Yu \< uu, then only the fraction  
Yuluu of the fixed cost is included in the objective function. Another disadvantage of this  
approach is its inflexibility in accommodating additional constraints. If the problem to be  
solved has additional constraints, the network structure of the linear programming  
relaxation will be destroyed unless another technique such as Lagrangian relaxation is  
used.  
An FCPA for FN  
To improve the bounds obtained from the network flow relaxation and to accommodate  
additional constraints within the scope of a linear programming relaxation, we now  
consider an FCPA for FN that uses strong cutting planes.  
We will use three classes of valid inequalities for FN. Observe first that for fixed *i* E *V,*  
any solution of\(4.1\) satisfies  
\(i\)  
and  
\(ii\)  
Using the separation procedure described in Section 11.6.2, violated extended cover  
inequalities can be generated from knapsack set \(i\) if bi \> 0 and from \(ii\) if bi \< 0.  
Also observe that any solution of\(4.1\) satisfies  
Replacing this equality by two inequalities gives sets having the form of the single-node  
flow model introduced in Section 11.2.4. Thus to obtain the second and third types of  
inequalities we consider the region 4. Fixed-Charge Network Flow Problems 499  
wheren = *iN+uN-1.*  
For the set *ofT,* we have derived the class of valid inequalities \[see \( 4.4\) of Section 11.2.4\]  
\(4.4\)  
where *C* s;;\: *N+* is a dependent set \(i.e., A.= I a.i- b \> 0 and *L* s;;\: *N-\).*  
jEC  
These can be generalized to the following larger class of valid inequalities, called  
generalized flow cover inequalities \(GFC\)  
where C+ s;;\: *N+,* c- s;;\: *N-,* L s;;\: *N-* \\ c- and A.= 1\:jec• a.i -1\:jec- a.i - b \> 0. We leave it as an  
exercise to show that these are valid for *T.*  
The separation problem for the family \( 4.5\) is\: Given a point \(x\*, y\*\), check whether for  
any sets C+, c-, and *L,* inequality \( 4.5\) is violated.  
We let a E BIN\~ be the characteristic vector of c+ s;;\: *N+* and let *p* E *BW1* be the character-  
istic vector of c- s;;\: *N-.* The definition of A. yields the equality knapsack constraint  
\(4.6\) I a.ia.i - I a.iP.i = b +A., subject to A.\> 0.  
*jEN' jeN-*  
The violation to be maximized is  
where the last term is derived from the observation that for *j* E *N-* \\ c-, any violation is  
maximized by taking\} E *L* if A.xj \< yj and *j* E *N-* \\ \( c- U *L\)* if A.xj \> yj.  
The resulting separation problem is the nonlinear integer program  
max\{\( 4. 7\)\: \( 4.6\), a E *B1 N1*  
*, p* E *B1 N1\).* This problem is equivalent to solving the family of  
equality knapsack problems  
\(4.8\) ';. = max\{\(4.7\)\: \(4.6\), a E *BIN1, p* E *B'N1\)*  
for all positive integral values of *A..* Hence we have shown the following\:  
Proposition 4.2. An inequality of the form *\(4.5\)* with A.= A.\*\> 0 is violated by the point  
\(x\*, y\*\) *if* and only *if';.·* \> 0.  
Unfortunately, there are two difficulties with this separation problem. Equality knap-  
sack problems are hard to solve, and the function ';.is not well behaved as a function of A..  
Therefore we look for a heuristic solution to the problem of choosing the sets c+ and c- for  
which inequality \(4.5\) is niost violated by \(x\*,y\*\). As a first step we consider a subclass of  
the inequalities in which L = N- \\ c-, and then we relax these inequalities by reducing the  
term \(a.i- *A.t* \(1 - Xj\) to \(a.i -A.\) \(1 -xi\). The resulting valid inequalities are 500 11.6. Applications of Special-Purpose Algorithms  
\(4.9\) I *\[y1* + *\(a1* - ,1,\) \(1 - *x1\)\]* \~ *b* + I *a1* + I ..tx*1.*  
\~\~ \~\~ \~\~\~  
Finding the sets c+ and c- for which \( 4.9\) is most violated is still not computationally  
easy, so we take a second heuristic step which is to work with an upper bound on the  
violation for any set c+, c- in \(4.9\).  
Because y*1* \~ a*1*x*1* for all\), an upper bound on the violation of \(4.9\) is obtained by  
replacing yjby the possibly larger value a*1*xj. This gives the upper bound  
I \[a*1*xj + \(a*1* - ,1,\) \(1 - xj\)\] - b - I a*1* - I ..txj.  
jEC' jE\~ jEN-\\C-  
Substituting b = LJEC' a*1 -* LJE\~ a*1 -* ,1, and canceling terms, the upper bound on the  
violation of\(4.9\) is equal to  
..t\[-I \(1-xj\)+Ixj-\(I xj-1\)\]·  
;EC' ;EC- ;EN-  
To find the maximum value of this upper bound, we solve the knapsack problem  
\(4.10\) \~ = max\{ I \(xj- 1\)a 1 + I xjP *1\}*  
jEN' jEN-  
L a*1*a*1* - I a*1*P *1* \> b  
jEN' \)EN-  
We now know that if some inequality \(4.9\) is violated, then ..t\[\~- an upper bound on the value of the violation, must be positive.  
\(LJEN- xj- 1\)\], which is  
Proposition 4.3. *A* necessary condition for the violation of an inequality of the form *\(4.9\)*  
is \~ \> LJEN- xl- 1. This condition is also sufficient if Yl= aJXlfor all j E c+, where *C* is  
determined by an optimal solution to *\(4.10\).*  
The above discussion leads to the following heuristic separation algorithm for general-  
ized flow cover inequalities.  
Separation Algorithm for Generalized Flow Cover Inequalities  
Step 1\: Solve the knapsack problem \( 4.10\) exactly or approximately to obtain an optimal  
or "near-optimal" pair c+, c-.  
Step 2\: Given c+ and c-, test whether \(x\*, y\*\) violates the inequality \(4.5\), where for  
j *EN-\\* c- we put\) E *L* if ..txj \< yjandj *EN-\\ \(L* U c-\) otherwise.  
Note that even if \~ \< LJEN- xj- 1, the inequality \( 4.5\) may be violated because our  
arguments have been based on approximations to the violation of \( 4.5\).  
Example 4.1. Consider the mixed 0-1 constraint 4. Fixed-Charge Network Flow Problems 501  
with Y1 \~ 6500, Y1 E Rl, x E B3  
, and the point \(y7, xi, x!, x\:\) = \(6500 0 1 0.296\). We can  
rewrite the above constraints in the form of a single-node variable upper-bound set T\:  
with the additional constraints x1 = 1, Yz = 2250xz, y3 = 4500x3, Y4 = 6750x4.  
Applying the heuristic separation algorithm for generalized flow cover inequalities  
\( 4.5\), we obtain the knapsack problem \( 4.10\)\:  
\~=max Oa1 + OPz + 1P3 + 0.296P4  
6500al- 2250Pz- 4500P3- 6750P4 \> o  
*aEB1*  
*, PEB3*  
with optimal solution a *1* = 1, P *3* = 1, and \~ = 1.  
With c+ = \{1\} and c- = \{3\}, we have A= 2000 and L = \{2, 4\}, and the resulting inequal-  
ity \(4.5\) is  
y 1 + \(6500- 2000\) \(1 - x1\) \~ 0 + 4500 + 2000xz + 2000x4;  
or, using the additional constraint x 1 = 1, we obtain  
Y1 \~ 4500 + 2000xz + 2000x4,  
which is violated by *\(x\*,* y\*\).  
Note that because\~\> I.J\~z *xj-* 1 = 0.296, the necessary condition of Proposition 4.3 is,  
in fact, satisfied. Since *YT* is at its upper bound, the sufficient condition also happens to  
hold.  
Valid inequalities of the third class are called extended GFCs and are of the form  
\(4.11\) \~ b + I *aj-* I min\{A, *\[aj- \(a-* A\)J+\} \(1 - *xj\)*  
*jec- jec-*  
+ I max\{A, *aj* - *\(a- A\)\}xj* + I *Yh*  
*jEL- jEN-\\\(C-uL* -\)  
where *a=* maxjec+ *ah aj* = max\(a, *aj\), L* + \<;; *N+* \\ C+, and *L-* \<;; *N-* \\ c-, and we require  
a\~ A\> o.  
We do not develop a separation routine for the extended GFCs. Instead we use the sets  
c+ and c- derived in the separation routine for GFCs, together with sets L + and L-  
constructed by  
and  
*L-* = \{j *EN-\\* c-\: max\{A, *aj- \(a- A\)\}xj* \< *yj\},*  
to find a violated inequality of the form \(4.11\). 502 11.6. Applications of Special-Purpose Algorithms  
In summary, for each constraint \( 4.1a\) of \( 4.1\), we try to find violated extended cover  
inequalities, GFCs, and extended GFCs as indicated above. Note that since sets of the  
form *T* given by \(4.3\) arise in relaxations of general mixed 0-1 programs \(see Section  
II.2.4\), the FCPA can be used to generate violated inequalities for general mixed 0-1  
models.  
We now illustrate the FCPA by applying it to the fixed-charge uncapacitated transporta-  
tion problem.  
Solving a Fixed-Charge Uncapacitated Transportation Problem by an FCPA and  
Branch-and-Bound  
For a transportation problem we obtain V = \( V 1 U V2\), and all arcs are directed from V 1 to  
V2• Hence \( 4.1\) simplifies to  
min I I *hiJYiJ* + I I *ciJxiJ*  
iEV1 jEV, iEV1 jEV,  
I Yii = b; *fori* E V1  
jEV,  
I *YiJ* = d*1* for j E *V*2  
iEV1  
where *uiJ* = min\(b;, *dj\)* for *i* E V1 and\} E Vz; V1 = \{1, ... , m\}; Vz = \{1, ... , n\}; and  
L;EV, b; = LjEV, dj.  
The initial linear programming relaxation LP1 is obtained by replacing the integrality  
constraints by 0 \~ *xiJ* \~ 1 *fori* E *Vt.i* E *V*2• Note that LP1 is a transportation problem  
with *hij* = *hiJ* + *ciJ/uiJ* because we can set *xiJ* = *YiJ/uiJ fori* E *V1* andj E *V2•* However, once  
cutting planes are added we no longer have a transportation problem.  
In the cutting-plane part of the algorithm we add three types of cuts\:  
Step a\: Extended cover inequalities are obtained from the knapsack sets  
I *UijXij* \~ I *Uij-* b;, *\(x;l' ... 'x;n\)* E *sn fori* E *VI*  
jEV*2* jEV*2*  
and  
where *xij* = 1 - *XiJ.* These constraints are obtained from  
I *uiJxu;;\:.* b; and I *uuxu;;\:.* d1•  
jEV2 iEV1  
Steps b and c\: GFC and extended GFC flow inequalities are obtained from the following  
sets of inequalities\:  
\(i\) I *Yu* \~ b;, *Yu* \~ *uuxu forj* E Vz,  
jEV,  
*fori* E V1 4. Fixed-Charge Network Flow Problems 503  
\(ii\) - *L Yu* \~ -b;, *Yu* \~ *UuXu for\}* E v2, fori E *V1*  
\(iii\) \(iv\) jEV2  
L *Yu* \~ *dh Yu* \~ *uuxu fori* E *V1,* iEV1  
- *L Yu* \~ *-dh Yu* \~ *uuxu fori* E *V1,* iEV1  
*for\}* E V2  
*for\}* E V2.  
Example 4.2. problem given by the following data\:  
We solve the instance of the fixed-charge uncapacitated transportation  
*m* =4, n=6  
c69 0.64 0.71 0.79 1.70 283\)  
0.75 0.88 0.59 1.50 2.63  
\(hu\) = \~\:\~\~  
1.06 1.08 0.64 1.22 2.37  
1.94 1.50 1.56 1.22 1.98 1.98  
*c* 16 18 17 10 20 \) 14 17 17 13 15 13  
\(cu\) = 12 13 20 17 13 15  
16 19 16 11 15 12  
b = \(45 35 20 15\), *d* = \(35 30 25 15 5 5\).  
Phase I  
*Iteration 1\: zLP* = 185.6\. The corresponding solution is shown in Figure 4.3, where  
*\(yU, xb\)* is indicated for each edge.  
Figure 4.3 504 11.6. Applications of Special-Purpose Algorithms  
Applying the separation routine at source row 1, we combine the constraint  
Yu + Y12 + Yl3 + Yl4 + Y1s + Y16 = 45  
with the variable upper-bound constraints Yn.\:;;;; 35xn, Y12.\:;;;; 30xl2• y13.\:;;;; 25x13, Yl4.\:;;;;  
15xl4• Y1s.\:;;;; 5xls, and Y16.\:;;;; 5xl6 to obtain  
35xn + 30xl2 + 25xl3 + 15xl4 + 5xls + 5xl6 \~ 45,  
which is the knapsack inequality  
The separation routine for extended cover inequalities with x'= \(0 0.67 1 1 1 1\) then  
gives a violated constraint  
or  
Similarly, from source rows 2, 3, and 4, we obtain the violated inequalities  
X31 + X32 + X33 + X34 X41 + X42 + X43 + X44 and from demand row 2, we obtain the violated inequality  
\~ 1  
\~ 1  
\~1.  
6 5  
Figure 4.4 4\. Fixed-Charge Network Flow Problems 505  
Iteration 2\: After addition of the above constraints and reoptimizing the linear program,  
we obtain ZLP = 198.67 and the solution shown in Figure 4.4.  
Now the knapsack inequality  
35x2l + 30x22 + 25x23 + 15x24 + 5x2s + 5x26 \~ 35  
\(x21, ... , x26\) E *B*6  
yields the violated cover inequality  
Also, in Step b the set  
Y21 + Y22 + · · · + Y26 \~ 35,  
Y21 \~ 35x2\~\> Y22 \~ 30x22, Y23 \~ 25x23,  
Y24 \~ 15x24' Y2s \~ 5x2s, Y26 \~ 5x26  
yields the violated GFC inequality \(4.5\)  
Y21 + Y22 \~ 30 + 5x2\~\>  
which is obtained with C = \{1,2\} and c- = 0.  
Iterations 3 and 4\: We obtain ZLP = 200.4. The cuts \(4.11\)  
Y22 + Y23 + Y24 \~ 20 + l0x22 + 5x23 + l0x24  
and  
Y21 + Y22 + Y23 + Y24 \~ 20 + 15x2l + l0x22 + 5x23 + 10x24  
are both derived from source row 2, the first with *C* = \{2, 3\}, c- = 0, *L* + = \{4\}, and  
L- = 0 and the second with *c+* = \{2, 3\}, *c-* = 0, L + = \{1, 4\}, and L- = 0.  
Iteration 5\: The lower bound increases to *ztp* = 200.61. On Iteration 5 no more cuts are  
generated, so the cut generation phase terminates.  
Phase 2. Branch-and-bound is now applied. The solution shown in Figure 4.5 is found at  
node 3, and it is proved to be optimal at node 5. Its cost is 202.35.  
If the problem is solved directly by branch-and-bound, a tree containing 129 nodes is  
needed to prove optimality.  
For larger fixed-charge transportation problems, this FCPA is often successful in  
substantially increasing the lower bounds obtained from the linear programming relaxa-  
tion. However, it remains an open question to find and develop separation algorithms for  
other classes of valid inequalities that will make it possible to obtain lower bounds that are  
reliably close to the optimal cost. 506 11.6. Applications of Special-Purpose Algorithms  
35  
30  
25  
15  
5  
5  
Figure 4.5  
A Reformulation of the Single Source Problem \(SFN\)  
The idea of the reformulation is to decompose the flows by destination. We suppose that  
node 1 is the source and let *U* = \{k E *V\: bk* \< 0\}. Thus *b1* = *LkEu lbk* 1. Now let *zuk* be the  
flow in arc *\(i,j\)* destined for node *k* E *U.* The reformulation of\(4.1\) is  
min I *hiJYiJ* + I *cuxu*  
\(i,j\)Ed \(i,j\)Ed  
\(4.12a\) I *zuk* - I *ziik* = 0 for *i* E *V* \\ \{1, k\} and *k* E *U*  
\)Ec5'\(i\) jEJ-\(i\)  
\(4.12b\) - I *Zjkk* = *bk* for k E U  
*jEJ-\(k\)*  
\(4.12\) \(4.12c\) *zuk-min\(* I *bk* I, *uu\)xu* \~0 for \(i, *j\)* E .sli and k E U  
\(4.12d\) I *Zijk- YiJ* =0 for \(i, *j\)* E .sli  
*kEU*  
*YiJ- UijXij* \~0 for \(i, *j\)* E .sli  
*z* ER\~IIUI, yERI'/1, xEBidl.  
The important difference between \(4.12\) and \(4.1\) is the upper-bound constraints  
*zuk* \~ *lbk lxiJ,* which, for fractional X;Jo can restrict the flows more than *Yu* \~ *uuxu* can.  
Proposition 4.4. For SFN, the optimal cost of the linear programming relaxation of  
formulation \(4.12\) is not less than the optimal cost of the linear programming relaxation of  
formulation \(4.1\). and it may be strictly greater.  
Proof By summing the constraints \( 4.12a\), \( 4.12b\) over k for fixed i, we see that if z *iJk*  
is feasible in \(4.12\), then *Yu* = *LkEU Z;Jk* is feasible in \(4.1\). It follows that every solution  
*\(zijk\> yij, xij\)* to the linear programming relaxation of \( 4.12\) gives rise to a feasible solution  
ofthe linear programming relaxation of\(4.1\).  
Example 4.3 shows that the linear programming relaxation of\(4.12\) can yield a larger  
lower bound, and thus it completes the proof. • 4\. Fixed-Charge Network Flow Problems 507  
Example 4.3. The graph and data are shown in Figure 4.6.  
An optimal solution to the linear programming relaxation of \(4.1\) is x 12 = x 13 =  
!, Xz3 = 0, Y12 = YD = 1, Yz3 = 0 with cost\~- An optimal solution to the linear programming  
relaxation of \(4.12\) is X12 = Xz3 = 1, x 13 = 0, Y12 = 2, Yz3 = 1, y 13 = 0 with cost 4. Note that  
the constraint *z* 133 \~ *x* 1 3 makes it infeasible to have *x* 12 = *x* 1 3 = 1\: and y 13 = 1.  
Thus from the point of view ofbounds, \(4.12\) is preferable to \(4.1\). However, \(4.12\) has  
one major disadvantage-its size-which makes it impractical for all but very small  
problems. Benders' decomposition sometimes provides a way around this problem. We  
will illustrate this approach with the Steiner branching problem.  
For uncapacitated single-source problems, the reformulation \( 4.12\) can be simplified  
because the variables *Yu* can be eliminated. Thus for SUFNs we obtain the reformulation  
\(4.13\)  
min\{ I I *huz;jk* + I *cuxu\:* \(4.12a\), \(4.12b\),  
kEU *\(i,j\)Esfi* \(i,j\)Ed  
*ZiJk- bkxiJ* \~ 0 for *\(i,j\)* Ed and k E U, *z* E Rlf'IIUI, *x* E Bldl\}·  
Proposition 4.5. Formulation \(4.13\) is stronger than the formulation \(4.1\) for SUFNs.  
Steiner Branchings  
We now consider the formulation \(4.13\) for the Steiner r-branching problem that we  
defined at the beginning of the section. Recall that we want to find a minimum-weight  
r-branchingon a subset ofnodesD 5; *V.* Let r = 1 and *U* = *D* \\ \{1\}. The formulation \(4.13\)  
yields the constraints for each k E U\:  
\(4.14\)  
for *i* E U \\ \{k\} and k E U  
*fork* E *U*  
*zuk* \~xu z E Rlf'IIUI, x E Bldl.  
for *\(i,j\)* Ed and k E U  
Figure 4.6. The c;1 appear on each arc; uu = 2 and hu = 0 for \(i, j\) E s/1. 508 11.6. Applications of Special-Purpose Algorithms  
Observe that if we fix x, say x = x, \(4.14\) decomposes into 1 *U* 1 separate feasibility  
problems. The kth problem is to determine whether there exists a feasible flow of one unit  
from node 1 to node k with arc capacities\:Xij. By the max-flow-min-cut theorem of Section  
1.3.4, such a flow exists if and only if \:Eies \:EjeV\\s *Xij;;;;.* 1 for all *S* with 1 E *S,* and  
k E V \\ S. Hence if Benders' decomposition is applied to the linear programming  
relaxation of\(4.14\), the resulting master problem is  
min I CijXij  
\(i,j\)Ed  
I Xij;;;;. 1 for all s £ v with 1 E *S, \(V* \\ S\) n u *=I=* 0  
\(i,j\)eo•\(S\)  
which states that every cutset having 1 E *S* and \( *V* \\ S\) n *U* =1= 0 has weight at least 1. For  
x E B1 J41 this is precisely the requirement that the subgraph induced by the arcs with x *ij* = 1  
contains a !-branching that spans the nodes of *D.*  
Reformulations ofthe Uncapacitated Lot-Size Problem \(ULS\)  
We first introduced the uncapacitated lot-size problem \(ULS\) in Section 1.1.5 using the  
formulation  
\(4.15\)  
*T*  
min I \(PtYt + htst + CtXt\)  
1=1  
Y1 =dl +S1  
St-1 + Yt = dt + s, *fort* = 2, ... , *T*  
fort= 1, ... , *T*  
Sr= 0  
*s,yERr, xEBr,*  
and then we reformulated it as an uncapacitated facility location problem.  
We let S s R!T denote the set of feasible solutions to \(4.15\). In Section 11.2.4 we  
described the convex hull of *S,* and in Section 11.5.5 we gave an *O\(T'l\)* dynamic  
programming algorithm for solving ULS. Here we take a different point of view. We  
assume that ULS is part of a more complicated problem. For example, we can add  
capacity constraints on the productions or inventories or assume that the actual problem  
to be solved involves several items. In this case, the model contains a copy ofULS for each  
product, and they are linked together by capacity constraints.  
Our objective is to solve the complicated model by a linear-programming/branch-and-  
bound algorithm. Thus it is important that a tight formulation ofULS, with respect to the  
bounds obtained from linear programming relaxation, be used in the overall model. One  
possibility is to use the description of conv\(S\) given in Section 11.2.4. However, since this  
description contains an exponential number of constraints, an FCPA will be required.  
Here we consider some other options that are derived from the SUFN formulation  
\(4.13\) on the network of Figure 5.2 of Section 11.5.5 \(see Figure 4.7\). 4\. Fixed-Charge Network Flow Problems 509  
Figure 4.7  
To obtain the formulation \(4.13\), we introduce variables *YJk* equal to the amount  
produced in period\) to satisfy demand in period k and s*1k* equal to the stock at the end of  
period *j* destined for period k. This yields  
\(4.16a\)  
\( 4.16\) \( 4.16b\)  
T T T-1 T-1 T  
min I I *PJYJk* + I I *s1 k* + I *c1x 1*  
*\)=1 k=j \)=1 k=j+1 \)=1*  
*Sj-1,k* + *YJk* = *Sjk* for all j and k \> j  
for all k  
To simplify the presentation we will not bother to explicitly state the constraints y *1 1* = *d1*  
and *Y1k* = *s1k fork\>* 1, and we assume *dk* \> 0 *fork=* 1, ... , *T.*  
Now we use s1 k = L\)=1 *Y;k* to eliminate the inventory variables from the objective  
function \<;1nd constraints. This yields  
\( 4.17\)  
T T T  
min I I *\(PJ* + *hi* + · · · + *hk-1\)YJk* + I *CJXJ*  
*\)=1 k=\) \)=1*  
*fork=* 1, ... , *T*  
*YJk* \~ *dkxJ* for all\) and k \~ *j*  
*y* E RI\<T+1 \)12  
, X E BT.  
Since s*1 k* \~ 0 are implied by *Yii* \~ 0, no other conditions are needed. Note that \(4.17\) is  
precisely the formulation \(5.5\) given in Section 1.1.5. As noted there, we obtain a  
formulation ofULS as an uncapacitated facility location problem by letting w *1 k* = *YJk!dk*  
for all\) and k for which *YJk* is defined. This yields 510  
\( 4.18\) \( 4.18a\)  
\(4.18b\)  
11.6. Applications of Special-Purpose Algorithms  
*T T T*  
min I I *\(p1* + *h 1* + · · · + *hk-l\)dkwJk* +I *c1x1*  
J\~l k\~J \)\~I  
*k*  
I w *1 k* = 1 for k = 1, ... , T  
J\~l  
w *1 k* \~xi for all\} and k \~ *j*  
*wE RI\(T+I\)/2*  
*, X* E *BT.*  
Now if we were to solve the formulation \(4.18\), the original variables *Yt.S1 fort* = 1, ... ,  
*T* would be obtained from  
\( 4.19\) T T  
*Yt* =I *Ytk* =I *dkwJk*  
k\~t k\~t  
*T T* t *T* t  
*St* = I *Stk* = I *IY;k* = I I *dkwik·*  
k\~t+l k\~t+l i\~l k\~l+l i\~l  
The observation we need to make is that corresponding to any feasible solution in the  
original variables \(s, y, x\) there can be an infinity of feasible solutions in the variables  
\( w, x\) with the same cost. For example, corresponding to the solution shown in Figure 4.8,  
we note that x *1* = x*2* = 1, x3 = 0, w" = 1, *W12* = W23 = \~. *W13* = *W22* = 1-\~ is a feasible  
solution to \(4.17\) of cost c1 + c2 + 2p *1* + p *2* + h 1 + h *2* for any\~\~ 0.  
We claim that we will still have a valid formulation of ULS by adding the constraints  
\(4.20\) *w1r* \~ *w1,r-1* \~ · · · \~ *w11* for *j* = 1, ... , T- 1  
to \( 4.18\). In other words, we claim that the formulation  
*T T T*  
min I I *\(p1* + *h 1* + · · · + *hk-l\)dkwJk* +I *CJXJ*  
j\~l k\~J J\~l  
\( 4.21\) \( 4.21a\)  
*k*  
J\~l  
I w1 k = 1 for k = 1, ... , T  
\(4.21b\)  
*wJr* \~ *wJ,T-1* \~ · · · \~ *w 11* \~ x*1* for j = 1, ... , *T*  
*w* E *RI\(T+I\)/2*  
*, X* E *BT.*  
is valid for ULS.  
To establish this claim, note that we have already shown in Section 11.5.5 that every  
extreme point of conv\(S\) is of the following form\: For some subset *\{i* 1, ••• , i,\} c\:; \{1, ... ,  
n\} with 1 = *i 1* \< i2 \< · · · \< i, we obtain  
X;,= 1 for *l* = 1, ... , *r, x*1 E \{0, 1\} otherwise  
i/+!-1  
Y;, = I *dt,* y*1* = 0 otherwise.  
*t=ij*  
Using \(4.19\), this corresponds to a feasible solution *\(x, w\)* of\(4.21\) with the same values of  
*x1* for *j* = 1, ... , *T,* and with w;,*1* = 1 *fort=* i 1 , ••• , *i1 +1* -1 and l = 1, ... , *r* and *Wu* = 0  
otherwise. 4. Fixed-Charge Network Flow Problems 511  
Figure 4.8  
Now let Q\( QLP\) be the image in *\(y,* s, x \)-space under transformation \( 4.19\) of the points  
*\(w, x\)* feasible in \(4.21\) \[the linear programming relaxation of \(4.21\)\]. The above discus-  
sion shows that\:  
Proposition 4.6. *S* = Q *\(and* conv\(S\) s QLP\)o  
Our interest in \( 4.21\) arises from a final formulation ofULS as a minimum-weight path  
problem. *Fork* = 2, ... , **T,** we subtract the *\(k* - I\)st constraint from the kth constraint in  
\(4.21a\). This leads to the T- 1 constraints  
\(4.22\)  
k-1  
*wkk-* I *\(wj*0*k-l* - *wjk\)* = 0 *fork=* 2, ... , *T.*  
j\~l  
Now define *Zjk* = *Wjk* - *wjok+l* \~ 0 for 1 ,;;;;; *j* ,;;;;; k \< T, and define *zjT* = *wjT* \~ 0 for *j* = 1,  
0 •• , *T.* Then *wkk* = *r.T\:\:,k zk1 ,* and we obtain the reformulation  
\(4.23a\) = 1  
k-1 T  
\(4.23\) \(4.23b\) -I *zj*0*k-l* +I *Zkt* = 0 *fork=* 2,. 0. , *T*  
j\~l l\~k  
\(4.23c\) ,;;;;; 0 *fork* = 1, ... , *T*  
where \(4.23c\) comes from *wkk,;;;;; Xk* for all *k.*  
If *ck* \~ 0, then in the linear programming relaxation of\(4.23\) we can take \(4o23c\) as an  
equality. Substituting this equality into \(4.23a\) and \(4.23b\) yields 512  
11.6. Applications of Special-Purpose Algorithms  
\(4.24a\)  
= 1  
\(4.24\)  
\(4.24b\)  
k-1  
-I *zj,k-1* + *xk* = 0 *fork* = 2, ... , *T*  
j\~1  
\(4.24c\)  
*T*  
t\~k  
I *zkt* - *xk* = 0 *fork=* 1, ... , *T.*  
Now observe that by constructing a digraph with node set \{ 1, ... , *T* + 1, 1', ... , T'\}  
and by letting *Xk* be the flow from *k* to *k'* and letting *Zjk* be the flow *from\}'* to *k* + 1, then  
\(4.24b\) and \(4.24c\) are the flow conservation equations at nodes k = 2, ... , T and at  
nodes k = 1', ... , *T',* respectively. See Figure 4.9 for T = 3.  
Moreover, if *ck* \< 0 we can set *Xk* = 1, and \(4.23c\) is superfluous. In terms of the graph,  
the arc corresponding to *xk* is deleted, and the nodes *k* and *k'* are coalesced.  
Proposition 4.7. The linear programming relaxations o/\(4.21\) and \(4.23\) have optimal  
solutions with x E BT for any objective function \(p, h, c\).  
An important consequence of Propositions 4.6 and 4. 7 concerns the polyhedron QLP  
representing the set of feasible solutions to the linear programming relaxation of\(4.21\) in  
terms of the original variables. ·  
Theorem 4.8. QLP = conv\(S\).  
Proof We show that QLP \~ conv\(S\). If not, let \(y\*, s\*, x\*\) obtained from  
\(y\*, s\*, x\*, w\*\) be an extreme point of QLP that is not in conv\(S\). There exists an objective  
function *\(p, h,* c\) for which \(y\*, s\*, x\*\) is the unique optimal solution to  
min\{py + hs +ex\: \(y, s, x\) E QLp\}. But this implies that \(x\*, w\*\) is a feasible solution to  
\(4.21\) whose objective value is less than that of any point \(x, w\) corresponding to an  
extreme point of conv\(S\). Thus x\* \$. B *T,* which contradicts Proposition 4. 7. Hence QLP \~  
conv\(S\). QLP 2 conv\(S\) was shown in Proposition 4.6. •  
It can be shown that Proposition 4.7 also holds for formulation \(4.18\). Thus we can  
conclude that the corresponding version of Theorem 4.8 holds for formulation \(4.18\).  
We now consider the problem stated at the beginning of this section of choosing a  
formulation to embed in a more complicated model. To formalize the problem, we wish to  
solve  
\(4.25\) z' = min\{py + hs +ex\: \(y, s, x\) E S n P'\},  
where P' \~ R!T represents the set of complicating constraints. For each of the three  
models \( 4.18\), \( 4.21\), \( 4.23\), it is easy to represent the constraints of P' in terms of the new  
Figure 4.9 5\. Applications of Basis Reduction  
Table 4.1.  
513  
Formulation  
Number of variables Number of constraints  
\(4.18\) and \(4.21\)  
\(4.23\)  
\(4.15\) & conv\(S\)  
variables, so we then obtain three reformulations of problem \( 4.25\). The values of their  
respective linear programming relaxations are denoted by zLp for i = 1, 2, 3.  
For each of these formulations we are interested in two things, namely, the tightness of  
the linear programming relaxation and the size of the formulation. Considering the  
bounds first, we have, by Theorem 4.8 and the identical result for formulation \(4.18\), the  
following proposition\:  
Proposition 4.9. zl\_p = min\{py + hs +ex\: \(y, s, x\) E conv\(S\) n P'\}for i = 1, 2, 3.  
Hence each of the formulations is as tight as it can be made without studying the  
structure of the complicating constraints *P'.* Therefore to choose among the formulations  
we turn to the question of size, and we consider the number of variables and constraints in  
each formulation \(see Table 4.1\). The last formulation shown in Table 4.1 is to add the  
facet-defining inequalities described in Section 11.2.4 to the formulation \(4.15\) of ULS.  
The figures in the table suggest that a model based on \(4.23\) significantly dominates  
formulations \(4.18\) and \(4.21\) with respect to the number of constraints. Recently,  
problems with 200 items and *T* = 10 periods have been solved by a standard linear-  
programming/branch-and-bound algorithm using reformulation \(4.23\).  
The last formulation, which involves only O\(T\) variables but an exponential number  
of constraints, might be competitive with \(4.23\) using an FCP algorithm because the  
number of facet-defining inequalities needed at an optimal extreme point is bounded by  
the number of variables.  
5\. APPLICATIONS OF BASIS REDUCTION  
The use of basis reduction in lattices is new to integer programming. To indicate its  
potential, we outline two applications. The first is a simple heuristic algorithm to find a  
feasible solution to a 0-1 equality knapsack constraint. The second is an algorithm for  
integer programming that is polynomial for fixed n. Although this is an important  
theoretical result, the algorithm is not practical. The result has, however, motivated the  
application ofbasis reduction techniques to a variety of problems.  
The Subset Sum Problem  
Here we consider the .N'\:?P-hard problem of finding a feasible solution to a 0-1 knapsack  
equality constraint\:  
n  
\(5.1\)  
I *ajxj* =M,  
j\~I  
This problem is of particular interest in cryptography where problems of the form \( 5.1\) are  
constructed to have a unique solution that corresponds to a message to be transmitted.  
In such a system the coefficients *aj* for *j* EN are public information, the message 514 11.6. Applications of Special-Purpose Algorithms  
transmitted is M, and the problem \(5.1\) must have "very large" coefficients and be  
"impossible" to solve, except by the receiver who knows a trick for any *M.*  
Here we describe a fast heuristic algorithm for \(5.1\), which uses the reduced basis  
algorithm of Section 1.7.5. Let  
c1 = \( *I* 0\)  
-a M'  
where *a* = *\(a* I. ... , *an\)* and I is then x n identity matrix. Consider the lattice L\( C1\) s *Rn+l*  
given by \{v E zn+l\: v = C 1y, y E zn+l\}. Now observe that if *X* E Bn is a feasible solution to  
\(5.1\), then  
is an element of the lattice. Moreover, v 1 is a short vector in *L\(* C1\) because il v 111 \:S; n vz,  
which is much smaller than the bound given in Theorem 5.5 of Section 1.7 .5.  
In addition, by setting *x1* = 1 - *x1* for *j* = 1, ... , *n* and by treating  
n n  
2\: a*1x1* = M' = 2\: a*1* - M,  
J\~l J\~l  
similarly, we see that v2 = @ is a short vector in the associated lattice *L\(* C2\), where  
Now min\(llv 1ll, llv2 11\) \:S; \(n/2\)lf2•  
The idea of the algorithm is that if *v;* is a very short, and possibly the shortest, vector in  
*L\(* C\) for *i* = 1 or 2, there is a good chance that it will appear in a reduced basis for *L\(* C;\).  
Thus it suffices to check whether the reduced basis contains a vector of the form\(±;\) with  
xEBn.  
The Reduced Basis Algorithm to Find a Solution of \(5.1\)  
Step 1\: Consider the lattice L\( C1\) s *Rn+l,* where C1 is the matrix given above.  
Step 2\: Find a reduced basis *ir* of L\( C\).  
Step 3\: Check if *13\** contains a column of the form\(±\~'\) with x 1 E Bn. If so, stop. x 1 solves  
\(5.1\).  
Step 4\: Repeat Steps 1 to 3 with C replaced by C2• If a vector x 2 E Bn is found, 1 - x 2  
solves \(5.1\). Otherwise, stop. No solution has been found.  
The reduced basis algorithm has a very high probability of finding a feasible solution  
for certain classes of knapsack problems. We define the density *d\(a\)* of a set of weights  
*\(ab* ... *'an\)* by  
*d\(a\)= .,-------,-----n------,--* log\(max1 aJf 5\. Applications of Basis Reduction 515  
It can be shown, under appropriate distribution assumptions, that there exist constants  
a and p such that\:  
a. For "nearly all" feasible instances \(5.1\) with *d\(a\) \<a,* \(5.1\) has a unique solution  
x E *Bn;* this solution is the shortest nonzero vector in *L\(C\).*  
b. For "nearly all" feasible instances with *d\(a\)* \< P/n, the reduced basis algorithm  
finds a solution.  
The proof of statement b is demonstrated by showing that all other vectors in the lattice  
*L\(C\)* are much longer than v1 = \(\~\). In particular, if llwll \~ 2"-1llv1ll for all  
wE *L\(C\)* \\ \{0, v1\}, then we know by Theorem 5.5\(iii\) of Section 1.7.5 that ±v 1 is in the  
reduced basis.  
The Linear Inequality Integer Feasibility Problem  
Here we outline an algorithm for the linear inequality integer feasibility problem  
\(5.2\) Find x E P n zn or show P n zn = 0,  
where *P* = \{x E *R"\:* Ax\~ b\} and *n* is fixed. From Section 1.5.4, it can be assumed that if  
*p \** 0 there exists *WA,b* such that *IXJ* I \~ *WA,b* for some *X* E *p* n zn.  
The algorithm is essentially enumerative. If we could show, for all x E *P,* that lx*1* 1 \~ y  
where y is any function polynomial in log BA,b, where BA,b is the largest coefficient in *\(A, b\),*  
then we would immediately obtain a polynomial algorithm by enumerating the *\(2y* + lY  
points with *IXj* I \~ *y* and *X* E zn. Since the bound *WA,b* is not polynomial in log *eA,b,* this  
simple approach does not work. However, by using a reduced basis it is possible to obtain  
a polynomial-time enumeration algorithm.  
The first important concept in the algorithm is the idea of a family of polytopes being  
"round". Let *S\(p, r\)* be ann-dimensional sphere with center *p* and radius *r.*  
Definition 5.1. that  
A family of full-dimensional polytopes in Rn is round if there exist a  
function c I such that for each p in the family, there exist rationals p E R n' r' q E R l such  
i. *S\(p, r\)* s; P s; *S\(p, q\)* and  
ii. *qjr* \~ c1•  
To motivate the algorithm, let us first consider the solution of problem \(5.2\) for a full-  
dimensional and round family of polytopes. Here we will see that straightforward  
enumeration is polynomial. There are two cases to be considered, as demonstrated in  
Figure 5.1.  
*Case 1. r* \~ \~n 112 • In this case, the unit hypercube with center pis contained in *S\(p, r\)* and  
hence in *P.* Now let *p\** be a closest integer point *top;* that is, *p*1 = \[p1j + jj for *j EN,*  
*pj=* \[PJJ ifjj \~ \~' *andpj=* \[p1j + 1 otherwise. Thenp\* E *P* n zn, and hence \(5.2\) is solved. 516 11.6. Applications of Special-Purpose Algorithms  
# •  
# •  
• • • •  
• • • • • •  
p  
• • \~s;,,, •  
• • • •  
• • • • • •  
# • • • •  
# •  
\(a\) \(b\)  
Figure 5 .1. \(a\) Case I\: The closest integer point p\* top is feasible. \(b\) Case 2. Enumerate the integer points in  
*S\(p,* q\).  
*Case 2. r* \< \~ *nlf2•* In this case, *q* \< \~n 1 1 2 c 1 • But now because *P* \~ *S\(p, q\),* we have that if  
X E *P,* then  
for all\). Thus total enumeration gives a polynomial algorithm for fixed n.  
Now we indicate how for any polytope P, we can find a linear transformation  
K\: Rn ... Rn depending on P such that the transformed family of polytopes \{K\(P\)\} is round.  
For simplicity, we consider only the case of full-dimensional polytopes.  
Using linear programming and Gaussian elimination, we start by finding n + 1 affinely  
independent extreme points \{vi\}7\~o of P. The convex hull of n + 1 affinely independent  
points in *R"* is called *ann-simplex.* Thus, \{v0  
, *v1*  
*,* ••• , v"\} is ann-simplex *Q* \~ *P.*  
Next we find a "large" n-simplex *Q'* \~ *P.* In particular, for each *i* = 0, 1, ... , n, we  
attempt tO find a new extreme point Vi of *p* SO that the Simplex \{v0  
, V*1*  
*, ••• ,* Vi-I, Vi, Vi+l,  
... , v"\} has a volume more than 50% larger than that of *Q.* To do this, for each *i* we find  
the facet nix= ai of the simplex opposite the vertex vi. We find ni ERn by using Gaussian  
elimination to solve the linear system nivi = ai for all *j \** i. We then solve the linear  
program max\{nix\: x E P\} whose optimal solution is vi.  
If I nivi- ai I \> \~ 1 nivi- ai 1, we replace vi by vi and start again with a larger simplex. If  
not, we replace ni by -ni and resolve the linear program. Every time the simplex changes,  
its volume increases by at least 50% \(see Figure 5.2a\). Thus the number oflinear programs  
that need to be solved cannot be too large. We stop when no change occurs for any *i* = 0,  
1, ... , n. The final simplex *Q'* = \{v0  
, *v* 1  
, ••• , *vn\}* is a "large" simplex within *P.* Further-  
more, we know that Plies inside a polytope with no more than 2n + 2 facets, namely, the  
polytope  
\(see Figure 5.2.b\). 5\. Applications of Basis Reduction 517  
\(a\)  
e d  
\(b\)  
Now a linear transformation K can be found so that K\(Q'\) becomes a regular  
n-simplex. Clearly K\(Q'\) \~ K\(P\) \~ K\(P\*\).  
What has been achieved? Taking p = \[1/\(n + 1\)\] \~?=0 *K\(vi\),* we can construct a hyper-  
sphere S\(p, r\) inside K\(Q'\). K\(P\*\) has no more than 2n + 2 facets, so its vertices can be  
computed, and we can construct a hypersphere S\(p, q\) containing K\(P\*\). Hence  
S\(p, r\) \~ K\(P\) \~ S\(p, q\). Simple calculations give that qfr \< kn 312 for some constant k.  
Figure 5.3 shows Figure 5.2 after the transformation K; it also shows the spheres S\(p, r\)  
andS\(p, q\).  
Although many technical details have been omitted, we have motivated the result that  
the family of all n-dimensional polytopes can be made round by a suitable linear  
transformation.  
Proposition 5.1. There exists a constant c1 such that for any n-dimensional polytope  
P = \{x ERn\: Ax =\:;; b\}, there exists a rational nonsingular matrix K and rationals  
p ERn, r E R.!. and q E R.!. such that K\(P\) = \{y ERn\: AK-1y =\:;; b\} satisfies  
i. S\(p, r\) \~ K\(P\) \~ S\(p, q\) and  
ii. qfr =\:;; c1•  
K\(fl  
K\(e\)  
Figure 5.3 518 11.6. Applications of Special-Purpose Algorithms  
The initial problem \(5.2\) has now been transformed to the problem  
\(5.3\) Find a vector y E *K\(P\)* n *L\(K\),*  
where *L\(K\}* is the lattice with basis *K.* In addition, we have rationals *p, q,* and *r* such that i  
and ii of Proposition 5.1 hold. This resembles the earlier situation in that we have a family  
of polytopes that is round, but now *L\(K\)* has replaced zn.  
The second transformation we introduce involves finding a reduced basis B for the  
lattice *L\(K\).* Problem \(5.2\) is now equivalent to the problem  
\(5.4\) Find a vector y E *K\(P\)* n *L\(B\).*  
The geometry for *n* = 2 is shown in Figure 5.4, where *S\(p, r\)* s;;; *K\(P\)* s;;; S\(p, *q\)* and the  
points of the lattice *L\(B\)* are given.  
We will now show how \(5.4\) can be solved by enumeration in polynomial time. As  
before, the algorithm breaks up into the case where *r* is large and the case where *·r* is small.  
Previously when *r* was large, we used rounding to find a lattice point "close" *top.* Now we  
will use a simple construction underlying the proof of the following proposition.  
Proposition 5.2. Given a lattice L\(B\) and p ERn, there exists z E L\(B\) such that  
liz- Pii 2 \~ \~L.f..t *llbjll*2  
.  
We now consider the two cases of r large and small. Without loss of generality we  
assume that the columns of Bare ordered so that maxj=t ... . *,n llbjll* = *llbnll·*  
# •  
# •  
0 bl  
Figure 5.4. Note that 0 denotes points of *L\(B\).* 5\. Applications of Basis Reduction Case 1. Ifr;\:\:\: 1nl!2llbnll, apply Proposition 5.2 to find a pointy E L\(B\) such that  
n  
IIY- Pll 2 .5 ± *L* llbjll 2 .5 ±nllbnll2•  
j\~l  
519  
Since  
it follows that y E *S\(p, r\)* \~ K\(P\). Therefore y E K\(P\) n L\(K\), and *x* = K-1 y is feasible in  
\(5.2\).  
Case 2. If r \< 1n1!2llbnll, we will show that it is possible to enumerate in the direc-  
tion of bn and only test feasibility for a polynomial number of points. Let L n-I = L\(b h ... ,  
b n-I \), and let H"-1 be the associated subspace. We let h denote the distance from b n to H"-1  
.  
By Definition 5.2 of Section 1.7.5, we have det B = h det\(b1, ... , bn-I\). Since B is a  
reduced basis, we know from Theorem 5.5iii of that section that  
n  
j\~l  
llllbjll .5 2n\(n-!\)14d\(L\) = 2n\(n-!\)l4h det\(bl, ... , bn-I\),  
and we know from Hadamard's inequality that det\(bh ... , bn-I\) .5 *ITJ\:i* llbjll. Therefore,  
by canceling terms we obtain  
Nowobservethatify E L\(B\)withy = Bz,z E Z", theny = yn-I + bnzn,Zn E 2 1  
, where  
yn-! E L n-!, and hence y E Hn-! + bnZn for some Zn E 2 1  
. Hn-! + bnZn, Zn E 2 1  
, is a family  
of hyperplanes separated by a distance *h.* The number *y* of such hyperplanes that can  
intersect *S\(p, q\)* is no more than *2qjh* + 1 \(see Figure 5.5\).  
--.........\_ *S\(p,* q\)  
# \~  
/ \~  
*I* \\  
*I* \\  
• *p*  
# \~\}h  
\\ *J* Hn-1 +2bn  
/ Hn-1+bn  
\~ / Hn-1  
# ""'  
Hn-l-bn  
Figure 5.5 520 11.6. Applications of Special-Purpose Algorithms  
We now have that y s 2q/h + 1 and q/r s c1 because K\(P\) is round, h ;;\:\:\: llbnll2-n\(n-I\)/4  
because the basis is reduced, and *r* \< \~n 112 llbnll by assumption. Thus we have  
2q 2r nlf2llb II  
Y - 1 \<-\<-cl \< -h-h - h -  
n cl \< nlf2c12n\(n-1\)/4  
.  
Therefore it is possible to enumerate over these *y* possible values of z E Z 1  
, and each of the  
resulting problems reduces to finding an integer point in a polytope whose dimension is no  
greater than n - 1.  
In integer programming terms we have shown that  
So we have outlined a basic inductive step. Either *P* is not full-dimensional, or we find a  
point in *P* n zn, or we reduce to a polynomial number of similar problems in n - 1  
variables. It can be verified that for fixed n, all the steps indicated above can be carried out  
in time that is polynomial in the input length. Hence, we obtain Lenstra's theorem\:  
Theorem 5.3. feasibility problem *\(5.2\).*  
For fixed n, there is a polynomial algorithm for the linear inequality integer  
Using bisection on the objective function value and the bounds given in Theorem 4.1 of  
Section 1.5.4, Theorem 5.3 leads immediately to a result for integer programs.  
Theorem 5.4. problem.  
For fixed n, there is a polynomial algorithm for the integer programming  
Another immediate consequence of Theorem 5.3 is\:  
Theorem 5.5. For fixed m, there is a polynomial algorithm for the linear inequality  
integer feasibility problem \(5.2\) and for the integer programming problem.  
Proof If m ;;\:\:\: n, the claim is immediate. If m \< n, we find the Hermite normal form  
of A given by \(H, 0\) = AC. This can be done in polynomial time \(see Section 1.7.4\). Now  
if y = *c-*1 x, then \{x\: Ax s b, x E zn\} \* 0 if and only if \{y\: ACy s b, y E zn\} \* 0.  
Hence the problem is reduced to the feasibility problem for \{u E zm\: Hu s b \}, where  
u = \(y\~, ... , Ym\). Thus by Theorems 5.3 and 5.4 the claim follows. •  
**6.** NOTES  
Section 11.6.1  
The dynamic programming recursion \(1.2\) and the asymptotic properties given in  
Propositions 1.1 and 1.2 appeared in Gilmore and Gomory \(1966\); also see Shapiro and  
Wagner \(1967\).  
The superadditive dual algorithm is due to Johnson \(1973, 1980b\). 6. Notes 521  
The heuristic analysis presented here also applies with some small variations to the 0-1  
knapsack problem, and the references cited analyze either the integer or 0-1 knapsack  
problem or both. Sahni \(1975\) combined the greedy heuristic with enumeration to obtain  
a polynomial approximation scheme for the knapsack problem. Ibarra and Kim \(1975\)  
used scaling to obtain a fully polynomial approximation scheme. The scaling/rounding  
heuristic is due to Lawler \(1979\). By adding the rounding feature, Lawler improved the  
running time of Ibarra and Kim's heuristic by a multiplicative factor of epsilon. A  
different fully polynomial approximation scheme for the 0-1 knapsack problem was given  
by Magazine and Oguz \(1981\).  
Gomory \(1965\) gave a dynamic programming algorithm for solving the group problem.  
Shortest-path algorithms for the group problem were given by Shapiro \(1968a\), Glover  
\(1969\), and Hu \(1970\). A comparison of algorithms for solving the group problem was  
presented by Chen and Zionts \(1976\).  
Shapiro \(1968b\) used the group problem and branch-and-bound to obtain an algorithm  
for general pure-integer programs \[also see Garry and Shapiro \(1971\), Garry, Shapiro, and  
Wolsey \(1972\), and Crowder and Johnson \(1973\)\]. An extensive computational study with  
this type of algorithm was carried out by Garry, Northup, and Shapiro \(1973\). A shortest-  
path enumeration scheme was described in general terms by Lawler \(1972\) and was  
developed in the context of the group/branch-and-bound algorithm by Wolsey \(1973\).  
The increasing group algorithm is a variant of an algorithm of Bell and Shapiro \(1977\).  
Shapiro \(1971\), Fisher and Shapiro \(1974\), Bell and Fisher \(1975\), and Fisher, Northup,  
and Shapiro \(1975\) investigated how the group theoretic and Lagrangian dual approaches  
can be combined to solve general integer programs.  
Kolesar \(1967\) gave one of the first branch-and-bound algorithms for the 0-1 knapsack  
problem. The computational efficiency of the basic algorithm has been improved by  
many researchers who have refined the node selection, branching and pruning rules, the  
variable fixing tests, and the method of solving the linear programming relaxation \[see,  
among others, lngargiola and Karsh \(1973\), Fayard and Plateau \(1975, 1982\), Lauriere  
\(1978\), Suhl \(1978\), and Balas and Zemel \(1980\)\]. The presentation given here is largely  
based on the article by Lauriere \(1978\). Martello and Toth \(1979\) have given a comprehen-  
sive survey of methodology and an empirical comparison of algorithms. Another survey  
was given by Salkin and de Kluyver \(1975\).  
Despite the excellent empirical results that have been obtained in solving knapsack  
problems by branch-and-bound, there are difficult families of knapsack problems for  
which any branch-and-bound algorithm with linear programming relaxations will enu-  
merate an exponential number of nodes of the search tree \[see Chvatal \(1980\)\].  
Some of the approaches and results for knapsack problems have been generalized to  
deal with problems having more than one knapsack-type constraint \(i.e., the multidimen-  
sional knapsack problem\). Polynomial approximation schemes have been obtained by  
Chandra et al. \(1976\) and Frieze and Clarke \(1984\). However, the problem of finding a  
fully polynomial approximation scheme for the multidimensional knapsack problem is  
.N2P-hard. Korte and Schrader \(1980\) showed this for the 0-1 problem, and Magazine and  
Chern \(1984\) obtained the result for bounded and unbounded integer variables. Various  
practical heuristics have been proposed and evaluated \[see, e.g., Loulou and Michaelides  
\(1979\), and Martello and Toth \(1981b\)\]. Martello and Toth \(1981a\) also have given a  
branch-and-bound algorithm.  
There have also been many studies of the knapsack problem with general upper-bound  
constraints including heuristics, branch-and-bound algorithms, and efficient methods for  
solving the linear programming relaxation \[see Frieze \(1976\), Sinha and Zoltners \(1979\),  
Zemel \(1980, 1984\), Johnson and Padberg \(1981\), and Dyer \(1984\)\]. 522 11.6. Applications of Special-Purpose Algorithms  
Section 11.6.2  
The heuristic for BIP, called pivot and complement, is due to Balas and Martin \(1980\).  
The FCP/branch-and-bound algorithm is from Crowder, Johnson and Padberg \(1983\)  
\[also see Johnson and Padberg \(1983\), Johnson, Kostreva, and Suhl \(1985\), and Hoffman  
and Padberg \(1985\)\]. The algorithm has been implemented in the mathematical program-  
ming systems PIPEX ofiBM, MPSARX ofScicon, and XMP ofMarsten \(1981\). Example  
2.4 is a test problem from Crowder et al. \(1983\), and the results were obtained using the  
MPSARX system \[see Van Roy and Wolsey \(1987\)\].  
Earlier approaches for solving BIPs emphasized implicit enumeration \[see Balas \(1965\),  
Geoffrion \(1967\), and Petersen \(1967\)\]. This type of algorithm was improved by the  
addition of surrogate constraints \[see Balas \(1967\), Glover \(1968c\), and Geoffrion \(1969\)\].  
Spielberg \(1979\) gave a survey of these algorithms.  
Specialized versions of the implicit enumeration approach have been used to solve set-  
partitioning and -covering problems \[see Pierce \(1968\), Garfinkel and Nemhauser \(1969\),  
Pierce and Lasky \(1973\), and Marsten \(1974\)\].  
Several implicit enumeration and branch-and-bound algorithms for set-covering and  
-partitioning problems have incorporated special techniques fqr solving the linear pro-  
gramming relaxation and tightening it. Etcheberry \(1977\) gave an implicit enumeration  
algorithm that uses Lagrangian relaxation and subgradient optimization. Nemhauser,  
Trotter, and Nauss \(1974\) used a combinatorial relaxation based on finding a minimum-  
weight chain decomposition in a partially ordered set. This relaxation can be solved as a  
network flow problem. Combined with Lagrangian duality to accommodate side con-  
straints, it yields the same bound as the linear programming relaxation. Nemhauser and  
Weber \(1979\) used a weighted matching problem relaxation that, when combined with  
Lagrangian duality to accommodate side constraints, yields a tighter bound than the linear  
programming relaxation. Ali and Thiagarajan \(1986\) reformulated the set-covering prob-  
lem as a network flow problem with side constraints. Again, Lagrangian duality is used to  
accommodate the side constraints. This relaxation yields a bound equal to the bound  
obtained from the linear programming relaxation. Marsten and Shepardson \(1981\) gave a  
linear programming based branch-and-bound algorithm.  
Fulkerson, Nemhauser, and Trotter \(1974\) gave a family of set-covering problems  
arising in the statistical design of experiments that are difficult to solve. Avis \(1980\) showed  
that these problems cannot be solved in polynomial time by branch-and-bound algo-  
rithms that use linear programming relaxations.  
Some work has been done on using Gomory cuts to solve covering and partitioning  
problems \[see, e.g., Balinski and Quandt \(1964\) and Salkin and Koncal \(1973\)\]. Other  
cutting-plane approaches have been investigated by Bellmore and Ratliff\(1971\) and Balas  
\(1980\). Balas' cutting-plane approach, which is based on conditional bounds, has been  
implemented into an algorithm that also uses heuristics and subgradient optimization \[see  
Balas and Ho \(1980\)\].  
Another approach to solving the set-partitioning problem uses Proposition 2.1. Balas  
and Padberg \(1972, 1975\) have given an algorithm that starts with an integer feasible  
solution and then uses pivoting to obtain a sequence of integer solutions of increasing  
weight which terminates with an optimal solution. The sequence is short, and its length is  
bounded by m, but exponential time may be required to find the appropriate pivots. Ikura  
and Nemhauser \(1985\) extended these ideas on pivoting from an integer solution to an  
adjacent one. For the set-packing problem, they showed that starting with any integer  
feasible solution and an associated unimodular basis matrix \(see Section III.1.2\), there  
exists a short sequence of primal simplex pivots, where each pivot element equals 1, to an 6. Notes 523  
optimal solution. This result also applies to set partitioning since, as shown by Lemke et al.  
\(1971\), by a linear transformation ofthe weight vector, a set-partitioning problem can be  
reformulated as either a set-packing or a set-covering problem.  
Heuristics have been used to obtain good solutions to very large set-covering problems  
\[see Baker \(1981\) and Baker and Fisher \(1981\)\]. Worst-case analyses of the bounds between  
heuristic, optimal, and dual solutions have been given by Lovasz \(1975\), Chvatal \(1979\),  
Dobson \(1982\), Fisher and Wolsey \(1982\), Hochbaum \(1982\), and Wolsey \(1982a\). The  
analysis of the greedy heuristic given here comes from Fisher and Wolsey.  
Crew-scheduling problems have been a fertile application area for set-covering and  
-partitioning models \[see Arabeyre et al. \(1969\) and Marsten and Shepardson \(1981\)\].  
Theorem 2. 7 is due to Nemhauser and Trotter \(1975\). They used this result to develop a  
branch-and-bound algorithm for the node-packing problem. This property has been  
studied further by Picard and Queyranne \(1977\). Grimmett and Pulleyblank \(1985\)  
showed that in large random graphs, LNP with a cardinality objective function is very  
unlikely to have an optimal solution with any of the variables equal to an integer.  
Nemhauser and Sigismondi \(1988\) gave an FCP /branch-and-bound algorithm for node  
and set packing. The algorithm uses classes of facets for the convex hull of node packings  
\[see Padberg \(1973, 1975a, 1977\), Nemhauser and Trotter \(1974\), and Trotter \(1975\)\].  
Facets of the convex hull of set covers have been studied by Sassano \(1985\), Balas and  
Ng \(1985\), and Cornuejols and Sassano \(1986\).  
The literature on packing and covering problems for which the polyhedron of the linear  
programming relaxation has only integer extreme points will be presented in the notes for  
Chapter Ill. I.  
Surveys on covering and partitioning problems have been given by Garfinkel and  
Nemhauser \(1972b\), Christofides and Korman \(1975\), Balas and Padberg \(1976\), and  
Padberg \(1979\). An annotated bibliography on combinatorial aspects of packing and  
covering was given by Trotter \(1985\).  
Section 11.6.3  
All of the material presented in this section and much more can be found in the collection  
of survey articles on the traveling salesman problem, edited by Lawler, Lenstra, Rinnooy  
Kan, and Shmoys \[LLRS \(1985\)\].  
Although the section treats the symmetric traveling salesman problem \(STSP\), many of  
the articles cited here deal with the slightly more general asymmetric problem \(ATSP\)-  
that is, the problem on a directed graph. We generally do not distinguish between these two  
versions in the citations and just use the acronym TSP.  
As we have observed previously, the linear programming relaxation of \(3.3\)-\(3.5\) was  
introduced by Dantzig, Fulkerson, and Johnson \(1954\). The integer 2-matching or  
assignment problem relaxation was used by Eastman \(1958\) and Little et al. \(1963\).  
Although this is the weakest of our bounds, Balas and Toth \(1985\) reported a statistical  
experiment with 400 randomly generated problems in which the ratio of the cost of an  
optimal assignment solution to the cost of an optimal TSP solution is 99.2%. A modified  
assignment problem relaxation that tends to avoid the difficulty of creating numerous  
small subtours was given by Jonker, Deleve et al. \(1980\). Bellmore and Malone \(1971\)  
introduced the 2-matching relaxation. A tighter relaxation is the 2-matching problem  
where triangles are excluded. Cornuejols and Pulleyblank \(1982, 1983\) gave a polynomial-  
time algorithm for the integer 2-matching problem where triangles are excluded. They also 524 11.6. Applications of Special-Purpose Algorithms  
showed that the problem of finding 2-matchings with no circuits of size 5 or smaller is  
.Ng\}l-hard \[also see Cornuejols, Naddef, and Pulleyblank \(1983\)\].  
Held and Karp \(1970, 1971\) introduced the 1-tree relaxation and, by combining it with a  
Lagrangian relaxation with respect to the degree constraints, arrived at the relaxation  
\(3.11\). Related work on this approach was done by Christofides \(1970\) and Helbig-Hansen  
and Krarup \(1974\). Balas and Christofides \(1981\) used the 2-matching relaxation in  
conjunction with a Lagrangian relaxation with respect to the subtour elimination con-  
straints to obtain the relaxation \(3.13\).  
The tightest relaxations have been used by Padberg and Grotschel \(1985\) and Padberg  
and Rinaldi \(1987a,b\). Their cutting-plane algorithms use the degree constraints, all of the  
active subtour elimination constraints, and some 2-matching and comb inequalities.  
The LLRS collection of articles contains three surveys on the analysis of heuristics for  
the TSP\: empirical analysis by Golden and Stewart \(1985\), worst-case analysis by Johnson  
and Papadimitriou \(1985b\), and probabilistic analysis by Karp and Steele \(1985\) \[also see  
Golden et al. \(1980\)\].  
Interchange heuristics for the TSP were developed by Croes \(1958\), Lin \(1965\), and Lin  
and Kernighan \(1973\). The k-interchange heuristic of Lin and Kernighan, where k varies  
by iteration, has proved to be very powerful. It is, however, much more complicated than  
using k = 2 or 3 and repeating the procedure from several initial tours. An alternative way  
of using an interchange heuristic is to combine it with simulated annealing \[see Bonomi  
and Lutton \(1984\)\].  
Insertion procedures were introduced by Clarke and Wright \(1964\). Some rules for  
choosing the next node to insert and where to insert it are described by Rosenkrantz et al.  
\(1977\) and Norback and Love \(1979\).  
Several composite heuristics that begin with a tour construction procedure followed by  
an interchange procedure were investigated by Golden and Stewart \(1985\). They also  
discussed the statistical comparison of heuristics.  
Karp \(1972\) proved that determining whether an arbitrary graph contains a Hamil-  
tonian circuit is .N'g\}l-complete. Subsequently, many special cases have been shown to be  
.Ng\}l-complete \[see Johnson and Papadimitriou \(1985a\) for a survey of these results\]. Sahni  
and Gonzales \(1976\) proved Proposition 3.2.  
Papadimitriou and Steiglitz \(1977, 1978\) have analyzed the worst-case behavior of  
interchange algorithms. They have shown that, ifg\> *\* .Ng\>,* interchange algorithms whose  
neighborhood search time is polynomially bounded cannot be guaranteed to find an  
optimal solution, even with an exponential number of iterations.  
Rosenkrantz et al. \(1977\) have analyzed the worst-case behavior of several tour  
construction heuristics for TSPs that satisfy the triangle inequality. The spanning-tree/  
perfect-matching heuristic and Theorem 3.6 are due to Christofides \(1975b\). Cornuejols  
and Nemhauser \(1978\) showed that this bound is tight.  
Fisher, Nemhauser, and Wolsey \(1979\) gave worst-case bounds for several heuristics for  
the maximum-weight Hamiltonian circuit problem; and Jonker, Kaas, and Volgenant  
\(1980\) gave data-dependent bounds for the general TSP. Frieze, Galbiati, and Maffioli  
\(1982\) analyzed the worst-case performance of some algorithms for the ATSP. Much more  
information on the worst-case analysis of heuristics for the TSP is contained in Johnson  
and Papadimitriou \(1985b\).  
The probabilistic analysis of TSP algorithms was surveyed by Karp and Steele. Karp  
gave two polynomial-time algorithms that asymptotically have a very high probability of  
finding an optimal solution. The first algorithm, by Karp \(1977\) \[also see Halton and  
Terada \(1982\)\], is for random euclidean problems on ad-dimensional cube. \(Originally,  
Karp considered random points on a unit square.\) The idea of the algorithm is to divide 6. Notes 525  
the square into a large number of very small subsquares. On each subsquare the problem  
can solved for an optimal solution in polynomial time. Finally, the small cycles are  
assembled into a tour. The second algorithm, by Karp \(1979\) and Karp and Steele \(1985\),  
deals with the ATSP with costs taken from the uniform distribution. Here the idea is to  
solve the assignment problem relaxation and then to patch the subtours together.  
Surveys ofbranch-and-bound algorithms for the traveling salesman problem have been  
presented by Carpento and Toth \(1980\) and Balas and Toth \(1985\). The branching rule  
shown in Figure 3.16 is due to Garfinkel \(1973\). Cutting-plane/branch-and-bound algo-  
rithms for the TSP were initiated by Dantzig, Fulkerson, and Johnson \(1954, 1959\).  
Systematic algorithms of this type were developed by Miliotis \(1976, 1978\), Padberg and  
Hong \(1980\), Crowder and Padberg \(1980\), Grotschel \(1980a\), Padberg and Grotschel  
\(1985\), and Pad berg and Rinaldi \(1987a,b \). The Padberg-Grotschel article surveyed these  
results and reported computational experience. The Padberg-Hong algorithm uses primal  
cutting planes; the other algorithms are FCPAs of the type described in this section. The  
Padberg-Rinaldi FCPA has solved a 2,392-city problem to optimality.  
The separation algorithm for subtour elimination constraints is due to Gomory and Hu  
\(1961\). The shrinking procedure illustrated in Figure 3.23 is taken from Padberg and  
Grotschel \(1985\). Padberg and Rao \(1982\) have given a polynomial-time separation  
algorithm for 2-matching inequalities. In particular, they have shown that the separation  
problem is a minimum odd-cut problem \(see Section III.3.7\).  
An interactive computer package with various TSP heuristics and exact algorithms has  
been developed by Boyd et al. \(1987\).  
Some very restricted families ofTSPs can be solved in polynomial time. An application  
of this type of result appeared in Ratliff and Rosenthal \(1983\), and a survey of these results  
was given by Gilmore et al. \(1985\).  
A generalization of the traveling salesman problem is the vehicle routing problem in  
which there are k salesmen located at a given city, and each must choose a subtour so that  
all cities are covered. Bodin et al. \(1983\), Christofides \(1985a,b\), and Golden and Assad  
\(1986\) surveyed results on this problem. Cullen et al. \(1981\) presented an approach that  
formulates routing problems as set-partitioning problems. Also see Laporte et al. \(1985\),  
Fisher, Greenfield et al. \(1982\), and Kolen et al. \(1987\).  
Another generalization is the quadratic assignment problem \[see Burkhard \(1984\) for a  
survey of this topic\].  
Section 11.6.4  
The fixed-charge network flow problem belongs to a family of problems known as  
network design problems. Magnanti and Wong \(1984\) gave a survey of models and  
algorithms in this area, and Wong \(1985\) gave an annotated bibliography. A Benders'  
decomposition approach to network design has been given by Magnanti et al. \(1986\), and  
a heuristic approach has been given by Lin \(1975\).  
A branch-and-bound algorithm of the type described can be found in Barret al. \(1981\)  
\[also see Cabot and Erenguc \(1984\), Guignard \(1982\), MacKeown \(1981\), Neebe and Rao  
\(1983\), and Suhl \(1985\)\].  
The generalized flow cover inequalities \(see the notes for Section II.2.4\) and their  
separation heuristics come from Padberg, Van Roy, and Wolsey \(1985\), Van Roy and  
Wolsey \(1986, 1987\), and Wolsey \(1987\).  
Example 4.2 is problem 2 from Gray \(1971\). The results were obtained using MPSARX.  
The multicommodity reformulation is part of the folklore. It can be found explicitly in  
Rardin and Choe \(1979\). 526 11.6. Applications of Special-Purpose Algorithms  
Algorithms for finding optimal Steiner trees and branchings appeared in Shore et al.  
\(1982\), Beasley \(1984\), Wong \(1984\), and Prodon et al. \(1985\).  
Reformulations of the lot-size problem were discussed in Sections 1.1.5 and 11.5.5. The  
shortest-path reformulation given here is due to Eppen and Martin \(1988\), and our  
development is based on Pochet and Wolsey \(1988\) \[also see Martin \(1987\)\]. Versions of  
Theorem 4.8 appeared in Rosling \(1983\) and Barany et al. \(1984\).  
The idea of introducing auxiliary variables to tighten a formulation has recently  
attracted considerable attention. Balas and Pulleyblank \(1983\) gave an example, like the  
lot-size problem, where the convex hull of solutions has an exponential number offacets,  
but the enlarged system contains a polynomial number of constraints and variables.  
Martin \(1984\) discussed how a dynamic programming algorithm can be used to derive a  
tight formulation with auxiliary variables, and in Martin \(1987\) it is observed how a linear  
programming separation algorithm also leads to a reformulation with auxiliary variables.  
Section 11.6.5  
The feasibility algorithm for the subset sum problem is due to Lagarios and Odlyzko  
\(1985\). Frieze \(1986\) gave simpler proofs of these results. Related results have been  
obtained by Furst and Kannan \(1987\).  
See Lenstra \(1984\) for a general discussion of integer programming and cryptography.  
The polynomial-time algorithm for the integer feasibility problem for fixed n is due to  
Lenstra \(1983\) \[also see Kannan \(1983\)\]. Earlier results for n = 2 were obtained by Kannan  
\(1980\) and Scarf \(1981a,b\). Rubin \(1985\) gave a polynomial-time algorithm for  
*m* x *\(m* + 1\) integer programs.  
7. EXERCISES  
1. Solve the knapsack problem  
max 18xt + 7x2 + 5x3 + X4  
9xt + 4x2 + 3x3 + 2x4 \~ b  
xEZ!  
by dynamic programming for all values of b from 1 to 100.  
2\. Prove Proposition 1.2.  
3\. Apply the superadditive dual algorithm to the instance in exercise 1 with b = 16.  
4. i\) Suggest other superadditive functions to be used in the superadditive dual  
algorithm.  
ii\) Interpret the dynamic programming algorithm as a superadditive dual algo-  
rithm.  
5\. Use the SR heuristic to find a solution to the knapsack problem\:  
max 537xt + 636x2 + 849x3 + 712x4 + 834xs + 219x6 + 832x?  
924x*1* + 1123x2 + 1501x3 + 1402x4 + 1579x5 + 498x*6* + 1649x7 \~ 23,762  
# xEz\:,  
which is within 1% of optimal. 7. Exercises  
6. 527  
Solve the integer program \(see exercise 13 of section 11.1.9\)  
max 2xt + Sx2  
4xt + x2\:\:\:; 28  
Xt + 4x2\:\:\:; 27  
7. 8. 9. 10. 11. i\) by the shortest-path enumeration algorithm and  
ii\) by the increasing group algorithm.  
Describe a fully polynomial approximation scheme based on a scaling/rounding  
heuristic for the 0-1 knapsack problem.  
Solve the 0-1 knapsack problem  
max 43xt + 41x2 + 27x3 + 32x4 + 15x5 + 50x6 + 19x7 + 2lxs  
20xt + *I9xz* + 14x3 + 16x4 + 7xs + 28x6 + 12x7 + 14xs \:\:\:; 61  
xEB8  
by the branch-and-bound algorithm of Section l.  
For the 0-1 knapsack problem,  
i\) propose a neighborhood search algorithm,  
ii\) propose a simulated annealing algorithm, and  
iii\) suggest alternative neighborhoods for use in i and ii.  
Consider the 0-l knapsack problems with cja*1* =constant for all\} EN. Why might  
these be difficult? Suggest a way to solve such problems.  
Propose heuristic algorithms for the 0-l multidimensional knapsack problem  
max\{ I *c1 x1\:* I *aux1* \:\:\:; b; for i EM, *x* E Bn\},  
jEN jEN  
where *au* E Z\~ for all *i* E M,j EN.  
12. Describe an efficient algorithm for the linear programming relaxation of the  
multiple-choice knapsack problem  
*L Xj* jEQ;  
\:\:\:; 1 for i E *J+* U 1-  
where r n /- = 0, Q; n Qk = 0 fori *\** k, and N = uiE/'U/- Q;. 528 13. 11.6. Applications of Special-Purpose Algorithms  
Apply the simplex-based heuristic BIP to the following problems.  
i\) max 9xl + 2x2- 3x3  
4x 1 + x2 - 5x3 \:\:\:\:\:\: 1  
4x 1 - 2x2 + 6x3 \:\:\:\:\:\: 7  
x EB3;  
ii\) the covering problem min\{lx\: Ax\~ 1, *x* E B9\}, where  
*I*  
# z  
0  
*I*  
1  
0  
14. Let S = \{x E B6 \: 40x 1 + 40x2 + 35x3 + 35x4 + 15x5 + 15x6\:\:\:;; 100\}. Find violated  
inequalities for *S* that cut off  
i\) Xa = \(1 \~ 0 \~ \~ 1\),  
1 1 1 1  
3 3 3 1\).  
15. Apply the FCP/branch-and-bound algorithm to the problem  
max 43xl + l0x2 + 18x3 + 12x4 + 36xs + 22x6  
12xl + 2x2 + 3x3 + 2x4 + 4xs + 3x6 \:\:\:;; 20  
3xl + 8x2 + 12x3 + 13x4 + 20xs + 14x6 \:\:\:;; 36  
*xEB*6•  
**16.** In Section 11.2.2 the extended cover inequalities  
2.\: *X;\:\:\:;;* I c I - 1  
*\}EE\(C\)*  
were defined for 0-1 knapsack problems. Formulate the separation problem for  
extended cover inequalities and propose a heuristic algorithm to solve it.  
17\. Let  
i\) Show that x 1 + X2 + X4 + x 5 + x 6\:\:\:;; 2 is a valid inequality.  
ii\) Find a valid inequality that cuts off *x\** = \(0 0 \~ 0\).  
iii\) Formulate the separation problem for the families of valid inequalities in  
exercise 9 of Section 11.2.6. 7\. Exercises 529  
18\. Consider the set S = S n \{x E Bn\: ex s c0\} with S = \{x E Bn\: Ax \~ b \}. Given t  
inequalities *LjeQ. Xj* \~ 1 fork = 1, ... , t, suppose there exists *v* E R\~ such that  
I  
I *vkscj* forjEN and I *vk\>Co.*  
*\(k\:Q.3j\)* k\~J  
Show the following\:  
i\) If *x* E S, then for some *k* E \{1, ... , t\}, we have *xj* = 0 fQr allj E Qk \(i.e., at least  
one of the t inequalities is violated by every point xES\).  
ii\) If S = \{x E Bn\: *LjeN, Xj* \~ 1 fori EM\}, then for any subset \{i\(1\), ... , *i\(t\)\}* of M,  
' I *Xj* \~ 1  
jE .'-;!, *\(Ni!k\)\\Qk\)*  
is a valid inequality for S.  
iii\) Apply these observations to the covering problem with an additional constraint  
Derive the valid inequalities x 1 s 0 and X4 \~ 1.  
19\. Apply the greedy heuristic to the set-covering problem  
+ Xs \~  
+ Xs \~  
What lower bounds on the optimal value are given by the heuristic? Can you derive  
stronger lower bounds?  
20\. Prove Proposition 2.4.  
21. Show that the bound of Theorem 2.5 can be achieved asymptotically.  
22\. Solve the weighted node-packing problem on the graph shown in Figure 7.1 530 11.6. Applications of Special-Purpose Algorithms  
4 4  
1 3  
Figure 7.1  
23. 24. i\) by solving LNP and fixing variables,  
ii\) by adding cuts of the type discussed Section 11.2.1.  
Show that LNP can be solved as an assignment problem.  
Find a minimum road distance tour of the midwest visiting each city exactly once  
and returning to the city from which you started. Distances are in tens of miles  
\(revised since Chapter 1.3\)\:  
2 3 4 5 6 7 8 9 10  
1.  
2.  
3.  
4.  
5.  
6.  
7.  
8.  
9.  
10.  
Chicago 92 99 50 41 79 46 29 50 70  
Dallas 78 49 94 21 64 63 42 37  
Denver 60 84 61 54 86 76 51  
Kansas City 45 35 20 26 17 20  
Minneapolis 80 36 55 59 64  
Oklahoma City 46 50 29 16  
Omaha 45 37 30  
St. Louis 21 45  
Springfield \(Mo.\) 25  
Wichita  
i\) Use any method that you like, but you must prove optimality of your solution.  
Your grade will be decreasing function of the length of your proof.  
**ii\)** Calculate as many of the bounds given in Figure 3.7 as possible.  
iii\) Test the primal heuristics given in Section 3.  
iv\) Solve by an FCP algorithm.  
25\. Some other heuristic algorithms for the symmetric traveling salesman problem  
include\:  
i\) Furthest insertion.  
ii\) Sweep. Locate an "origin" in the center of the map, and then denote each city by  
its rectangular coordinates *\(r,* B\). Order the cities by increasing *e.*  
Apply them to the examples in Section 3.  
26\. Prove Proposition 3.2. 7\. Exercises 531  
27\. Devise a simulated annealing algorithm for the asymmetric traveling salesman  
28\. 29. 30. 31. 32\. problem.  
Find a family of graphs for which the worst-case bound of the spanning-tree/perfect-  
matching heuristic is asymptotically achieved.  
Find one or more violated inequalities for the fractional solution shown in Figure  
3.28.  
i\) Solve the uncapacitated fixed-charge network problem exhibited in Figure 7.2  
by branch-and-bound.  
ii\) Find generalized flow cover inequalities that cut off the initial linear program-  
ming solution.  
**iii\)** Solve the multicommodity reformulation of the problem.  
Consider the following fixed-charge transportation problem with 3 suppliers and 7  
customers. The supplies are 15, 25, and 33; the demands are 5, 7, 8, 10, 12, 15, and 16;  
and the variable and fixed costs are  
*hij* = \( \~ 1  
4 2  
2  
3  
3  
2 1  
2 1 3  
3 2 2  
31  
12  
32  
10 6  
32 46  
17 16  
19  
29  
15  
12  
11  
24  
14 \)  
14 .  
12  
Solve this problem by an FCP /branch-and-bound algorithm.  
Show that the inequalities \(4.5\) are valid.  
33. Another way to reformulate a fixed-charge network flow problem is to define  
variables *y\{j,* where *y\{j* is the flow along the path *p* passing through arc \( i, j\). Write out  
and solve the arc-path reformulation for Example 4.3.  
Figure 7.2. Costs *\(hu, cu\)* appear on each arc. Bounds are *U;J* = 3 for *\(i,j\)* E *Jil.* 532 11.6. Applications of Special-Purpose Algorithms  
34. Show that it is always possible to convert a capacitated fixed-charge network  
problem into an uncapacitated problem by increasing the number of nodes and arcs.  
35\. Consider the problem of finding a minimum-weight !-branching. It can be shown  
that the linear program  
min I wuxu  
\{i,j\)Ed  
\(i,j\)Eo'\(S\)  
I xu 2'\: I for *S* C *V* with 1 E *S*  
*X* ERJsil  
always has an optimal solution with x E Bn when w E Rjsi I, and it is unbounded  
otherwise.  
i\) Show that this linear program solves the minimum-weight branching\_problem.  
ii\) Give a linear program having a polynomial in I VI number of constraints and  
variables that solves the minimum-weight branching problem.  
iii\) Give a linear program with similar characteristics that solves the minimum-  
weight spanning-tree problem.  
36\. Consider the problem of finding a minimum-weight Steiner !-branching when there  
are two demand nodes \( I D I = 2\)\:  
i\) What structure do the branchings have?  
ii\) Give a polynomial algorithm to solve this problem.  
37\. Prove that the linear programming relaxation of \(4.18\) always has an optimal  
solution with x E BT. Part III  
# COMBINATORIAL  
OPTIMIZATION **111.1**  
# Integral Polyhedra  
**1. INTRODUCfION**  
In Part III we will continue to study feasible regions of the form S = *\{x* E Z\~\: *Ax..;; b\},*  
where *\(A, b\)* is an *m* x *\(n* + 1\) integral matrix, and integer programs of the form  
max\{cx\: *xES\}.* However, for most of the problems considered here a nice description of  
conv\(S\) is known. This is the essential distinction between the Part II and Part III  
problems.  
We will encounter some problems with the property that conv\(S\) = *\{x* E R\~\: *Ax..;; b\},*  
and we will encounter others for which we can specify an explicit set of constraints  
*A* x..;; *b'* such that  
conv\(S\) = *\{x* E R\~\: *Ax..;; b, A* x..;; *b/\}*  
Frequently, in these cases, we also obtain an efficient combinatorial algorithm for solving  
the linear optimization problem. Conversely, such an algorithm for solving the optimiza-  
tion problem may provide a proof that the inequalities define the convex hull.  
The minimum-weight *s-t* path problem on a digraph q; = *Cv,.s4\)* \(see Section 1.3.2\) is  
an example of a Part III problem. It is a network flow problem where we require one unit  
of flow out of node s, one unit of flow into node *t,* and conservation of flow at all other  
nodes. For this formulation, we gave an algorithm in Section 1.3.6 which, for an arbitrary  
weight function, either \(a\) yields an integral optimal solution and thus provides a mini-  
mum-weight *s-t* path or \(b\) shows that the objective value is unbounded. Thus, the  
algoritlim provides a proof that the polyhedron of feasible solutions only has integral  
extreme points. In Section 2, we will establish an important property of node-arc inci-  
dence matrices, which gives a different proof of this result.  
A second formulation arises from considering the relationship between s *-t* dipaths and  
*s-t* dicuts. Let *\(U, U\)* be any partitiol!...9f *V* with s E *U* and *t* E *U.* Then the set of arcs  
whose tail is in *U* and whose head is in *U* is an *s-t* dicut. Let Id I = *n, M* = \{l, ... , *m\}* be  
the index set of all *s-t* dicuts, and let *ai* E *Bn* for *i EM* be the incidence vectors of the  
*s-t* dicuts. Now a dipath must intersect each dicut. Thus, if *x* E *Bn* is the incidence vector  
of an *s-t* dipath, then *aix* \~ 1 for all *i EM.* It is not difficult to show that all of the  
incidence vectors of *s-t* dipaths are extreme points ofthe polyhedron  
*\{x* E R\~\: *aix* \~ 1 for all *i EM\}.*  
Much more significantly, they are the only extreme points.  
535 536 111.1. Integral Polyhedra  
Now let *K* be the index set of all *s-t* dipaths, and let *Xi* E *Bn* for *i* E *K* be the  
corresponding incidence vectors of the *s-t* dipaths. Then, by polarity, we obtain another  
integral polyhedron  
*\{a* E R\~\: *Xi a* \~ 1 for all *i* E *K\}*  
whose extreme points are the *s-t* dicuts. This approach will be pursued further in  
Section 6.  
In both formulations of the minimum-weight path problem, we obtain a polyhedron  
having only integral extreme points. We now state this property precisely.  
*Definition* 1.1\. A nonempty polyhedron *P* £ *Rn* is said to be *integral* if each of its  
nonempty faces contains an integral point.  
It is sufficient to consider the minimal faces. Now, since each minimal nonempty face is  
an extreme point if and only ifrank\(A\) = *n* \(see Proposition 4.2 of Section 1.4.4\), we have  
Proposition 1.1. *A nonempty polyhedron P* = *\{x* E *Rn\: Ax* \~ *b\} with* rank\(A\) = *n is*  
*integral if and only if all of its extreme points are integral.*  
Also, if *P* = *\{x ERn\: Ax* \~ *b\}* £ R\~ and is not empty, then rank\(A\) = *n.* Thus, we have  
the following corollary\:  
Corollary 1.2. *A nonempty polyhedron P* £ R\~ *is integral if and only if all of its extreme*  
*points are integral.*  
We assume hereafter, unless otherwise stated, that nonempty polyhedra have extreme  
points.  
Consider the linear programming problem over the polyhedron *P* given by  
\(LP\) ZLP = max\{cx\: *x* E *P\}.*  
Integral polyhedra can be characterized by optimal solutions to LP.  
Proposition 1.3. *Thefollowing statements are equivalent.*  
1. *P is integral.*  
2. LP *has an integral optimal solution for all cERn for which it has an optimal*  
*solution.*  
3. LP *has an integral optimal solution for all* c E *zn for which it has an optimal*  
*solution.*  
4. ZLP *is integralfor all* c E *zn for which* LP *has an optimal solution.*  
*Proof* 1 .... 2. If LP has an optimal solution, it has an optimal solution at an extreme  
point of *P* \(see Theorem 4.5 of Section 1.4.4\).  
2 .... 3 and 3 .... 4 are obvious.  
4 .... 1. We prove the contrapositive using the fact that if *x* E *P* is an extreme point, there  
is acE *zn* such that *x* is the unique optimal solution to LP \(see Theorem 4.6 of  
Section 1.4.4\). 1. Introduction 537  
Thus if statement 1 is false, there exists c E *zn* such that *x'* is the unique optimal  
solution to LP, and some component of *x',* say *xi,* is fractional. Now it follows that there  
exists a \(suitably large\) integer *q* such that *x'* is also optimal for the objective vector  
*c'* = c + *\(l/q\)ej* and for the objective vector *qc '* = *qc* + *ej.* But *qc'x'* - *qcx'* = *xi,* which  
means that *ZLP* is fractional for at least one of the objectives *qc'* or *qc.* Hence statement 4 is  
\~. .  
Statement 4 of Proposition 1.3 provides a technique for establishing the integrality of *P*  
by studying the dual polyhedron.  
*Definition* 1.2. A system of linear inequalities *Ax* \~ *b* is called *totally dual integral*  
\(TDI\) if, for all integral c such that ZLP = max\{cx\: *Ax* \~ *b\}* is finite, the dual  
*min\{yb\: yA* = c, *y* E *R'\:'\}* has an integral optimal solution.  
Note that the definition is not given in terms of a polyhedron *P* but, instead, is given  
more specifically in terms of a linear inequality description of it.  
Corollary 1.4. *If Ax* \~ *b* is TDI *and b* is *integral, then P* = *\{x ERn\: Ax* \~ *b\} is integral.*  
*Proof* Since the dual has an integral optimal solution and *b* is integral, the optimal  
objective value of the dual is integral. Hence for all c for which ZLP is finite, *ZLP* is integral.  
Now the result follows from Proposition 1.3. •  
*Example* 1.1. We are given a complete bipartite graph with node partItion *Vi =*  
\{l, ... *,m\}, Vi* = *\{m* + 1, ... *n\},* node weights Cj for\} E *V;* U *Vi* = *V,* and edge weights *bij*  
for *i* E *V;* and\} E *V2•* The problem is to assign node numbers *Xj* for\} E *V* to solve the  
linear program  
max I *CjXj*  
*JEV*  
*Xi* + *Xj* \~ *b ij* for *i* E *Vi* and\} E *V2•*  
Its dual is  
I *Yij* = Ci for *i* E *Vi*  
*JEV,*  
*iEV,*  
I *Yij* = Cj for\} E *Vi*  
*Yij* ;;;. 0 for *i* E *Vi* and\} E *Vi,*  
The dual problem is the transportation problem \(see Section 1.3.5\), which has an  
integral optimal solution if it is feasible and if Cj is integral for allj E *V.* Hence, the linear  
system *Xi* + *Xj* \~ *bijfor i* E *Vb\}* E *V2 is* TDI. Hence, if the b;/sareintegers, the polyhedron  
*\{x ERn\: Xi* + *Xj* \~ *bi\}* for *i* E *Vb\}* E *V2\}* is integral.  
The fact that *Ax* \~ *b* with *bERm* is a TDI system says nothing about integrality unless  
*b* E *zm.* In fact, the TDI property is sensitive to scaling the rows of *A.* 538 III.I. Integral Polyhedra  
Proposition 1.5. *If Ax* \~ *b is any linear system with rational coefficients, there exists a*  
*positive integer q such that \(ljq\)Ax* \~ *\(ljq\)b is* TO!.  
*Proof* Consider the dual constraints *yA* = c, *Y* E *R';'* with c E *zn.* By Proposition 3.1  
of Section 1.5.3, there is a positive integer *q* such that every extreme point can be written as  
*y* = *\(1!q\)\(PI\>'" ,Pm\),* where *Pi* is an integer for *i* = 1, ... , *m.* Now let *y'* = *qy.* Hence  
every extreme point of *\(l/q\)y\:4* = c, *Y'E R';'* is integral, and the dual system  
*\(1!q\)Ax* \~ *\(l/q\)b* is TOL •  
Corollary 1.6. *inequality system.*  
*Any polyhedron P* = *\{x ERn\: Ax* \~ *b\} can be represented by a* TOI *linear*  
Integral polyhedra are distinguished by the existence of a TOI representation with an  
integral right-hand side.  
Proposition 1.7. *If P* = *\{x ERn\: Ax* \~ *b\} is an integral polyhedron, then P can be repre-*  
*sented as P* = *\{x ERn\: Ax* \~ *b'\}, where Ax* \~ *b' is* TOI *and b' is integral.*  
*Proof* Consider acE *zn* for which *max\{cx\: x* E *P\}* is bounded. Let *F* be the face of *P*  
of optimal solutions with equality set *MI\<.* Now consider the polyhedral cone  
*C\(F\)* = *\{d ERn\: d* = I *Uia i, U* E *RJ M*  
*,* I\}.  
*iEM,*  
By Theorem 6.1\(ii\) of Section 1.4.6, *C\(F\)* n *zn* is finitely generated with generators  
*nk* E *zn* for *k* E *K\(F\);* that is,  
*C\(F\)* n *zn* = *\{d ERn\: d* = I *kEK\(F\)*  
*Ykn\\ y* E *ZJK\(F\)* I\}.  
Also, since *P* is integral and *nk* E *zn,* we obtain *max\{nkx\: x* E *P\}* = nZ E zt. In addition  
since *nk* E *C\(F\),* we have \~x = nZ for all *x* E *F.*  
We now add the finite set of inequalities *nkx* \~ n\~ for *k* E *K\(F\)* to the description of *P*  
for each of the finite number off aces *F* of *P.* This gives the dual problem  
min I *Uibi* + I I Ykn\~  
*F kEK\(F\)*  
*UER';', Y* E *RlIK\(F\)1*  
Now since c E *C\(F\)* n *zn,* there exists y\~ E Zl, *k* E *K\(F\)* such that *e* = *LkEK\(F\)* y\~\~.  
Finally, *y\** is an optimal dual solution because for any optimal primal solution *x·* E *F,* we  
have *ex'* = *LkEK\(F\)* y\~\~x' = *LkEK\(F\)* y\~nZ since \~x· = nZ. •  
A linear inequality description of an integral polyhedron may not be TOI. This is  
illustrated in the following example. 1. Introduction 539  
*Example* 1.2\. linear program\:  
The problem is to find a minimum cardinality covering of the nodes of a  
graph by its edges. In particular, given a complete graph on four nodes we consider the  
min Xl2 + Xl3 + Xl4 + *X23* + *X24* + *X34*  
Xl2 + X13 + Xl4 \~ 1  
\(1.1\) \~ 1  
Xl3  
Xl4 + *X24* + X34\~ 1  
xER\~.  
There are three optimal solutions\: Xu = *X34* = 1, *xij* = 0 otherwise; X13 = *X24* = 1, *xij* = 0  
otherwise; *X* 14 = *X* 23 = 1, *X ij* = 0 otherwise.  
It can be shown that these solutions, together with the four solutions obtained by setting  
the edge variables for the edges incident to node *i* equal to 1 and the others equal to zero,  
are the only extreme points of the linear system \(1.1\). We leave the details to the reader.  
Now consider the dual, which is the fractional node-packing problem  
max\{\~ *Yi\: Yi* + *Yj* \~ 1 for all *i* andj withj \> *i, Y* E *R!\}.*  
Its unique optimal solution is *Yi* =! for *i* = 1, ... ,4. Hence the linear system *Ax* \~ 1,  
*x* \~ 0 is not TDI, but the polyhedron *P* = *\{x ERn\: Ax* \~ 1, *x* \~ 0\} is integral.  
All of the results of this section hold regardless of whether *P* has extreme points or not.  
However, for full-dimensional integral polyhedra, Proposition 1.7 can be strengthened.  
Proposition 1.8. *side.*  
*For a/ull-dimensional integral polyhedron, there exists a unique mini-*  
*mal \(with respect to removing constraints\)* TDI *representation with an integral right-hand*  
We now outline the rest of this chapter and briefly describe the topics of the following  
two chapters. In Section 2, we describe a class of matrices for which the integrality of  
*P\(b\)* = *\{x* E R\~\: *Ax* \~ *b\}* holds for all integral *b.* A subset of these matrices, including  
node-arc incidence matrices of digraphs, are studied in Section 3. We provide a recogni-  
tion algorithm for these matrices and observe that the associated linear programming  
problem can be solved by network flow algorithms.  
Thereafter, we consider packing polytopes of the form *P* = *\{x* E R\~\: *Ax* \~ l\} and  
covering polyhedra of the form *Q* = *\{x* E R\~\: *Ax* \~ n, where *A* is a \(0, 1\) matrix. In  
Section 4, we describe matrices, in terms of forbidden submatrices, such that both the  
packing and covering polyhedra are integral. Here the linear systems are TDI, and for a  
subclass of the matrices we give a recognition algorithm and an efficient combinatorial  
algorithm for solving the associated linear programming problems. 540 111.1. Integral Polyhedra  
In Section 5 we give a complete description of integral packing polytopes  
*p* = *\{x* E R\~\: *Ax* \~ 1\}. Here the matrices *A* are defined by incidence vectors of cliques of a  
class of graphs, and the extreme points of the polytopes are incidence vectors of node  
packings. Then by invoking antiblocking polarity we obtain a proof of the famous perfect  
graph theorem. In Section 6, we study blocking polarity and obtain results of the type  
exemplified by the polarity between incidence vectors of paths and cuts in a graph.  
Chapters III.2 and III.3 deal with combinatorial objects known as matchings and  
matroids, respectively. Matchings generalize network flows and matroids generalize  
forests of a graph. Both of these combinatorial settings yield interesting polyhedral results  
and efficient optimization algorithms.  
2. TOTALLY UNIMODULAR MATRICES  
*Definition* 2.1. An *m* x *n* integral matrix *A* is *totally unimodular* \(TU\) if the determinant  
of each square submatrix of *A* is equal to 0, 1, or -1.  
It is evident that *aij* = 0,1, or -1 if *A* is TU; that is, *A* is a \(0, 1, -1\) matrix.  
*Example* 2.1. The matrix  
is not TU since I det *A* ' I = 2, where *A'* is the submatrix of *A* consisting of the first three  
rows and columns.  
Note that the example illustrates that recognizing TU matrices is in *CfioJV2P.* That is, to  
give a short proof that a matrix is not TU, we only need to give an appropriate submatrix  
because determinants can be calculated in polynomial time \(see Section 1.5.3\). On the  
other hand, the definition does not give a clue about how to give a short proof that a matrix  
is TU, since the number of square submatrices is exponential in the description of the  
matrix. We will discuss the recognition question in the next section.  
The following proposition, which follows directly from the definition of total unimodu-  
larity, provides ways of constructing other *TV* matrices from a given *TV* matrix.  
Proposition 2.1. *Thefollowing statements are equivalent.*  
1. *A* is *TV.*  
*2. The transpose of A* is *TV.*  
*3. \(A, l\)* is *TV.*  
*4. A matrix obtained by deleting a unit row \(column\) of A* is *TV.*  
*5. A matrix obtained by multiplying a row \(column\) of A by* -1 is TU.  
*6. A matrix obtained by interchanging two rows \(columns\) of A is TV.*  
*7. A matrix obtained by duplicating columns \(rows\) of A is* TU.  
*8. A matrix obtained by a pivot operation on A is TV.* 2. Totally Unimodular Matrices 541  
We will only prove statement 8. Suppose I aij I = 1. Recall from Section 1.2.3  
that a simplex pivot on a \(0, ±l\) matrix *A* with pivot element aij involves the following  
*Proof* steps.  
1. If aij = -1, multiply the *ith* row of *A* by -1. Call the new row *ai •*  
2. For *k* \*' *i,* we obtain  
Now consider a square submatrix *B* of.4. Let *B* be the matrix obtained after the pivot  
has been executed. We will prove that det *B* E \{-1, 0, l\}.  
*Case* 1. The *ith* row of *A* appears in *B.* Then Idet *B* 1= Idet *B I.*  
*Case* 2. The jth column, but not the *ith* row, appears in *B.* Then I det *B* I = 0.  
*Case* 3. Neither the ith row nor thejth column appear in *B.*  
Let  
C= \: *B .*  
\( \~ij ... *aiP \)*  
*alj*  
Then after pivoting we have  
Hence I det *B* I = I det C I = I det CI. •  
Proposition 2.2. *which it is not empty.*  
*If A is TV, then P\(b\)* = *\{x* E *RZ\: Ax.\:\:;; b\) is integral for all* bE *zm for*  
*Proof* Consider the linear program with constraint set *Ax* + *Iy* = *b, x* E R\~, *Y* E *R\:,*  
where *A* is *TV* and *b* is integral. Let *\(A, I\)* = *\(A B, AN\)'* where *A B* is a basis matrix for the  
linear program. By statement 8 of Proposition 2.1, it follows that *All* is an integral matrix.  
Thus *A Ii b* is integral, so the correspondence between basic feasible solutions and extreme  
points yields the result. •  
A similar argument yields the following generalization of Proposition 2.2.  
Proposition 2.3. *If A is TV, if b, b', d, and d' are integral, and if P\(b, b', d, d'\) =*  
*\{x ERn\: b'* .\:\:;; *Ax* .\:\:;; *b, d'* .\:\:;; *x* .\:\:;; d\} *is not empty, then P\(b, b', d, d'\) is an integral*  
*polyhedron.*  
Because the transpose of a *TV* matrix is *TV,* the dual polyhedron is also integral. 542 II1.l. Integral Polyhedra  
Corollary 2.4. *If A is* TU, c *is integral, and Q\(c\)* = *\{u* E *R'\:'\: uA* ? c\} *is not empty, then*  
*Q\(c\) is an integral polyhedron.*  
The sufficiency of total unimodularity for *P\(b\)* to be integral is not the least bit  
surprising. But the converse is not so obvious.  
Theorem 2.5. *If P\(b\)* = *\{x* E R\~\: *Ax* \~ *b\} is integral for all b* E *zm for .vhich it is not*  
*empty, then A is* TO.  
*Proof* Let *A* I be an arbitrary *k* x *k* nonsingular submatrix of *A,* and let  
be the *m* x *m* nonsingular submatrix of *\(A, I\)* generated from *A* I by taking the appropriate  
*m-k* unit vectors from *I.* Let *b* = *AZ* + *e;,* where z E *zm* and *e;* is the ith unit vector. Then  
*A* -I *b* = z + a\:;I, where *ail* is the ith column of *A* -I. Choose z so that z -+- *ail?* 0. Thus  
z + *ail* is the vector of basic variables of an extreme point of *P\(b\).* By hypothesis,  
*z* + *ail* E *zm* and *z* E *zm;* hence *ail* E *zm* and *A* -I is an integral matrix. Thus *A* II is an  
integral matrix.  
Finally, det *A* I and det *A* II are integers and  
Thus, Idet *A* II = 1. •  
Theorem 2.5 is false if *P\(b\)* = *\{x* E R\~; *Ax* = *b\}.* A counterexample is given in exer-  
cise 5.  
Now we consider sufficient conditions for a matrix to be totally unimodular.  
Proposition 2.6. *column, and if'f.; au* = ° *if column j contains two nonzero coefficients, then A is* TO.  
*If the* \(0, 1, -1\) *matrix A has no more than two nonzero entries in each*  
This result is very easy to prove; but rather than giving a direct proof, we will establish it  
as a corollary to a much more general result. Its significance is that it implies that a node-  
arc incidence matrix of any digraph is TU, thus establishing that the sets of feasible  
solutions to a network flow problem and its dual are integral polyhedra. Consequently,  
linear programming duality yields integral min-max results such as the max-flow-min-  
cut theorem \(see Theorem 4.1 of Section 1.3.4\).  
We now present a characterization of total unimodularity that yields Proposition 2.6  
and some other sufficient conditions as corollaries.  
Theorem 2.7. *Thefollowing statements are equivalent.*  
i. *A is* TO.  
ii. *For every* I s; *N* = \{l, ... , *n\}, there exists a partition* II. 12 *of* I *such that*  
I I *au* - I *a;jl* \~ I *for* i = 1, ... , *m.*  
*jEll jEl,* 2\. Totally Unimodular Matrices 543  
*Proof* i .... ii. Let *J* be an arbitrary subset of *N.* Define *Z* by *Zj* = 1 if j E *J, Zj* = 0  
otherwise. Also let *d'* = 0, *d* = *z,* g = *Az, hi* = *bi* = -!gi if gi is even, and *bi* = -!\(gi - 1\),  
*b;* = *hi* + 1 if g; is odd. Now consider  
*P\(b, b', d, d'\)* = *\{x* ER\~\: *b'* \~ *Ax* \~ *b, d'* \~ *x* \~ *d\}.*  
Note that *x* = *z/2* E *P\(b, b', d, d'\).* Since *A* is *TV,* we have *b', b* E *zm, d',* dE *zn* and  
*P* =\#= 0. Proposition 2.3 states that *P* is integral. Thus there exists *XO* E *P* n *Bn* with *xJ* = 0  
for j EN \\ *J* and *xJ* E \{D, l\} for j E *J.* Note that *Zj* - *2xJ* = ± 1 for j E *J.*  
*LetJI* = \{j E *J\: Zj* - *2xJ* = 1\} *andJ2* = \{j E *J\: Zj* - *2xJ* = -l\}. We have  
'" '" \_ '" \( 2 0\) \_ *\{g;* -g; = 0 if gi is even  
L..., *aij* - L..., *aij* - L..., *aij Zj* - *Xj* -  
. •  
*JEJI jEJ, jEJ* g; - *\(g;* ± 1\) = ±1 If g; IS odd.  
Thus  
I I *aij* - I *aijl* \~ 1 for *i* = 1, ... , *m.*  
*JEJI jEJ,*  
ii .... i. *IJ* I = 1 in statement ii yields *aij* E \{D, ±1\} for all *i* andj. The proof is by induction  
on the size of the nonsingular submatrices of *A* using the hypothesis that the determinant  
of every *\(k* - 1\) x *\(k* - 1\) submatrix of *A* equals 0, ±1.  
Let *B* be a *k* x *k* nonsingular submatrix of *A,* and let *r* = I det *B* I. Our objective is to  
prove that *r* = 1.  
By the induction hypothesis and Cramer's rule, we have *B-1* = *B\*/r,* where *bij* = \{D, ±1\}.  
By the definition of B\~ we have *Bbi* = *reh* where bt is the first column of B\~  
Let *J* = \{i\: MI =\#= O\} and *J;* = \{i E *J\:* MI = 1\}. Hence for *i* = 2, ... , *k,* we have  
*\(Bbi\);* = I *bij* - I *bij* = O.  
*JEJi jEJ\\Ji*  
Thus I \{i E *J\: b* ij =\#= O\} I is even; so for any partition \(JI\> *J2\)* of *J,* it follows that  
*I\:.jEJI bij* - \~EJ, *bij* is even for *i* = 2, ... *,k.* Now by hypothesis, there is a partition *\(JI\> J2\)*  
of *J* such that I *I\:.jEJI bij* - *I\:.jEJ, bij* I \~ 1. Hence  
I *bij* - I *bij* = 0 for *i* = 2, ... , *k.*  
*JEJI jEJ,*  
Now consider the value of *al* = I *I\:.jEJI blj* - *I\:.jEJ, blj* I. If *al* = 0, define *y* E *Rk* by *y;* = 1  
for *i* E *J* 10 *y;* = -1 for *i* E *J* 2, and *y;* = 0 otherwise. Since *By* = 0 and *B* is nonsingular, we  
have *y* = 0, which contradicts *J* =\#= 0. Hence by hypothesis we have *al* = 1 and *By* = *±el'*  
However, *Bbi* = *rei.* Since *y* and *bi* are \(0, ±1\) vectors, it follows that *bi* = ±yand Ir I = **1..**  
Note that because *A* is *TV* if and only if its transpose is *TV,* statement ii can  
equivalently be phrased in terms of partitions of subsets of rows of *A;* that is, for every  
*Q* s.;; *M* = n, ... , *m\},* there exists a partition *QI\> Q2* of *Q* such that  
I I *aij* - I *aijl* \~ 1 forj = 1, ... , *n.*  
;EQI ;EQ, 544 III.1. Integral Polyhedra  
Corollary 2.8. *Let A be a* \(0, 1, -1\) *matrix with no more than two nonzero elements in*  
*each column. Then A is* TU *if and only if the rows of A can be partitioned into two subsets*  
Q 1 *and* Q2 *such that if a column contains two nonzero elements, the following statements*  
*are true\:*  
a. *Ifboth nonzero elements have the same sign, then one is in a row contained in* QI  
*and the other is in a row contained in Q2'*  
b. *If the two nonzero elements have opposite sign, then both are in rows contained in*  
*the same subset.*  
*Proof* The partitioning of statement ii of Theorem 2.7 is applied to the rows of *A.*  
Conditions a and b provide the partition for any Q \~ *M. •*  
Corollary 2.8 immediately yields Proposition 2.6 as well as the following corollary\:  
Corollary 2.9. *The node-edge incidence matrix of a bipartite graph is TV.*  
Another consequence of Corollary 2.8 is a linear-time algorithm for recognizing  
whether a \(0, 1, -1\) matrix *A* with, at most, two nonzero entries per column is TU.  
Without loss of generality, assume that every column of *A* contains two nonzero elements  
and every row of *A* contains at least one nonzero element. Let *B\(j\)* = \{i\: *au* \* O\}.  
Arbitrarily put row 1 in Q I. Then Corollary 2.8 fixes the assignment of all rows *i* such that  
there *existsj* with *B\(j\)* = \(l, *n.* Once *i* is assigned, Corollary 2.8 fixes the assignment of all  
rows *k* such that there existsj with *BU\)* = \{i, *k\}.* The process is repeated in this way until  
either the partition is completed or an incompatibility with the conditions of the corollary  
is discovered. The latter occurs when a row already assigned is required to be placed in the  
complementary set. Note that for a \(0, 1\) matrix, the procedure simply tests whether the  
graph of the given node-edge matrix is bipartite.  
*Definition* 2.2. *i* \< *I* \< *k.*  
An *m* x *n* \(0, 1\) matrix *A* is called an *interval matrix* if in each column  
the I's appear consecutively; that is, if *au* = *akj* = 1 and *k* \> *i* + 1, then *au* = 1 for alII with  
Corollary 2.10. *Interval matrices are TV.*  
*Proof* This follows from statement ii of Theorem 2.7 by observing that the interval  
property of a matrix is closed under row deletions and, for Q = \{l, ... *,m\},* taking  
QI = \{i\: *i* is odd\} and Q2 = Q \\ QI' •  
An integer programming problem that involves assigning workers to shifts can be  
modeled using an interval matrix. Suppose the work day consists of *m* hours. A shift is a  
set of consecutive hours. Suppose that *n* different shifts are possible. The *jth* shift is  
represented by a 0-1 m-vector *aj,* where *aij* = 1 if hour *i* is in the *jth* shift. Thus *A* is an  
*m* x *n* interval matrix of specified shifts. Let *b* E *Z,\:"* where *b* i is the minimum number of  
workers required in the ith hour. The set of feasible solutions is given by  
S = *\{x* E Z\~\: *Ax* \~ *b\},* where *Xj* is the number of workers assigned to thejth shift.  
In the next section we will show that if *A* is an interval matrix and *b* E *Z,\:"* then integer  
programs of the form  
\(2.1\) min\{cx\: *Ax* \~ *b, x* E Z\~\} 2. Totally Unimodular Matrices 545  
are network flow problems. Moreover, for any \(0, 1\) matrix, a problem of the form \(2.1\)  
can be relaxed to another nontrivial problem of the form \(2.1\) in which the constraint  
matrix is an interval matrix.  
To see this, suppose that the \(0, 1\) matrix *A I* is not an interval matrix. Then some  
column of *A I* is not an *m* x 1 interval matrix \(column\). However, any noninterval column  
can be uniquely written as the sum of *p* interval columns, where *p* \< *ml2* \(see Figure 2.1\).  
Now replace each noninterval column *a;* by the *p* interval columns defined in its  
decomposition, and give the new columns an objective function coefficient *Cjk* for *k* = 1,  
... *,p* with *Cj* = L£\~l *Cjk.* We then obtain a problem of the form \(2.1\) in which the matrix *A*  
is an *m* x s interval matrix with s \~ *mn 12.* This is a relaxation of the original problem  
since we have omitted the constraints that each of the variables associated with the *p*  
interval columns that have replaced *aj* must be equal.  
We leave as an exercise the comparison of the bounds obtained from this relaxation  
with those obtained from the linear programming relaxation. The advantage of this  
relaxation lies in the efficiency of solving flow problems with side constraints.  
We close this section by presenting a composition procedure for TU matrices that is  
used in Section 3 to describe a characterization ofTU matrices.  
**Proposition 2.11.** *Let*  
\(\~ *a*  
1 *and d*  
° *a\)* \(1 *dO B b \)*  
*be m* x *nand n* x *m* TU *matrices respectively, where A is \(m* - 1\) x *\(n* - 2\), *a is \(m* - 1\)  
x *1,cis1x\(n-2\),Bis\(n-1\)x\(m-2\),bis1x\(m-2\),dis\(n-1\)x* 1, *and* ° *and* 1  
*are scalars. Then the \(m* + *n* - 2\) x *\(m* + *n* - 4\) *matrix*  
isTU.  
This proposition can be proved by applying Theorem 2.7.  
0 0  
I I 0 0  
0 0 0 0  
0 0  
I 0 + + 0  
I 0 I 0  
0 0 0 0  
0 0  
Figure 2.1 546 IIU. Integral Polyhedra  
*Example* 2.2\. Given that  
*A·=\(-i* J J -g -n and *A,=*  
o 0  
1 1 0  
# 111  
# 001  
# 001  
are *TV,* Proposition 2.11 yields the *TV* matrix  
1 0 1 0  
-1 1 0 0  
*A3=* 0 -1 -1 0  
0 -1 -1 1  
0 0 0 1  
0 0 0 1  
3\. NETWORK MATRICES  
This section relies on, and is motivated by, the graphical representation of a system of  
equations *A 'x* = *b,* where *A'* is the node-arc incidence matrix of a digraph \(see Sec-  
tion I.3.6\). We begin with a brief review of the results of Section 1.3.6 that are needed here.  
Let g; = *\(V, SIt\)* be a digraph with *m* + 1 nodes and *n* arcs, and let *A'* be the node-arc  
incidence matrix ofg;. Suppose the graph underlying g; is connected.  
1. rank\(A '\) = *m.* Since it convenient to work with a matrix offull row rank, we *letA* be  
the *m* x *n* matrix obtained by deleting any row of *A'.*  
2. Let *A* = *\(A* 1, *A 2\)* where *A* 1 is an *m* x *m* nonsingular submatrix of *A.* The arcs  
*\(el,* ... *,em\)* that correspond to the columns of *Al* induce a spanning tree in g;,  
denoted by *fF* = *\(V, SIt 1\)'*  
3. The representation ofa column of *A 2,* corresponding to the arc *ej* = *\(u,* v\) as a linear  
combination of the columns of *A* h is given by the incidence vector *aj* of the unique  
dipath *P j* in *fF* from *u* to v, where  
if *Pj* passes *ei* in a forward direction  
if *Pj* passes *ei* in a backward direction  
otherwise.  
*V* sing the terminology oflinear programming, *A* 1 is a basis matrix and *A* 2 = *AliA* 2 is the  
incidence matrix of the dipaths corresponding to the columns *aj.* Since we are not  
concerned with primal or dual feasibility here, *b* and an objective vector are both  
irrelevant.  
*Definition* 3.1. Given a directed tree *fF* = *\(V, SIt* 1\) and a digraph g; = *\(V, Slt2\),* where  
IVI = *m* + 1, I *SIt* 1 I = *m,* and I *SIt* 2 I = *n,* the *m* x *n* arc-dipath incidence matrix *M\(fF,* g;\)  
corresponding to the dipaths in *fF* whose endpoints are defined by the arcs of *SIt* 2 is called a  
*network matrix.* \(For convenience, it is desirable to allow *Slt2* to contain edge repetitions.\) 3\. Network Matrices **547**  
Note that in this definition the arcs offJ mayor may not be arcs *of'2lJ.* This gives us the  
freedom to avoid, if we wish, having an identity matrix as a submatrix of every network  
matrix.  
4. Let *er* E.sIl I and *es E.sIl2,* and suppose that *fJ'= \(V,* \(.sill \\ *\{er\}\)* U *\{es\}\)* is acyclic. A  
. pivot in *M\(fJ, '2lJ\)* corresponds to forming the tree *fJ'* and the digraph *'2lJ'* = *\(V,*  
*\(.sIl2* \\ *\{es \}\)* U *\{er\}\)* and then computing the updated incidence matrix *M\(fT', '2lJ'\).*  
Hence a network matrix is precisely a matrix whose columns represent arcs of a node-  
arc incidence matrix of a digraph after one row has been deleted and any number of  
simplex pivots have been executed.  
***Example* 3.1.** The incidence matrix of *qy* is  
Consider the digraph *'2lJ* and tree *fJ* shown in Figure 3.1.  
*c* 0 0 -1 1 \} , 1 0 0 0 1 2  
*A* = \~ -1 -1 0 -1 o 3  
0 0 -1 4  
*el e2 e3 e4 es e6*  
Let *A* be the submatrix consisting of the first three rows of *A* " and let  
1  
*C* 0  
-D *Al=* \~  
-1  
be the submatrix consisting of the first three rows and columns of *A.* Then we obtain the  
network matrix *M\(fT, '2lJ\)* given by  
# o  
1  
# o  
# o  
o -1  
PI *P2 P3 P4 Ps P6*  
Network matrix *M\(fJ, '2lJ\)*  
Forming a new tree *fJ'* by adding *e5* to *fJ* and excluding *e2* as shown in Figure 3.1, we  
obtain the network matrix *M\(fJ', '2lJ'\)* given by  
# o  
1  
# o  
# o  
o 0  
-1 1  
# o  
Note that *M\(fJ', '2lJ'\)* is obtained by pivoting on the second row and fifth column of  
*M\(fJ, '2lJ\).* 548  
I1I.1. Integral Polyhedra  
# "\~\~  
# \~  
1 e4 4  
Figure 3.1  
**Proposition** 3.1. *Network matrices have thefollowing properties\:*  
1. *They are closed under row and column deletions and duplications.*  
*2. They are closed under multiplication of a column by -1.*  
*3. If A is a network matrix, then \(A, I\) is a network matrix.*  
*4. They are closed under pivoting.*  
*5. They are TV.*  
*Proof*  
1. Deleting a column means just to ignore the corresponding dipath. Duplicating a  
column means simply to repeat the representation of the corresponding dipath.  
Removing a row is equivalent to removing the corresponding arc \[say, *e* = *\(u, v\)\]*  
from *fJ* and then constructing the tree *fJ'* by "identifying" nodes *u* and v as shown in  
Figure 3.2a. This operation is called a *contraction* of *e.* Duplicating a row is  
c4uivalent to splitting the corresponding arc as shown in Figure 3.2b.  
2. Multiplying a column by -1 means to reverse the direction of the corresponding  
path.  
e *v* x  
\(al  
e *u'* e'  
\(bl  
Figure 3.2. \(a\) Contracting *e.* \(b\) Splitting *e.* 3\. Network Matrices  
549  
s  
Figure 3.3  
3. Here we add a path for each tree arc.  
4 and 5. These have been shown above.  
# •  
Two classes of TV matrices presented in Section 2 are network matrices. In each case,  
we obtain the result simply by giving the appropriate class of trees.  
Proposition 3.2. *column, then A is a network matrix.*  
*If A is* TV *and contains no more than two nonzero elements in each*  
*Proof. fF* = *\(\{s\)* U QI U *Q2,* .511 1 \), where  
Let *Q\\, Q2* be the partition of the rows of *A* defined in Corollary 2.8, and let  
.511 1 = *\{\(u, s\)\:* for all *u* E QJl U *\{\(s, v\)\:* for all *v* E *Q2\}*  
\(see Figure 3.3\). Let 9.0 = *\(\{s\)* U QI U *Q2,* .5112 U .511 3 U *.5114\),* All of the arcsind2 are from one  
node in *Q* 1 to another in *Q* 1 and correspond to the columns of *A* with two elements of  
opposite sign. The arcs in .511 3 are from a node in QI to a node in *Q2* \(or vice versa\) and  
correspond to those columns of *A* with two elements of the same sign. The arcs in .5114 are  
arcs of *fF* and correspond to columns with only one nonzero entry. •  
Proposition 3.3. *Interval matrices are network matrices.*  
*Proof.* .Let *V* = \{l, ... *,m* + n. *fF* is a path from node 1 to node *m* + 1; that is,  
.511 1 = *\{\(t, i* + 1\)\: *i* = 1, ... , *m\}.* A column of *A* whose first 1 is in row *p* and whose last 1 is in  
row *q* is represented by the arc *\(p, q* + 1\) E *.511 2• •*  
*Example* 3.2\. Consider a linear program with constraint set *\{\(x, y\)* E *R,\:+n\:*  
*Ax* + *Iy* = *b\},* where  
*A=*  
o 0  
1 0  
1 1  
# o  
# o  
This is a network flow problem over the network shown in Figure 3.4. For the basic  
solution with *y* = *b, x* = 0, the tree arcs corresponding to basic variables are *\{e\\,* .•. , *es\};*  
the digraph arcs corresponding to nonbasic variables are *\{e6, e7,* eg\}. Note that with the  
supplies shown in Figure 3.4, it follows that *Yi* = *bi* for *i* = 1, ... ,5 is a feasible flow. 550 III.1. Integral Polyhedra  
Figure 3.4  
Example 3.2 illustrates that if *A* is a network matrix associated with a known tree  
*g* = *\(V,* d 1\) and digraph *!jlJ* = *\(V,* d 2\), we can model and solve the linear program  
*max\{cx\: Ax* \~ *b, x* E R\~\} as a network flow problem. Furthermore, there is no need to  
transform *A* into a node-arc incidence matrix. We immediately obtain a basic solution  
*Y* = *b, x* = ° by setting *Yi* = *bi,* where *Yi* is the flow on the tree arc *ei* for *i* = 1, ... , *m.* The  
digraph arcsd 2 represent the nonbasic variables *x.* Then if *b* ;\:;. 0, we have an initial primal  
feasible basic solution for the network simplex algorithm of Section 1.3.6.  
We now turn to the question of recognizing network matrices. This problem is in *.H?J*  
since, given the appropriate digraph and spanning tree, it is easy to verify that the matrix is  
the desired arc-dipath matrix. On the other hand, it is not so obvious how to give a short  
proof that a matrix is not a network matrix. There are, however, polynomial-time  
algorithms for recognizing network matrices. Before describing one, we note that it is  
extremely unlikely for a random \{O, 1, -l\} matrix to be a network matrix. Thus it would  
not be wise to use a network recognition algorithm unless there was some reason to believe  
that the matrix being checked had appropriate structure.  
By Proposition 3.2 and the algorithm based on Corollary 2.8 for recognizing *TV*  
matrices with no more than two nonzero elements in each column, we have a polynomial-  
time algorithm for recognizing whether a matrix with no more than two nonzero elements  
per column is a network matrix. The following recursive algorithm uses this result, by  
reducing the general question to a suitably small set of recognition problems in which each  
matrix has no more than two nonzero elements in each column. In the following  
presentation, we assume for simplicity that *A* has no zero rows or columns and no row or  
column duplications.  
The algorithm has two parts. In the first part, we ignore the signs of the coefficients and  
determine whether the matrix is an edge-path incidence matrix ofa tree \(i.e., a connected  
forest\). In the second part, we consider the orientations of the edges.  
*LetM* = \{l, ... , *m\} andN* = \{l, ... *,n\}.*  
***Definition*** 3.2\. The *m* x *n* \(0, 1\) matrix *A* is the *edge-path incidence matrix of a tree* \(an  
EPT matrix\) if there is a tree *T* on *m* + 1 nodes such that each column of *A* is the  
characteristic vector of the edges ofa path in *T.*  
***Definition*** 3.3\. The *row intersection graph G\(A\)* of an *m* x *n* \(0, 1\) matrix *A* is the graph  
with node set *M* that has an edge between nodes *i* and *k* if there is a column *j* of *A* with  
*aij* '" ° and *akj* '" 0. 3\. Network Matrices 551  
If *G\(A\)* contains *k* \> 1 components, then *A* has the structure  
*Ai* °  
*\(*  
° *A2*  
# ° °  
Thus the *A* i for *i* = 1, ... , *k* can be considered separately. So we assume *k* = 1.  
By ignoring edge orientations, we see that any \(0, 1\) network matrix is an EPT matrix.  
However, as shown below, the converse is false.  
Note that any \(0, 1\) matrix with no more than two l's in each column is an EPT matrix  
ofa *star* \(see Figure 3.5\). The reader can easily check that the EPT matrix  
is not a network matrix because the required orientations cannot be achieved.  
We need to establish some properties of EPT matrices. The following proposition is  
analogous to statement 1 of Proposition 3.1. Its proof is left as an exercise.  
Proposition 3.4. *If A is an* EPT *matrix, then every submatrix of A is an* EPT *matrix.*  
Every edge of a tree *T* is a *cut edge* in the sense that if *e* = *\(u, v\)* is deleted \(not  
contracted\) from *T,* then the resulting subgraph is a forest with two components *Tu* and *Tv.*  
An edge is called a *proper cut edge* if each component of the resulting forest contains at  
least one edge. Otherwise, the edge is called an *end edge.*  
Let *Bi* be the submatrix of *A* with row *i* deleted and all columns\} with *au* = 1 deleted,  
and let *G\(Bi\)* be the row intersection graph of *Bi.* If *au* = 1 for all\}, take *Bi* to be an identity  
matrix of size *m* - 1.  
Propositiori 3.5. *If A is an* EPT *matrix and Li au* "" 3 *for some\}, then\:*  
1. *There exists a row k such that G\(Bk\) contains at least two components.*  
*2. For any k such that G\(Bk\) contains at least two components, A is an* EPT *matrix of*  
*some tree T for which ek is a proper cut edge.*  
Figure 3.5. Star graph. 552 111.1. Integral Polyhedra  
*Proof*  
1. By hypothesis, any tree *T* for which *A* is an EPT matrix contains at least one path of  
length at least 3. Hence *T* contains a proper cut edge, say *ek.* Now, since *Bk* is an EPT  
matrix of the tree obtained from *T* by contracting *ek* and ignoring all of the paths  
that contain *ek,* it follows immediately that *G\(Bk\)* contains at least two components.  
2. Suppose *A* is an EPT matrix of *T.* If *ek* = *\(u,* v\) is a proper cut edge of *T',* there is  
nothing to prove. So suppose that *e"* is an end edge of *T.* Since *G\(Bk\)* contains at  
least two components, there exists a partition *\(Mb M 2\)* of the rows of *M* \\ *\{k\}* such  
that no path contains edges from both *Ml* and *M 2•* Furthermore, without loss of  
generality, it can be assumed that the subforests obtained from the edges of *Ml* and  
*M2* meet at *u* and that v is of degree 1 \(see Figure 3.6a\). Hence *A* is also an EPT  
matrix for the tree *T* shown in Figure 3.6b, and *ek* is a proper cut edge of *T. •*  
*Exampie3.3*  
1  
# o  
# o  
*Bi* = C\) for *i* = 1,2,3. Hence *G\(Bi\)* is connected for all *i,* and *A* is not an EPT matrix.  
In the following presentation the index *k* is fixed since we are assuming that *ek* is a  
proper cut edge. For simplicity of notation, the dependence on *k* is suppressed.  
Let *U* = \{l, ... ,t\} index the components of *G\(Bk\)* where *t* \~ 2. The components  
induce a partition of *M* \\ *\{k\}.* Let *Qq* = \{i *EM\: i* is in the *qth* component\}. Let  
*Ri* = \{j\: *aij* = 1\}, let *R;* = *Ri* n *Rk* for *i* EM \\ *\{k\},* and let *Rq* = \{U *R;\: i* E *Qq\}* for all  
*q* E *U.* If *A* is an EPT matrix of a tree *T,* then for each *q* the set of edges indexed by *Qq* is  
the edge set of a subtree P of *T, Ri* is the set of paths containing *ei, R;* is the set of paths  
containing *ei* and *ek,* and *Rq* is the set of paths that contain *ek* and some edge from P.  
Note that *Rq* "" 0 for any *q* since *G\(A\)* is connected.  
Now if *A* is an EPT matrix of *T* and *ek* = *\(u,* v\) is a proper cut edge of *T,* there exists at  
least one bipartition of *U* -say, *\(Uu, Uv\)* with *Uw* "" 0 for *w* E *\{u,* v\}-such that if *q* E *Uw,*  
then *Tq* is on the *w* side of *ek.*  
*v*  
*\(a\) \(b\)*  
Figure 3.6 3\. Network Matrices 553  
Now we try to decide whether two subtrees *Tq* and P' can lie on the same side of *ek.*  
Suppose  
\(3.1\) for some *i* E *Qq* and *q* \* *q'.*  
Then there is a path containing *ek, e;,* and an edge of *Tq,;* and there is another path  
containing *ek* and an edge of *Tq"* but not *ej.* Note that \(3.1\) does not preclude P and *Tq'*  
from being on the same side of *ek* \(see Figure 3.7\). But if\(3.1\) is true and *Tq* and *Tq'* are on  
the same side of *ek,* every path that contains *ek* and an edge of P must contain precisely  
the same set of edges from *Tq'.* This establishes an ordering between P and *Tq"* since *ek*  
must be closer to *Tq,* than to P. We say that *q' precedes q* when \(3.1\) holds.  
Similarly, when  
\(3.2\) *R;* n *Rq* \* 0 and *Rq* \\ *R;* \* 0 for some *I* E *Qq'* and *q* \* *q'*  
holds, we say that *q precedes q'.* Now it follows that if\(3.1\) and \(3.2\) hold, then P and p'  
must be on opposite sides of *ek.*  
This discussion motivates the use of the graph *Hk* = *\(U, Ek\)* to determine which pairs of  
subtrees must lie on opposite sides of *ek,* where *\(q, q'\)* E *Ek* ifand onlyif\(3.1\) and \(3.2\) are  
true for the pair *\(q, q'\).*  
We now give necessary and sufficient conditions for *A* to be an EPT matrix. Moreover,  
the conditions yield an efficient and constructive algorithm for determining whether *A* is  
an EPT matrix.  
Theorem 3.6. *A is an* EPT *matrix if and only if,for any k such that G\(Bk\) contains at least*  
*two components, the following statements are true\:*  
a. *Hk is bipartite.*  
b. *The submatrices Aq with column index set N and row index set Qq* U *\{k\} are* EPT  
*matrices/or all q* E *U.*  
*Proof* Suppose *A* is an EPT matrix. By Proposition 3.4, condition b must hold.  
We have already shown that *ek* = *\(u, v\)* is a proper cut edge and thatif\(q, *q'\)* E *E\\* then  
P and pi must be on opposite sides of *ek.* But this cannot hold for all such pairs of  
subtrees if *Hk* contains an odd cycle. Hence if *A* is an EPT matrix, condition a must be  
true.  
Figure 3.7 554 III.I. Integral Polyhedra  
Now we show that if conditions a and b are true, then *A* is an EPT matrix. From  
condition a, there exists a bipartition of *U* with the property that *if\(q, q'\)* E *Ek,* thenq and  
*q'* are in different subsets; let *\(Uu, Uv\)* be any such partition. For wE *\{u, v\},* let *Aw* be  
the submatrix of *A* consisting of the column index set *N* and row index set  
*Qw* = *UqEUw Qq* U *\{k\}.*  
The substance of the proofis to show *thatAw* is an EPT matrix ofa tree *Tw* with *ek* as an  
*end edge.* If this is true, it then follows immediately that *A* is an EPT matrix of the tree *T*  
obtained by joining *Tu* and *Tv* together on *ek* as sh9wn in Figure 3.8.  
We now show how to construct *Tw* from *Aw.* We begin by constructing a partial order on  
the set *Uw.* Consider *q, q'* E *Uw.* Since *\(q, q'\)* \$. *Ef,* either *q* and *q'* are unrelated, or *q*  
precedes *q'* or *q'* precedes *q,* but not both.  
We claim that if *q* precedes *q'* and *q'* precedes *q",* then *q* precedes *q".* Since *q* precedes  
*q'* and *q'* does not precede *q,* \(3.2\) and the complement of\(3.1\) yield\:  
i. *R;* n *Rq* '\* 0 for some *I* E *Qq,*  
ii. Either *Rj* n *Rq,* = 0 or *Rq'* \\ *Rj* = 0 for all i E *Qq.*  
By statement i, we obtain *Rj* n *Rq,* '\* 0. Hence by statement ii, we have *Rj* 2 *Rq'.* Since  
*Rq* 2 *Rj,* we have *Rq* 2 *Rq'* if *q* precedes *q'.* Now, since *q'* precedes *q"* and *Rq* 2 *Rq"* it  
follows that *R;* n *Rq* '\* 0 and *Rq* \\ *R;* '\* 0 for some *r* E *Qq".* Hence *q* precedes *q".*  
So we have a partial order of the elements of *Uw.* We represent the partial order by any  
sequence ql, *q2,* ... *,qtw* with the property that for 2.;;; *r';;; tw* and *r'* \< *r, q,* does not  
precede *q,'* \(see Figure 3.9\).  
By hypothesis b of Theorem 3.6, we have that for all *r,* the matrix *Aq,* with column  
index set *N* and row index set *Qq,* U *\{k\}* is an EPT matrix of some tree. Furthermore, by  
the choice of *k* and *Qq"* there exists some such tree, say *T',* with the property that *ek* is an  
end edge of *T'.* This is true because the row intersection graph ofthe matrix *A q,* with row *k*  
deleted is connected; that is, it defines a component of *G\(Bk\).*  
Let ex = UI\~1 *Qq,.* Now we proceed by induction with the hypothesis that the matrix  
with row index set *Qtw-I* U *\{k\}* and column index set *N* is an EPT matrix of some tree *1'tw-1*  
in which *ek* is an end edge.  
We must show how to construct *1'twfrom 1'tw-1* and *Plw. Sinceqt* doesnotprecedeq,for  
any *r* \< *tw.* we have either *Rj* n *Rtw* = 0 or *Rtw* \\ *Rj* = 0 for all i E *Qtw-I.* For those i  
satisfying *Rj* n *Rtw* = 0, we have *aij* = 0 for allj E *Rtw;* for those i satisfying *Rtw* \\ *Rj* \~ 0,  
we have *aij* = 1 for allj E *Rtw.* In other words, *ifj,j'* E *Rtw,* then *aij* = *aij'* for all i E *Qtw-I*  
*.*  
*T*  
Figure 3.8 3\. Network Matrices 555  
*qg*  
Figure 3.9  
Let S = \{i E *Qlw -l\: Rlw* \\ *Rj* = 0\}. \(S may be empty.\) By the induction hypothesis, there  
is a pathp\* in *1'lw-1* containing precisely the edges *ei* for *i* E S. One end node *ofp\** is *u,* call  
the other end node *u\*.* \(If S is empty, then *U\** = *u.\)*  
The construction of *1'lw* is shown in Figure 3.10.  
Finally, since *Tu* = *1'1.* and *Tv* = *1'\\* the proof is complete. •  
Proposition 3.5 and Theorem 3.6 yield a recursive polynomial-time algorithm for  
recognizing EPT matrices. The algorithm has two fundamental subroutines. The first one  
finds the components of a row intersection graph. The second one checks whether a graph  
is bipartite.  
Algorithm for Recognizing an EPT Matrix  
*Step* 1\: Given a \(0, 1\) matrixA\: \(a\) Ifl\:i *aij";\:;* 2 for *allj,* then *A* is an EPT matrix ofa star;  
\(b\) otherwise, partition *A* according to the components of its row intersection graph,  
and treat each component separately.  
*Step* 2 *\(Component Finding\)\:* Let *k* = 1, and let *Bk* be the matrix obtained from *A* by  
deleting row *k* and all columnsj with *akj* = 1 unless *akj* = 1 for *allj.* In the latter case, let  
*Bk* be an *\(m* - 1\) x *\(m* - 1\) identity matrix. Let *G\(Bk\)* be the row intersection graph of  
*v*  
*v*  
*v*  
*u\**  
Figure 3.10 556 III.I. Integral Polyhedra  
*Bk.* Determine *t,* the number of components of *G\(Bk\).* If *t* \> 1, go to Step 3. Otherwise,  
if *k* \< *m,* then *k* .... *k* + 1, and go to Step 2; and if *k* = *m,* then *A* is not an EPT matrix.  
*Step* 3 *\(Bipartite Test\)\:* Construct the graph *Hk* = *\(U, Ek\),* where *U* = \{l, ... , t\} and *Ek* is  
determined by \(3.1\) and \(3.2\). If *Hk* is not bipartite, *A* is not an EPT matrix. Otherwise,  
let *\(Uu , U.\)* be any bipartition of *U* with the property that if *\(q, q'\)* E *E,* then either  
*q* E *Uu* or *q'* E *Uu,* but not both.  
*Step* 4 *\(Recursion\)\:* Construct the matrices *Aw* for *w* E *\{u, v\},* where *Aw* consists of row *k*  
and the rows *i* of *A,* with *i* in the qth component of *G\(Bk\)* and *q* E *Uw.* Mark row *k* of  
*Aw ,* and call the algorithm for the matrices *Au* and *Avo* with the exception that marked  
rows may not be selected in Step 2.  
*Step* 5 *\(Constructing the Tree\)\:* If the recursion ends in Step 1, each pair of terminal  
submatrices is joined on the edge specified by the marked row. This procedure is  
applied recursively to determine some tree represented by *A.*  
To show that the algorithm runs in polynomial time, we first calculate *f\(m\),* the  
maximum number of passes through Steps 1-3 for a matrix with *m* rows. In Step 4, a  
matrix with *m* rows is split into two matrices, one with *i* rows and the other with *m* - *i* + 1  
rows where 2 ..;; *i* ..;; *m* - 1. Hence for *m* ;;;. 3, we obtain  
\(3.3\) *f\(m\)* = max *\[f\(i\)* + *f\(m* - *i* + 1\)\] + 1  
*2.;,.;m-1*  
and *f\(2\)* = 1. It is a simple exercise to show that the unique solution to \(3.3\) is *f\(m\) =*  
*2m -3.*  
Both Steps 2 and 3 can be executed in polynomial time by well-known algorithms.  
Step 2 dominates. It may require up to *m* executions offorming a row intersection graph  
and finding its components. The dominant step in each execution is the pairwise  
comparison of the rows *ofA.* Hence Step 2 is *O\( m 3n\)* for an *m* x *n* matrix, and the overall  
time complexity is *O\(m4n\).*  
As suggested by Theorem 3.6, the algorithm can be modified to yield a finer decompo-  
sition of *A* at each step. In particular, instead of decomposing *A* into *Au* and *Avo* we  
decompose *A* into *Aq* for *q* E *U.* Then if *Aq* for all *q* E *U* are EPT matrices, we use the  
partial orders of the nodes in *Uu* and *Uv* to construct *T.* This will be illustrated in  
Example 3.5.  
*Exampie3.4*  
0 0 0 1 1 1 1\)  
100 1 100  
\(  
*A* = 010 1 0 10·  
001 100 1  
The components of *G\(B'\)* are *QI* = \{2\}, *Q2* = \{3\}, *Q3* = \{4\}. *R;* = *R i- '* = \{4, 3 + i\} for  
i = 2, 3, 4; for each pair ofindices, \(3.1\) and \(3.2\) are true. Hence *HI* is a triangle, and *A* is  
not an EPT matrix. 3\. Network Matrices 557  
*e5* el el *e4* el *e6*  
.\~ **•**  
**• • • • • • • • •**  
*u v U v U v U v*  
*T3 T2 T4*  
Figure 3.11  
*Exampie3.5*  
1 2 3 4 5 6 7 8 9 10  
0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 0 0 2  
*A=* 1 0 0 0 0 0 0 0 0 3  
0 0 1 0 0 0 1 1 0 4  
0 0 0 1 0 0 0 0 1 1 5  
0 0 0 0 0 0 0 6  
*1=k*  
QI = \{2, 3\}, Q2 = \{4\}, Q3 = \{5\}, Q4 = \{6\}. *R;* = \{6, 7, 8\}, *R2* = \{7, 8, 1O\}, *R3* = \{9, 1O\},  
*R;'* = \{7, 8\}.  
*HI* = *\(U, EI\),* where *U* = n, 2, 3, 4\} and *EI* = \{\(1, 2\), \(1, 4\), \(2, 3\)\}. *HI* is bipartite with  
bipartition *Uu* = \{l, 3\} and *Uv* = \{2, 4\}. In the set *Uu,* nodes 1 and 3 are unrelated; and in the  
set *U"* node 2 precedes node 4. The matrices *A q* for *q* = 1, ... , 4 yield the stars shown in  
Figure 3.11.  
Since nodes 1 and 3 are unrelated and node 2 precedes node 4, the trees are put together  
as shown in Figure 3.12.  
Only a small modification of the EPT matrix recognition algorithm is required to  
obtain a recognition algorithm for network matrices.  
**• • • •**  
*v u v*  
# •  
*T*  
Figure 3.12 558 III.I. Integral Polyhedra  
Theorem 3.7. *The* \(0, 1, - 1 \) *matrix* C *is a network matrix if and only if\:*  
a. *the matrix A obtainedfrom* C *by replacing each element ofC by its absolute value* is  
*an* EPT *matrix; and*  
b. *the submatrices of* C *with no more than two nonzero elements in each column,*  
*corresponding to the submatrices of A with no more than two* 1 *S in each column that*  
*are produced* in *the* EPT *recognition algorithm, are network matrices.*  
*Proof* The necessity of condition b follows, since all submatrices of network matrices  
are network matrices. The necessity of condition a follows, since a dipath in a directed tree  
must be a path in the underlying tree.  
To prove sufficiency we only need to show that two directed subtrees *gu* and fJ., can be  
merged as in Figure 3.8. This is clear if both directed trees contain the arc *\(u,* v\) or both  
contain *\(v, u\).* So suppose that *gu,* the arc-dipath matrix of *Au,* contains *\(u,* v\) and that fJ."  
the arc-dipath matrix of *Av,* contains *\(v, u\).* Now observe that by reversing the direction of  
every arc in fJ." we obtain another directed tree g\~ that also is an arc-dipath matrix of *Av,*  
since a dipath in g\~ represented by the arc *\(r,* s\) corresponds to a dipath in g\~ represented  
by the arc *\(s, r\). •*  
*Example* 3.6  
2 3 4 5 6 7 8 9  
0 0 0 0 1 -1 1 -1 1  
-1 1 0 0 0 1 0 0 0 2  
1 0 0 0 1 0 0 0 0 3  
0 1 0 0 0 0 1 0 0 4  
C= 0 0 0 1 0 0 1 0 5  
0 0 -1 0 0 1 0 0 0 6  
0 0 0 0 0 0 1 7  
0 0 0 1 0 0 1 0 0 8  
0 0 0 0 0 0 0 -1 9  
Let *A* be the matrix obtained from C by ignoring the signs of the coefficients.  
2 3 4 5 6 7 8 9  
0 0 0 0 1 1 1 1 1  
1 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0  
*A=* 0 0 0 1 0 0 1 0  
\~ \} *Q2*  
0 0 1 0 0 1 0 0 0  
0 0 0 1 0 0 1 0 1 \~\} *Q3*  
0 0 0 1 0 0 1 0 0  
0 0 0 0 0 0 0 9\} *Q4*  
*!\}Q'* 1 0 0 0 1 0 0 0 0 3\. Network Matrices 559  
Now we determine if *A* is an EPT matrix. Since *G\(B* I\) has four components, the edge *el*  
corresponding to row 1 must be a proper cut edge. To see which subtrees lie on opposite  
sides of eJ, we let *HI* = *\(U, EI\),* where *U* = \{t, 2,3, 4\} andEI = \{\(I, 2\), \(1,3\), \(2,4\), \(3, 4\)\}.  
*HI* is bipartite, and *UUI* = \{l, 4\}, *UV1* = \{2, 3\} is a bipartition of *U.* The components *QI* and  
*Q4* are unrelated, as are the components *Q2* and *Q3.* Hence we carryon, under the  
assumption that subtrees *TI* and r lie on one side of *el* and that subtrees *T2* and T3 lie on  
the other side of *e* I.  
We now need to show that the matrices *A* i corresponding to potential subtrees *Ti* U *\{e* I\}  
are tree matrices with *el* an end edge.  
1 2 5 6 7 8 9  
1 0 1 0 0 2  
A'\~ \(I 0 1 1 1 1 0 1 \*  
0 1 0 0 0 3  
1 0 0 1 0 4  
c,\~u 0 1 1 1  
1 0 1 0 0 2  
0 1 0 0 0 3  
-D 1 \*  
0 0 0 4  
5 6 7 8 9  
*A 4 =* \(\~ 1 1 \~\) 1 \*  
0 0 9  
*C4 =* \(\~ 1 -1 -D 1 \*  
0 0 -1 9  
A'\~ *\(*  
3 5 6 7 8 9  
0 1 1  
1 0 0 1 5  
0 0 0 6  
\~\) 1 \*  
C2 = 1 1 0 0 1 0 5  
C 1 1 -1 1 -I\) 1 \*  
-1 0 1 0 o 0 6  
Since *A* 1, *A* 2, and *A* 4 contain no more than two 1 's in each column, we can immediately  
check that the corresponding submatrices of C are all dipath incidence matrices. The  
corresponding trees are shown in Figure 3.13. 560 111.1. Integral Polyhedra  
Vj  
Figure 3.13  
4 5 6 7 8 9  
0 0 0 7  
A;\~O 1 1 1 1\) \*  
0 0 0 8  
needs to be decomposed further. *G\(B7\)* has two components; *QS* = \{1\} and *Q6* = \{8\}.  
*H7* = *\(U, E 7 \),* where *U* = \(5, 6\) and *E7* = \{\(5, 6\)\}. *H7* is bipartite; *UU7* = \{5\} and *UV7* = \{6\}.  
4 5 6 7 8 9  
*AS* = \(\~ 1 1 1 \~\) \*  
0 0 0 7 \*  
*CS* = \(\~ 1 1 -1 1 -\~\) \*  
0 0 1 0 7 \*  
4 7 9  
*A 6=C* \~\) 7 \*  
8  
C6 =C \~\) 7 \*  
8  
The corresponding trees and their merger are shown in Figure 3.14.  
ej e7 e7 es  
**. .. • \)' . • \)I • \)I •**  
Uj Vj *=U7 V7 U7 V7*  
*lYu7 IYv7*  
ej e7 es  
**... •** \)I **• \)I •**  
Uj *Vj v7*  
1'7 3  
Figure 3.14 3\. Network Matrices 561  
*e6*  
eJ *e7* ea  
vJ uJ vJ  
*eS*  
*ffuJ ffvJ*  
Figure 3.15  
The merging of gl and *g4* and of *g2* and *g3* are shown in Figure 3.15.  
The final merging of *gUt* and\:!!v, is shown in Figure 3.16, and C is a network matrix of  
this tree.  
The final topic of this section is a brief discussion of the recognition problem for totally  
unimodular matrices. The following two matrices are not network matrices but are TU\:  
1 -1 0 0 -1 1 1 1  
-1 1 -1 0 0 1 0 0  
0 -1 1 -1 0 0 1 0  
0 0 -1 -1 0 0 1  
-1 0 0 -1 0 0  
These two matrices and network matrices are the fundamental building blocks for  
constructing all TU matrices. This result is a deep theorem whose proof is beyond the  
scope of this presentation.  
Theorem 3.8. *Every* TU *matrix that is not a network matrix or one of the two matrices*  
*given above can be constructed/rom these matrices using the rules of Proposition* 2.1 *and*  
*Proposition 2.11.*  
A consequence of this theorem is that the TU recognition problem is in *\}\(,\{JjJ,* since a  
short proof of total unimodularity for matrix *A* is to give easily recognizable TU matrices  
and the rules to construct *A* from them. Theorem 3.8 also yields a polynomial-time  
algorithm for the recognition problem and a polynomial-time algorithm for solving linear  
programs with TU constraint matrices. But the conclusion of practical importance to be  
drawn from Theorem 3.8 is that "nearly all" TU matrices are network matrices.  
Figure 3.16 562 111.1. Integral Polyhedra  
4. BALANCED AND TOTALLY BALANCED MATRICES  
In the remainder of this chapter, we will study packing and covering problems. *LetA* be an  
*m* x *n* \(0, 1\) matrix.  
The *fractional packing* problem we consider is the linear program  
\(FP\) *max\{cx\: x* E *P\},*  
where *P* = *\{x* E R\~\: *Ax* ",;; n. Its dual is  
\(DFP\) *min\{y1\: yA* \~ c, *y* E *R';'\}.*  
Here it makes sense to eliminate primal unboundedness and dual infeasibility by assum-  
ing that *A* contains no zero columns; that is, *a j* "\* ° for\) E *N* = \{l, ... , *n\}.* We also assume  
that *Cj* \> ° for\) E *N* since if *Cj* ",;; 0, there is an optimal solution to FP with *Xj* = 0.  
The *fractional covering* problem we consider is the linear program .  
\(FC\) *min\{cx\: x* E *Q\},*  
where *Q* = *\{x* E R\~\: *Ax* \~ 1\}. Its dual is  
\(DFC\) *max\{yl\: yA* ",;; c, *y* E *R';'\}.*  
Here it is sensible to eliminate primal infeasibility and dual unboundedness by assuming  
that *A* contains no zero rows; that is, *a i* "\* ° for *i* EM = \{l, ... *,m\}.* We also assume that  
*Cj* \> ° for\) E *N* because if *Cj* \< 0, Fe is unbounded, and if *Cj* = 0, we can set *Xj* = 1.  
We can view the rows of *A* as incidence vectors ofa family of subsets *Ni* \~ *N* for *i EM.*  
To describe *P,* the maximal rows are necessary and sufficient; and to describe *Q,* the  
minimal rows are necessary and sufficient. Hence, in both cases, we can assume that the  
rows of *A* are incomparable \(0, 1\) vectors; that is, they are the incidence vectors of a set of  
subsets called a *clutter.*  
Our goal is to determine classes of matrices and classes of combinatorial optimization  
problems for which these linear programs have integral optimal solutions. Thus the  
fundamental questions are\:  
1. When is *P* an integral polytope?  
2. When is the system *Ax* ",;; 1, *x* \~ ° TDI?  
3. When is *Q* an integral polyhedron?  
4. When is the system *Ax* \~ 1, *x* \~ ° TDI?  
As we have already seen, total unimodularity of *A* is a correct answer to all four  
questions. But, as we shall see, there are larger classes of\(O, 1\) matrices for which *P* and *Q*  
are integral, and the packing and covering systems are TDI. Let *P\(b\)* = *\{x* E R\~\: *Ax",;; b\)*  
and *Q\(b\)* = *\{x* E R\~\: *Ax* \~ *b\).* Note that if *P* is integral, then *P\(b\)* is integral for all  
determined by setting *Xj* = ° for all\) with *aij* = 1 for some *i* E *M\(b\).*  
bE *Bm.* This is true since if bE *B m* and *M\(b\)* = \{i *EM\: bi* = O\}, then *P\(b\)* is the face of *P*  
The integrality of *P* and *Q* are, in general, unrelated. To relate them it is necessary to  
consider families of polyhedra that are obtained by eliminating constraints. For the  
constraint *aix* \~ *bi,* setting *bi* = ° is just a way of saying that the ith constraint is 4\. Balanced and Totally Balanced Matrices 563  
superfluous or has been eliminated. Similarly, for the constraint *aix* \~ *bi,* setting  
*bi* \~ LJ\~l *au* has the same effect. Here we use the notation *bi* = 00.  
**Proposition 4.1.** *statements are equivalent.*  
*Let A be a* \(0, 1\) *matrix with no zero rows or columns. The following two*  
1. *PCb\) is integralforall b with bi* E \(l, *oo\}for i EM.*  
*2. Q\( b\) is integra/for all b* E *Bm.*  
*Proof* Each member of each of the families is nonempty since ° E *P\(b\)* and 1 E *Q\(b\).*  
Consider *Q\(b\)* with *bi* = 1 for *i* E *K* \~ *M* and with *bi* = ° otherwise. Suppose *x* is a  
fractional extreme point of *Q\(b\).* Then there exist N\~ \~ *N* and K\~ \~ *K* such that  
IN\~ I + IK\~ I = *n* and *x* is a solution to  
. \{I *a'x =*  
*ai* \~ 1  
for *i* E K\~  
otherwise, \{o forjEN\~  
*Xj* = *pj* \~ ° otherwise.  
But then *x* is an extreme point of *PCb\),* where *bi* = *bi* for *i* E K\~ and *bi* = 00 otherwise.  
Thus 1 =\> 2. The proof of2 =\> 1 is similar. •  
The matrices we study in this section are precisely those for which statements 1 and 2 of  
Proposition 4.1 hold. Let..4tk, *k* \~ 3, be the family of *k* x *k* \(0, 1\) matrices, all of whose row  
and column sums equal 2, that do not contain the submatrix  
***Definition*** 4.1. *in..4tk* for any *k* \~ 3.  
A \(0, 1\) matrix is *totally balanced* \(TB\) ifit does not contain a submatrix  
***Definition*** 4.2. *k* \~ 3 and odd.  
A \(0, 1\) matrix is *balanced* ifit does not contain a submatrix *in..4tk* for any  
Note that a matrix *A* in..4tk, *k* \~ 3, that does not contain a submatrix in *..4t1, I* \< *k,* is a  
node-edge incidence matrix of a cycle \(see Figure 4.1\). By permuting rows and columns,  
we can write such a matrix in a canonical form with *ajj* = *aj+l,j* = 1 for *j* = 1, ... , *k* - 1,  
*akk* = *alk* = 1, and *au* = ° otherwise. Then IdetA 1= 2 when *k* is odd, and IdetA 1= °  
when *k* is even.  
0 0 0  
r\) 0 1 1  
# \(!  
1 0 0  
o 0 1 2  
# 110  
4  
Figure 4.1 564 111.1. Integral Polyhedra  
We now give simple consequences of the above definitions.  
Proposition 4.2. *If A is a \(totally\) balanced matrix, then the following matrices are*  
*\(totally\) balanced.*  
1. *\(A, I\).*  
*2. The transpose of A.*  
*3. Any matrix obtained by permuting rows or columns of A.*  
*4. Any submatrix of A.*  
Proposition 4.3. *If A is a* \(0, 1\) *TV matrix, then A is balanced.*  
*Proof* If *A* is not balanced, then it contains a submatrix *A'* E *.;\{,\(k* where *k* \~ 3 and odd.  
Hence I det *A* 'I = 2. •  
On the other hand, a \(0, 1\) *TV* matrix may not be a TB matrix. For example, any matrix  
in .;\{,\(4 is a *TV* matrix. The following example illustrates some of the properties of TB  
matrices.  
*Example* 4.1. Let  
The reader should check the following statements.  
1. *A* is not a *TV* matrix.  
*2. A* is a TB matrix.  
*3. P* is integral. \(Its extreme points are the null vector and the four unit vectors.\)  
*4. P\(b\)* with *b* = \(2 1 1 1\) contains the extreme point *\(i iii\).*  
5. The matrix  
where *A* 'E .;\{,\(4 is balanced, but not a *TV* or TB matrix.  
The relationship among these classes is given in Figure 4.2.  
Although there are nice polyhedral results for balanced matrices, no polynomial-time  
combinatorial methods are known for solving the corresponding linear programming  
problems. Moreover, the recognition problem also is unsolved except for the obvious  
result that it is in *Cf6o.N'\(fjJ.*  
In contrast, the results for TB matrices are much richer because both optimization and  
recognition problems can be solved by efficient combinatorial methods. Hence, for most  
of the remainder of this section, we consider TB matrices. Also, since the theory and  
algorithms for FP \(the fractional packing problem\) and Fe \(the fractional covering 4\. Balanced and Totally Balanced Matrices 565  
Balanced  
Figure 4.2  
problem\) are essentially the same for TB matrices, we only need to consider one of them in  
detail. Since more general results for FP will be given in the next section, we consider FC  
and its dual DFC here. We will give a polynomial-time algorithm that obtains integral  
optimal solutions to FC and DFC. We first need some preliminary results.  
*Definition* 4.3\. submatrix  
A \(0, 1\) matrix *A* is called a *row inclusion matrix* ifit does not contain the  
*F=C* \~\).  
In other words, all of the rows *i* with *aij* = 1 are ordered by inclusion with respect to the  
columns\}, ... , *n.* The reader should keep in mind that the row inclusion property of a  
matrix is sensitive to the ordering of its rows and columns. Later in this section we will  
address the issue of whether the rows and columns ofa given \(0, 1\) matrix can be permuted  
to obtain a row inclusion matrix.  
Two obvious properties of row inclusion matrices are given in the following proposi-  
tions.  
Proposition 4.4. *The recognition problem for row inclusion matrices is solvable in*  
*polynomial time.*  
Proposition 4.5. *Row inclusion matrices are totally balanced.*  
*Proof* SupposeA contains a submatrixB in.4lb *k* \~ 3. Then there exists *i,\}, k, I* with  
*i* \< *k* and\} \< *I* such that *bij* = *bi!* = *bkj* = 1. Then *bkl* = 0, since otherwise *B* contains the  
submatrix \(\: \:\). Hence *A* contains the submatrix *F. •*  
The converse of Proposition 4.5 obviously is false, but later we will show that by row-  
and-column permutations of a TB matrix we can obtain a row inclusion matrix.  
Our next objective is to show that when *A* is a row inclusion matrix, the fractional  
covering problem \(FC\) and its dual \(DFC\) are easily solved. DFC is solved by greedily  
packing the rows of *A* into the vector c. That is, we first take as much as possible of row *ai,*  
then row *a2*  
*,* and so on. When a positive multiple of a row is taken, we note the largest  
column index\} for which the remaining amount of *Cj* is reduced to zero. These columns  
are then used to find a primal optimal solution. The primal solution also is constructed  
greedily by processing these columns in the reverse order from which they were selected. 566  
111.1. Integral Polyhedra  
Algorithm for DFC and FC for Row Inclusion Matrices  
DFC\:  
*Initialization\:* Let *Ni* = \{j *EN\: aij* = 1\} for *i* = 1, ... , *m, i* = 1, c1 = C \> 0, *Jo* = 0.  
*Iteration i\:* Let *Yi* = min\{c\)\: *j EN;\}.* If *Yi* \> 0, then let *Ci+1* = ci  
- *Yiai* and *J i* = *J i-1* U *\{k\},*  
where *k* = max\{j E *N i\: Yi* = *cj\}.* Let *a\(k\)* = *i.* Otherwise *J;* = *J i-1* and *Ci+ 1* = *ci*  
*.* If *i* = *m,*  
stop; *\(y\),* ... *,Yrn\)* is an optimal solution. Otherwise *i* ... *i* + 1.  
FC\:  
*Initialization\:* Let *b* = 1, *Jrn* = *\{k\),* ... *,kp \}* \(from DFC\), where *a\(ki\)* \< *a\(ki+l\)* for *i* = 1,  
... *,p* - 1. *Setxj* = ° *forj* \$. *Jrn ,* and set *1= p.*  
*Iteration I\:* Set *Xk,* = max\(O, *bCl.\(k,\)* and *b* ... *b* - *ak,xk,.* If *1=* 1, stop; *x* = *\(x\),* ... *,xn\)* is an  
optimal solution. Otherwise *I* ... *I* - 1.  
Theorem 4.6. *row inclusion matrix.*  
*The algorithm gives integral optimal solutions to* DFC *and* FC *when A is a*  
*Proof* It is clear that the solutions are integral and that the dual solution is feasible.  
Throughout the proof, we use the following facts\: \(a\) If *J rn* = *\(k\),* ... *,kp\},* then  
*a\(k;\)* \< *a\(ki+ 1\)* for *i* = 1, ... *,p* - 1, and \(b\) if *Yi* \> 0, then there is a *k* E *Jrn* with *a\(k\)* = *i,*  
and either *cj* \> ° or *au* = ° for allj.  
We now consider primal feasibility. By construction, we have *x* \~ 0. The proof of  
*aix* \~ 1 for *i* = 1, ... , *m* is divided into two cases.  
*Case* 1 *\(Yi* \> 0\). Then for some *k,* E *Jm ,* we have *a\(k,\)* = *i* and *aik,* = 1. At Step *I,* we set  
*Xk,* = max\(O, *bi\),* where *bi* is the current value of *bi.* If *bi* \~ 0, then *aix* \~ 1. If *bi* = 1, then  
*Xk,* = 1 and *aix* \~ 1.  
*Case* 2 *\(Yi* = 0\). Let M = *Ni* n \{j *EN\:* c5 = O\}. Since *Yi* = 0, we obtain M \*' 0. Let  
*jl=max\{j\:jEM\}.* Suppose *jl \$.Ji-l-* Now, since *c5,=0,* there is an *il\<i* such that  
*Yi,* = *cj\:* \> 0, and *ai,j,* = 1. *Sincejl* \$. *J i-\),* there is ah E *J i-1* withj2 \> *jl* such that *il* = *a\(h\),*  
*Yi,* = *cj;,* and *ai,h* = 1. Also, by the definition *ofj\),* we have *au,* = 1 and *au,* = 0\. Hence we  
have the following submatrix of *A\:*  
\~\)  
Since *A* is a row inclusion matrix, this is not possible, so *j* 1 E *Ji - 1•*  
Now we can define *i* 1 by *a\(jl\)* = *il* \< *i.* Hence if *Xj,* = 1, then *aix* \~ 1.  
On the other hand, if *Xj,* = 0, leth E *J* be such that *xh* = 1 and row *i* 1 was first covered  
by columnh. This means that *ai,h* = *ai,j3* = 1, where *i3* = *a\(h\)* \> *i 1•* In addition, *ai3j,* = °  
since *a\(jl\)* = *il* and *Yi3* \> 0\. Now *ifj3 \<j\),* we have 4\. Balanced and Totally Balanced Matrices Henceh \> *jl'* Then, if *aij,* = 0, we have  
567  
*jl* h  
G \~\) ;1  
Hence *aij,* = 1 and *aix* \~ *aij,x3* = 1.  
Next we establish the complementarity conditions  
\(4.1\) *Xj\(Cj* - I *aijYi\)* = 0 *forj* = 1, ... , *n,*  
1=1  
\(4.2\)  
*Yi\(± aijXj* - 1\) = 0 for *i* = 1, ... , *m.*  
\}=I  
The conditions \(4.1\) are satisfied by the construction of *Jm* and *Xj* = 0 if *j* \$. *Jm•* Now  
consider \(4.2\) and suppose *Yi* \> 0 and *a\(k\)* = *i.* We need to show that  
*L aijxj* = I *aijXj* + *aikXk* + *L aijXj* = 1.  
*jEJm jEJm \:* a\(j\)\<i *jEJm \:* a\(j»i  
1. *l\:jEJm \: a\(j\)\<i aijXj* = 0 since *a\(j\)* = *i* 1 \< *i* implies cj = 0, and thus *Yi* \> 0 implies *aij* = O.  
2\. Since *a\(k\)* = *i,* it follows that *aik* = 1. By the construction of the primal solution,  
*Xk* = 1 ifand only *ifl\:jEJm\:!\>U»; aijxj* = O.  
3. Now it suffices to show that *l\:jEJm \: aU»; aijXj* .;;; 1. If not, there exists *j* I, *h* E *J m* such  
that *aij1xjl* + *aij,xh* = 2 and *a\(h\)* \> *a\(jl\)* \> *i.* Then *a a\(j,\),j* I = 0 since *C,!;\(h\)* = 0, and  
*aa\(jl\),h* = 0 since *Xjl* = *Xh* = 1. Hence we have  
*h jl*  
or \(11 0 1\) *i a\(j2\).*  
Since neit\)\:1er of these is possible, we have *l\:jEJm \: aU»i aijxj* .;;; 1.  
# •  
*Example* 4.2. *min\{cx\: Ax* \~ 1, *x* E R\~\} with c = \(2 3 3 1\) and  
\( 1 1 0 0\)  
o 1 1 0  
*A=* 0 0 1 1 .  
1 1 1 1  
It is easy to check that *A* is a row inclusion matrix.  
DFC\:  
1. *YI* = 2, c2 = \(2 3 3 1\) - \(2 2 0 0\) = \(0 1 3 1\), *k* = 1, *J 1* = \{l\}, *a\(l\)* = 1.  
*2. Y2* = 1, c3 = \(0 0 2 1\), *k* = *2,J70.* = \{l, 2\}, *a\(2\)* = 2.  
*3. Y3* = 1, c4 = \(0 0 1 0\), *k* = *4,J3* = \{l, 2, 4\}, *a\(4\)* = 3.  
*4. Y4* = 0, *J4* = \{l, 2, 4\}; *Y* = \(2 1 1 0\) is an optimal solution. 568 1II.1. Integral Polyhedra  
Fe\:  
*1. J4* = \{l, 2, 4\}, *X3* = 0, *b* = \(1 1\).  
2. *1=* 3, *k3* = 4, *X4* = *b3* = 1, *b* = \(1 0 0\).  
3. *1=* 2, *k2* = 2, *X2* = *b2* = 1, *b* = \(0 0 0 -1\).  
4. *1=* 1, kl = 1, XI = b l = O. Stop; *X* = \(0 1 0 1\) is an optimal solution of value 4.  
Next we show that the rows and columns of a TB matrix can be permuted so that the  
resulting TB matrix is a row inclusion matrix.  
Given a \(0, 1\) matrixA, let iii = *\(ain,* ... *,ail\)* for *i* = 1, ... *,m* be the elements of row *i*  
in reverse order, and let *iij* = *\(amj,* ... , *a* Ij\) for *j* = 1, ... , *n* be the elements of *columnj* in  
reverse order.  
*Definition* 4.4. The \(0, 1\) matrix *A* is called *totally reverse lexicographic* \(TRL\) if  
-i+1 *L* -i *c* . 1 1 d ·f - *L* - *c* . 1 1  
*a* \~ *a* lor *l* = , ... , *m* - an 1 *aj+1* \~ *aj* lOr\} = , ... , *n* - .  
We now give an algorithm which shows that\:  
Proposition 4.7. *to a* TRL *matrix in polynomial time.*  
*By permuting rows and columns, any* \(0, 1\) *matrix can be transformed*  
*Proof* For any partition *M\[,* ... *,Mt* of the rows of *A,* let *dj* E Z\~ be given by  
*dj* = \( *L aij,* ... , *L aij\)* for *j* EN.  
*iEM, iEM,*  
Initially, let *t* ='1 and *MJ* = *M* = \{l, ... *,m\}.* Hence *dj* = *1\:iEM aij'* Suppose *dj, =*  
maxjEN *dj •*  
We now begin to construct the TRL permutation of *A* by making the following row and  
column permutations\:  
*1. j n* is the last column.  
*2. MI* = \{i *EM\: aij,* = O\} and *M2* = *M* \\ *MI.* Hereafter, all rows in *MI* precede those in  
*M 2•*  
Hence we have the *m* x 1 TRL matrix  
# o  
and regardless of how we permute the rows withinMI andM2, we have *iij* J; *iij , forj"\* jn.*  
Now for *j"\* jn,* let *dj* = *\(1\:iEM2 aij, 1\:iEM, aij\)* and suppose *dj'\_l';; dj* for *j* EN \\ *Un\}.*  
*PartitionMk* for *k* = 1,2 *intoMk* = *\{i* E *M k \: aij'\_l* = O\} *andMk* \\ *M k.* Now put the rows in 4\. Balanced and Totally Balanced Matrices 569  
*Mk* before those in *Mk* \\ *Mk* and call the new partition Mh ... *,MI ,* where *t* \~ 4\. Also  
move column *I n-l* to the *\(n* - l\)st position. Now we have an *m* x 2 TRL matrix  
\}n-l *\}n*  
# o  
# o  
# o  
# o  
# o  
1  
# o  
# o  
# o  
1  
1  
1  
with *iij* J; *iijn\_1* for\) EN \\ *Un-l,\}n\}'*  
The process can be continued by choosing a\} E *N* \\ *U n-h\} n\}* such that *dj* is lexico-  
graphically largest and then partitioning each of the *M;* to maintain the lexicographic  
ordering of the rows. •  
*Example 4.3.*  
1 2 3 4 5 6 7  
0 1 0 0 0 0 1 1  
0 0 1 0 0 2  
*A=* 1 0 0 1 1 1 3  
1 0 0 0 1 1 4  
0 1 0 1 0 0 5  
0 0 0 0 6  
*Step* 1\: *dj* = maxj=l ..... 7 *dj* = 4. Hence\}7 = 3, MI = \{l, 6\}, andM2 = \{2, 3,4, 5\}.  
*Step* 2\: *d5* = \(3 0\) \{; *db \}"\** 3. Hence\}6 = 5, MJ = \{l, 6\}, *Mz* = \{4\}, andM3 = \{2, 3, 5\}.  
*Step* 3\: *d l* = \(1 1 1\) \{; *dz, d4, d6, d7•* Hence \}5 = 1, MI = \{l\}, *Mz* = \{6\}, *M3* = \{4\},  
*M4* = \{2, 5\}, and *M5* = \{3\}.  
Continuing in this manner, we obtain the TRL matrix  
4 2 6 7 1 5 3  
0 1 0 1 0 0 0 1  
1 1 0 0 1 0 0 6  
0 0 1 1 1 0 4  
1 0 0 0 0 2  
0 1 0 0 0 1 1 5  
0 0 1 1 3  
Now we can establish the equivalence of totally balanced TRL matrices and row  
inclusion matrices. 570 111.1. Integral Polyhedra  
Proposition 4.8. *not contain the sub matrix F*  
*Let A be a* \(0, 1\) TRL *matrix. A is totally balanced if and only if it does*  
*Proof* If *A* is not TB, then it contains *F* \(see Proposition 4.5\). Now suppose *A*  
contains  
# F=C  
Consider rows *i l* and iz, and leth be the last column of *A* with *ai,j,* \* *ai2 j,'* Since *A* is  
TRL,we obtaini3 \> iz, *ai,h* = 0, andai2h = 1. Similarly, consideringcolumnsil andiz, with  
*i3* being the last row with *ai3j,* \* *ai3h,* we obtain *i3* \> i z, *ai3j,* = 0, and *ai3 h* = 1. Let  
*i3*  
1  
# °  
LJ *i* I  
iz  
*i3*  
If *ai3j3* = 1, *thenA3 E.!U3* and *A* is not TB. If *ai3h* = 0, we repeat the argument using rows *i2*  
and *i 3 .* So we obtaini4 \> *i3* with *ai2j,* = 0, *ai3j,* = 1, and *ai,j* = *ai3j* for alIi\> *i4'* Now we also  
iz andh, we obtain *i4* \> *i3* with *ai,j,* = *ai,h* = ° and *ai,j3* = 1. Let  
observe that by the definition *ofi3,* it follows that *ai,j,* = *ai,j,* = 0\. Similarly, from columns  
1  
° 1  
# °  
° 1  
° 1 .  
# ° \)  
# ° °  
*ai,j,*  
Again, if *ai,j,* = 1, then *A* is not TB; and if *ai,j,* = 0, then *i4* andi4 cannot be the last row and  
*columnofA.*  
After *k* steps, we get  
But this process is finite, so for some *k* we have *Ak* E *.!Uk.* •  
Now we have a polynomial-time algorithm for recognizing TB matrices.  
Algorithm for Recognizing TB Matrices  
*Step* 1\: Given an arbitrary \(0, 1\) matrix *A,* permute its rows and columns to obtain a TRL  
matrix *A* '. \(By Proposition 4.1, *A* is TB if and only if *A'* is TB. It can be shown that the  
algorithm in the proof of Proposition 4.7 runs in *O\(nZm\)* time\).  
*Step* 2\: Check all 2 x 2 submatrices of *A'* for the matrix *F* and then apply Proposition 4.8.  
\(This takes *O\(mZnZ\)* time.\) 4\. Balanced and Totally Balanced Matrices 571  
2 11  
Figure 4.3  
Suppose *A* is TB and TRL. Then, by Proposition 4.8, *A* is a row inclusion matrix, and  
FC and DFC for *A* can be solved for integral optimal solutions by the algorithm given  
on p. 566. Furthermore, since submatrices of TB matrices are also TB, it follows that  
FC and DFC have integral optimal solutions for all bE *Bm.*  
Theorem 4.9. *If A* is *a* TB *matrix, then Q\(b\)* = *\(x* E *R\:\: Ax* \~ *b\)* is *integral, and* DFC  
*has an integral optimal solution for all* bE *Bm.*  
By proceeding in exactly the same way, we obtain analogous results for FP and DFP.  
Theorem 4.10. *If A* is *a* TB *matrix, then PCb\)* = *\(x* E *R\:\: Ax* \~ *b\)* is *integral, and* DFP  
*has an integral optimal solution for all b, with bi* E \{ 1, 00\) *for all* i.  
Totally balanced matrices arise in the formulation of some location problems as set-  
covering problems. Let *T* = *\(V, E\)* be a tree with nonnegative weights on its edges. The  
weight of the unique path joining nodes i and\}, denoted by *dij,* is the sum of the edge  
weights over all edges in the path. In addition, for each\} E *V,* there is an *rj* \~ 0 called the  
*radius* of node\}.  
A *neighborhood subtree* of *T* rooted at node\} is an induced subgraph *Tj* = *\(Jij, Ej \),*  
where *Jij* = \{i E *V\: dij* \~ *r\). Jij* is the set of nodes that can be served by a facility placed at  
node\}. Let *Cj* be the cost of *Tj.*  
The problem of finding a minimum-cost set of neighborhood subtrees that covers *V* is  
the set-covering problem min\{cx\: *Ax* \~ 1, *x* E *Bn \),* where *aij* = 1 if i E *Jij* and *aij* = 0  
otherwise ..  
*Example 4.4.* We are given the tree in Figure 4.3. Let *rj* = 1 for all\} E *V,* and let *dij* be the  
number of edges on the unique path joining nodes i and\}. Then each neighborhood  
subtree contains a node and all of the nodes adjacent to it, and *A* is the node-star incidence  
matrix *ofT.*  
*A=* 0 0 0 0 0 0 0 0 0  
1 1 1 1 0 0 0 0 0 0  
0 1 0 0 0 0 0 0 0 0  
0 0 1 0 0 0 0 0 0 0  
0 1 0 0 0 1 0 0 0  
0 0 0 0 1 0 0 0 0  
0 0 0 0 0 1 1 0 0 0 0  
0 0 0 0 1 0 0 1 0 1  
0 0 0 0 0 0 0 1 0  
0 0 0 0 0 0 0 0 1 1 0  
0 0 0 0 0 0 0 0 0 572 **111.1.** Integral Polyhedra  
*v* 2  
Figure 4.4  
Proposition 4.11. *If A is a node by neighborhood subtree incidence matrix, then A is total-*  
*ly balanced.*  
*Proof* Suppose *A* is not TB. Then we can assume that for some *k* \~ 3, *A* contains the  
*k* x *k* matrix ofacycle; that is, *ajj* = *aj+I,J* = 1 for\} = 1, ... *,k* - 1, *akk* = *alk* = 1, \~nd *aij* = 0  
otherwise. So for\} = 1, ... , *k* - 1, *1j* contains nodes\} and\} + 1 but no other nodes from \{t,  
.. \: *,k\},* and *Tk* contains nodes 1 and *k* but no nodes in \(2, ... *,k* - 1\). *LetpJ* be the unique  
path in *T* joining nodes\} and\} + 1, and let *Pk* be the unique path joining nodes 1 and *k.*  
Suppose *k* = 3. Then since *T I, T2,* and *T3* are neighborhood subtrees, nodes 1,2, and 3  
cannot lie on a common path. Hence the paths *Ph P2,* and *P3* intersect at some node *v* other  
than nodes 1, 2, or 3 \(see Figure 4.4\).  
Now let *d/v* = mini=I,2.3 *div .* Suppose *I* = 1. Let *u* be the node closest to *V2* on *P2.* Hence  
*diu';;; max\(d2u , d3u \),* and *T2* contains node 1. By symmetry the same argument applies if  
*1=* 2 or 3.  
Now suppose *k* \> 3. Define *v* to be the node closest to node 3 on the path PI' \(Now we  
can have *v* = 1 or 2 since *T3* contains nodes 3 and 4.\) Define  
. ;, \{minU\: *v* is on the path joining nodes 3 and *i* + 1,3,;;; i ,;;; *k* - 1\}  
\) *k* otherwise.  
If\} \< *k,* the path from 3 to\} does not contain *v,* and the path from 3 to\} + 1 does. Thus  
*v* is on the path *Pj* \(see Figure 4.5a\). If\} = *k,* the path from 3 to *k* does not contain *v.* But  
since *v* is on the path joining nodes 1 and 3, *v* is on *Pk* \(see Figure 4.5b\).  
In Figure 4.5a let *dqv* = mini=I,2.J.J+1 *div'* Then *TI* and *Tj* contain *q,* which is a contra-  
diction. Similarly, in Figure 4.5b, let *dqv* = mini=I,2.3.k *div'* Then *T2* and *Tk* contain *q,*  
which again is a contradiction. So the *k* x *k* cycle matrix cannot occur and *A* is totally  
balanced. •  
*j+l*  
*v* 2 1 *v* 2  
C\<. **•**  
*\(a\) \(b\)*  
Figure 4.5 5\. Node Packing and Perfect Graphs 573  
The nodes Df a graph are trivial neighborhood subtrees *Ti* = «vJ, 0\) for i = 1, ... ,  
*m.* Hence a node by neighborhood-subtree incidence matrix is a special case of a  
neighborhood-subtree by neighborhood-subtree incidence matrix. Given two families of  
neighborhood subtrees ofa tree *T,* namely, *Ti* = *\(Vi'* EJ for *i* = 1, ... *,m* and *Tj* = *\(Vj,*  
*E\}\)* forj = 1, ... *,n,* let  
*aij* = ° otherwise.  
\{ I ifv;nV;\*0  
By using an argument similar to the one given in the proof of Proposition 4.11, we obtain  
the following generalization\:  
Proposition 4.12 *If A is a neighborhood subtree by neighborhood subtree incidence*  
*matrix, then A is totally balanced.*  
We conclude this section by mentioning some results about balanced matrices. Note  
that if *A* is not balanced, and therefore contains a submatrix *A'* E *AlZk+l* for some *k* \~ 1,  
then Theorem 4.9 is false. This is an immediate consequence of the fact that the unique  
solution to *A'x* = 1 is *x* = 10 ... 1\). Hence with *bi* = 1 for the rows of *A'* and *bi* = °  
otherwise, *Q\(b\)* is not integral. The main result, which we will not prove here, is that  
Theorems 4.9 and 4.10 are still true when *A* is balanced.  
Theorem 4.13. *Let A be a* \(0, 1\) *matrix with no zero rows or columns. The following*  
*statements are equivalent.*  
1. *A is balanced.*  
*2. PCb\)* = *\{x* E R\~\: *Ax* \~ *b\} is integralfor all b with bi* E \{l, oo\}.  
*3. Q\(b\)* = *\{x* E R\~\: *Ax* \~ *b\} is integralfor all* bE *Brn.*  
In Section 5 we will study matrices *A* for which *P* = *\{x* E R\~\: *Ax* \~ 1\} is integral but  
where *A* is not balanced. In Section 6 we will consider some matrices *A* for which  
Q = *\{x* E R\~\: *Ax* \~ 1\} is integral but where *A* is not balanced.  
5. NODE PACKING AND PERFECT GRAPHS  
Integrality results for the fractional packing polytope *P* = *\{x* E R\~\: *Ax* \~ 1\} can be general-  
ized to a larger class of\(O, 1\) matrices than that considered in Section 4. These matrices are  
clique matrices of a family of graphs known as *perfect graphs.* Recall that in Section 11.2.1  
we used clique matrices in the formulation of the node-packing problem.  
For completeness, some definitions are repeated here.  
*Definition* 5.1. A *node packing* on a graph G = *\(V, E\)* is a *U* \<;; *V* with the property that  
no pair of nodes in *U* is joined by an edge.  
*Definition* 5.2\. pair of nodes in C is joined by an edge.  
A *clique* in a graph G = *\(V, E\)* is a C \<;; *V* with the property that every  
Unless otherwise specified, when we use the term *clique* we mean a *maximal* clique. 574 111.1. Integral Polyhedra  
4  
*K=* \(j 1 1 0 0  
0 1 0 1  
!\) 1 0 1 0  
1 1 0 0  
5 6  
3  
Figure 5.1  
*Definition* 5.3\. G.  
The *clique matrix K* of a graph G is the \(0, 1\) incidence matrix whose  
rows correspond to all of the cliques of G and whose columns correspond to the nodes of  
*Definition* 5.4. The *fractional node-packing polytope* of a graph G is *P* = *\{x* E R\~\:  
*Kx.\:;;* 1\}, *wheren* = I *VI.*  
*Definition* 5.5\. A graph G is *perfect* if its fractional node-packing polytope is integral.  
This polyhedral definition of a perfect graph is not the standard definition. Later in this  
section, we will show that it is equivalent to the standard definition, which is given purely  
in graphical terms. Originally, graphs that satisfied Definition 5.5 were called *pluperfect.*  
*Example* 5.1\. A graph and its clique matrix are shown in Figure 5.1.  
Matrix *K* is not balanced. Nevertheless, G is perfect since *P* is integral. The reader can  
check that its only extreme points are *x* = 0, *x* = *ej* for\) = 1, ... , 6, *x* = *ej* + *ek* with  
\) + *k* = 7, and *x* = \(0 0 0 1 1 1\). This is not inconsistent with Theorems 4.10 or  
4.13 since with *b* 1 = 00 and *bi* = 1 otherwise, *P\(b\)* contains the extreme point  
1\(1 1 0 0 0\), which is also an extreme point of *Q* = *\{x* E R\~\: *Kx* \~ 1\}.  
Before studying some classes of perfect graphs, we explain why it suffices to consider  
clique matrices. In particular, we will show that if *A* is the incidence matrix of clutter that  
is not a clique matrix, then *P* = *\{x* E R\~\: *Ax* .\:;; 1\} is not integral.  
Proposition 5.1. *are equivalent.*  
*Let A be the m* x *n incidence matrix of a clutter. The following statements*  
1. *A is a clique matrix.*  
*2. \[fA containsap xpsubmatrixA' wherep* \~ 3 *and all of the row and column sums of*  
*A' equal p* - 1, *then A' is contained in a \(p* + 1\) x *p submatrix that contains a row of*  
*alii's.*  
*Proof* 1 .... 2\. Let *G\(A\)* be the intersection graph of *A* and, without loss of generality, let  
*A'* be the submatrix of *A* consisting of the first *p* rows and columns of *A.* If statement 2 is  
false, then the sets \{l, ... *,p\}* \\ \{i\} are contained in cliques for *i* = 1, ... *,p,* but no clique  
contains \{l, ... *,p\}.* This is impossible for a clique matrix.  
2 .... 1. Let *Ni* = \{j\: *a* ij = 1\} for *i* = 1, ... , *m.* If statement 1 is false, there exists a minimal  
C \~ *V* with I C I \~ 3 such that the subgraph of *G\(A\)* induced by C is complete, and there  
is no *i* such that *Ni* \:\:2 C. Since C is minimal, for each\) E C, there is a distinct *i\(j\)* such  
that *Ni\(j\)* n C = C \\ *\{j\}.* Hence *A* contains a *k* x *k* submatrix *A',* all of whose row and 5\. Node Packing and Perfect Graphs 575  
column sums equal *k* - 1, but there is no *\(k* + 1\) x *k* submatrix that contains *A I* and has a  
*k-vector* of ones. Thus statement 2 is false. •  
**Proposition** 5.2. *Let A be an m* x *n incidence matrix of a clutter. If A is not a clique matrix,*  
*then P* = *\{x* E R\~\: *Ax* \~ 1\} *is not integral.*  
*Proof A* contains the submatrixA *I* of Proposition 5.1. Suppose the columns of *A I* are  
indexed 1, ... *,p.* Then it is easy to see that *Xj* = *l!\(p* - 1\) for *j* = 1, ... *,p,* and *Xj* = 0  
otherwise is an extreme point of *P. •*  
We now consider two well-known classes of perfect graphs and a necessary condition  
for a graph to be perfect.  
**Proposition** 5.3. *Bipartite graphs are perfect.*  
*Proof* The cliques of a bipartite graph are its edges. Hence *K* is the edge-node  
incidence matrix of the graph. Thus, by Corollary 2.9, *K* is totally unimodular and, by  
Proposition 2.2, *P* is integral. •  
***Definition* 5.6** A *chord* of a cycle is an edge joining two nodes of the cycle that are not  
adjacent on the cycle. A graph with *k* nodes, *k* \~ 4, corresponding to a cycle without  
chords is called a *k-hole.* A graph that is the complement of a k-hole is called a *k-antihole.*  
A hole or antihole is *odd \(even\)* if *k* is odd \(even\).  
A 5-hole and a 7-antihole are shown in Figure 5.2.  
**Proposition 5.4.** *odd antihole, then* G *is not perfect.*  
*If a graph* G *contains a node-induced subgraph that is an odd hole or an*  
*Proof* Kas  
Suppose G contains an odd hole on nodes \{l, ... *,2k* + n. Then we can write  
where Kl is the edge-node incidence matrix of the odd hole, and *K3* contains at most two  
positi ve elements in each row. Hence *Xj* = *i* for *j* = 1, ... , *2k* + 1, and *Xj* = 0 otherwise is an  
extreme point of *P.*  
5 2  
6  
3  
4 4  
5-hole 7 -anti hole  
Figure 5.2 576 III.1. Integral Polyhedra  
On the other hand, if G contains an odd antihole on nodes \{t, ... , *2k* + 1\}, then each  
maximum clique of the subgraph is of size *k.* Hence *Xj* = *11k* for *j* = 1, ... , *2k* + 1, and  
*Xj* = 0 otherwise is an extreme point of *P. •*  
Many classes of graphs without induced odd holes or antiholes are known to be perfect.  
However, the converse of Proposition 5.4 is unresolved. It is known as the *perfect graph*  
*conjecture* and is considered to be one of the most challenging and difficult problems in  
graph theory and polyhedral combinatorics.  
We now consider a fundamental class of perfect graphs.  
*Definition* 5.7\. A graph is called *chordal* if it does not contain any k-holes for *k* \~ 4.  
Proposition 5.5. *If matrix A is a totally balanced incidence matrix of a clutter, then it is*  
*the clique matrix of a chordal graph.*  
*Proof* If *A* is not a clique matrix, then by Proposition 5.2 it follows that  
*P* = *\{x* E R\~\: *Ax* \~ 1\} is not integral. Hence by Theorem 4.10, *A* is not TB. If *A* is a clique  
matrix of a graph that is not chordal, then by Definition 5.7 it follows that *A* contains a  
member of *.;Uk* for some *k* \~ 4. Hence *A* is not TB. •  
The chordal graph of Figure 5.1 shows that the converse is false.  
There is a nice characterization of chordal graphs that yields an efficient recognition  
algorithm as well as an algorithm that gives an integer solution \(node packing\) to the linear  
programming problem over the fractional node-packing polytope. Let *N\(v\)* = *\{u* E  
*V\: \(u,* v\) E *E\}* be the nodes adjacent to v-that is, the set of *neighbors* ofv.  
*Definition* 5.8\. clique.  
A node v of G = *\(V, E\)* is called *simplicial* if it and its neighbors form a  
In the graph of Figure 5.1, nodes 4, 5, and 6 are simplicial, but nodes 1, 2, and 3 are not.  
*Definition* 5.9. *vn\}.*  
An ordering of *V,* \(J' = \[Vb *V2,* ••. , *vn \]* is a *perfect elimination scheme*  
\(PES\) if, for i = 1, ... , *n* - 1, viis a simplicial node of the subgraph induced by \{v i, ... ,  
In the graph of Figure 5.1, \[6 5 4 3 2 1\] is a PES.  
It is easy to see that the existence of a PES implies that a graph is chordal. For graph G let  
v be the first node of a PES that is contained in a k-hole with *k* \~ 4. If v does not exist, G is  
chordal. Otherwise, v and the two nodes *u* and *w* adjacent to v on the cycle are contained  
in a clique. Hence *\(u, w\)* E *E* so that the cycle contains a chord, and G is chordal.  
Moreover, it can be shown that every chordal graph has a PES.  
Theorem 5.6. *A graph is chordal if and only if it contains a* PES.  
By considering the nodes sequentially, a PES can be constructed in polynomial time or  
can be shown not to exist. S. Node Packing and Perfect Graphs S77  
Now suppose that \[1 2 *n\]* is a PES for G. Let *ki* be the characteristic vector of  
the clique containing node *i* in the subgraph induced by the nodes \(i, ... *,n\).* Then the  
*n* x *n* matrix *K* whose rows are *k1*  
*,* \(0, *kZ\),* ... , *en* contains the incidence vectors of all of  
the \(maximal\) cliques of G. *K* can, of course, also contain dominated rows corresponding  
to non maximal cliques. However, it is convenient to work with *K* since *kii* = 1 for *i* = 1,  
... , nand *ki\)* = 0 for\} \< ;. Let S\), ... , *Sm* be any partition of\{1, ... , *n\)* with the property  
that if *IE Si,* then *kl* \~ *k i.*  
For any c E Z\~, we can write the fractional node-packing problem as  
\(FNP\) max\{cx\: *Kx* \~ 1, *x* E *R\:\)* = max\{cx\: *Kx* \~ 1, *x* E R\~\)  
and its dual as  
\(DFNP\) *min\{1u\: uK;;.* c, *u* E R\~\) = *min\{1y\: yK;;.* c, *y* E *R'\:'\).*  
We obtain a feasible *y* from a feasible *u* with *lu* = *ly* by *Yi* = *LIES, UI* for *i* = 1, ... , *m.*  
We now give a greedy algorithm that finds integral optimal solutions to DFNP and FNP  
for chordal graphs. The algorithm is very similar to the greedy algorithm given in the  
previous section for the fractional covering problem with totally balanced matrices.  
Algorithm for DFNP and FNP for Chordal Graphs  
DFNP\:  
*Initialization\:* ; = 1, c1 = C, *Jo* = 0 \[12 ... *n\]* is a PES.  
*Iteration i\: Ui* = max\{O, cD. If *Ui* \> 0, let *Ji* = *Ji- 1* U \{i\) and *Ci+1* = ci  
- *uk.* If *Ui* = 0, then  
*Ji* = *Ji- 1* and *Ci+1* = *ci•* If *i* = *n,* stop; *\(u\),* ... , *un\)* is an optimal solution. Otherwise,  
*;\<-i+1.*  
FNP\:  
*Initialization\: J* = *J n* \(from DFNP\), *Xj* = 0 for\} tf- *J.*  
*Iteration\:* Let *I* be the last element of *J.* Set *x,* = 1 and *J* \<- *J* \\ \(i\: *kit* = 1\). If *J* = 0, stop;  
*x* = *\(x\),* . \: • , *xn\}* is an optimal solution. Otherwise repeat.  
Proposition 5.7. *The algorithm gives integral optimal solutions to* DFNP *and* FNP.  
*Proof* By construction, the solutions are integral and the dual solution is feasible.  
Suppose that the ith constraint of FNP \(with respect to the matrix *K\)* is violated. Then  
there exists\}l *=I=\}2* with *kulxjl* = *kij,xj,* = 1. Suppose\}2 follows\}1 in the PES. Then *i* =\}1 or *i*  
precedes\} 1 in the PES. Since\} 1 was not deleted from *J* when we set *xj,* = 1, it follows that  
*kjlj,* = O. But this contradicts the assumption that *i* is a simplicial node of the subgraph  
induced by \{i, ... ,\}\), ... ,\}2, ... *n\}.*  
To complete the proof, we show that complementary slackness is satisfied. By construc-  
. 1 *N •*  
tion, we have 2\:\{\:1 *kijUi* \> c\) only if *cj* \< O. But then *J\)* = *J\)-I.* Hence\} tf- *J* and *Xj* = O. Now  
suppose *Ui* \> 0, *i* E *J.* By the construction of the primal solution, either *Xi* = I or there  
exists an *I* that comes after *i* in the PES with *ki/x,* = 1. Hence, L\~1 *kijxj* = 1. • 578 111.1. Integral Polyhedra  
Corollary 5.8. *Chordal graphs are perfect.*  
*Example* 5.1. *\(continued\).* We use the PES \[4 5 6 1 2 3\]. Then  
4 5 6 1 2 3  
1 0 0 1 1 0  
0 1 0 1 0  
*K=* 0 0 1 0  
0 0 0 1  
0 0 0 0 1  
0 0 0 0 0  
Letc=\(3 1 2 4 6 3\)= c l  
.  
DFNP\:  
1. *UI* = 3, c2 = \(0 1 2  
2. *U2= 1,c3 =\(0* 0 2  
3. *U3* = 2, c4 = \(0 0 0  
4. *U4* = 0, *CS* = *c4*  
*, J4* = *J3•*  
5. *Us= 1,c6 =\(0* 0 0  
6. *U6* = 0, *J6* = *Js•*  
1  
0  
0  
3 3\), *J I* = \{4\}.  
3 2\),12 = \{4, 5\}.  
*0\), J 3* = \{4, 5, 6\}.  
0  
o *-1\),1s* = \{4, 5, 6, 2\}.  
FNP\:  
*1. J* = \{4, 5, 6, 2\}, *X2* = 1.  
*2. J* = \{5\}, *Xs* = 1.  
An optimal solution to FNP is *x* = \(0 1 0 0 1 0\). Let *Yi* = *Ui* for *i* = 1,2, 3 and let  
*Y* 4 = *U4* + *Us* + *U6.* An optimal solution to DFNP is *y* = \(3 1 2 1\).  
We now give some general properties of perfect graphs and the corresponding  
polytopes.  
Let *E* = *\{e\: e* \$. *E\};* then G = *\(V, E\)* is the complement of G. Let *K* be the clique matrix  
of G. Perfect graphs always come in pairs because\:  
Proposition 5.9. *Gis perfect if and only ifG is perfect.*  
*Proof* If G is perfect, then by definition it follows that *P* = *\{x* E R\~\: *Kx* \~ 1\} is an  
integral polyhedron whose extreme points are the incidence vectors of node packings of G.  
Since there is a one-to-one correspondence between cliques of G and maximal node  
packings of G, the maximal extreme points of *P* are the rows of *K.*  
Now by antiblocking polarity-in particular, Proposition 5.8 of Section 1.4.5-  
*P* = *\{x* E R\~\: *Kx* \~ 1\} is an integral polyhedron. Hence G is perfect.  
The converse follows trivially since G is the complement of G. • 5\. Node Packing and Perfect Graphs 579  
Figure 5.3  
*Example* 5.2. In the graph of Figure 5.3, *VI* = \(1, 2, 3\) and *V2* = \{4, 5, 6\} are cliques.  
Hence this graph is the complement ofa bipartite graph. By Proposition 5.3 and Proposi-  
tion 5.9, it is perfect.  
The fractional node-packing polytope on the subgraph induced by *V* \\ *U* is  
*P* n *\{x* E R\~\: *Xj* = 0 forj E *U\},* which is a face of *P* and therefore is integral if *P* is integral.  
Hence we have the following proposition\:  
Proposition 5.10. *Node-induced subgraphs of perfect graphs are perfect.*  
Now consider *P\(b\)* = *\{x* E R\~\: *Kx* \~ *b\}* with *b* E *Em. P\(b\)* is the face of *P* with *Xj* = 0 if  
there is an *i* with *b* i = 0 and *k ij* = 1. Hence if *P* = *P\(l\)* is integral, then *P\( b\)* is integral for all  
*bEEm.*  
Let *a\( G\)* be the *size of a maximum cardinality node packing* in *G,* and let *B\( G\)* be the  
*minimum number of cliques required to cover all the nodes* of *G.* For any graph *G* with *n*  
nodes and *m* cliques, by relaxation and duality we have  
*a\(G\)* \~ max\{lx\: *Kx* \~ 1, *x* E *R1\}*  
*= min\(1y\: yK* \~ 1, *Y* E *R'\:'\}* \~ *B\(G\).*  
For perfect graphs, the first inequality is an equality; for an odd hole on *2k* + 1 nodes,  
however, *a\(G\)* = *k, B\(G\)* = *k* + 1, and the linear programming relaxations have value  
*k* + t.  
To generalize *a* and *B* for graphs with node weights, let  
*a\(G,* c\) = *max\{cx\: Kx* \~ 1, *x* E *En\}*  
*z\(G,* c\) = *max\{cx\: Kx* \~ 1, *x* E R\~\}  
*= min\{1y\: yK* \~ c, *Y* E *R'\:'\}*  
*B\(G,* c\) = *min\(1y\: yK* \~ c, *Y* E Z'\:'\}.  
Hence *a\(G,* 1\) = *a\(G\), B\(G,* 1\) = *B\(G\),* and for any c E *En* with *U* = \{j E *V\: Cj* = l\} we  
have *a\(* G, c\) = *a\(H\)* and *B\( G,* c\) = *B\(H\),* where *H* is the subgraph of G induced by *U.* By  
duality and relaxation, we obtain  
*a\(G,* c\) \~ *z\(G,* c\) \~ *B\(G, c\).*  
For perfect graphs, we know that the first inequality is an equality for all c and, in  
particular, for c E *En.* The following result, which is the fundamental theorem of this  
section, establishes the second equality for c E *En* and also yields some interesting  
corollaries. 580 III.1. Integral Polyhedra  
Theorem 5.11. *Thefollowing statements are equivalent.*  
1. *P* = *\{x* E R\~\: *Kx* .",; 1\} *is integral.* \(G *is perfect.\)*  
*2. a\(H\)* = *\(\)\(H\) for all node-induced subgraphs H of* G.  
*Proof* 1 = 2. Given that G is perfect, by Proposition 5.10 every subgraph *H* of G is  
perfect. Hence if *H* is the subgraph induced by *U* and if *Cj* = 1 for\} E *U* and *Cj* = 0  
otherwise, we obtain  
\(FNP\) *a\(H\)* = max\{cx\: *Kx* .",; 1, *x* E R\~\}.  
Thus, by linear programming duality, we need to prove that  
\(DFNP\) *min\{1y\: yK* ? c, *Y* E *R';'\}*  
has an integral optimal solution for all c E *Bn.*  
The proof is by induction on the number of positive components of c, which equals  
1 *U* I. Note that if c = 0, then *y* = 0 is an optimal solution to DFNP. Now it suffices to  
assume the hypothesis for all proper subgraphs of G and to prove that  
*min\{1y\: yK?* 1, *Y* E *R';'\}* has an integral optimal solution.  
Let the rows of *K* be *k i* for *i* = 1, ... , *m.* Since *y* = 0 is not feasible to DFNp, there is an  
*i,* say *i* = *r,* such that *y,* \> 0 in an optimal dual solution. Hence by complementary  
slackness, *k'x* = 1 for every optimal solution to FNP.  
If *k'* = 1, then *r* = 1 and an optimal solution to DFNP is *Yl* = 1. If *kr* \< 1, by the  
induction hypothesis, *min\{1y\: yK?* 1 - *k', y* E *R';'\}* has an integral optimal solution,  
say *yO.*  
*Claim* 1\: *yO* + *e,* is an optimal solution to DFNP. Note it is feasible since  
*\(yO* + *e,\)K?* 1 - *k'* + *k r* = 1. Because G is perfect, any maximum cardinality node pack-  
ing on G is an optimal solution to FNp, and hence, by the definition of *r,* every maximum  
cardinality node packing on G contains a node in the clique C = \{j E *V\: k rj* = 1\}. So for the  
subgraph *H* induced by *V* \\ C, a maximum cardinality packing is obtained by deleting a  
node from C from a maximum cardinality packing on G. Hence *a\(H\)* = a\( G\) - 1, and  
*1yO* + 1 = *\(\)\(H\)* + 1 = *a\(H\)* + 1 = *a\(G\).",; \(\)\(G\).",; 1\(yO* + *e,\)* = *1yo* + 1.  
So *yO* + *e* r is an integral optimal solution to DFNP.  
2 = 1. We will prove that *a\(G,* c\) = *z\(G,* c\) for all c E z\~. Statement 2 says that  
*a\(G,* c\) = *z\(G,* c\)for all c E *Bn.* Now for any c E *zn* \\ z\~ let *Cj* = *Cj* if *Cj* ? 1 and let *Cj* = 0  
otherwise. Since *Cj* \< 0 implies *Xj* = 0 in both the fractional and integer node-packing  
problems, *a\(G, c\)* = *a\(G,* c\) and *z\(G, c\)* = *z\(G, c\).* Hence from statement 2 we have  
a\( G, c\) = z\( G, c\) for all C E *zn* with C .",; 1.  
The proof for C E z\~ is by induction with the hypothesis *a\(G, c'\)* = *z\(G, c'\)* for all  
c' \< c. Consider c E z\~ with *Cj* ? 2 for some\}. Let c' = c - *ej.* Since *C;?* 1, it follows from  
complementary slackness that there is an *r* such that *krj* = 1 and *k'x* = 1 in every optimal  
solution to max\{c'x\: *x* E *P\}.* Let c = c - *kr.* Hence by the induction hypothesis, we have  
*a\(G,* c\) = *z\(G, c\).*  
*Claim* 2\: *a\(G,* c\) = *a\(G,* c\) + 1. Since c \> C, we have *a\(G, c\)? a\(G, c\).*  
Let *y* be an optimal solution to  
*z\(G,* c\) = *min\{1y\: yK?* C, *Y* E *R';'\}.* 5\. Node Packing and Perfect Graphs 581  
duplicate 1  
1 '  
duplicate 2  
1 '  
Figure 5.4  
Since *\(y* + *er\)K;;.* e + *k'* = c, we have *z\(G,* c\) \~ *z\(G,* e\) + l. Since *a\(G,* c\) \~ *z\(G,* c\) and  
*a\(G,* e\) = *z\(G, e\),* we have  
*a\(G,* e\) \~ *a\(G,* c\) \~ *a\(G,* e\) + 1.  
Finally, since the a's are integers, *a\(* G, c\) = *a\(* G, e\) or *a\(* G, c\) = *a\(* G, e\) + 1.  
Suppose *a\(G,* c\) = *a\(G,* e\) and let *x* be the characteristic vector of any node packing  
with *ex* = *a\(G, e\).* Then *k'x* = 0 and *c'x* = *a\(G, c'\)* since *a\(G,* e\) \~ *a\(G, c'\)* \~ *a\(G, c\).*  
This is a contradiction because we have already shown that *k' x* = 1 for any node packing *x*  
with *c'x* = *a\(G, c'\).* Hence *a\(G,* c\) = *a\(G,* e\) + 1.  
Now we have  
*a\(G,* c\) \~ *z\(G,* c\) \~ *z\(G,* e\) + I = *a\(G,* e\) + 1 = *a\(G, c\).*  
Hence z\( G, c\) = *a\(* G, *c\),* and the theorem is proved. •  
The standard definition of a perfect graph is statement 2 of Theorem 5.11. In the proof  
of 2 = 1 we first used the trivial implication 2 = 3, where  
3. max\{cx\: *x* E *P\}* has an integral optimal solution for all c E *Bn,*  
and thus we proved 3 = 1. Hence the real content of 2 = 1 is the following result.  
Corollary 5.12. *P* = *\{x* E R\~\: *Kx* \~ 1\} is *integral if* max\{cx\: *x* E *P\} has an integral*  
*optimal solutionfor all* c E *Bn.*  
Corollary 5.12 is rather surprising since, in general, we need integral optimal solutions  
for all c E *zn* to conclude that a polytope is integral.  
There is another interesting interpretation of this result, which involves duplicating the  
nodes of a graph. By duplicating a node *v* of a graph G, we mean that a new node *v'* is  
added to G and that *v'* is joined to all of the neighbors *ofv* but not to *v* \(see Figure 5.4\).  
It is easy to see that if G' is the graph obtained by duplicating node *j Cj* - 1 times in the  
graph G, then *a\(G'\)* is the weight ofa maximum-weight node packing in G with weight *Cj*  
on node *j.* Hence if statement 2 holds for G and all of the graphs obtained from G by 582 111.1. Integral Polyhedra  
duplicating nodes, then statement 1 is true. Thus Corollary 5.12 can also be interpreted in  
graphical terms.  
Corollary 5.13. *then* G' *is perfect.*  
*If* G is *a perfect graph and* G' is *obtained from* G *by duplicating nodes,*  
In polyhedral terms, since G' is perfect ifand only *ifa\(H'\)* = *B\(H'\)for* all subgraphsH'  
of *G',* we have that if G is perfect, then a\( G, c\) = *B\(* G, c\) for all c E *zn.* In other words\:  
Corollary 5.14. *IfG is a perfect graph, the linear system Kx* \~ 1, *x* \~ 0 *is* TO!.  
Yet another corollary to Theorem 5.11 is obtained from Proposition 5.9. Let *w\(G\)* be  
the size of a maximum cardinality clique of G. Since cliques in G correspond to maximal  
node packings in G and conversely, we have  
*w\(G\)* = *a\(G\)* and *w\(G\)* = *a\(G\).*  
Also define the *chromatic number* of G, denoted by *y\( G\),* to be the minimum number  
of colors required to color the nodes of G so that no adjacent nodes have the same color.  
The celebrated four-color theorem says that every planar graph \(a graph that can be drawn  
in the plane without crossing edges\) has *y\(G\)* \~ 4. The complete graph on 4 nodes is a  
planar graph with *y\(G\)* = 4. A minimum cardinality node coloring for the graph of  
Figure 5.1 is shown in Figure 5.5.  
Note that in any feasible coloring, all of the nodes of the same color form a node  
packing. Thus *y\(G\)* is the minimum number of node packings needed to cover all ofthe  
nodes. Hence we have  
*y\(G\)* = *B\(G\)* and *y\(G\)* = *B\(G\).*  
Now from Proposition 5.9 and Theorem 5.11, we immediately obtain the following  
theorem\:  
Theorem 5.15. *Thefollowing statements are equivalent.*  
1. *a\(H\)* = *\(J\(H\) for all node-induced subgraphs H of* G.  
*2. w\(H\)* = *y\(H\)for all node-induced subgraphs H ofG.*  
Theorem 5.15 is known as the *perfect graph theorem.*  
B  
Yellow Blue Red  
Figure 5.5. *w\(G\)* = *y\(G\)* = 3. 5\. Node Packing and Perfect Graphs 583  
8  
3  
5 4  
Figure 5.6 G is perfect by Theorem 5.17.  
It would take us too far from the subject matter of this book to study additional classes  
of perfect graphs. The following two theorems, given without proofs, illustrate some of the  
progress that has been made in identifying classes of perfect graphs. They do not, however,  
give the most general results.  
**Theorem 5.16.** *chords.*  
G *is perfect if each odd cycle of length at least* 5 *contains at least two*  
**Theorem 5.17.** G *is perfect if for each odd cycle there is an edge \(i, j\) of the cycle with the*  
*property that every clique that contains i and j also contains another node of the cycle.*  
Theorem 5.17 is illustrated in Figure 5.6. Edge \(1, 7\) satisfies the hypothesis of the  
theorem, since the only clique that contains \(1, 7\) is C = \{l, 6, 7\}. However, the edge \(6, 7\)  
does not satisfy the hypothesis since it is contained in the clique \{6, 7, 8\}.  
Theorem 5.17 is an immediate corollary to Theorem 4.13, since the class of graphs  
defined in Theorems 5.17 is balanced. This follows since the hypothesis of the theorem  
forbids a submatrix in *';uk* for *k* \~ 3 and odd by requiring that any *\(2k* + 1\) x *\(2k* + 1\)  
submatrix with all row and column sums at least 2 has at least one row with row sum at  
least 3.  
Although no characterization of perfect graphs is known in graphical terms, there is an  
important result which characterizes \(0, 1\) matrices that are clique matrices of perfect  
graphs in terms offorbidden submatrices. We will not prove this theorem.  
**Theorem 5.18.** *are equivalent.*  
*Let A be the m* x *n incidence matrix of a clutter. The following statements*  
*1. A is the clique matrix of a perfect graph.*  
*2. If A contains a p* x *p nonsingular submatrix A' whose row and column sums are all*  
*equal to p,* 2 \~ *P* \~ *\[n/2J,* then there is a *\(p* + 1\) x *p submatrix that contains A' and*  
*also contains a row with row sum greater than p or a row with row sum p that is not*  
*equal to any row of A'.*  
The implication 1 = 2 is easy to prove since if statement 2 is false we obtain a fractional  
extreme point with *Xj* = *p-'* for each column of *A* " and *Xj* = 0 otherwise. 584 111.1. Integral Polyhedra  
If statement 2 is false, then *p* = *p* - 1 implies that *A* is not a clique matrix \(see  
Proposition 5.1\), and *P* = 2 or *\[p* /2\] for *p* odd implies that the graph contains an odd hole  
or an odd anti hole \(see Proposition 5.4\).  
Thus, one approach to the perfect graph conjecture is to consider *minimal imperfect*  
*graphs-that* is, graphs that are imperfect but all of whose node-induced subgraphs are  
perfect. If the perfect graph conjecture were true, Theorem 5.18 says that for a minimally  
imperfect graph, statement 2 must have *p* odd and *P* = 2 or *\[p /2\].*  
For some classes of graphs, the perfect graph conjecture is known to be true. For  
example, the planar graphs without odd holes and odd antiholes are perfect.  
Theorem 5.18 also establishes that the recognition problem for imperfect graphs is in  
*JIPf'.* This follows since \(a\) the clique matrix of an imperfect graph must have a *\(p* + 1\) x *p*  
submatrix for which statement 2 is false and \(b\) such a matrix can be validated in  
polynomial time. However, it is not known whether the recognition problem for perfect  
graphs is in *.N'PJ'.*  
In addition, it is not known whether recognizing graphs that contain no odd hole or  
antihole is in *.N'PJ'.* Obviously these two recognition problems are equivalent if the perfect  
graph conjecture holds.  
We close this section with a brief discussion of algorithms for solving node-packing  
problems. For general graphs, the maximum cardinality node-packing is .N'PJ'-hard \(see  
Section I.5.6\), and even the maximum-weight fractional node-packing problem is *.N'PJ'-*  
hard \(see Section I.6.3\). However, strong fractional cutting-plane algorithms \(which use  
heuristics to find violated clique and other inequalities, and good feasible solutions\) are  
quite successful in solving a variety of instances.  
For general perfect graphs, there is an ellipsoid algorithm that solves the maximum-  
weight node-packing problem in polynomial time. However, the fractional node-packing  
polytope is not the basis of the algorithm since the separation problem for clique  
inequalities is another weighted node-packing problem on a perfect graph. Instead, the  
algorithm uses a convex constraint set which, for a general graph, is contained in the  
fractional node-packing polytope and contains the convex hull of node packings. Hence  
for perfect graphs it coincides with the convex hull of node packings. The separation  
problem for this convex constraint set is solvable in polynomial time. But it is necessary to  
use a generalization of the ellipsoid algorithm to accommodate the nonlinear constraints.  
For some classes of perfect graphs, efficient combinatorial algorithms are known for the  
recognition problem and for solving the maximum-weight node-packing problem. We  
have already solved these problems for bipartite and chordal graphs. More generally, for  
the perfect graphs given in Theorem 5.16, the recognition problem and the maximum-  
weight node-packing problem can be solved in polynomial time.  
Efficient node-packing algorithms are not restricted to perfect graphs.  
*Definition 5.10.* A *line graph L\(G\)* ofa graph G is obtained by replacing each edge ofG  
by a node and joining two nodes by an edge if the two edges in G are incident to a common  
node \(see Figure 5.7\).  
4 5  
\:k8\~3 1  
7 6  
G *L\(G\), L\(G\)* is not perfect  
Figure 5.7 5\. Node Packing and Perfect Graphs 585  
Figure 5.8  
It is easy to see that a subset of nodes in *L\(* G\) is a packing if and only if the  
corresponding set of edges in G is a matching. Hence for line graphs, the maximum-weight  
node-packing problem in *L\(* G\) is equivalent to a maximum-weight matching problem in  
G \(see Chapter 1II.2\).  
The graph in Figure 5.8 is called a *claw.* A graph is called *claw-free* if it does not contain  
a claw as a node-induced subgraph. By drawing a few pictures, the reader can establish that  
line graphs are claw-free, but the converse is false.  
It is easy to see that claw-free graphs need not be perfect since a 5-hole is claw-free. An  
interesting property of claw-free graphs is illustrated in Figure 5.9. The black nodes of the  
graph are a node packing. Nodes \{l, 2, 3, 4, 5\} induce a path whose nodes alternate  
between white and black and whose end nodes are white. By interchanging the colors of  
the nodes on this path, we increase the cardinality of the packing.  
This means of increasing the size of a packing works for claw-free graphs because if  
there were any edges between the nodes \{l, 3, 5\} or between one of these nodes and a black  
node not on the path, the graph would contain a claw. This approach leads to an efficient  
algorithm for solving the maximum-weight node-packing problem on claw-free graphs.  
The algorithm is closely related to the matching algorithm discussed in Section 1II.2.3.  
Claw-free graphs without odd holes and odd antiholes are perfect; that is, the perfect  
graph conjecture is true for these graphs. However, no description of the convex hull of  
node packings is known for claw-free graphs.  
2 3 4 5  
\:"'Q-------!\)'7  
8  
Figure 5.9 586 111.1. Integral Polyhedra  
6\. BLOCKING AND INTEGRAL POLYHEDRA  
In the previous sections of this chapter, we have considered the following types of  
questions\: \(1\) Given a family of polyhedra of the form *P* = *\{x* E R\~\: *Ax* \~ *b\},* under what  
conditions on *\(A, b\)* will *P* be integral? \(2\) When does the dual linear program  
*min\{yb\: yA* ;?; c, *y* E *R'\:'\}* have an integral optimal solution? In particular, in the last  
section we completely characterized when *P* = *\{x* E R\~\: *Ax* \~ 1\} is integral when *A* is a  
0-1 matrix. Here we consider the question of when *Q* = *\{x* E R\~\: *Ax* ;?; 1\} is integral, and  
when the corresponding inequality system is TDI. However, there is no nice characteriza-  
tion known, so we start from a different point of view. Given a finite set *N* = \{l, ... *,n\}*  
and a set *\:JP* of subsets of *N,* we consider the problem  
\(6.1\) *min\{w\(F\)\: FE \:JP\},*  
where *w* E R\~ is a weight function on the elements of Nand *w\(F\)* = *LjEF Wj.*  
We consider two questions\:  
a. How can we formulate \(6.1\) as an integer program?  
b. How can we formulate \(6.1\) as a linear program?  
We will formulate \(6.1\) as an integer program of the form  
min\( *wx\: Ax* ;?; 1, *x* E *Bn\}*  
where *A* is a 0-1 matrix, and then we will ask when the polyhedron *Q* = *\{x* E R\~\: *Ax* ;?; 1\} is  
integral.  
If *Q* is not integral, then to formulate \(6.1\) as a linear program  
\(6.2\) min\{ *wx\: x* E *Q\*\}*  
we will describe the polyhedron *Q\** whose extreme points are the characteristic vectors *xF*  
for *FE\:JP,* and such that min\{wx\: *x* E *Q\*\}* is unbounded if and only ifw *ERn* \\ R\~.  
Many familiar examples of \(6.1\) are associated with graphs. Let G = *\(V, E\)* be a  
complete graph, let *N* = *E* and *Wj* be the weight of *ej* E *E.* Some problems are given below.  
1. *The minimum-weight s-t path problem. FE\:JP* if *F* is the edge set of an *s-t* path.  
*2. The minimum-weight s-t cut problem. FE \:JP* if *F* is the set of edges of a minimal  
*s-t* cut.  
*3. The minimum-weight covering of nodes by edges. FE\:JP* if *F* is a minimal set of  
edges with the property that every node is met by some edge in *F.*  
*4. The minimum-weight star problem. F* E *\:JP* if *F* is the set of edges incident to a node.  
*F* is called a *star.*  
*5. The traveling salesman problem. FE \:JP* if *F* is the edge set of a Hamiltonian cycle.  
A significant difference between problem 4 and the others is that in problem 4, I\:JP 1 =  
I *V* I, while in the others I\:JP 1 grows exponentially with 1 *V* I. Hence problem 4 is easily  
solved by enumeration. Problems 1 and 2 are network flow problems \(see Sections 1.3.2  
and 1.3.4\). Problem 3 is closely related to the matching problems considered in Chap- 6\. Blocking and Integral Polyhedra 587  
ter III.2 and will be considered in Section III.2.4. It can be solved in polynomial time.  
Problem 5 is ,NflP-hard.  
To develop integer and linear programming formulations of\(6.1\), we consider another  
clutter.  
*Definition* 6.1. The *blocking clutter* of \~ is the clutter B\(\~\) whose members *H* satisfy the  
following two conditions.  
1. *Intersection\: H* n *F* '\*' 0 for all *F* E\~.  
*2. Minimality\:* If *H'C H,* then *H'n F* = 0 for some *F* E\~.  
*Example* 6.1. Suppose \~ is represented by the rows of the matrix  
\(1 1 1 0\)  
o 1 0 1 .  
o 0 1 1  
The reader can check that its blocking clutter is specified by the rows of the matrix.  
Proposition 6.1. *B\(B\( g;»* = \~.  
*Proof* For any clutter\~, let \~+ = *\{R\: R* \:2 *F* for some *F* E \~\}. Suppose FE *\$P.* By  
the definition *ofB\(\$P\)* we havethatifH E B\(\~\), thenF n *H* '\*' 0. *HenceF* E \(B\(B\(\~»t.  
Now we need to prove that the members of *\$P* are the minimal elements of *\(B\(B\(\$P»t.*  
Suppose *T* \~ \~+. Then for any G E\~, we obtain G r;t *T.* Hence G n *\(N* \\ *T\)* '\*' 0 for  
all G E *\$P.* So *N* \\ *T* E *\(B\(\$P\)t* and thus *T* \~ *\(B\(B\(\$P»t.* Hence the minimal elements of  
\(B\(B\(\~»t are precisely the members of\~; that is, \~ = B\(B\(\~». *•*  
Thus we can interchange the roles of \~ and *B\( \$P\)* and simply refer to a pair of clutters *\$P*  
and \~ as blocking clutters when \~ = B\(\~\) or \~ = B\(\~\).  
The proof of Proposition 6.1 establishes the following theorem of the alternative, which  
characterizes blocking pairs of clutters.  
Corollary 6.2. *The clutters \$P and* \~ *are a pair of blocking clutters if and only if for all*  
*T* s; *N, there is either an F* E \~ *with F* s; *T or an H* E *'J\{ with H* s; *N* \\ *T but not both.*  
*Proof* We have already shown that if *T* \~ *\$P+,* then *N* \\ *T* E *\(B\(\$P»+.* Both statements  
cannot be true because of the intersection condition. The converse is proved similarly .•  
*Example* 6.2\. Suppose \~ is the clutter of *s -t* paths in a connected graph. We have proved  
in Section 1.3.4 that G containsans-t path ifand onlyifevery *s-t* cutis nonempty. Hence  
every *s-t* path contains an edge belonging to every *s-t* cut and conversely. Thus B\(\~\) is  
the clutter of minimal *s-t* cuts. Figure 6.1 shows a graph and the matrices of incidence  
vectors of *s-t* paths and minimal *s-t* cuts. 588 III.1. Integral Polyhedra  
*u*  
8  
\[\)  
*ej ez e3 e4 es ej ez e3 e4 es*  
1 0 0 1 0 1 1 0 0 0  
1 0 0 1 1 0 0 1  
0 1 1 0 0 1 1 0  
0 0 0 0 0 0 0  
Path matrix Cut matrix  
Figure 6.1  
*Example* 6.3\. Let\:J' be the clutter of edge covers \(covers of nodes by edges\) in a graph  
G = *\(V, E\)* without isolated nodes. *E'* s; *E* is an edge cover if and only if every node in the  
subgraph G = *\(V, E'\)* has degree at least l. Hence *B\(\:J'\)* are the stars ofG. Note that the star  
matrix is the node-edge incidence matrix of G. For the graph of Figure 6.1, the incidence  
matrices of minimal edge covers and stars are given below.  
*ej e2 e3 e4 es ej ez e3 e4 es*  
1 0 1 1 0 1 1 0 0 0  
1 0 0 0 1 0 1 0  
0 1 1 0 1 0 1 1 0  
0 1 0 0 0 0 0  
Minimal edge cover Star matrix  
matrix  
*Example* 6.4. There are some obvious members of the blocking clutter of tours. For  
example, every tour contains at least two edges incident to every node. Thus stars with an  
edge deleted are members of the blocking clutter. But a complete description of the  
minimal edge sets whose deletion would make the graph non-Hamiltonian is not known.  
From the perspective of integer programming, the importance of knowing *B\(\:J'\)* is that  
it gives a formulation of \(6.1\) as a set-covering problem. We use binary vectors *xF* for  
FE \:J' to represent the elements of\:J' and use binary vectors *aH* for HE *B\(\:J'\)* to represent  
elements of *B\(\:J'\).* Let  
Q = *\(x* E R\~\: *ally;* ;?; 1 for all *H* E *B\(\:J'\)\}.*  
*Q\*=* conv\{x E z\~\: *X;?; x F* for some *F* E *\:J'\}.*  
*QB* = *\{a* E R\~\: *x F a;?;* 1 for all *F* E *\:J'\}.*  
QiJ = conv\{a E Z\~\: *a* ;?; *aH* for some *H* E *B\(\:J'\)\}.* 6\. Blocking and Integral Polyhedra *Example* 6.1 *\(continued\).* 589  
Q is the polyhedron given by  
*xER!.*  
The reader can check that \(a\) its extreme points are the incidence vectors of the members  
*of!lF* and the point \(1 1 1 1\) and \(b\) its extreme rays are the 4 unit vectors. Hence  
Q n Z4 is the set of integer vectors equal to or greater than some incidence vector of a  
member *of!lF.* Thus QI3 is the polyhedron given by  
*a ER!.*  
Its extreme points are the incidence vectors of the members of 'Je and the point  
\(0 1 1 1\). Again *QB* n Z4 is the set of integer vectors equal to or greater than some  
incidence vector ofa member *of'Je,* and Q1 = *conv\(QB* n *Z4\).*  
Proposition 6.3. *The following statements are true.*  
1. Q n *zn* = *\{x* E *zn\: x* \~ *x F for some F* E .9'\}.  
2. Q\* = conv\(Q n *zn\).*  
*3. QB* n *zn* = *\{x* E *zn\: a* \~ *aH for some H* E *B\(.9'\)\}.*  
4. Q1 = conv\(QB n *zn\).*  
*Proof* We will establish statement 1. Statement 2 follows immediately from state-  
ment 1. Statements 3 and 4 are proved similarly.  
If *x* E *zn* and *x* \~ *x F* for some *FE.9',* then *ally;* \~ 1 for all *HE B\(.9'\).* Hence  
*x* E Q n *zn.* Conversely, if *x* E Q n *zn* but *x* \~ *x F* fails to hold for all *FE.9',* let  
*T* = \{j\: *Xj* \> O\}. Then it follows from Corollary 6.2 that there exists *HE B\(.9'\)* with  
*H!;; N* \\ *T.* Hence *aHx* = 0 and *x\$.* Q n *Z". •*  
Since *w* E R\~ and any *x* E Q n *zn* satisfies *x* \~ *x F* for some *FE.9',* \(6.1\) can be  
reformulated as the set-covering problem *min\{wx\: x* E Q n *Bn\}.* Moreover, since the  
extreme points of Q\* are precisely *xF* for *F* E .9', we obtain  
*min\{wx\: x* E Q\*\} = *min\{wx\: x* E Q n *Bn\}.* 590 111.1. Integral Polyhedra  
We also obtain analogous results for the problem min\{ *w\(H\)\: HE B\(\:¥\)\}.* In particular,  
*min\{w\(H\)\: H* E *B\(\:¥\)\}* = *min\{wa\: a* E *QB* n *Bn\}*  
*= min\{wa\: a* E Q\~\}.  
Note that *min\{w\(F\)\: FE\:¥\}* can be formulated as the set-  
*Example* 6.1 *\(continued\).* covering problem  
min *WIXI + W2X2 + WyX3 + W\<!X4*  
Xl + X4 \~  
X2 + X3 \~  
X2 + X4 \~  
X3 + X4 \~  
*xEB4.*  
Also *min\{w\(H\)\: H* E *i1t'\}* can be formulated as the set-covering problem  
min *Wlal + W2a2 + W3a3 + W4a4*  
*al + az + a3* \~  
*az* + *a4* \~  
*a3 + a4* \~  
*a EB4.*  
We now investigate the relationships among *Q,* Q\~ *QB,* and Q\~.  
Proposition 6.4. *The following statements are true.*  
a. *Q\* and QB are a blocking pair of polyhedra.*  
b. *Q and* Q\~ *are a blocking pair of polyhedra.*  
*Proof* The extreme points of *Q\** are *XF* for *FE\:¥.* Hence by Proposition 5.7 of  
Section 1.4.5, its blocker is  
An identical argument yields statement b. •  
The relationships are summarized in Figure 6.2.  
Blockers  
Q-\<\<-------------\~\>\~Q;  
Convex hUll! 1 Convex hull  
Blockers  
Q\* E \> *QB*  
Figure 6.2 6\. Blocking and Integral Polyhedra 591  
*Example* 6.1. *\(continued\).* We have shown that the extreme points of *QB* are the  
incidence vectors of the members of'J\{ and the point \(0 ! ! \~\). Hence min\{ *w\(F\)\: F*  
E \~\} can be reformulated as the linear program  
min *WIXI + W2X 2 + W3X 3 + W04*  
*Xl* + *X4* \~  
*X2 + X3* \~  
*X2* + *X4* \~  
*X3 + X4* \~  
*X2 + X3 + X4* \~  
*xER!.*  
2  
since these constraints define the polyhedron Q\~  
Similarly, *min\{w\(H\)\:* HE *'J\{\}* can be reformulated as  
*al + a2 + a3* \~  
*a2* + *a4* \~  
*a3 + a4* \~  
*al + a2 + a3 + a4* \~  
*a ER!.*  
2  
since these constraints define the polyhedron Q\~.  
Now when *Q* is integral, *Q* = *Q\*.* Hence their respective blockers Q\~ and *QB* are equal  
\(see Figure 6.2\). Thus we obtain \(see Theorem 5.10 of Section 1.4.5\) a pair of max-min  
relationships.  
Theorem 6.5. 1. *Q is integral.*  
*2. QB is integral.*  
*3. For all w* E R\~, *we have*  
*Thefollowing statements are equivalent.*  
*4. For all w* E R\~, *we have*  
The max-min equality of statement 4 says that for all wE *RZ,* the weight of a  
minimum-weight element of \~ equals the maximum number of elements of the blocking 592 111.1. Integral Polyhedra  
clutter that can be packed fractionally into the weight vector *w.* Can more be said for  
*w* E z\~? In general, the answer is no; we will consider some examples later. However,  
when the packing problem has an integral optimal solution for all *w* E z\~, we say that the  
*max-min equality holds strongly.* This is equivalent to the system *aHx* \~ 1 for *HE B\(\:¥\)*  
and *x* \~ 0 being TDI, since if *w* E *zn* \\ z\~, the packing problem is infeasible. By  
Proposition 6.1, all of the remarks made in this paragraph about statement 4 also apply to  
statement 3.  
The results of Proposition 6.4 and Theorem 6.5, together with the polynomial equiva-  
lence of optimization and separation \(see Theorem 3.3 of Section I.6.3\), relate the  
computational complexity of the linear programs over Q, Q\~ *QB,* and Q\~.  
Theorem 6.6. *Each member of the following pairs of problems is solvable in polynomial*  
*time if and only if the other member of the pair is solvable in polynomial time.*  
1. *The linear programs over* Q\* *and QB for all w* E R\~.  
*2. The linear programs over* Q\~ *and Qfor all w* E R\~.  
*3. The linear programs over* Q *and QB when* Q *is integral.*  
*Example* 6.2 *\(continued\).* The problem  
can be interpreted as the maximum number of *s-t* paths that can be packed fractionally  
into the weight or capacity vector *w.* Since we can think of each path as a flow of one unit  
from s to *t,* \(6.3\) is a formulation of the max-flow problem. Hence by the max-flow-min-  
cut theorem \(see Theorem 4.1 of Section 1.3.4\), statement 3 of Theorem 6.5 holds for all  
*w* E R\~. Thus Q and *QB* are integral polyhedra. The extreme points of *QB* are the  
incidence vectors of all minimal s *-t* cuts, and the extreme points of Q are the incidence  
vectors of all s *-t* paths. Note from Figure 6.1 that neither the incidence matrix of s *-t* paths  
nor the matrix of minimal *s-t* cuts is balanced, which means that if certain rows were  
dropped from those matrices the corresponding polyhedra would no longer be integral.  
The weighted min-cut problem formulation given by the dual of\(6.3\),  
\(6.4\) *min\{wa\: axF* \~ 1 for *FE\:¥, a* E R\~\},  
can be solved by a constraint generation algorithm since for any *a\** E R\~, it follows that  
*a\*xF* \~ 1 for all *FE* \:¥ifand only if the weight of a mini mum-weight *s-t* path with weight  
vector *a\** is at least 1. Although this algorithm is not practical, it illustrates the connection  
between optimization and separation and how the ellipsoid algorithm is used to prove that  
a combinatorial linear program with a large number of constraints can be solved in  
polynomial time.  
The max-min equality in statement 4 of Theorem 6.5 also holds strongly; that is, the  
maximum number of *s-t* cuts that can be packed into *w* E R\~ equals the weight of a  
minimum-weight *s-t* path, and the packing problem has an integral optimal solution.  
Moreover, Dijkstra's algorithm can be used to construct an integral optimal solution to the  
cut packing problem.  
To show this, we refer to the algorithm in Section I.3.2, and we replace each edge by a  
pair of directed arcs. 6\. Blocking and Integral Polyhedra 593  
Let *g\(j\)* be the weight of a minimum-weight path from node *s* to node\}, and let  
*g\(s\)* = O. Let *VO* = *\{s\}.* At iteration *i,* we have a set *Vi* with  
max *g\(j\)* \:s\:; min *g\(j\)* with *Vi* = *V* \\ *Vi.*  
*JEU' JEU'*  
Let *s* = \}o, and define\}i to be any\} E *Vi* that satisfiesg\(ji\) = *maXjEU' g\(j\).* Hence  
*g\(ji+l\)* = max *g\(j\)* \:s\:; min *g\(j\).*  
*JEVal jEU'*  
The cut *\(Vi, Vi\)* is assigned the weight *YU'* = *g\(j* i+l\) - *g\(j* i\) for *i* = 0, 1, .... Thus  
\(6.5\) *i i*  
*L YUI-l* = *L \(g\(jt\)* - *gUt-I»* = *gUJ*  
t\~1 t\~1  
Now if *t* = \} *k,* we claim that an optimal integral solution to the cut packing problem is  
given by *YU'* = *g\(ji+l\)* - *g\(ji\)* for *i* = 0, ... , *k* - 1 and by *Yu* = 0 otherwise.  
Given *W* E Z\~, we have *YU'* E *Z!,* and by \(6.5\), we obtain L7\~1 *YUi-l* = *get\).* Thus it  
remains to be shown that  
*L YU'-I\:S\:; We* for all *e* E *E.*  
*\(i\:U'3e,i\<;k\)*  
Let *e* = \(j *p,\) q\),* where *q* \> *p.* By definition of *g\(j\),* it follows that *g\(j q\)* ;;;. *g\(j p\)* and  
*we;;;' g\(jq\)* - *g\(jp\).* By \(6.5\), we have *g\(jq\)* - *g\(jp\)* = L7\~p+I *YU'-l* and  
min\(k,q\)  
*L YU'-I\:S\:; L YU'-l*  
*U\:* U!3e,i\~k\} *i\:\:\:=p+l*  
*ifq* \> *k*  
Let *W* = \(3 1 2 4\) be a weight vector for the graph of Figure 6.1. Oijkstra's  
algorithm yields *Vo=\{s\}, g\(s\)* = 0; VI *=\{s, v\}, g\(v\)* = 1; *VZ=\{s,v,u\}, g\(u\)=2;*  
*V3* = *\{s, v, u,* t\}, *get\)* = 4. Hence an optimal integral solution to the cut packing problem  
is obtained by assigning weigh\~f\(v\) - *g\(s\)* = 1 to the cut *\(VO, If\)* = *\{ell ez\},* weight  
*g\(u\) =-l\(v\)* = 1 to the cut *\(VI, V\)* = *\{e\), e3, es\},* and weight *get\)* - *g\(u\)* = 2 to the cut  
*\(VZ, V\)* = *\{e4, es\}.*  
Example 6.2 shows the nicest possible behavior. *Q* and *QB* are integral, and both  
polyhedra are represented by TOI systems. Example 6.3 reveals other possibilities.  
*Example* 6.3 *\(continued\)*  
A. *Bipartite Graphs.* Since the matrix whose rows are the incidence vectors of stars in G  
is the node-edge incidence matrix, it is totally unimodular \(Corollary 2.9\). Hence, the  
polyhedron *Q* is integral and the linear system of inequalities is TO!. Since the packing of  
stars is the same as node packing, we obtain from statement 4 of Theorem 6.5 that the  
weight of a minimum-weight edge cover equals the maximum number of stars or nodes 594 111.1. Integral Polyhedra  
that can be packed into *w* E Z\~. In particular, for *w* = 1, this is the classical result that the  
minimum number of edges needed to cover all of the nodes equals the size of a maximum-  
cardinality node packing.  
Since *Q* is an integral polyhedron, so is *QB'* It can be shown that the packing problem in  
statement 4 of Theorem 6.5 has an integral optimal solution for *w* = 1. This says that the  
maximum number of edge disjoint edge covers equals the degree of the minimum degree  
node.  
B. *General Graphs. Q* is not integral for all graphs. For example, if G is a triangle, *Q*  
contains the extreme point \(1 1 1\).  
The edge-covering problem on the complete graph on 4 nodes, which we considered in  
Example 1.2, is interesting in that it reveals that the packing problems in statements 3 and  
4 of Theorem 6.5 can have different behavior. *Q* is integral, but with *w* = 1 the star packing  
problem has a unique optimal fractional solution. On the other hand, it can be shown that  
the problem of fractionally packing the edge covers has an integral optimal solution for all  
*w* E Z\~. Thus, we have an example of a blocking pair of integral polyhedra for which the  
max-min equality holds strongly for one but not for the other.  
There is an analogous theory, which we consider only briefly, for finding a maximum-  
weight element of a clutter *gjP.*  
*Definition* 6.2\. satisfy the following two conditions.  
The *antiblocking* clutter of *gjP* is the clutter *A\(gjP\)* whose members *H*  
1. Minimum intersection\: I *H* n *F* I \~ 1 for *F* E *gjP.*  
2\. Maximality\: If *H'\:\:J H,* then I *H'* n *F* I \> 1 for some *F* E *gjP.*  
A familiar example of the antiblocking relation arises in the maximum-weight node-  
packing problem. Here *gjP* is the set of maximal node packings in a graph G, and *A \(gjP\)* = cg  
is the set of maximal cliques. Given the weight vector *w* E R\~ on the nodes, the maximum-  
weight node packing problem is  
*max\{w\(F\)\:* FE *gjP\}* = max\{wx\: *x* E *P* n *Bn\},*  
where *P* = *\{x* E R\~\: *Fx* \~ 1 for all C E *cg\},* and *kC* is the incidence vector of the clique C.  
*P* is the fractional node-packing polytope for G.  
In Section 5, we showed that if *P* is integral \(G is perfect\), then the system *kCx* \~ 1 for  
C E cg, *x* \~ 0 is TD I. We used the antiblocking theorem for packing polytopes correspond-  
ing to Theorem 6.5 \(see Proposition 5.8 and Theorem 5.10 of Section 1.4.5\) to show that G  
is perfect if and only if the complement of G is perfect. We also established that these  
results for perfect graphs characterize antiblocking pairs of integral polyhedra. In contrast,  
no simple characterization of blocking pairs of integral polyhedra is known.  
Integer Rounding  
We close this section by considering a related integrality issue regarding the packing  
problems 6\. Blocking and Integral Polyhedra  
595  
\(6.6\)  
*z\(w\)* = *max\{1y\: yA* \~ w, *Y ERr;'\)*  
\(6.7\)  
ZIP\(W\) = *max\{1y\: yA* \~ *w, y* E *Zr;'\),*  
where the rows of *A,* namely, *ai* E Z\~ \\ ° for *i* = 1, ... , *m,* are incomparable vectors and  
w E Z\~. The problem is to determine when ZIP\(W\) = lz\(w\)J for all w E Z\~.  
*Definition* 6.3. The system *\{y ERr;'\: yA* \~ w\) is IRD *\(integer round down\)* if  
ZIP\(W\) = lz\(w\)j for all *W* E Z\~.  
Let Q = \{w E Z\~\: w \~ 1\:7!1 *Aiai,* 1\:7!1 *Ai* = 1 for some *A* E *R';'\),* and let *kQ* = *\{kw\:*  
w E *Q\),* where *k* is a positive integer. Note that *kQ* \~ *\(k* + l\)Q for *k* = 1, 2, ....  
Proposition 6.7. *For any positive integer r, z\(w\)* \~ *r if and only ifw* E *rQ.*  
*Proof z\(w\)* \~ *r* \~ for some *y ERr;',*  
*m m*  
*L Yi* = *rand L Yiai* \~ w \[by \(6.6\)\]  
i=1 i=1  
\~ for some *A ERr;',*  
\~wErQ. •  
Corollary 6.8. *r* \~ *z\(w\)* \< *r* + 1 *if and only ifw* E *rQ* \\ *\(r* + *1\)Q.*  
Hence IRD holds if and only if for all w E Z\~, w E *\(rQ* \\ *\(r* + *1\)Q\)* n Z\~, implies  
ZIP\(W\) = *r.*  
Let *Sk* = *kQ* n *zn* for *k* = 1,2, ....  
*Definition* 6.4\. Q is *integrally decomposable* if for each integer *k* \~ 1 and each w E S *k,*  
there exist *ai,* ... , *ak* E SI \(not necessarily distinct\) such that w = 1\:7=1 *ai*  
# .  
To show that Q is integrally decomposable, it suffices to show that the minimal integral  
points of *kQ* can be expressed as a sum of *k* integral points of Q. This follows since if Wi,  
w2 E *Sk,* w2 \> Wi, and Wi = 1\:7=1 *ai,* where *ai* E SI for *i* = 1, ... *,k,* then  
*k-l*  
w2 = *L ai* + *\(ak* + w2  
i=1  
- Wi\),  
Theorem 6.9. *The system \{y ERr;'\: yA* \~ w\) *is* IRD *if and only if* Q *is integrally*  
*decomposable.*  
*Proof* We show that if *r* \~ *z\(w\)* \< *r* + 1 and Q is integrally decomposable, then  
ZIP\(w\) = *r.* For *r* = 0, we have 0= ZIP\(W\) \~ *z\(w\)* \< 1. Now suppose that *r* is a positive  
integer. By Proposition 6.7, wE *rQ.* Hence there are *ai* E Q n Z\~ for *i* = 1, ... *,r* such  
that *1\:i=1 ai* = w, and there are minimal points *alU\)* E Q n Z\~, not necessarily distinct, such 596 111.1. Integral Polyhedra  
that *al\(i\)* .;;; *iii* for *i* = 1, ... , *r* and I\:\~=I *al\(i\)* .;;; *w.* Now let *yi* be the number of times that *al\(i\)*  
appears in I\:\)=I *al\(i\).* Hence *y\** is a feasible solution to \(6.7\), and I\:7!1 *yj* = *r* = *Iz\(w\)J.*  
To prove the converse, we observe that a feasible solution of value *r* to \(6.7\), together  
with the remark that preceded the statement of Theorem 6.9, yields a suitable decomposi-  
\~. .  
*Example* 6.5\. Suppose  
*A=* 1 0 1 .  
\(1 1 0\)  
o 1 1  
Note that with *w* = \(1 1\), the unique solution to \(6.6\) is *y* = \(1 1 1\) and *z\(w\)* = *i.*  
Now we show that *Q* is integrally decomposable. It is easy to check that all minimal  
points of *kQ* are of the form  
\(AI + *A2,* Al + *A3,* A2 + *A3\),* Al + A2 + A3 = *k,* A \~ 0  
*= \(al\> a2, 2k* - al - *\(2\),* 0.;;; *al\> a2* .;;; *k,* al + *a2* \~ *k.*  
So we need to show that foral\> *a2* E Zl, *al\> a2';;; k,al* + *a2* \~ *k,* thereisay E Z\~such that  
I\:t=1 *Yi* = *k* and  
*YI* + *Y2* = al  
*YI* + *Y3* = *a2*  
*Y2* + *Y3* = *2k* - al - *a2·*  
A solution is *YI* = al + *a2* - *k, Y2* = *k* - *a2,* and *Y3* = *k* - al.  
Different behavior is observed for the matrix  
1 1  
*A* = 1 0  
\(  
o 1  
o 0  
1 0 0 0\)  
o 1 1 0  
# 0011·  
o 1  
Note that  
1\) E *2Q.*  
But there are not two integral vectors in *Q* whose sum is *w.* Hence *Q* is not integrally  
decomposable. In particular, *ZIP\(W\)* = 1 and *z\(w\)* = 2.  
We now consider a network flow model whose integral solutions define a matrix *A*  
such that *\{Y* E *R';.'\: yA* .;;; *w\}* is IRD. Let *gy* = *\(V, SIl\)* be a directed graph with ISilI = *n.* A  
vector bE ZiVl with *I\:vEV bv* = 0 is called a *supply-demand vector.* The nodes  
*L* = \{v E *V\: bv* \> O\} are called *supply nodes,* and the nodes *T* = \{v E *V\: bv* \< O\} are called  
*demand nodes. Afeasible flow* is a vector *a* E R\~ that satisfies the conservation equations  
\(6.8\) *bv* + *L auv* - *L avu* = 0 for v E *V.*  
*uEo-\(v\) UEo+\(v\)* 6\. Blocking and Integral Polyhedra 597  
Let *A* be the matrix whose rows are the vectors of minimal, integral feasible flows in *qj\).*  
The problem we consider is packing the rows of *A* into *w* E Z\~.  
*Example* 6.6\. minimal integral feasible flows is  
Consider the data given in Figure 6.3. It can be shown that the matrix of  
*A=* 2 0 1 2 0  
2 0 2 1 1  
2 0 3 0 2  
0 2 0 1 1  
0 2 1 0 2  
0 2 0  
1 1 1  
2 0 2  
It is easy to see that the packing problem does not have an integral optimal solution for  
all *w* E Z\~ for which it is feasible; for example, take *w* = \(1 0 1 1 0\).  
To show that the system *\{y* E *R';'\: yA* .;;; *w\}* is IRD, we need to introduce a capacity  
vector *d* E Z\~ on the arcs of *qj\).*  
Proposition 6.10. *Given any d* E Z\~, *the following two statements are equivalent.*  
i. *There exists an a* E Z\~ *that satisfies* \(6.8\) *and a* .;;; *d.*  
ii. *For all U* 5; *V,*  
\(6.9\) I *bv ';;;* I *de.*  
*vEU eE6'\(U\)*  
*Proof* i =\> ii is obvious since for any *U* 5; *V,* the flow out of *U* must be at least *1\:vEU bv•*  
The proof of ii =\> i uses the max-flow-min-cut theorem on the graph *qj\)' =*  
*\(V* u *\{s, t\}, .sil'\),* where  
*.sil' =.sII* U *\{\(s, v\)\: vEL\}* U *\{\(v, t\)\: v* E n,  
The capacity of *e* E.sII is *de.* the capacity of *esv* for *vEL* is *bv,* and the capacity of *evt* for  
*vET* is *-bv•* We only sketch the proof.  
If \(6.9\) holds for all *U* 5; *V,* then it can be shown that a minimum-weight cut in *qj\)'* is  
given by the set of arcs *\{\(s, v\)\: v* E L\}-that is, the cut generated by the node partition  
*\(\{s\}* U *\(V* \\ *L\),* \{t\} U *L\).* Then by the max-flow-min-cut theorem of Section I.3.4, there is  
*b3 = -1*  
Figure 6.3 598 111.1. Integral Polyhedra  
an integral *s-t* flow in *qj\)'* of size *LVEL by.* Thus in every maximum flow, the flow on *eSV* is *bv*  
for all *vEL.* It then follows that statement i is true. •  
Theorem 6.11. *If A is an m* x *n matrix whose rows are the minimal integral flows in a*  
*digraph qj\) with supply-demand vector b* E z111, *then \{y* E *R'\:'\: yA* \~ *w\} is* IRD.  
*Proof* Let *Sk* = *kQ* n *zn* for *k* = 1,2, .... By Theorem 6.9, it suffices to prove that  
for any *k* and *wE Sk, w* can be written as the sum of *k* integral points in Sl. This is a  
triviality for *k* = 1. Now suppose it is true for *Sk-l* where *k* \~ 2.  
We must show that for any *w* E *Sk,* there exist an *a* E Sl such that *w* - *a* E *Sk-lo* Note  
that *wE Sk* means that for the supply-demand vector *kb,* there is a flow *ak* \~ *w.* But since  
the supply-demand system is totally unimodular, we can choose *ak* E z\~. Hence  
*kbv* + I a\~v - I a\~u = ° for *v* E *V,*  
*uEO-\(v\) uEO'\(v\)*  
and for any *U* \~ *V* with *LVEU bv* \~ 0, we have  
I a\~ = *k* I *bv* + I a\~ \~ *k* I *bv* \~ I *by.*  
*eEO'\(U\) vEU eEO-\(U\) vEU vEU*  
Now taking *ak* to be the capacity vector in Proposition 6.10, there exists an *a* E z\~ satisfies \(6.8\) and *a* \~ *ak.* Thus *a* E S\), *ak* - *a* E z\~, and  
*\(k* - *l\)bv* + I \(a\~v - *auv\)* - I \(a\~u - *avu \)* = ° for *v* E *V.*  
*eEO-\(v\) uEO'\(v\)*  
that  
Hence *\(ak* - *a\)* E *Sk-l* and, since *w* \~ *ak,* we have *\(w* - *a\)* E *Sk-lo* •  
Theorem 6.11 generalizes to capacitated supply-demand systems where, in addition to  
\(6.8\), the flow must satisfy *a* \~ c where c E z\~. This can be shown by transforming a  
capacitated supply-demand system to an uncapacitated one. It also generalizes to  
circulations; that is, *bv* = ° for all *v* E *V,* and *I* \~ *a* \~ c where *I,* C E z\~. Thus a circulation  
is a solution to *Ga* = 0, *I* \~ *a* \~ c, where G is a node-arc incidence matrix. Finally, packing  
the minimal solutions of *Ga* = 0, *I* \~ *a* \~ c is IRD for any totally unimodular matrix G.  
7. NOTES  
Section **111.1.1**  
The study of integral polyhedra has its roots in the theory of network flows \[see Ford and  
Fulkerson \(1962\)\] and, in particular, in the max-flow-min-cut theorem. Two early proofs  
of this theorem illustrate fundamental techniques in the theory of integral polyhedra.  
Dantzig and Fulkerson \(1956\) proved it using linear programming duality, and Ford and  
Fulkerson \(1956\) proved it by giving an algorithm that produces a feasible flow and an *s-t*  
cut of weight equal to the value of the flow \(see Section I.3.4\).  
Proposition 1.3 is due to Hoffman \(1974\). Edmonds and Giles \(1977\) independently  
proved Proposition 1.3 and Corollary 1.4, and they coined the term *total dual integrality*  
and expounded upon its significance. They also developed the notion of box TDI systems\:  
A system *Ax* \~ *b,* c \~ *x* \~ *d* is *box TDI* ifit is TDI for all vectors c, *d.* 7. Notes 599  
Giles and Pulleyblank \(1979\) proved Proposition 1.7. Schrijver \(1981\) proved Proposi-  
tion 1.8.  
Cook \(1983a\) studied operations that preserve total dual integrality \[also see Cook  
\(1986\) for box TDI systems\]. Computational issues regarding TDI systems have been  
studied by Chandrasekaran \(1981\) and Cook, Lovasz, and Schrijver \(1984\).  
Edmonds and Giles \(1984\) gave a survey of theoretical results on total dual integrality  
and classes ofTDI systems.  
Schrijver \(1986b\) gave a survey of proof techniques for establishing integrality and  
related properties of polyhedra.  
Section 111.1.2  
Hoffman and Kruskal \(1956\) proved Theorem 2.5 and thus established the fundamental  
part of the connection between total unimodularity and integer programming \[also see  
Hoffman \(1979\)\]. A substantially simpler proof, the one presented in the text, was  
discovered by Veinott and Dantzig \(1968\).  
Theorem 2.7 was proved by Ghouila-Houri \(1962\). The results on characterizations of  
totally unimodular matrices with no more than two nonzero elements in each column are  
due to Heller and Tompkins \(1956\), Hoffman and Kruskal \(1956\), and Dantzig and  
Fulkerson \(1956\).  
Interval matrices were studied by Fulkerson and Gross \(1965\). The relaxation of a set-  
covering problem to a problem with an interval constraint matrix was given by  
Nemhauser, Trotter, and Nauss \(1974\).  
Other conditions for total unimodularity were given by Camion \(1965\), Chan-  
drasekaran \(1969\), Heller \(1957, 1963\), Heller and Hoffman \(1962\), Padberg  
\(1976a, 1988\), Tamir \(1976\), and Truemper \(1977, 1978\). See Padberg \(1975b\) for a survey.  
In a study of the integrality of the matching polytope, Hoffman and Oppenheim \(1978\)  
proposed the idea oflocal unimodularity and thus gave another technique for establishing  
the integrality of a polyhedron.  
Section 111.1.3  
The significance of recognizing network structure has been stimulated, in part, by a  
number of practical linear programming models that can be reformulated as network flow  
problems \[see Zangwill \(1966\), Cunningham \(1983\), and Bland \(1988\)\] and was also  
motivated by the efficiency of network codes \(see the notes for Chapter 1.3\).  
The definition of network matrices was proposed by Tutte in his study of graphic  
matroids \[see Tutte \(1965\)\]. Further references to matroids will be given in the notes for  
Chapter III.3.  
Iri \(1966\) gave a polynomial-time algorithm for recognizing network matrices. A much  
more efficient algorithm was obtained by Bixby and Cunningham \(1980\). Their presenta-  
tion is in terms of matroids. The algorithm given here is adapted from Schrijver \(1986a\).  
Recently, attention has been given to finding large network submatrices \[see Bixby and  
Cunningham \(1980\) and Bixby \(1984\)\]. Several researchers have developed heuristics for  
this problem \[see Brown and Wright \(1984\) and Gunawardane et al. \(1981\)\]. The problem  
of finding a largest network submatrix is ,N'9P-complete \[see Bartholdi \(1981\)\].  
Theorem 3.8 and the algorithm for recognizing totally unimodular matrices are due to  
Seymour \(1980\). For a restricted class of totally unimodular matrices, Yannakakis \(1985\)  
gave efficient recognition and optimization algorithms. 600 III.I. Integral Polyhedra  
Section 111.1.4  
Balanced matrices were introduced by Berge \(1972\). He proved the fundamental result  
given by Theorem 4.13. Several other results on the integrality of polyhedra associated  
with balanced matrices were obtained by Fulkerson, Hoffman, and Oppenheim \(1974\). In  
particular, they showed that if *A* is balanced and the system *Ax* = 1, *x* \~ 0 is feasible, then  
the polytope defined by this system is integral. This result on set-partitioning polytopes  
can be used to prove Theorem 4.13.  
The restriction to totally balanced matrices was apparently proposed by Lovasz  
\(1979b\). The main results on totally balanced matrices given here \(Proposition 4.4 through  
Theorem 4.10\) come from Hoffman, Kolen, and Sakarovitch \(1985\). Proposition 4.11 is  
due to Giles \(1978\) and was used by Kolen \(1983\) to obtain integrality results for a class of  
uncapacitated facility location problems. Tamir \(1983\) gave the generalization stated in  
Proposition 4.12. Further generalizations were given by Tamir \(1987\).  
Farber \(1983\) and Anstee and Farber \(1984\) independently obtained nearly the same  
results as Hoffman et al. \(1985\). Their characterization of totally balanced matrices is in  
terms of node-node incidence matrices of graphs. Extensions have been obtained by  
Lubiw \(1982\) and Chang and Nemhauser \(1984, 1985\). Also see Sakarovitch \(1975, 1976\)  
and Farber \(1984\).  
Section 111.1.5  
The concept of perfect graphs is due to Berge \(1960\). It has led to a vast literature, mainly  
on graph theory, which we barely cite here. Instead, we refer the reader to the book by  
Golumbic \(1980\), the collection of articles edited by Berge and Chvatal \(1984\), and the  
chapter entitled "Stable Sets in Graphs" in the book by Grotschel, Lovasz, and Schrivjer  
\(1987\).  
Duchet \(1984\) presented a survey of classic results on perfect graphs. Fulkerson \(1970b,  
1971, 1972, 1973\) made the connection between perfect graphs and polyhedral combina-  
tories, and he introduced the concept of pluperfect graphs.  
Dirac \(1961\) established the connection between simplicial nodes and chordal graphs.  
Gavril \(1972\) solved the cardinality node-packing problem and the corresponding clique-  
covering problem for chordal graphs. Frank \(1975\) solved the weighted versions of these  
problems essentially by the linear-time algorithm given in the text.  
Theorem 5.11 and the perfect graph theorem, Theorem 5.15, were proved by Lovasz  
\(1972\). However, he acknowledges that much credit should be given to Fulkerson who had  
already shown that these theorems were true if and only if Corollary 5.12 was true. The  
proof of Theorem 5.11 given here comes from Chvatal \(1975\).  
Theorem 5.15 was proved by Meyniel \(1976, 1984\). A polynomial-time agorithm for  
recognizing these graphs has been obtained by Burlet and Fonlupt \(1984\). Theorem 5.16  
was proved by Berge \(1972\).  
Theorem 5.17 was proved by Padberg \(1974\). Some other articles related to Padberg's  
work on minimally imperfect graphs are by Padberg \(1975b, 1976b, 1984\), Bland, Huang,  
and Trotter \(1984\), and Whitesides \(1984\).  
A polynomial-time ellipsoid algorithm for maximum-weight node packing in perfect  
graphs was given by Grotschel, Lovasz, and Schrijver \(1984a\). Recently, they have  
obtained a polynomial-time ellipsoid algorithm for maximum-weight node packing in  
graphs for which the node-packing polytope is described by the clique and odd hole  
constraints \[Grotschel, Lovasz, and Schrijver \(1988\)\]. These graphs are called *t-perfect.* 7. Notes 601  
Hsu \(1984\) gave a survey of graphs for which the strong perfect graph conjecture is true.  
It was proved for claw-free graphs by Parthasarathy and Ravindra \(1976\). Polynomial-  
time algorithms for solving the weighted node-packing problem on claw-free graphs have  
been given independently by Minty \(1980\) and Sbihi \(1980\). The convex hull of node  
packings for these graphs has been studied by Giles and Trotter \(1981\). Polynomial-time  
algorithms for maximum-weight cliques, minimum-weight clique covers, and minimum  
colorings for claw-free perfect graphs have been obtained by Hsu \(1981\) and Hsu and  
Nemhauser \(1981, 1982, 1984\). These problems are .N'\~-hard for general claw-free graphs.  
Section III.1.6  
The theory of blocking and antiblocking polyhedra was developed in a series of articles by  
Fulkerson \(1968, 1970a, 1971, 1972\). Fulkerson's work was motivated by a 1965 paper of  
Lehman which was not published until 1979. A survey of results obtained in the 1970s has  
been presented by Tind \(1979\). \[Also see Tind \(1974, 1977\), Johnson \(1978\), and Huang  
and Trotter \(1980\).\]  
Proposition 6.1 was proved by Edmonds and Fulkerson \(1970\). Propositions 6.3 and 6.4  
and Theorem 6.5 were proved by Fulkerson \(1970a\).  
Fulkerson \(1968\) showed that the max-min inequality holds strongly for the *s-t* path  
and *s-t* cut clutters. There are several interesting pairs of clutters for which the max-min  
inequality holds, but not strongly, and for which one or both of the dual problems has an  
optimal solution that is half-integer for all nonnegative integers *w.* \(A vector is said to be  
half-integer if each of its components is either an integer or an integer divided by 2.\) An  
example where both of the clutters have this property is 2-commodity cuts and flows in  
graphs \[see Hu \(1969\) and Seymour \(1978\)\]. The max-min inequality holds for the *T-join,*  
*T-cut* clutters to be studied in Section III.2.4. However, here one of the packing problems  
has the half-integer property and the other does not \[see Edmonds and Johnson \(1973\) and  
Seymour \(1979\)\].  
In general, the problem of characterizing pairs of clutters for which the max-min  
inequality holds \(or holds strongly\) or for which the half-integer property is obtained for  
one or both of the packing problems is unresolved. However, Seymour \(1977\) character-  
ized the strong max-min inequality for an interesting class of clutters known as *binary*  
*clutters.* Some other blocking relations will be studied in Section 111.2.4 and Chapter  
111.3.  
The connection between the integer round-down property and integral decomposabil-  
ity was established by Baum and Trotter \(1977, 1981\). Further results along these lines were  
obtained by McDiarmid \(1983\).  
The IRD property for network flows given in Theorem 6.11 is due to Fulkerson and  
Weinberger \(1975\). Additional integer-rounding results for network flow problems have  
been obtained by Weinberger \(1976\) and Trotter and Weinberger \(1978\).  
Marcotte \(1985, 1986a\) has established some families of knapsack problems for which  
the cutting stock problem has the integer-rounding property and has also given an instance  
of the cutting stock problem where the gap is equal to 1.  
Some literature on integer-rounding results for matroid problems will be cited in the  
notes for Section III.3.8.  
Computational complexity issues associated with problems with the IRD property have  
been studied by Baum and Trotter \(1982\) and Orlin \(1982\). 602 III.I. Integral Polyhedra  
8\. EXERCISES  
1. Consider the polytope *P* described by the linear inequality system  
*x ",,0.*  
2. i\) ii\) Show that *P* is an integral polytope.  
Show that the linear inequality system is not TO!.  
iii\) Find the unique minimal TOI representation with an integral right-hand side.  
Find a TOI representation for the polytope  
*P* = *\{x* E R\~\: *4Xl* + *X2* \<S; 28, *Xl* + *4X2* \<S; 27, *Xl* - *X2* \<s; n.  
3. 4.  
A linear inequality system *Ax* \<s; *b* is *box* TOI if *Ax* \<s; *b, I* \<s; *X* \<s; *u* is a TDI system  
for alII and *u ERn.*  
i\) Show that the system of exercise l\(iii\) is not box TO!.  
ii\) Show that the system *Xl* + *X2* + *X3* \<s; 4, *X""* 0 is box TO!.  
iii\) Show that the system of Example 1.1 is box TO!.  
Verify that the top two matrices are TU but the bottom two are not.  
-1 0 0 -1 1 1  
-1 1 -1 0 0 1 0 0  
0 -1 1 -1 0 0 1 0  
0 0 -1 -1 0 0 1  
-1 0 0 -1 0 0  
1 1 0 1 0  
0 1 1 0 0 0  
-D 0 0 0 1 1 1  
1 0 0 0 -1  
0 0 0  
U 0 -1  
5.  
Show that  
*A* \~ \(-;  
1 1\)  
1 0  
o 0  
6\. is not *TV.* Then show that *P\(b\)* = *\{x* E R\~\: *Ax* = *b\}* is integral for all *bE zn* for  
which it is nonempty.  
Show that if *A* is a 0, 1, -1 matrix in which the sum of the entries of every square  
submatrix with even row and column sums is divisible by 4, then *A* is *TV.* 8\. Exercises 603  
7. Suppose that the 0, 1 matrix *A* is not an interval matrix and that the integer program  
\(2.1\) is relaxed by splitting columns as described. If each column is split into, at most,  
*p* columns, compare the bound from this relaxation with that from the standard  
linear programming relaxation.  
8. Prove Proposition 2.11.  
9. Verify whether the following are network matrices or not.  
i\)  
0 -1 0 1 -1 1 0 -1 0  
0 1 0 0 -1 0 -1 1 0  
1 0 0 0 0 0 0 0  
0 1 -1 0 0 0 0 1 0  
1 1 0 0 0 0 0 0 0  
0 0 0 0 0 0 1  
0 0 0 0 -1 1 -1 0 0  
0 -1 0 0 0 0 0 -1 -1  
0 0 -1 0 0 0 0  
ii\)  
1 0 0 1 -1 0  
0 1 -1 0 0 0  
0 0 0 1 -1 -1  
0 0 0 0 -1  
1 0 0 1 0  
0 -1 0 -1 0 0  
0 0 0 0  
iii\)  
-1 0 0 -1  
-1 -1 0 0  
0 -1 -1 0  
0 0 -1 1 -1  
-1 0 0 -1  
**10.** Modify the network recognition algorithm so as to find a maximal network sub-  
matrix.  
**11.** Let *A* be a 0, 1 matrix with no zero rows or columns. Show that *\{x* E R\~\: integral if and only if statement lor statement 2 of Proposition 4.1 holds.  
*Ax* = 1\} is  
**12.** Are interval matrices \(i\) balanced, \(ii\) TB? 604 IIU. Integral Polyhedra  
13. i\) Show that  
1 1 0 0 0  
0 0 0 0  
*A=* 0 0  
1  
0 1 1  
0 0 0  
14. is a row inclusion matrix.  
ii\) Solve min\{cx\: *Ax* \~ 1, *x* E Z\~\} with c = \(4 2 7 1 3 5\).  
Convert the following matrix to a TRL matrix.  
# 001  
o 0  
1 1 0 0  
o 0  
001 o 0  
1 1  
# 100  
# 010  
# 001  
# 000  
1 1 0  
# 100  
# 000  
o 0  
# 000  
# 101  
# 000  
# 000  
# 001  
# 000  
# 010  
# 000  
# 000  
1 1 0 0 0 0  
o 0 0 0 0 0  
o 0 0 0 0 0  
o 0 0 0 0 0  
1 1 0 0 0 0  
1 1 0 0 0 0  
000 1  
o 0 1 0  
# 00001  
o 010  
o 100  
o 000  
Then give a short proof that the matrix is not totally balanced.  
15\. Solve the problem of finding a minimum-weight set of nodes that can serve every  
node on the graph shown in Figure 4.3, where *rj* = 1 when\} is even, *rj* = 2 when\} is  
odd, and *dij* is the number of edges on the path joining i and\}. The weights are given  
by c = \(4 2 7 3 12 8 10 5 7 3 12\).  
16\. Prove Proposition 4.12.  
17\. Prove Theorem 4.13.  
18. Let  
where *E* is the matrix of alII's.  
i\) Prove that *A* \* is a neighborhood matrix.  
ii\) Prove that *A* is TB if and only if *A* \* is TB.  
19\. A *strong elimination ordering* of a graph G = *\(V, E\)* is a perfect elimination  
ordering Vb ••• , *Vn* of *V* satisfying the following additional conditions for each i,\}, *k,* 8\. Exercises 605  
*I\:* If i *\<j* \< *k* \< *I* and *\(Vi, Vk\), \(Vi, VI\), \(Vi\> Vk\)* E *E,* then *\(vi\> VI\)* E *E.* A graph is  
*strongly chordal* if it has a strong elimination ordering.  
i\) ii\) Show that the graph of Figure 5.1 is not strongly chordal  
Show that the graph of Figure 8.1 is strongly chordal.  
iii\) Show that a graph is strongly chordal if and only if its neighborhood matrix is  
balanced.  
20. Is *P* = *\{x* E R\~\: *Ax* \~ l\} integral for the following matrices? Why?  
i\)  
1 0 0 0 1  
0 1 1 0 0 1  
*A=* 1 0 0 1 0 0  
0 1 0 0 1  
0 0 0 0  
ii\)  
21. 22\. 23. 24. 25. 0 1 0 0 1  
0 0 0 1  
0 0 0 1 0 1  
*A=* 0 0 0 0 1 0 0  
0 0 0 0 0  
1 0 0 0  
0 0 0  
A graph G = *\(V, E\)* is called an *interval graph* if there is an assignment of an interval  
of the real line to each V E *V* such that *\(u,* v\) E *E* if and only if the intervals  
corresponding to *u* and V intersect. Show that interval graphs are chordal and that  
there is a greedy algorithm for solving max\{cx\: *Ax* \~ 1, *x* E *Bn\}* when *A* is an  
interval matrix.  
Describe a polynomial algorithm to check whether *A* is the clique matrix of some  
graph.  
i\) ii\) Give a polynomial algorithm to find an odd cycle in a graph.  
Use this to devise a heuristic algorithm to detect odd holes in a graph.  
Give an O\( I *E* I\) algorithm to construct a PES or to show that a graph is not chordal.  
Let *N* be a set of subtrees of a tree. Let *A* be the resulting node-tree incidence matrix.  
Show that *A* is the clique matrix of a chordal graph and conversely.  
# \<lZ1\>  
Figure 8.1 606 111.1. Integral Polyhedra  
26\. Let *A* be the clique matrix of a chordal graph with PES = \{l, ... *,n\}.* Give an  
algorithm to solve max\{cx\: *Ax* \~ *b, x* E z\~\}, where *b l* \~ *b2* \~ ••• \~ *bn •*  
27\. 28. 29. 30. 31. Give a polynomial algorithm for node coloring of chordal graphs.  
G = *\(V, E\)* is a *comparability* graph if there is an orientation of each edge *e* E *E*  
giving a digraph qz; = *\(V, d\)* having the properties that if *\(i,* i\), *\(j, k\)* Ed, then  
*\(i, k\)* E *d.* Show that comparability graphs are perfect.  
i\) ii\) What is the rank 1 hull of the node-packing problem, where S = *P* n *zn* and  
*P* = *\{x* E R\~\: *Xi* + *Xj* \~ 1 for *e* = *\(i,\}\)* E *E\}?*  
Show that the rank 1 hull is not integral if G contains a node-induced subgraph of  
the form shown in Figure 8.2.  
Describe the convex hull of incidence vectors of node packings for line graphs.  
A clutter *fliP* is represented by the rows of the matrix  
1 0 0 0  
1 0 1 0 0  
*A=* 0 1 0 1 0  
0 0 1  
0 0 0  
i\) Find its blocking clutter *B\(\:Ji\).*  
ii\) Find the polyhedron Q\~ of the form *\{x* E R\~\: *Bx* \~ 1\} with *B* \~ 0 having the  
incidence vectors of the members of *B\(fliP\)* as extreme points.  
iii\) Find a polyhedron Q\* of the above form having the incidence vectors of the  
members of *fliP* as extreme points.  
32. Given a connected graph G = *\(V, E\),* let *A* be the incidence matrix of spanning trees  
by edges; that is, each row of *A* is the incidence vector of a spanning tree.  
i\) Give a polynomial-time ellipsoid algorithm for solving the linear program  
min\{cx\: *Ax* \~ 1, 0 \~ *x* \~ n.  
ii\) Specify *r, q,* and any other information needed by the ellipsoid algorithm.  
iii\) Give a combinatorial interpretation of the problem. Do you know an efficient  
combinatorial algorithm for solving it?  
33\. Let *\:Ji* be the clutter of spanning trees of a graph G.  
i\) Find its blocking clutter *B\(fliP\).*  
ii\) Give an example to show that Q = *\{x* E RI;I\: *aHx* \~ 1 for HE *B\(\:Ji\)\}* is not  
integral.  
Figure 8.2. "Odd *K 4,,;* each wavy line denotes a path with an odd number of edges. 8\. Exercises  
607  
Figure 8.3  
34\. Let\:Ji be the clutter ofbranchings rooted at node 1 in \~.  
i\) Find its blocking clutter *B\(\:Ji\).*  
ii\) Show that *Q* = *\{x* E R\~I\: *aHx* \~ 1 for *HE B\(\:Ji\)\}* is integral.  
iii\) Let *A* be the branching by arc incidence matrix.  
a\) Does the max-min inequality hold strongly for rooted branchings? That is,  
does  
max\{ly\: *yA* \~ W, *Y* E Z,\:,\} = min *\{aHw\:* w E Z\~\~?  
*HEBW\)*  
b\) Does the IRD property hold for *\{y* E *R'\:'\: yA* \~ *w\}?*  
35\. Let *\:Ji* be the clutter of cycles in a graph G.  
i\) Find its blocking clutter *B\(\:Ji\).*  
ii\) Use the graph in Figure 8.3 with w = \(4 3 2 1 8\) to show that *Q* is not  
integral.  
iii\) Give a polynomial combinatorial algorithm to find a minimum-weight cycle  
when w E *Rif'.*  
iv\) Give a polynomial combinatorial algorithm to find a minimum-weight element  
of *B\(\:Ji\).*  
36. For the graph of Figure 8.4, find the maximum number of *s-t* cuts that can be  
packed into w, where w is indicated in the figure.  
37\. Let \~ be the set of minimal feasible solutions to S = *\{x* E *Bn\: LjEN ajxj* \~ *b\}.*  
i\) Find the blocking clutter *B\(\:Ji\).*  
ii\) Use this to give a reformulation *ofmin\{cx\: xES\}* with c E R\~.  
iii\) Compare the inequalities of this form with the valid inequalities generated in  
Section 11.2.2.  
Figure 8.4 111.2  
# Matching  
1. INTRODUCTION  
In a graph G = \(V, E\), the number of edges that meet node i is called the degree 9f node i.  
Matching problems involve choosing a subset of the edges subject to degree constraints on  
the nodes. The simplest case is i-matching \(or just matching\). A matching M 5; E is a  
subset of edges with the property that each node in the subgraph G\(M\) = \(V, M\) is met by  
no more than one edge. Every graph G contains a matching, namely M = 0. An obvious  
generalization of I-matching is b-matching in which node i is met by no more than bi  
edges, where bi is a positive integer. In a b-matching problem, we may impose the  
restriction that each edge is chosen no more than once \(O-i b-matching\) or allow an edge to  
be chosen a nonnegative integer number of times \(integer b-matching\). A b-matching is  
called perfect if each of the degree constraints holds with equality. In particular, in a perfect  
i-matching each node is met by exactly one edge. Another variation on matchings is to  
require that each node i be met by at least b i edges. These problems are called node  
covering by edges.  
Let Ce be the weight of e E E and let c\(E'\) = .l\:eEE' Ce be the weight of E' 5; E. The  
weighted b-matching problem is to find a b-matching of maximum weight. In the case of  
perfect matchings, it also makes sense to consider minimum-weight matchings. When  
Ce = 1 for all e E E, the optimization problem is called an unweighted or cardinality  
problem.  
An integer programming formulation of the weighted 0-1 b-matching problem is  
maxcx  
Ax"",b  
where A is the node-edge incidence matrix of the graph, I E I = n, and x e = 1 means that e  
is in the matching.  
The important property of A for matching problems is that each of its columns contains  
exactly two is; in other words, .l\:i aij = 2 for allj E E. Note that if the graph is bipartite,  
then A is totally unimodular so that the extreme points of \{x E R\~\: Ax "'" b\} are precisely  
the b-matchings. However, when G contains an odd cycle, the constraint set ofthe linear  
programming relaxation can contain fractional extreme points. For example, in the graph  
of Figure 1.1, x = \(! ! n is the unique optimal solution to the linear programming  
relaxation with c = \(1 1 1\) and b = \(1 1 1\).  
The classic application of matching deals with the pairing of objects from two disjoint  
sets \(e.g., workers with jobs, men with women, etc.\). The perfect matching problem  
608 1. Introduction 609  
Figure 1.1  
associated with such pairings is on a bipartite graph, and the optimization problem is the  
assignment problem \(see Section 1.3.5\). Pairings, however, do not necessarily involve  
disjoint sets \(e.g., the selection of roommates in a college dormitory\). So we see that the  
weighted perfect I-matching problem is a meaningful generalization of the assignment  
problem.  
We have already mentioned some other applications of matching in connection with  
relaxations and heuristics for the traveling salesman problem \(see Section II.6.3\). For  
example, a perfect 0-1 2-matching is a relaxation of the traveling salesman problem. We  
have also used weighted I-matching in the spanning-tree matching heuristic for the  
euclidean traveling salesman problem.  
Another application of weighted I-matching is to the postman problem. Given a graph  
G with weights on the edges, the postman problem is to find a minimum-weight set of  
edges to add to G so that the resulting multigraph MG contains a eulerian cycle \(i.e., a  
closed walk containing each edge of MG exactly once; see Section II.6.3\). The eulerian  
cycle on MG translates to a minimum-weight closed walk on G in which each edge is  
visited at least once and therefore generates a minimum-weight delivery route for the  
postman.  
Recall that a multigraph is eulerian if and only if each node is of even degree. Let V' be  
the nodes of odd degree in G, and let C ij be the weight of a minimum-weight path between  
nodes i and\} in V' \(see Figure 1.2\). Now consider the complete graph G' = \(V', E'\), and  
b 3 d  
'\<\]-' \{?,f  
c 4 e  
G  
Nodes of odd degree \(b, c, d, e\)  
Cbe = 2, Path \(b, a, c\)  
Cbd = 3, Path \(b, d\)  
Cbe = 6, Path \(b, a, c, e\)  
Ced = 5, Path \(c, a, b, d\)  
Gee = 4, Path \(c, e\)  
Cde = 4, Path \(d,/, e\)  
# 2\~4  
Minimum-weight perfect matching  
\{\(b, c\), \(d, e\)l  
c 4 e  
G'  
b d  
Postman solution  
\(a, b, a, C, b, d, e,/, d,/, e, c, a\)  
c e  
MG  
Figure 1.2 610 111.2. Matching  
let cij be the weight of e = \(i,\)\) in E'. Let M' be a minimum-weight perfect matching on  
G'. For each edge \(i,\)\) EM', add to G a minimum-weight path joining i and\). We leave it  
as an exercise to show that the resulting multigraph generates an optimal solution to the  
postman problem.  
Matching problems are celebrated in the history of combinatorial optimization as the  
first true integer programs \(i.e., integer programs that cannot be solved merely from the  
linear programming relaxation\) for which polynomial-time algorithms were obtained.  
Moreover, these algorithms use a class of valid inequalities for the convex hull of  
matchings and, in fact, prove that these inequalities, together with the degree and  
nonnegativity constraints, give a linear inequality description of the convex hull of  
matchings.  
We will mainly study the weighted I-matching problem stated as the integer program  
\(WM\)  
max I CeXe  
eEE  
eEJ\(v\)  
I Xe \~ 1 for v E V  
xEB".  
The more general problem of weighted b-matching, as well as other generalizations that  
allow any constraint matrix A with L; I aij I \~ 2 for all\), are examined in Section 4.  
We say that U \~ V is an odd set if I U I \~ 3 and is odd. We have already given the valid  
inequalities, called odd-set constraints\:  
\(1.1\) I x \~ llQlJ for all odd sets U \~ V  
eEE\(U\) e 2  
for the convex hull of matchings. Recall that they are valid since each matching edge "uses  
up" two nodes. They can be obtained by one iteration ofthe integer-rounding procedure  
\(see Sections II.1.1 and n.1.2\). They are, of course, also valid when I U I is even, but they  
are not interesting because they can be obtained as a nonnegative linear combination of  
the degree and nonnegativity constraints.  
The main results of this chapter are a polynomial-time algorithm for WM and a linear  
inequality description of the convex hull of matchings. The algorithm solves the linear  
program  
I Xe \~ 1 for v E V  
\(1.2\)  
eEJ\(v\)  
I x \~ llQlJ for all odd sets U \~ V  
eEE\(U\) e 2  
xER\~  
and obtains an integral optimal solution, and therefore a matching, for any objective  
function vector c. Hence it provides a proof that the convex hull of matchings is given by  
the degree, non negativity, and odd-set constraints.  
An algorithm for maximum-weight matching can also be used to find a maximum-  
weight perfect matching, when one exists, by a simple transformation of the objective 2. Maximum-Cardinality Matching 611  
function. Let k = I VI/2, a = maxeEE max\(ce, 0\), b = mineEE Ce, \(J = k\(a - b\) + 1, and  
c; = Ce + \(J for e E E. The ranking of perfect matchings by weight is the same for C and c'.  
Moreover, with respect to the objective function c', a lower bound on the weight of a  
perfect matching is k\( b + \(J\), and an upper bound on the weight of an imperfect matching  
is \(k - 1\) \(a + 0\). Now  
k\(b + \(J\) - \(k - l\)\(a + 0\) = 0 + kb - \(k - l\)a = a + 1 \> 0  
so that any perfect matching has greater weight than any imperfect matching.  
Our approach to solving \(1.2\) is by a primal-dual algorithm similar to the algorithm we  
gave for the transportation problem in Section 1.3.5. The main difficulty to overcome is  
the exponential number of odd-set constraints.  
The dual of \(1.2\) is  
w = min I llv + I II U 2 I J Yu  
vEV odd sets U  
I llv + I Yu \~ C e for e E E  
\(1.3\) v\:eEo\(v\) U\:eEE\(U\)  
llv \~ 0 for v E V  
Yu \~ 0 for all odd sets U.  
The algorithm to be presented maintains primal and dual feasibility and achieves  
optimality when the complementary slackness conditions are satisfied. At each major  
iteration the cardinality ofthe matching is increased. This is done by solving a cardinality  
matching problem. So we begin the presentation of the general algorithm by studying  
maximum cardinality matching.  
2\. MAXIMUM-CARDINALITY MATCHING  
In our study of the maximum-flow problem \(see Section 1.3.4\), we gave necessary and  
sufficient conditions for a flow to be maximum in terms of augmenting paths. That is, the  
flow could be increased if and only if an augmenting path existed with respect to the  
current flow. We then gave an efficient procedure for finding an augmenting path or  
showing that none existed. We use the same idea to find a maximum-cardinality  
matching. Thus we begin by defining an augmenting path with respect to a matching.  
Given a graph G and a matching M, a path in G is said to be alternating relative to M if  
its edges alternate between M and E \\ M. \(See Figure 2.1, where edges in M are repre-  
sented by wavy lines.\) A node v is said to be exposed relative to M if no edge of M meets v.  
A path in G is augmenting relative to M if it is alternating and both of its end nodes are  
exposed. This definition is natural since, if there is an augmenting path relative to M, a  
new matching M' with one more edge is obtained by deleting from M the matching edges  
in the path and adding to M the nonmatching edges in the path \(see Figure 2.2\).  
e EM e E M  
• • • • ..  
Alternating path  
Figure 2.1 612 111.2. Matching  
Augmenting path  
Figure 2.2  
The interesting result is that ifthere is no augmenting path, the matching is maximum.  
Theorem 2.1. A matchingM is not maximum if and only if there exists an augmenting  
path relative to M.  
Proof Let E' be the edge set of the augmenting path, and let M' = \(M U E'\) \\  
\(M n E'\). Then M' is a matching, and 1M' I = 1M I + 1. This formally establishes our  
claim that the existence of an augmenting path implies that the matching is not maximum.  
We now show that if M is not maximum, then there exists an augmenting path relative  
toM. If M is not maximum, there exists a matchingM' with 1M' I = 1M I + 1. LetD be  
the symmetric difference of M and M'; that is, D = \(M U M'\) \\ \(M n M'\). Thus  
IDI = IMI + IM'I-2IMnM'1 =21MI + 1-2IMnM'I.  
Hence I D I is odd.  
Consider the subgraph G\(D\) = \(V, D\). Since M and M' are matchings, the degree of  
each node is no more than 2; and if the degree is 2, then one edge is from M and the other  
is from M'. Hence each component of G\(D\) is either an isolated node, a cycle containing  
an even number of edges, or an alternating path relative to both M and M'. Since I D I is  
odd, there must be at least one alternating path of odd length. Moreover, since I M' I =  
I M I + 1, one of these alternating paths of odd length must be augmenting with respect  
\~M •  
The basic idea of the augmenting-path algorithm is to grow a tree of alternating paths  
rooted at an exposed node. Then if a leaf of the tree is also exposed, an augmenting path  
has been found. We begin by describing an augmenting-path algorithm for bipartite  
graphs. Finding an augmenting path in a bipartite graph is much simpler than finding one  
in a general graph. In fact, in the primal-dual algorithm for the transportation problem,  
we have shown that an augmenting path in a bipartite graph can be found by finding a flow  
augmentation in a maximum-flow problem. The algorithm given below is essentially a  
flow-augmentation algorithm described with augmenting-path terminology. This termi-  
nology will be useful in the description of the general algorithm.  
Cardinality Matching Algorithm for Bipartite Graphs  
Initialization\: M is an arbitrary matching. All nodes are unlabeled and unscanned.  
Step 1 \(Optimality Test\)\: Ifno nodes are exposed and unlabeled, the current matching is  
maximum. Otherwise choose an exposed and unlabeled node r. Label it \(E, -\). \(Here  
E stands for even and should not be confused with the usual use of E for an edge set.  
The first component of a node label is either E or O. A labeled node is said to be even if  
the first component of its label is E; otherwise it is odd.\) 2. Maximum-Cardinality Matching 613  
Step 2 \(Grow an Alternating Tree\)\: Choose a labeled and unscanned node i. If it is even, let  
J = \(j E V\: j is an unlabeled neighbor of i\). Label all j E J with \(0, i\). Node i is  
scanned; go to Step 3. If i is odd and exposed, go to Step 4. If i is odd and not exposed,  
label the node joined to i by a matching edge \(E, i\). Node i is scanned; go to Step 3.  
Step 3\: If there is a labeled and unscanned node, go to Step 2; otherwise go to Step 1.  
Step 4 \(Augmenting-Path Identification\)\: Use the second components of the labels to  
identify the augmenting path from node r to node i. Remove all labels, update the  
matching, and return to Step 1.  
Theorem 2.2\. The algorithm produces a maximum-cardinality matching on a bipartite  
graph.  
Proof In Steps 2 and 3, we grow a forest of alternating paths. An odd node i yields an  
alternating path between rand i with an odd number of edges. Hence if i is exposed, the  
path is augmenting.  
Now we show that if there are no exposed and unlabeled nodes, the final matching MO  
is maximum. We do this by giving a feasible solution to the dual problem \(1.3\) with  
w = I \~ I. One way to obtain a feasible solution to \(1.3\) is to find a subset of nodes W s V  
such that each e E E is incident to a node in W. Then we set '/rv = 1 for all v E W, '/rv = 0  
otherwise, and Yu = 0 for all odd sets. Here our objective is to produce a dual feasible  
solution of this form with I WI = IMo I.  
When the algorithm terminates, we have a set oflabeled trees Ti = \(Vi, Ei\) for i = 1,  
... , s - 1, and we also have a set of unlabeled nodes V, \(see Figure 2.3\).  
Since no nodes in V, are exposed, the subgraph induced by V" \(V" Es\) contains a  
perfect matching. Let \(Vl, \~\) be a bipartition of v" and let V? C V; be the odd nodes of Ti  
for i = 1, ... , s - 1. Set W = Uf\:\: V? U Vl.  
0 E  
x E  
Tl Xx T2 0  
Xx  
x\~  
E  
E  
# •  
\<== Xx  
0 x E 0  
x  
0 E \) 0 E  
x  
Figure 2.3. Crosses mean that the edge cannot be in E. 614  
m.2. Matching  
\(E, -\) \(0,5\) \(E,7\)  
2  
\(0,3\)  
Augmenti ng path  
\(5,8,3,6\)  
4  
New matching edges  
Maximum matching\: MO = \{\(2, 7\), \(3, 6\), \(4, 10\), \(5, 8\)\}  
O . al d 1 1 . \{ 1 for v = 3, 7, 8, 10  
pnm ua so utlOn\: n, = 0 th .  
a efWlse.  
Figure 2.4 2. Maximum-Cardinality Matching 615  
W generates a dual feasible solution since\:  
a. Each e E Ui\~l Ei is incident to a node in W.  
b. In the subgraph induced by Vi, i = 1, ... , s - 1, there cannot be an edge joining two  
even nodes; otherwise there would be an odd cycle.  
c. There cannot be aI\). edge joining even nodes in different trees; otherwise one of  
these nodes would have been labeled from the other.  
d. There cannot be an edge joining an even node and an unlabeled node; otherwise the  
unlabeled node would have been labeled from the even node.  
To show strong duality, note that  
# •  
Example 2.1. An example of the algorithm is given in Figure 2.4.  
The algorithm may fail to find an augmenting path if the graph is not bipartite. An  
example is shown in Figure 2.5. Here there are two paths between nodes 1 and 4. The odd-  
length path is augmenting, but we find it only by labeling in a particular way.  
We now develop a procedure that circumvents this problem. Let M be a matching.  
Suppose in the process of growing an alternating tree using the algorithm given above, we  
find that there are two alternating paths to node i, one of even length and the other of odd  
length. This can happen in two ways \(see Figure 2.6\)\:  
a. Node i is even and is adjacent to another even node in the tree;  
b. Node i is odd, adjacent to another odd node in the tree, and the edge that joins them  
isinM.  
By tracing the two paths back toward the root of the tree until the node where they  
intersect is reached, we identify a set of labeled nodes U £; V with I U I odd and  
1M n E\( U\)I = II U /211. Note that in both cases the intersection node, denoted by b\( U\), is  
even.  
\(0, 1\)  
2  
\(0, 1\)  
2  
\(E, -\)  
3  
\(0,1\)  
Node 4 cannot be labeled  
4  
3  
\(E,2\)  
Augmenting path found  
4  
\(0,3\)  
Figure 2.5 616  
III.2. Matching  
E  
o  
E E  
o o  
o  
E  
o  
E  
An even node is adjacent to  
another even node in the tree  
Two odd nodes are joined by  
a match i ng edge  
Figure 2.6  
Thus relative to M, the odd-set constraint for U is satisfied at equality. The subgraph  
\(U, E\( U\)\) is called a blossom relative to M. Each u E U \\ b\( U\) is met by an edge in M n  
E\( U\). Node b\( U\), which is called the base of the blossom, is either the root of the tree or is  
adjacent to a matching edge in the tree.  
Now we shrink the blossom as described below and illustrated in Figure 2.7.  
Procedure for Shrinking a Blossom  
Construct a reduced graph G by replacing \(U, E\( U\)\) by a node B\( U\) called a pseudonode.  
In G, each node that is adjacent to a node in U in the original graph is joined to the  
pseudonode. All of these edges are nonmatching edges unless there is a matching edge  
adjacent to b\( U\), in which case that edge remains a matching edge in the shrunken graph.  
The remainder of the graph remains the same. The resulting reduced matching on G is  
denoted M. B\( U\) receives the label previously assigned to b\( U\), and any node not in U  
that has been labeled from a node in U has the second component of its label changed to  
B\( U\). Also record the triple \(B\( U\), b\( U\), U\). After a blossom is shrunk, the labeling  
process continues on the reduced graph G. A reduced graph may be shrunk again, and it  
may happen that a blossom to be shrunk contains a pseudonode. In this case the  
pseudonode is treated like an ordinary node. Both the terminology reduced graph and the  
notation G are used for any graph that contains a pseudonode; and correspondingly, M is  
used to indicate a matching on the reduced graph.  
When an augmenting path is found in a reduced graph G with matching M, we also find  
an augmenting path in G with respect to M. A procedure for finding an augmenting path  
in G is given below and illustrated in Figure 2.8.  
Procedure for Obtaining an Augmenting Path in G  
Let B\( U\) be a pseudonode on the augmenting path in G. Let a\( U\) be the node adjacent to  
B\(U\) on the augmenting path that is joined toB\(U\) by a nonmatching edge, let G' be the  
graph obtained by replacing B\(U\) by the blossom \(U, E\(U\)\), and let b'\(U\) be a node in U  
that is adjacent to a\(U\) in G'. By construction, \(U, E\(U\)\) contains an even-length  
alternating path p joining b\( u\) and b' \(u\). Replace B\( U\) in the augmenting path on G by  
the path p. This yields an augmenting path in G'. The procedure is repeated for each  
pseudonode on the augmenting path. Note that old pseudonodes may reappear when a  
pseudonode is replaced by an alternating path. 2. Maximum-Cardinality Matching  
\(U, E\(lJ\)  
E  
\<0, b\(u»  
\(E, -\)  
GandM  
An associated forest, with the odd set U  
\(E,k\) p\(lJ\) i  
\(E, -\)  
- -  
GandM  
\(0, i\)  
k  
The associated reduced forest  
Figure 2.7  
• • B\(lJ\)  
# ,  
\(E, -\) 0  
a\(u\)  
• •  
0 E  
# -  
Augmenting path in G  
• 0  
E  
• D---  
b\(u\) b'\(u\)  
.. Pathp  
b'\(u\)  
;\('" To be expanded  
• .. • B\(U'\) \~ • • • • b\(u\)  
\(E, -\) b\(u\) b'\(u\) a\(u\) E 0 \(U, E\(lJ\)  
Augmenting path in G'  
Figure 2.8  
617  
a\(u 618 m.2. Matching  
General Cardinality Matching Algorithm I  
Initialization\: M is an arbitrary matching. a = G. Sf = M. All nodes are unlabeled and  
unscanned.  
Step 1 \(Optimality Test\)\: Ifno nodes in a are exposed and unlabeled, Sf is maximum in a  
and M is maximum in G. Otherwise choose an exposed and unlabeled node r in V.  
Label it \(E, -\).  
Step 2 \(Grow an Alternating Forest\)\: Choose a labeled and unscanned node or pseu-  
donode i E V. If there is none, go to Step 1.  
a. If i is even and has an even neighbor, go to Step 4.  
b. If i is even and does not have an even neighbor, label all unlabeled neighbors of i  
with \(0, i\). Node i is scanned; go to Step 3.  
c. If i is odd and is exposed, go to Step 5.  
d. If i is odd and is not exposed, label the endpoint of the matching edge adjacent to  
node i with \(E, i\). Node i is scanned; go to Step 3.  
Step 3\: If there is a labeled and unscanned node or pseudonode in a, go to Step 2.  
Otherwise go to Step 1.  
Step 4 \(Shrink a Blossom\)\: Use the second components of the labels on node i and its  
neighbors to identify a blossom \(U, E\(U» and its base b\(U\). Use the shrinking  
procedure described above to replace \(U, E\( U» by a pseudonode B\( U\). Complete the  
scanning of B\( U\) as in Step 2 \[B\( U\) has an even label\] and then go to Step 3.  
Step 5 \(Augmentation in G\)\: Use the second components of the labels to identify an  
augmenting path in a, and use the procedure given above for identifying an augment-  
ing path in G. Find a new matching M' in G, and M .... M', a .... G, and Sf +- M. Return  
to Step 1.  
Theorem 2.3. The algorithm produces a maximum-cardinality matching.  
Proof When the algorithm terminates in Step 1, we have a reduced graph a, the  
associated matching Sf, and the matching MO in the original graph. We also have a set of  
labeled trees T; = \(Vi, Ei\) for i = 1, ... ,s - 1 and a set of unlabeled nodes f"s. The  
subgraph as induced by Vs contains a perfect matching since no nodes in f"s are exposed.  
Moreover, all of the matching edges e E Sf are either tree edges, edges internal to a  
shrunken blossom, or edges internal to as.  
We show that MO is maximum by giving a feasible solution to the dual linear program  
\(1.3\) with w = IMo I. The dual solution and the proof of its feasibility and optimality are  
similar to those given in the proof of Theorem 2.2. So only the details that are different are  
given here.  
When the algorithm terminates, all labeled pseudo nodes in the shrunken graph a are  
even. Let B\( U\) be a labeled pseudonode in a, and let  
R\(U\) = \{v E V\: v E U or v is in a blossom nested in B\(U\)\}.  
Since U is an odd set and R \(U\) is obtained from U by replacing pseudo nodes by odd sets,  
R\(U\) is an odd set. The dual constraints for the edges in the graph induced by R\(U\) are  
satisfied by setting YR\(U\} = 1. 2. Maximum-Cardinality Matching 619  
Now consider Gs. and let  
Q\(V;\) = \{v E V\: v E V; or v is in a blossom nested in a pseudonode in G s\}.  
Note that since 1 V;I is even and Q\(V;\) is obtained from V; by replacing pseudonodes by  
blossoms, 1 Q\( V;\) 1 is even and there is a perfect matching on the subgraph induced by  
Q\( V;\). If 1 Q\( V;\) 1 = 2, the dual constraints for edges in the subgraph are satisfied by setting  
'ltq = 1 for anyone q E Q\( V;\). Otherwise let q be an arbitrary node in Q\( V;\). Then  
Q\(V;\) \\ \{q\} is an odd set containing \(I Q\(V;\) 1 - 2\)/2 matching edges. Hence the dual  
constraints for the edges in the subgraph induced by Q\( V;\) \\ \{q\} are satisfied by setting  
YQ\(V,\)\\\(q\} = 1.  
Let V? £; V; be the odd nodes of 1j for i = 1, ... , s - 1. A feasible solution to \(1.3\) is  
given by  
'ltv = 1 if v E V? for i = 1, ... , s - 1  
'ltq = 1 for anyone q E Q\( V;\)  
'ltv = 0 otherwise  
YR\(U\) = 1 if B\(U\) is a labeled pseudonode in the final graph  
YQ\(V,\)\\\(q\} = 1 if 1 Q\(V;\) 1 \> 2  
Yu = 0 otherwise.  
Now following the argument in the proof of Theorem 2.2, we obtain  
L 'ltv + L II U 2 I J Yu'\:\:\:; IMo I·  
vEV odd sets U  
So by weak duality, w = IMo I. •  
Let II U /21 J be the weight of the odd set U.  
Corollary 2.4. The maximum number of edges in a matching equals the minimum  
number of nodes plus weighted odd sets needed to cover all the edges.  
Now we consider the complexity of the algorithm, where m = I V I and n = I E I. The  
number of augmentations is no more than m /2\. Between augmentations we need to create  
an alternating forest, contract pseudonodes, and then reexpand to find the new matching.  
Using the labels and storing blossoms appropriately, these steps can be carried out in such  
a way that each edge is considered only a constant number of times.  
Proposition 2.5. The complexity of the cardinality matching algorithm is O\(mn\). 620 III.2. Matching  
Example\].\]  
G and a matching  
1. We grow an alternating tree rooted at node 1. Node 2 is chosen next, and \(2,3\) E M  
indicates the blossom with U, = \{l, 2, 3\} and b\(U,\) = 1.  
\(0,1\)  
Blossom indication  
\(0,1\)  
2. The pseudo node B\(U,\) = B, is created and becomes the root of the tree. B, is  
scanned. Node 4 is scanned. In scanning node 5, a blossom is identified with node set  
U2 = \{B" 5, 6\} and b\(U2\) = B,.  
\(E,4\)  
Blossom indication 2. Maximum-Cardinality Matching 621  
3. The pseudonode B2 = B\(U2 \) is created and becomes the root of the tree. In scanning  
B2 , we find the blossom with U3 = \{Bz , 4, 7\} and b\(U3 \) = B2•  
\(0, B2\)  
Blossom indication  
4. The pseudo node B3 = B\(U3 \) is created and scanned.  
Node 8 is odd and exposed. Hence we find the augmenting path \(B3, 8\).  
5. The graph G and the new matching M are shown below.  
To find the corresponding matching M, we start with M = \{\(8, B3\)' \(9, IO\)\} and then  
expand B3• Node 8 is joined to node 4 of B3• So next we find an even-length path from 622 III.2. Matching  
node 4 to B2 , which is the base of B3• From the path \(4, 7, B2 \) we identify the matching  
edges \(8,4\) and \(7, B2\)' Hence M = \{\(8, 4\), \(7 \~B2\)' \(9, to\)\}.  
Next we expand B 2. Node 7 is joined to node 6 of B 2, and the base of B 2 is B 1. From the  
even-length path \(6, 5, B 1\), we identify the matching edges \(6, 7\) and \(5, B 1\), and we find  
the matching M = \{\(8, 4\), \(6, 7\), \(5, B 1\), \(9, to\)\}.  
FinallybyexpandingBb weobtainM = M = \{\(8, 4\), \(7,6\), \(5,2\), \(3,1\), \(9, to\)\}. The new  
matching, along with the blossoms B 1, B 2, B 3, is shown below. 2. Maximum-Cardinality Matching 623  
6. We grow an alternating tree rooted at node 11. Nodes 11, 10, 9, 1, 3, 2, and 5 are  
scanned. In the process of scanning node 6, a blossom with U4 = \{2, 3, 5,6, 7\} and  
b\( U 4\) = 3 is identified.  
\(E, -\) \(0,11\) \(E,lO\) \(0,9\)  
7. The pseudo node B4 = B\( U4\) is created, and labeling continues.  
\(E, -\) \(0,11\) \(E,lO\) \(0,9\)  
8. All n9des are labeled so that the current matching M is maximum.  
An optimal solution to the dual is given by  
lri = 0 otherwise  
Yu, = 1, Yu = 0 otherwise. 624 m.2. Matching  
E  
E ... ----\_\~\_---\_\_t1 B\(U\)  
o !---\_ .... EE-- Indication of augmentation  
a\(U\)  
Graph with  
\(u, E\(U\)  
expanded  
b\(u\)=B\(U'\)  
b'\(U\)  
A----.... --... 'a\(U\)  
V3 ,.,...;.. \_\_ .... V5  
aCU'\)  
Graph with  
CU', E\(U'»  
expanded  
btU'\) =Vl  
\(U\~ E\(U'»  
e- •  
B\(U\) \~I------" a\(U\)  
... •  
.-e- ----e-. --q t  
Figure 2.9 2\. Maximum-Cardinality Matching 625  
We have two reasons for modifying Algorithm I. First of all, restarting from scratch  
with a new matching after an augmentation is found can be inefficient, since many of the  
pseudonodes we had before may be recreated. Secondly, in the weighted algorithm given in  
the next section, we will need to keep some pseudo nodes after an augmentation is found.  
In Algorithm II given below, when an augmentation is found in G we update the  
matching in G, but we grow a new alternating forest in G with respect to the new matching  
M in G. Algorithm II has the same complexity as Algorithm I; however, for the reason  
mentioned above, it is likely to be more efficient. Some additional steps are needed as  
explained below.  
After an augmentation and a new matching M' are found in G, the bases of the  
pseudonodes in G on the augmenting path, as well as all of the pseudonodes nested within  
these pseudonodes, must be updated. This is done during the recursive process of finding  
the new matching M' in G and is illustrated in Figure 2.9.  
In addition, when we grow a new alternating forest for G, a previously created  
pseudonode may receive an odd label. In this case there may be no augmenting path in the  
reduced graph, whereas there is an augmenting path in the graph in which this odd  
pseudonode is expanded. Hence when a pseudonode B\( U\) receives an odd label, we  
expand it as shown in Figure 2.10; and in the alternating forest, we replace B\( U\) by a node  
a\( U\) E U that is joined by a nonmatching edge to the node from which B\( U\) was labeled.  
Note that in this case, the new matching contains fewer than II U 1/2\] edges from the  
blossom B\( U\). This observation is important for the weighted matching algorithm that  
will be described in the next section.  
General Cardinality Matching Algorithm II  
Steps 1, 3, and 4\: These are the same as in Algorithm I.  
Step 2'\: Modify Step 2c,d by\: Ifi is an odd pseudonode go to Step 6.  
Step 5' \(Augmentation in G and G\)\: Modify Step 5 with the following additions\:  
a. Use the augmenting path in G to find a new matching M' in G. Update the bases of  
all the pseudo nodes on the augmenting path and all of the pseudonodes nested  
within these pseudonodes.  
b. M \<- M', M \<- M', and return to Step 1.  
Step 6' \(Expand a Pseudonode\)\: Pseudonode i = B\(U\) has the label \(O,\}\). Change G by  
expanding the blossom B\(U\) = \(U, E\(U». Find a\(U\) E U with \(j, a\(U» E E \\ M.  
Replace B\( U\) by a\( U\) in the alternating forest and give a\( U\) the label \(0 ,i\). If a\( U\) is  
a pseudonode, the process is repeated. Go to Step 2' .  
Example 2.2 \(continued\). Here we apply Algorithm II. The first five steps are the same  
as before. Referring to the graph at the end of Step 5, we see that b\(U3\) = 4, b\(Uz \) = 6, and  
b\(Ul \) = 2.  
6'. We grow an alternating tree rooted at node 11.  
# \~----\~\~----\~\{V\~--\~\~  
\(E, -\) \(0, 11\) \(E, 10\) \(0,9\) 626  
III.2. Matching  
The labeled tree does not  
indicate an augmenting path  
E o  
\(E, -\)  
Expansion of blossom  
E o  
\(E, -\)  
Labeling continues and augmenting path is found  
E  
E o E o  
,...-.....  
# --  
\_\_\_ -\_r-Indication of augmenting path  
E ....  
# --  
....  
E o \(E, -\)  
Figure 2.10  
7'. B 3 is odd, so it is expanded. a\(B 3\) = B 2; hence B 2 replaces B 3 in the tree. B 2 is odd, so  
it is expanded. a\(B2\) = B 1; hence BI replaces B2 in the tree. BI is odd, so it is expanded.  
a\(B I\) = 1; hence 1 replaces B 1 in the tree. Now the tree is  
# \~\~--\~\~\~--\~\~\~--\~G\)  
\(E, -\) \(0,11\) \(E,10\) \(0,9\)  
8'. Nodes 1,3,2, and 5 are scanned. In the process of scanning node 6,--\(carry on  
with point 6 of Example 2.2\). 3\. Maximum-Weight Matching 627  
3\. MAXIMUM-WEIGHT MATCHING  
Here we give a primal-dual algorithm for the linear program \(1.2\) and prove that the  
solution is integral for any objective function vector c. Such a solution is therefore a  
solution to the weighted matching problem. We can assume Ce \> 0 for all e E E since  
Ce \~ 0 implies that there is an optimal solution with Xe = O.  
Given a matching M, let Xe = 1 for e EM, let Xe = 0 otherwise, and let  
C; = I nv + I Yu - Ceo  
v\:eEa\{v\) odd sets U\:eEE\(U\)  
The complementary slackness conditions for the linear programs \(1.2\) and \(1.3\) are  
\(3.1\) C;Xe = 0 for e E E \(either c; = 0 or e tf. M\)  
\(3.2\) \(ll U 1/2\] - I Xe\) Yu = 0 for odd sets U  
eEE\(U\)  
\(3.3\) \(either Yu = 0 or M n E\( U\) = II U I /2\]\)  
\(1 - I Xe\) nv = 0 for v E V \(either nv = 0 or v is met by an e EM\).  
eE,j\(v\)  
The primal-dual algorithm maintains primal and dual feasibility and also maintains  
the conditions \(3.1\) and \(3.2\). Therefore, optimality is achieved when \(3.3\) is satisfied.  
An initial integral primal feasible solution and a dual feasible solution that satisfy \(3.1\)  
and \(3.2\) are given by  
Xe = 0  
for e EE  
YU=O  
for odd sets U  
\(3.4\)  
1  
nv = 2 \~\~x Ce  
for v E V.  
Note that c;, = 0 for all e' E E such that Ce' = maXeEE Ceo  
Let E"= \{e E E\: c; = O\}. The graph G' = \(V, E'\) is called the equality-constrained  
subgraph. Throughout the course of the algorithm, \(3.1\) is maintained by settingxe = 0 for  
e E E \\ E'. We maintain \(3.2\) by requiring Yu = 0 unless \(U, E\(U» is a blossom in the  
equality-constrained subgraph that has been shrunk into a pseudonode.  
To see if \(3.3\) can be satisfied, we find a maximum-cardinality matching in the  
equality-constrained subgraph G'. Again, we will be dealing with reduced subgraphs \(;' of  
G' that contain pseudonodes. There are two possibilities\:  
i. A matching £1 is found in \(;' with nv = 0 for all exposed nodes. The corresponding  
matching M in G' is an optimal solution to the weighted matching problem.  
ii. For the reduced graph \(;' and matching £1, no augmenting path is found.  
In the latter case a dual change is made that maintains dual feasibility and also  
maintains \(3.1\) and \(3.2\). Then the equality-constrained subgraph G' and its reduced  
subgraph \(;' are updated. In addition the edges of the alternating forest F' in \(;' still have  
c; = 0, so that F' is kept. After a small number of dual changes, either an augmentation is  
obtained or nv = 0 for all exposed nodes. 628 III.2. Matching  
After either a dual change or augmentation, all pseudonodes B\( U\) with dual variables  
Yu = 0 are expanded. However, pseudo nodes with Yu \> 0 are not expanded. The implica-  
tion of this is that an augmenting path ofthe type shown in Figure 2.10 may not be found  
immediately. It will be necessary to reduce Yu to zero before such an augmenting path can  
be found. The reason for this change is to maintain the complementary slackness  
condition \(3.2\).  
When new edges are added to a' after a dual change, we continue with the development  
of the alternating forest F' by adding edges, labeling nodes, and creating pseudonodes as  
described previously unless an edge \(u, v\) is added where u and v are both even and  
contained in different trees of F'. In this case, F' contains an augmenting path joining the  
roots of the two trees as shown in Figure 3.1.  
E  
\(E'-'\:-\~\)---O\~----E?-------\~ E  
o  
E 0  
\(E, -\) o v  
o  
E  
Figure 3.1  
Weighted Matching Algorithm  
Initialization\: Start with the primal and dual solutions given by \(3.4\). Let  
E' = \{e E E\: c; = O\}, G' = \(V, E'\), a' = G', if = M = 0, andF' = 0.  
Step 1\: Continue with the construction of the alternating forest F'. If an augmenting path  
is found, go to Step 2. Otherwise, go to Step 3.  
Step 2 \(Augmentation\)\: Update the primal solution M and expand all pseudonodes B\( U\)  
with Yu = O. Update the bases of the remaining blossoms. Let a' be the reduced  
equality-constrained subgraph with matching if'. If lrv = 0 for all exposed nodes, the  
current primal and dual solutions are optimal. Otherwise set F' = 0 and go to Step 1.  
Step 3 \(Dual Change\)\: Apply the dual change given by \(3.5\) and \(3.6\) below. If lrv = 0 for  
all exposed nodes, the current primal and dual solutions are optimal. Otherwise update  
a' and expand all pseudonodes B\( U\) with Yu = O. If an e = \(u, v\) has been added to a'  
where u and v are both even and contained in different trees of F', then identify an  
augmenting path and go to Step 2. Otherwise keep F' intact and go to Step 1.  
We need to ensure that \(3.1\) and \(3.2\) remain satisfied if an augmentation or a dual  
change occurs. In addition, we must ensure that dual feasibility and that part of the 3\. Maximum-Weight Matching 629  
equality subgraph corresponding to the alternating forest are preserved when a dual  
change occurs.  
Proposition 3.1. If conditions \(3.1\) and \(3.2\) are satisfied prior to an augmentation, then  
they are satisfied by the matching obtained/rom the augmentation.  
Proof Since c; = 0 for all edges in the equality-constrained subgraph, it is clear that  
\(3.1\) remains satisfied. The only way that \(3.2\) can be violated is by an augmentation that  
reduces the number of matching edges in E\(U\), where U is an odd set with Yu \> O.  
However, ifyu \> 0, \(U, E\(U» is represented by apseudonodein CT'. Any augmentation in  
G' translates into a new matching M with 1M n E\( U\) I = I U 1/2\. •  
We now describe the dual change used in Step 3. Define the following sets\:  
OU+ = \{odd set U\: U is the node set ofa shrunken blossom represented  
by an even pseudonode\}  
ou-= \{odd set U\: U is the node set of a shrunken blossom represented  
by an odd pseudonode\}  
ou-= \{odd set U\: U is the node set of a shrunken blossom represented  
by an unlabeled pseudonode\}  
P = \{v E V\: v is even or v E U for some U E OU+\}  
V- = \{v E V\: v is odd or v E U for some U E OU-\}  
V- = \{v E V\: v is unlabeled or v E U for some U E OU-\}  
E+ = \{e = \(i,j\) EE\: i E P,j E V-\}  
E++ = \{e = \(i,j\) E E\: i E P,j E V+, no U E ou+ contains both i andj\}.  
Let  
ifOU- =\#= 0  
ifOU-= 0  
ifE++=\#=0  
ifE++=0  
ifE+ =\#= 0  
ifE+ = 0  
and 0 = min\(o" 02, 03, 04\)'  
The dual change is given by  
\(3.5\) and  
\(3.6\) for v E V+  
# r-\~  
ftv = ttv + 0 for v E V-  
7tv otherwise  
rU+2\~ for U EOU+  
Yu= yu-2J for U EOU-  
Yu otherwise. 630 111.2. Matching  
The effect of the dual change on node weights trv and edge weights c; is shown in  
Figure 3.2.  
Proposition 3.2. The dual change is bounded and not degenerate; that is, 0 \< 6 .;;  
\~ maXeEE Ceo  
Proof 1. \(61 \> 0\). Before the dual change, there is an exposed node u E V+ with tru \> 0;  
otherwise the algorithm would have terminated prior to the dual change. Moreover, u has  
been exposed before all previous dual changes since once a node is met by a matching edge,  
it remains covered thereafter. Thus tru = minvEv' trv since tru has been decreased at every  
dual change and we started with tr v equal to a constant for all v E V. Hence  
\{vEV+\:7rv =0\}=0.  
2. \(62 \> 0\). All pseudo nodes with Yu = 0 are expanded after dual changes and augmen-  
tations. Thus the only possibility for a labeled pseudonode with Yu = 0 is that U has been  
shrunk since the last augmentation or dual change. But then U E qj+. Hence  
\{U EUfr\: Yu= O\} = 0.  
3. \(63 \) 0\). If c; = 0, then e is in the equality-constrained subgraph. Ifboth ends of E are  
even and c; = 0, then either an augmenting path is identified or both endnodes of e are  
contained in a shrunken blossom. Thus \{e E E++\: c; = O\} = 0.  
4. \(64 \) 0\). If e E E+ and c; = 0, then the other end of e receives an odd label. Hence  
\{e E E+\: c; = O\} = 0. Finally 6 = min\(6I, 62, 63, 64\) \> 0 and 6 .;; 61 .;; \~ maXeEE Ceo •  
Proposition 3.3. If the primal and dual solutions satisfy \(3.1\) and \(3.2\), and c; = 0 for all  
edges of the alternatingforest, then these conditions are satisfied after a dual change.  
Proof a. \(nv \~ 0 for v E V\). We have  
nv \~ trv - 6 \(by \(3.5\)\)  
\~ trv - 61 \(by the definition of 6\)  
\~ 0 \(by the definition of 61\),  
b. CVu \~ 0 for odd sets U\). We have  
Yu \~ Yu - 26 \(by \(3.6\)\)  
\~ Yu - 262 \(by the definition of 6\)  
\~ 0 \(by the definition of 62\).  
c. \(c; \~ 0 for e E E\). By \(3.5\) and \(3.6\) we only need to consider e E E+ U E++ or e is in  
a blossom whose pseudonode is labeled; otherwise c; \~ c; \~ O.  
i. \(e E E+\). We have  
c; = c; - 6 \~ 0 \(by \(3.5\) and the definition of c;\)  
\~ c; - 64 \(by the definition of 6\)  
\(by the definition of 64\), 3\. Maximum-Weight Matching  
631  
o E  
Alternating  
forests  
+20 I /  
# D  
a¥. /  
/ 1-20  
E \_r?E  
-0 +0  
HI Lo  
& jJ  
In matching "- X/  
In forest  
--- / "-  
Not in forest G \~  
or matching  
Figure 3.2  
Unlabeled  
11. \(e E E++\). We have  
c; = c; - 2\<5 \~ 0 \(by \(3.5\) and the definition of c;\)  
\~ c; - 2\<5 3 \(by the definition of \<5\)  
\(by the definition of \<53\),  
111. \(e is in a blossom whose pseudonode is even\). Then both endnodes of e are in P.  
Thus  
c; = c; - 6 - 6 + 26 = c; \(by \(3.5\), \(3.6\), and the definition of c;\).  
iv. \(e is in a blossom whose pseudo node is odd\). Then both endnodes of e are in V-.  
Thus  
c; = c; + \<5 + \<5 - 2\<5 = c; \(for the reason given in iii\).  
Thus the new solution is dual feasible. 632 m.2. Matching  
Next, we establish that \(3.1\) is satisfied; that is, e EM implies c; = O. Since c; = 0 for  
e EM, it suffices to show that c; = c; for e EM. Consider an e EM and first suppose that  
e has not been shrunk; then either one endnode of e is odd and the other is even, or both  
endnodes are unlabeled. Then by \(3.5\), we have c; = c;. On the other hand, if e has been  
shrunk, then c; = c; by iii and iv above if the pseudonode is labeled, whereas c; = c; by  
\(3.5\) and \(3.6\) if the pseudonode is unlabeled.  
Next we show that \(3.2\) is satisfied; that is, if U is an odd set and 1 E\( U\) n M 1 \<  
\[I U 1/2\], then Yu = Yu = O. This follows directly from \(3.6\) since if 1 E\( U\) n M 1 \<  
\[I U 1 /2\], then \(U, E\( U» cannot be shrunk into a pseudonode.  
Finally, consider the edges of the alternating forest. If e is in a labeled blossom, we have  
shown above that c; = c; = O. Otherwise, one end of e is in v+ and the other end is in V-.  
Thus  
c; = c; - 0 + 0 = c; = O. •  
Proposition 3.4. There are no more than Sm/3 dual changes between augmentations.  
Proof We will use the fact that the alternating forest F' is kept between augmenta-  
tions. The effect of the dual change depends on arg\(min\{oI, 02, 03, 04\}\).  
a. \(0 = 01\)' Since all exposed nodes are even in C', and 7Cv = 01 for all exposed nodes, we  
have ft\~ = 0 for all expoded nodes. In this case the current primal and dual solutions  
are optimal.  
b. \(0 = 02\). Then Yu = 0 \< Yu for some odd pseudonode. After an augmentation there  
can be no more than m/3 odd pseudonodes, and all new pseudonodes created  
between augmentations are even. Hence 0 = 02 can happen no more than m/3  
times between augmentations.  
c. \(0 = 03\). Then c; = 0 \< c; for some e whose two endnodes are even. This edge is  
added to the equality-constrained subgraph. The result is an augmentation or the  
creation of a pseudonode. The latter can happen no more than m/3 times between  
augmentations because a newly created pseudo node B\( U\) is even so that its dual  
variable Yu can only increase between augmentations.  
d. \(0 = 04\). Then c; = 0 \< c; for some e with one endnode even and the other un-  
labeled. Then the unlabeled node becomes odd. This can happen no more than  
m - I times because there are no more than m - I unlabeled nodes. •  
Theorem 3.5. The weighted matching algorithm finds an integral optimal solution to  
\(1.2\) and also finds an optimal solution to its dual \(1.3\). Its complexity is O\(m2n\).  
Proof Integrality of the primal solution is maintained throughout the algorithm  
because each solution is a matching. When the algorithm terminates, both the primal and  
dual solutions are feasible and satisfy complementary slackness.  
The work between successive dual changes is O\(n\). By Proposition 3.4 the maximum  
number of dual changes between an augmentation is O\(m\), and the number of augmenta-  
tions is Oem\).  
Finally, observe that after p dual changes, it follows that 7C, y and c' are rationals with  
denominator 2k for some integer k, 0.;;; k .;;; p. Hence the numbers involved in the  
calculations remain polynomially bounded. • 3\. Maximum-Weight Matching 633  
Theorem 3.6. The polytope defined by the constraint set of \(1.2\) is the convex hull of  
matchings.  
Proof By Theorem 3.5, \(l.2\) has an integral optimal solution that is a matching for  
any objective function vector c. Thus, by Proposition 1.1 of Section III.1.1, each extreme  
point of the polytope defined by the constraints of\(l.2\) is integral. •  
Example 3.1  
C = \(cel' ... , Ce ,\) = \(8 9 8 7 9 4 5 2 1\)  
1. Initialization  
77\:v = 4.5 for all v E V  
yu=O for all U  
c' = \(1 0 I 2 0 5 4 7 8\)  
Equality  
constrained subgraph  
2. Equality-constrained subgraph and labels with M = \{ez, es\}  
o \(E,-\)  
\(E, -\)  
0\(E,-\) 634  
111.2. Matching  
3. Dual change  
n = \(3.5 4.5 4.5 4.5 4.5 3.5 3.5\)  
yu = 0 for all U  
c' =\(0 0 1 2 0 4 3 6 6\)  
el is added to the equality constrained subgraph.  
4. Equality-constrained subgraph and labels  
o \(E,-\)  
\(E, -\) \(0,1\) \(E,2\) o \(E,-\)  
5. Dual change  
61 = 3.5, 62 = 00, 63 = 3, 64 = min\{l 2 4 3 n = \(2.5 5.5 3.5 4.5 4.5 2.5 2.5\)  
yu = 0 for all U  
c' =\(0 0 0 1 0 3 2 5 4\)  
e3 is added to the equality constrained subgraph.  
6\} = c;, = 1, 6 = 64 = 1  
6. Equality-constrained subgraph and labels  
\(0,3\) \(E,-\)  
# o  
# o  
\(E,4\) \(E,-\) 3\. Maximum-Weight Matching  
7. Dual change  
635  
n = \(2 6 3 5 4 2 2\)  
yu = 0 for all U  
c' = \(0 0 0 0 0 3 2 4 3\)  
e4 is added to the equality-constrained subgraph.  
8. Reduced equality-constrained subgraph and labels  
\(E, -\)  
# o  
# G\)------\(01-------IG  
\(E, -\) \(0,1\) \(E,2\)  
U = \{3, 4, 5\}  
BI = B\(U\)  
b\(U\) = 3  
\(E, -\)  
9. Dual change  
03 = 1 min \(c;\) = 1,  
1=6,7,8,9  
n = \(1 7 2 4 3 1\)  
Yu = 2 for U = \(3, 4, 5\), Yu = 0 otherwise  
c' = \(0 0 0 0 0 1 0 2 1\)  
e7 is added to the equality-constrained subgraph.  
10. Augmentation in the reduced graph and new labeling. M = \(eb e4, e7\)  
\(E, -\)  
o 636  
III.2. Matching  
11. Dual change  
7C = \(1 7 2 4 3 0 1\)  
Yu = 2 for UI = \{3, 4, 5\), Yu = 0 otherwise.  
12. Optimal solution  
Primal\: xei = 1 for i = 1,4, 7, X ei = 0 otherwise  
Dual\: 7C = \(1 7 2 4 3 0 1\)  
YU1 = 2 for UI = \{3, 4, 5\), Yu = 0 otherwise.  
4\. ADDITIONAL RESULTS ON MATCHING AND RELATED PROBLEMS  
This section contains a potpourri of topics related to matchings. We begin by presenting  
further results on the convex hull of matchings. Then we describe the polytope of the  
convex hull of perfect matchings and relate matchings to the problem of covering nodes by  
edges.  
The next topic is the reduction of integer and \(0, 1\) b-matching problems to matching  
problems. These reductions may also be viewed as a technique for obtaining linear  
inequality descriptions of b-matching polytopes.  
We then introduce a pair of combinatorial objects known as T-joins and T-cuts. T-joins  
include perfect matchings, s-t paths, and eulerian subgraphs.  
The final topic of this section is the problem of coloring the edges of a graph so that no  
pair of edges that are incident to the same node have the same color. This edge coloring  
problem is equivalent to partitioning the edges of a graph into matchings.  
The Matching Polytope  
Here we demonstrate an interesting nonalgorithmic proof technique for showing that a set  
of inequalities describes the convex hull of a set S by proving Theorem 3.6; that is, the  
convex hull of matchings in a graph G = \(V, E\) is given by  
I xe\:;;; 1 for v E V  
eEO\(v\)  
\(4.1\)  
I xe\:;;; II U 2 I J for all odd sets U with I U I ;;. 3  
eEE\(U\)  
xER\~.  
Let .;U be the set of matchings on G, let w be a weight vector on the edges of G, let  
w\(M\) = LeEM We, let  
z\(w\) = max\{w\(M\)\: M E.;U\}.  
and let.;U\(w\) be the set of maximum-weight matchings. We use the following property of  
.;U\(w\). 4\. Additional Results on Matching and Related Problems 637  
Proposition 4.1. If w \> 0 and G is connected, then either  
1. there exists a v E V such that J\( v\) n M '\* 0 for all M E .tU\( w\), or  
2. IMI = II VI/2\]forall M E.tU\(w\), and I VI is odd.  
Proof Suppose that statement 1 is false and there exists an M E .tU\( w\) with  
1M I \< II V I /2\]. Since I M I \< II V I /2\], there are at least two exposed nodes relative to M.  
Now choose an M E.tU\(w\) so that there are exposed nodes u and v as close together as  
possible. Then \(u, v\) \$. E; otherwise M U \{\(u, v\)\} E.tU and w\(M U \{\(u, v\)\}\) \> z\(w\).  
Let t be any internal node on a minimum-length path joining u and v. By the choice of  
u and v, t is not exposed relative to M. Also, since statement 1 is false, there is another  
matching M' E .tU\(w\) such that t is exposed relative to M'.  
Now the graph a = \(V; MUM'\) consists of a node disjoint union of paths and cycles in  
which the degrees of nodes t, u, and v are equal to 1. \(If u or v was exposed relative to M',  
we would have a contradiction to the choice of M, u, and v.\) The component at = \(\~, Et \)  
of a containing t is therefore a path with t as one endpoint. Hence this component cannot  
contain both u and v. Since the edges of Et alternate between M and M', it follows that  
are matchings, and  
w\(M\) + w\(M'\) = w\(M\) + w\(M'\) = 2z\(w\).  
Since w\(M\), w\(M'\) .\:;;; z\( w\), we have w\(M\) = z\( w\) and M E .tU\( w\). This is a contradiction  
because \(a\) u and t are exposed relative to M and \(b\) the path between u and t is shorter  
than the path between u and v. So either statement 1 is true or I M I = II V I /2\] for all  
M E.tU\(w\).  
Finally, if I M I = II V I /2\] and I V I is even, M is a perfect matching, so statement 1 is  
true. Thus if statement 1 is false, statement 2 is true. •  
Proof of Theorem 3.6\. Let S be the set of incidence vectors of matchings in  
G = \(V; E\). Suppose  
\(4.2\) L WeXe'\:;;; Wo  
eEE  
defines a facet of conv\(S\). We consider two cases\:  
1. wo'\:;;; 0 or We \< 0 for some e E E. Since the set ofmatchings form an independence  
system, the only inequalities that define facets with Wo .\:;;; 0 or We \< 0 are -Xe .\:;;; 0 for  
e E E \(see Section 11.1.5\).  
2. wo \> 0 and we \~ 0 for e E E. \(i\) Since \(4.2\) defines a facet, Wo = z\(w\) and xM E S  
satisfies WXM = Wo if and only if M E .tU\(w\); and \(ii\) since conv\(S\) is full-dimen-  
sional, by Theorem 3.6 of Section 1.4.3, the set of equations  
\(4.3\) L weX\~ = Wo for M E .tU\(w\)  
eEE  
has a unique solution up to scalar multiplication. 638 111.2. Matching  
Suppose there is a v E V such that I J\( v\) n M I = 1 for all M E .;/;l\( w\). Then a solution to  
\(4.3\) is We = 1 for e E J\(v\), We = 0 otherwise, and Wo = 1. Hence \(4.2\) is of the form  
L eE6\(v\) Xe \~ 1.  
If no such v exists, let E' = \{e E E\: We\> O\} and G' = \(V', E'\) be the subgraph of G  
induced by E'. G' is connected; otherwise \(4.2\) is the sum of valid inequalities for G and  
thus cannot define a facet. Define w' on G' by w; = We fore E E', and let.;/;l'\(w'\) be the set  
of maximum-weight matchings on G'. Hence M' E.;/;l' \(w'\) if and only if M' = M n E'  
for some M E .;/;l\( w\). Hence, by hypothesis, statement 1 of Proposition 4.1 is false for the  
pair \(G', w'\). Thus 1M' I = II V'I/2j for all M' E.;/;l'\(w'\), and I V' I is odd. Now a solution  
to \(4.3\) is We = 1 for e E E\( V'\), We = 0 otherwise, and Wo = II V' 1/2j. Thus \(4.2\) is of the  
form LeEE\(V'\) Xe \~ II V' 1/2j, where I V' I \~ 3 and is odd. •  
For any pair \(G, w\), we have z\(w - 1\) \~ z \(w\) -II V1/2j. However, when statement 10f  
Proposition 4.1 is false and w is integral, it can be shown that z\(w - 1\) = z\(w\) -II V1/2j,  
which implies 1M I = II VI/2j for all M E .;/;l\(w\). Thus we can state a stronger version of  
Proposition 4.1 forintegral w.  
Proposition 4.2. If w \~ 1 and is integral, and G is connected, either  
a. there exists a v E V such that J\(v\) n M '" o for all ME .;/;l\(w\), or  
b. WI is odd and z\(w - 1\) = z\(w\) -II V1/2j.  
By using Proposition 4.2, we obtain a simple proof that the dual ofmax\{LeEE WeXe\: x  
satisfies \(4.1\)\} has an integral optimal solution for all w E zn.  
Theorem 4.3. The system of inequalities \(4,1\) is TDL  
Proof The proof is by induction on I V I + I E I + LeEE We. Clearly the result is true for  
a graph with two nodes and one edge. We can assume that G is connected; otherwise the  
induction hypothesis can be applied separately to each component. We can also assume  
w \~ 1; otherwise an edge can be deleted. Hence the hypotheses of Proposition 4.2 hold.  
Let n E R'\:' and y E R\~ be the dual variables for the degree constraints and odd-set  
constraints, respectively. There are two cases according to Proposition 4.2.  
1. Statement a of Proposition 4.2 is true for v. Let w' be defined by w; = We - 1 for  
e E J\(v\) and by w; = We otherwise. Clearly, z\(w'\) \~ z\(w\) - 1; but if z\(w'\) = z\(w\),  
then statement a of Proposition 4.2 is false. Hence z\(w'\) = z\(w\) - 1. Now by the  
induction hypothesis, there is an optimal dual solution \(n', y'\) E Z'\:'+P of cost  
z\(w\) - 1. Now define \(n, y\) E Z'\:'+P by nv = n\~ + 1, nu = n\~ otherwise, and y = y'.  
Then it is a simple calculation to show that \(n, y\) is an optimal dual solution for the  
weight vector w.  
2. Statement b of Proposition 4.2 is true. Let w' = w - 1. Hence z\(w'\) = z\(w\)-  
II V1/2j, and I VI is odd. By the induction hypothesis, there is an optimal dual  
solution \(n', y'\) E Z,\:,+p. Now define \(n, y\) E Z'\:'+P by n = n', Yv = y\~ + 1, and  
Yu = Yu otherwise. Again, it is easy to check that \(n, y\) E Z'\:'+P is an optimal dual  
solution for the weight vector w. •  
Perfect Matchings  
We now consider perfect matchings. Clearly, if I V I is odd, there are no perfect matchings. 4\. Additional Results on Matching and Related Problems 639  
Theorem 4.4. The convex hull of perfect matchings on a graph G = \(V, E\) with I VI even is  
given by  
\(a\) x E R\~  
\(b\) I Xe = 1 for v E V  
\(4.4\) eEo\(v\)  
\(c\) I x e ",;; II u 2 I J for all odd sets U s V with I U I \~ 3  
eEE\(U\)  
or by \(a\), \(b\), and  
\(d\) I Xe \~ 1 for all odd sets Us V with I U I \~ 3.  
eEo\(U\)  
Proof Since the convex hull of perfect matchings is the face of the matching polytope  
with1\:eEo\(v\) Xe = 1 for all v E V, the claim for \(a\), \(b\), and \(c\)followsfrom Theorem 3.6. We  
now show that an x satisfies \(a\), \(b\), and \(c\) if and only if it satisfies \(a\), \(b\), and \(d\).  
By summing the constraints of \(b\), we obtain  
I xe=1IVI.  
eEE  
Now since I VI is even and U is an odd set, V \\ U is an odd set. Hence \(c\) yields the  
inequalities  
- I Xe \~ \_lillj  
eEE\(U\) 2  
and  
- I Xe \~ - = -21 V I + -+ 1.  
llV\\Ulj 1 llUlj  
eEE\(v\\U\) 2 2  
Summing the last three constraints yields \(d\). •  
The system \(4.4 \(a\), \(b\), \(c» is TDI since it is obtained from the TDI system \(4.1\) by  
changing some inequalities to equalities. The system \(4.4 \(a\), \(b\), \(d» is not TDI for all  
graphs \(see exercise 9\). However, it can be shown that the dual problem always has an  
optimal solution in which each variable is an integer or an integer divided by 2.  
Edge Coverings  
The theory and algorithmic aspects of edge coverings completely parallel those for  
matching. We illustrate this with two results.  
Proposition 4.5. Let M be a maximum-cardinality matching, and let C be a minimum-  
cardinality covering of the nodes by edges in a graph G = \(V, E\). Then IMI + I CI = I VI. 640 III.2. Matching  
Proof Given M, let V be the set of nodes of degree zero relative to M. Thus IV I =  
I V I - 21 M I. Since we obtain a cover by adding I V I edges to M, we have  
ICI \~ IMI + IVI = IMI + IVI-2IMI = IVI-IMI.  
Given C, let M be a maximum-cardinality matching in \(V, C\), and let -0 be the set of  
nodes of degree zero relative to M in \(V, C\). Then  
and  
Hence ICI + IMI = I VI. •  
To cover an odd set of nodes V, we need at least II V 1/2J + I edges. Thus we obtain the  
valid inequalities  
L Xe + L Xe \~ II V 2 I J + 1 for all odd sets V.  
eEE\(U\) eEa\(U\)  
These inequalities, together with the degree and nonnegativity constraints, yield the  
convex hull of edge covers.  
Theorem 4.6. The convex hull of edge covers in a graph G = \(V, E\) is given by  
L Xe \~ I for v E V  
eEa\(v\)  
L x \~ llQlJ + I for all odd sets V  
eEE\(U\)UO\(U\) e 2  
xER\~.  
b-Matching  
The next topic deals with the reduction of b-matching problems to I-matching problems.  
These reductions may be viewed as modeling devices for transforming harder problems to  
easier ones, and they can be used in contexts other than matching. Although they are not  
necessarily polynomial reductions, they serve three useful purposes.  
1. The transformed problem may yield theoretical results-for example, polyhedral  
descriptions of the convex hull of solutions in the original space.  
2. The transformed problem can be solved by a standard matching algorithm. This  
may be preferred, even when the transformation is not polynomial, to constructing  
a new algorithm.  
3. An efficient algorithm can often be developed for the original problem by studying  
the \(possibly nonpolynomial\) algorithm on the transformed problem. 4\. Additional Results on Matching and Related Problems 641  
We first consider the integer b-matching problem on G = \(V, E\). Its constraints are  
LeEo\(v\) Xe \< bv for v E V and x E Z\~.  
Suppose that bv = 2 for all v E V. In this case the edges M = \{e E E\: Xe \> 0\} of a feasible  
solution produce a graph \(V, M\) whose components are paths in which Xe = 1 for all edges  
of the path, cycles in which Xe = 1 for all edges of the cycle, isolated nodes, and single edges  
with Xe = 2. In perfect integer 2-matchings, only the cycles and single edges with Xe = 2 can  
occur.  
In Section II.6.3, we used perfect 2-matchings as a relaxation for the traveling salesman  
problem, and we reduced the perfect integer 2-matching problem on G = \(V, E\) to a  
perfect matching problem on a bipartite graph H = \(Vl U P, E'\), where V l and Pare  
copies of V, and el = \(u l  
, v2\), e2 = \(v l  
, u2\) are in E' if and only if e = \(u, v\) E E. The  
reduction does not depend on the matching being perfect.  
Let Y E B2n be the incidence vector of a matching on H, and let Xe = Yel + Ye" Then  
x E Z\~ and  
2.\: Ye+ 2.\: Ye= 2.\: Xe·  
eEo\(v'\) eEo\(v'\) eEO\(v\)  
Thus to find a maximum-weight integer 2-matching on G with weight vector wE Rn,  
we can find a matching on H with weight vector w' E R2n  
, where W\~I = W\~2 = We for all  
eEE.  
The reduction also yields a linear inequality description of the convex hull of integer  
2-matchings.  
Proposition 4.7.  
\(4.5\)  
Proof The convex hull of integer 2-matchings on G = \(V, E\) is given by  
2.\: Xe \< 2 for v E V  
eEO\(v\)  
xER\~.  
The convex hull of matchings on the bipartite graph H is given by  
2.\: Ye \< 1 for Vi E Vi and i = 1, 2  
\(4.6\) eEO\(v'\)  
We need to show that the projection onto Rn of the points that satisfy \(4.6\) and  
Xe = Yel + Ye' for e E E is precisely those points in Rn that satisfy \(4.5\).  
Every point x of the projection lies in R\~ because Yel, Ye2 E R\~ and Xe = Yel + Ye" Also,  
every such point satisfies \(4.5\) because  
2.\: Xe= 2.\: Ye+ 2.\: Ye\<2  
eEo\(v\) eEo\(v'\) eEO\(v'\)  
by \(4.6\). It remains to show that every point x E R\~ satisfying \(4.5\) is a point of the  
projection. For this it suffices to take Yet = Ye' = \~xe for e E E. •  
This approach readily extends to integer b-matchings with bv even for all v E V and  
yields the result that when bv is even for all v E V, integer b-matching is a network flow  
problem on a graph with 21 V 1 nodes and 21E 1 edges. 642 m.2. Matching  
Figure 4.1  
For general b E Z\~, the transformation of integer b-matching on G to 1-matching is  
more complicated. Here each node is replaced by bv copies of itself, and for each pair of  
adjacent nodes in the original graph all the resulting copies are joined to form a complete  
bipartite graph. Formally, we construct a new graph H = \(UVEV VV, U\(u,V\)EE EU,V\), where  
VV = \(VI, ... , Vb,\) for v E V, EU'v = \(e l  
, ... , eb•b,\) for \(u, v\) E E, and \(VU U VV, EU,V\) is a  
complete bipartite graph with b u + b v nodes and bubv edges. Hence H contains 1\:vEV b v  
nodes and n\* = 1\:\(u,V\)EE bubv edges. An example is given in Figure 4.1. This is not a  
polynomial reduction, since the new description of the problem is not a polynomial  
function of 1\:vEV log by.  
Let y E Bn' be the incidence vector of a matching on H, and let x e = 1\:7\:1 Ye; for e E E.  
Then x E Z\~ and 1\:eE6 \(v\) Xe = 1\:eE6 \(V'\) Ye \~ b v for v E V. Hence x is an integer b-matching  
onG.  
Conversely, if x E Z\~ is the vector of an integer b-matching on G, then we get a  
matching on H by setting Ye; = 1 for Xe node disjoint edges for all e E E.  
For the graph of Figure 4.1, Table 4.1 gives the maximal b-matchings on G and the  
corresponding matchings on H.  
Table 4.1.  
Y!J Y\~J y!, y\~2 y!, y\~, y\~, y\:,  
o 1 0 0 0 0 0 1 0  
1 0 0 0 0 0 0 1  
0 1 0 0 1 0 0 0  
0 1 0 0 0 1 0 0  
o 0 0 1 0 0 1 0 0  
0 0 1 0 0 0 0 1  
0 0 0 1 1 0 0 0  
0 0 0 1 0 0 1 0  
o o 2 0 0 0 0 1 0 0 1  
0 0 0 0 0 1 1 0 4\. Additional Results on Matching and Related Problems 643  
We also obtain a linear inequality description of the convex hull of integer b-matchings.  
Theorem 4.8. The convex hull of integer b-matchings is given by  
L xe";;;; bv for v E V  
eE.5\(v\)  
\(4.7\)  
eEE\(U\) L Xe";;;; - L bv  
II J  
2 vEU for U S V with L bv odd  
vEU  
xER\~.  
Proof. Let y E R\~' be a point in the convex hull of matchings on H; and for  
e = \(u, v\) E E, let Xe = \~f\~f' Ye" Then x E R\~ and, as above,  
Now suppose \~VEU bv is odd. Let S = UvEU VV so that I S I = \~VEU by. Hence from the  
odd-set constraints we obtain  
L Xe = L Ye";;;; -lSI = - L bv .  
l1 J l1 J  
eEE\(U\) eEE\(S\) 2 2 vEU  
So x satisfies \(4.7\).  
Conversely, suppose x satisfies \(4.7\). We need to show that for each such x there is a  
Y ERn' lying in the convex hull of I-matchings on H. For e = \(u, v\) E E, let  
Yu',v j = xu,v/bub v for i = 1, ... , bu andj = 1, ... , by. Then Y E R\~' and  
L L Yu',vj = L Xbu,v,,;;;; 1 for vj E VV and v E V.  
uEV\\\{v\} u'EV" uEV\\\{v\} v  
Now consider an odd set S of nodes in H.  
Case 1. S = UVEU VV and \~VEU bv is odd. Hence I S I = \~VEV bv and  
L Ye = L Xe";;;; - L bv = -IS I  
II J II J  
eEE\(S\) eEE\(U\) 2 vEV 2  
Case 2. S contains kv \> 0 nodes from VV for v E U and for some wE U, kw \< bw .  
Hence I S I = \~VEU kv and  
\~ \~ kukv  
L. Ye = L. b b Xu,V'  
eEE\(S\) u,vEU u v  
We will show that  
L kbukb v Xu,v ,,;;;; -2 1 \( L kv + \(kw - 1»\) = l\~ IS I J.  
u,vEU U v vEU\\\{w\} 644 111.2. Matching  
For v E U \\ \{w\}, multiply the constraint LuEJ\(v\) Xu,v \~ bv by kv /2bv and multiply the  
degree constraint for w by \(kw - 1\)/2bw . By summing these inequalities and using x E R1  
to eliminate the coefficients of edges not in E\( U\), we obtain  
'" \(ku kv \) '" \(ku kw - 1\) \('" k k \)  
L. - + - XUV + L. - + -- Xuw \~ L. v + \( w - 1\)  
u,vEU\\\{w\} 2bu 2bv ' uEU\\\{w\} 2bu 2bw ' vEU\\\{w\}  
Now for u, v E U \\ \{w\}, we obtain  
For u E U \\ \{w\}, we obtain  
and  
since 1 \~ kw \~ bw - 1 and 1 \~ ku \~ bu. Hence,  
'" kukv '" \( ku kv \) '" \(ku kw - 1\) L. --x \~ L. -+- X + L. -+-- X  
u,vEU bubv u,v u,vEU\\\{w\} 2bu 2bv U,v uEU\\\{w\} 2bu 2bw U,w  
# •  
A triangle with bv = 2 for v = 1,2, 3 and c = \(1 1\) shows that the system \(4.7\) is not  
TDI. An interesting result that we will not prove is that by adding the superfluous  
constraints LeEE\(U\) Xe \~ t LVEU bv for U \~ V with LVEU bv even, we obtain a TDI system.  
Analogous to Theorem 4.4 and by an identical proof, which uses Theorem 4.8, we  
obtain the convex hull of perfect b-matchings.  
Corollary 4.9. The convex hull of perfect b-matchings is given by  
L Xe = bv for v E V  
\(4.8\) eEJ\(v\)  
L Xe \~ 1 for U \~ V with L bvodd  
eEJ\(U\) vEU  
xER1.  
This result will be used later to establish the convex hull of perfect binary 2-matchings.  
Binary b-matching problems can be reduced to integer b-matching problems. We will  
only study binary perfect 2-matching, denoted by BP2M. Here the feasible solutions are  
cycles that cover all of the nodes. We showed in Section II.6.3 that BP2M gives a tighter  
relaxation for the traveling salesman problem than does integer perfect 2-matching. 4\. Additional Results on Matching and Related Problems  
u e  
645  
v  
# •  
# •  
u e1 u' e 2 v'  
• • •  
bu = 2 bu' = 1 bu' = 1  
Figure 4.2  
e 3  
v  
# •  
bu = 2  
Given a BP2M problem on G = \(V, E\), we construct a new graph G' = \(V u V',  
E U E'\), where each e E E is replaced by a path with three edges as shown in Figure 4.2.  
Hence I V' I = I E' I = 21E I. We let bv = 2 for v E V and bv = 1 for v E V'.  
Now every perfect b-matching on G' has either \(i\) Xel = X e' = 1 andxe2 = 0 or \(ii\) Xe2 = 1  
and Xel = X e' = O. So there is a one-to-one correspondence between BP2M's on G and  
perfect b-matchings on G' given by Xe = Xel = X e' = 1 - X e' for e E E.  
This reduction, together with the reduction of perfect integer b-matching with  
bv E \(1, 2\) to perfect matching, gives a polynomial-time algorithm for BP2M \(i.e., the  
algorithm of Section 3\). Figure 4.3 shows the transformation of BP2M for a triangle to  
perfect I-matching, and it also shows a perfect I-matching on the resulting graph. This  
reduction also yields a linear inequality description of the convex hull of BP2M.  
In Section II.2.3, we derived the rank 1 inequalities  
\(4.9\) I Xe+ I.xe\~ IHI +lIE 2A IJ forHC V,  
eEE\(H\) eEE  
where E \~ J\(H\) is an odd set of node disjoint edges. Here we will show that these  
inequalities, together with the degree constraints and 0 \~ x \~ 1, define the convex hull of  
0-1 perfect 2-matchings.  
First we restate \(4.9\) by subtracting\:l\: LeEo\(V\) Xe = 1 for v E H. This yields  
1 1 llEIJ - I Xe -- I Xe \~ --  
2 eEE 2 eEo\(H\)\\E 2  
or  
I Xe - I Xe \~ IE I - 1  
eEE eEo\(H\)\\E  
or  
I \(1 - xe\) + I Xe \~ 1 for He V, E \~ J\(H\), IE I odd.  
eEE eEo\(H\)\\E 646 111.2. Matching  
2  
# 2\~2  
BP2M  
2  
2 2  
Perfect integer b-matching with bu E \{ 1, 2\}  
Perfect 1-matching  
Figure 4.3 4\. Additional Results on Matching and Related Problems 647  
Theorem 4.10. The convex hull of binary perfect 2-matchings is given by  
I Xe = 2 for v E V  
eEo\(v\)  
\(4.10\) I \(l - Xe\) + I Xe \~ 1 for H e V, where E \<;; J\(H\) is an odd  
eEE eEo\(H\)\\E  
set of node disjoint edges  
x \~ 1, XER\~.  
Proof We transform BP2M to an integer perfect b-matching problem with b i E \{t, 2\}  
as shown in Figure 4.3, and then we apply Corollary 4.9 to the graph G ' = \(V U V',  
EUE'\).  
Suppose y E R!n satisfies \(4.8\) for the graph G'  
. For e E E, let Xe = 1 - Ye2 = Yel = Ye3  
\(see Figure 4.3\). Since 0 \~ Ye2 \~ 1, we have 0 \~ Xe \~ 1 for all e E E. Also \~eEO\(v\) Xe =  
\~eEo\(vl Yel = 2.  
Now for He V and EeE with lEI odd, define we V U V' by W=HU\{u '\:  
\(u, v\) E E\}. Then \~VEW bv is odd. Also  
J\(W\) = \{\(u '  
, v'\)\: \( u, v\) E E\} U \{\(u, u'\)\: \(u, v\) E J\(H\) \\ E\}.  
Thus \~eEO\(W\) Ye \~ 1 yields  
I Ye 2 + I Y e ' \~ 1.  
e2\~\(u',v'\)\: \(u,v\)EE e'\~\(u,v\)Eo\(H\)\\E  
Transforming to the variables x E R\~ yields  
I \(1 - xe\) + I Xe \~ 1.  
eEE eEo\(H\)\\E  
Hence x sa\~isfies \(4.10\).  
Conversely, if x satisfies the constraints \(4.10\), then with Yel = Ye3 = 1 - Ye2 = x" we  
have Y E R!n; also, Y satisfies the degree constraints for G'  
. Now if \~VEW bv is odd,  
I W n V'I is odd and, in particular, I \{u' E W\: v I \$. W\} is odd. Define H = W n V and  
E = \{\(u, v\) E E\: u, u' E W, v, v' \$. W\}. Hence IE I is odd and J\(H\) = \{\(u, v\)  
EE\: u E W, v\$. W\}. Now from  
I \(1 - xe\) + I Xe \~ 1,  
eEE eEJ\(H\)\\E  
we obtain  
I Yed I Yel = I Ye \~ 1.  
e 2 \~ \(u',v'\)\:\(u,V\)EE e ' \~ \(u,v\) EJ\(H\)\\E eEo\(W\)  
Hence Y satisfies \(4.8\). • 648 111.2. Matching  
Theorem 4.10 generalizes to perfect 0-1 matchings. The convex hull of these matchings  
is given by  
x.;;; 1, xER\~  
L Xe = bv for v E V  
eEa\(v\)  
where H £; V and E £; J\(H\) are such that I\:VEH bv + I E I is odd.  
Moreover, these results can be generalized further to include the constraints x .;;; d for  
anydEZ\~.  
T-Joins and T-Cuts  
Our next topic introduces parity conditions into matching problems and includes the  
postman problem and the minimum-weight s-t path problem.  
Definition 4.1. Given G = \(V, E\) and T £; V with I T I even, a subset of edges E' £; E is a  
T-join if, in the subgraph G' = \(V, E'\), the degree of v is odd if and only if vET.  
Proposition 4.11. Minimal T-joins areforests.  
Proof By deleting all of the edges from all cycles of the T-join, we obtain a smaller  
\~\~. .  
Example 4.1. In the graph of Figure 4.4, if T = V, the T-joins are \{et, e4\} and \{e2, e3, e4\}'  
If T = \{l, 4\}, the T-joins are \{et. e2, e4\} and \{e3, e4\}. If T = \{3, 4\} the T-joins are \{e4\} and  
\{et. e2, e3, e4\}.  
By choosing different types of sets T, the minimal T-joins yield forests with interesting  
properties.  
1. If T = \{s, t\} and E' £; E is a minimal T-join, then the forest is an s-t path.  
2. If T = V, E' £; E is a minimal T-join, and the edges of E' form a matching, then the  
forest is a perfect matching.  
3. If T = \{u\: u is of odd degree\} and E' £; E is a minimal T-join, then E' is a minimal  
set of edges with the property that the multigraph obtained by duplicating E' is  
eulerian.  
2-------... 3  
4  
Figure 4.4 4\. Additional Results on Matching and Related Problems 649  
2\~---\~3  
4  
G  
Figure 4.5  
The minimum-weight T-join problem is solvable in polynomial time. We show this by  
reducing it to a perfect matching problem. Given G = \(V, E\) and T, replace each v E V by  
a clique Cv containing IJ\(v\) I + £Xv nodes, where  
\_ \{O £Xv - 1 otherwise.  
if vET and I J\( v\) I is odd, or v \$. T and I J\( v\) I is even  
Then for each \(u, v\) E E, join a node in Cu to a node in Cv in such a way that no two of  
these edges are incident to the same node. Call the new graph G' = \(V', E U E'\), whereE'  
are the clique edges.  
For the graph of Example 4.1 and T = \{l, 4\}, we have £Xl = £X3 = 1 and £X2 = £X4 = o. A  
perfect matching on G' and the corresponding T-join are sho\)J'n in Figure 4.5.  
Proposition 4.12. If E \~ E U E' is a perfect matching in G', then EnE is a T-join in G.  
Conversely, ijE\* is a T-join in G, there exists an E \~ E' so that E U E\* is a perfect matching  
in G'.  
Proof Suppose vET. By the definition of £Xv, I C v I is odd. Hence a perfect matching  
in G' contains an odd number of edges in J\(Cv\). Similarly, if v \$. T, then I Cv I is even and  
a perfect matching in G' contains an even number of edges in J\( C v\).  
The argument for the converse is similar. •  
Next we consider a class of valid inequalities for the convex hull of T-joins. The  
following definition generalizes the definition of s-t cuts in a graph.  
Definition 4.2\. Given G = \(V, E\) and T \~ V with I T I even, J\( U\) for U \~ V is a T-cut if  
I U n T I is odd. 650 111.2. Matching  
o\(U\) n E'  
U V\\U  
Figure 4.6  
Example 4.1 \(continued\). The minimal T-cuts with T = V are \{e\{, e2\}, \{e\{, e3\}, and \{e4\}.  
Suppose 6\( U\) is a T-cut and E' s; E is a T-join. Consider the graph G = \(V, E'\) shown  
in Figure 4.6. Since 6\(U\) is a T-cut, I U n TI is odd. Since E' is a T-join, the degree of  
each node in U nTis odd, and the degree of each node in U \\ T is even. Now if  
6\( U\) n E' = 0, the graph \(U, E\( U\) n E'\) would have an odd number of nodes of odd  
degree, which is impossible for any graph. Hence 6\( U\) n E' \*' 0, and  
I Xe \~ 1 for 6\(U\) aT-cut, Us; V  
eEtl\(U\)  
is a valid inequality for the convex hull of T-joins. Moreover, these inequalities yield a  
polyhedron where extreme points are the minimal T-joins.  
Theorem 4.13. A linear inequality description of the polyhedron whose extreme points are  
the incidence vectors of minimal T-joins in G = \(V, E\) and whose extreme rays are the n unit  
vectors is given by  
I Xe \~ 1 for Us; V with I U n TI odd  
\(4.11\) eEtl\(U\)  
xER\~.  
The proofinvolves showing that y satisfies y \~ 0 and the odd-set constraints of\( 4.2\) for  
G' ifand only if x satisfies \(4.11\) for G. The details are left as an exercise.  
We now show that blocking polarity can be used to determine a polyhedron whose  
extreme points are the minimal T-cuts.  
Proposition 4.14. For any graph G, the set of minimal T-joins and T-cuts are a pair of  
blocking clutters.  
Proof The proof is by Corollary 6.2 of Section III.1.6. In particular, we show that if  
E' C E does not contain a T-join, then E \\ E' contains a T-cut. Note that it suffices to  
take a maximal set E' that does not \~ntain a T-join; that is, for any e = \(u, v\) E E \\ E',  
E' U \{e\} contains a minimal T-join E.  
Each component of \(V, E\), and hence each component of \(V, E' U \{e\}\), contains an  
even number of nodes of T. Now let G = \(U, E\(U\) n \(E' U \{e\}\)\) be the component of  
\(V, E' U \{e\}\) containing e.  
We claim that \(U, JIJU\) n E'\) is disconnected. If not, there exists a cycle C in G  
containing e. But then E \\ C s; E is a T-join, contradicting the definition of E'. 4\. Additional Results on Matching and Related Problems 651  
Now let Vb V2 be a bipartition of V according to the components of\(U, E\(U\) n E'\).  
Since I U n T I is even and e E If, it follows that I VI n T I and I V2 n T I must be odd.  
Finally, since J\(VI\) n E' = 0, J\(VI\) is a T-cut with J\(VI\) \~ E \\ E'. •  
From Theorem 4.13, Proposition 4.14, and Theorem 6.5 of Section III.1.6, we obtain a  
description of a T-cut polyhedron.  
Theorem 4.15. A linear inequality description of the polyhedron whose extreme points are  
the incidence vectors of minimal T-cuts in G = \(V, E\) and whose extreme rays are the n unit  
vectors is given by  
I Xe \~ 1 for all minimal T-joins E' \~ E  
\(4.12\) eEE'  
xER\~.  
Since we have already given a polynomial-time algorithm for finding minimum-weight  
T-joins, it follows from the polynomial-time equivalence between optimization and  
separation that\:  
Corollary 4.16. The minimum-weight T-cut problem is solvable in polynomial time,  
In fact, there is an efficient combinatorial algorithm for solving the minimum-weight  
T-cut problem. It uses a max-flow algorithm as a subroutine and is closely related to the  
algorithm given in Section 11.6.3 for finding violated subtour inequalities.  
Edge Coloring  
The last topic of this section is the edge-coloring problem\: Given G = \(V, E\), color the  
edges of G, with a minimum number of colors subject to the restriction that no pair of  
edges incident to a common node has the same color.  
Edge coloring is related to matching since an edge coloring is feasible if and only if all of  
the edges with the same color are a matching. Hence we can formulate the edge-coloring  
problem as one of covering the edges of a graph with a minimum number of maximal  
matchings. This yields a minimum-cardinality set-covering problem with a huge number  
of variables of the form  
x\(G\) = min ly  
\(4.13\) yA \~ 1  
where the rows of A correspond to the maximal matchings in G, and x\( G\) is the minimum  
number of colors needed to obtain a feasible edge coloring. X\(G\) is called the chromatic  
index ofG.  
Let Ll\(G\) = maxvEv I J\(v\) I; that is Ll\(G\) is the degree of a node v\* of maximum degree.  
Since all of the edges incident to v\* require a different color, we have X\( G\) \~ Ll\( G\) for all  
graphs G.  
Proposition 4.17. X\(G\) = Ll\(G\)for bipartite graphs,  
Proof If Ll\( G\) \~ 2, the result is trivial. That is, if Ll\( G\) = 2, then G contains disjoint  
paths and even cycles, so two colors suffice. 652 111.2. Matching  
Now suppose that d\(G\);;\:. 3. Attempt to construct a feasible coloring with d\(G\) colors  
by coloring the edges in any order and not using a new color unless it is necessary to do so.  
Suppose that we have already used d\(G\) colors and that e = \(u, v\) requires a new color.  
This means that all of the d\( G\) colors except i have been used to color edges incident to u,  
i has been used to color an edge adjacent to v, and some color j has not been used to color  
edges adjacent to v \(see Figure 4.7\). Now consider the subgraph generated by e and all of  
the edges already colored either i or j. In this subgraph, each node is of degree no larger  
than 2 and there are no odd cycles; hence it is possible to color these edges with i and j  
alone. So now we have a coloring with no more than d\( G\) colors that includes e. •  
Note that the proof gives a polynomial-time algorithm for the edge-coloring problem  
on bipartite graphs.  
Next we consider the edge-coloring problem in general graphs. The graph of Figure 4.8  
has d\( G\) = 3 \< X\( G\) = 4\. A 4-coloring is shown in Figure 4.8; X\( G\) ;;\:. 4 since I E I = 7, and  
each maximal matching has two edges.  
Surprisingly, this example gives the largest possible value of x\( G\) - d\( G\). The following  
theorem, which we will not prove, is known as Vizing's theorem.  
Theorem 4.18. For any graph G, X\(G\) equals d\(G\) or d\(G\) + 1.  
We now comment on its implications on solving the edge-coloring problem. Since d\( G\)  
can be found for any graph in O\( I E I\) time and Vizing's proof provides a fast algorithm to  
color the edges with d\(G\) + 1 colors, we might hope that Theorem 4.18 could be used to  
find X\( G\) efficiently. Unfortunately, this is not the case since the decision problem "Does  
X\(G\) = d\(G\)?" is .N'9Jl-complete. Moreover, determining the chromatic index is difficult  
even if X\( G\) is small.  
u .... -----\~ ...  
# -'"----4u  
3  
Figure 4.7. i = 3,\) = I. 4\. Additional Results on Matching and Related Problems 653  
Theorem 4.19. The problem of deciding whether X\(G\) .;;; 3 is \}\(P\}J-complete.  
In other words, there is an infinite family of graphs with Ll\( G\) = 3, for which the  
problem of deciding whether X\( G\) = 3 or 4 is \}\(P\}J-complete. An immediate consequence of  
this result is\:  
Corollary 4.20. Unless P\}J = \}\(P\}J, no polynomial-time algorithm can yield afeasible edge  
coloring that requiresfewer than 11 x\(G\)J colorsfor all graphs.  
Despite these negative results regarding the polynomial solvability of the edge-coloring  
problem, we now show how Theorem 4.18 and the ellipsoid algorithm can, in certain  
cases, yield a polynomial-time algorithm for finding X\( G\). The linear programming  
relaxation of\(4.13\) is  
XLP\(G\) = min ly  
\(4.14\)  
yA \~ 1  
yER'\:'  
and its dual is  
LlLP\(G\) = max Ix  
\(4.15\)  
Ax.;;; 1  
xER\~.  
Although problem \(4.15\) has a constraint for each maximal matching, it can be solved  
in polynomial time since the separation problem is a maximum-weight matching prob-  
lem. That is, x·, 0 .;;; x\*.;;; 1, is a feasible solution to \(4.15\) if and only if a maximum-weight  
matching in G with edge weights x\* has value no greater than 1.  
Proposition 4.21. If LlLP\(G\) \> Ll\(G\), then X\(G\) = Ll\(G\) + 1.  
Proof Note that Ll\(G\).;;; LlLP\(G\) since a feasible solution to \(4.15\) is Xe = 1 for all  
e E t\:5\(v\*\), where v\* is a node of maximum degree. Now by linear programming duality,  
LlLP\(G\) = XLP\(G\).;;; X\(G\). Hence if LlLP\(G\) \> Ll\(G\), then X\(G\) \> Ll\(G\). Then by Theo-  
rem 4.18, we have X\(G\) = Ll\(G\) + 1. •  
In the graph of Figure 4.8 we have LlLP\( G\) \> 3, which implies X\( G\) = 4.  
Heuristics provide a practical approach for finding good colorings of large graphs. In  
fact, there are heuristics that achieve the performance bound of I\~ X\(G\)J, and it is also  
possible to realize asymptotic bounds of the form ax\(G\) + p with a \<\~. We will not give  
Figure 4.8 654 III.2. Matching  
any details, but the basic idea of many heuristic coloring schemes has been used in the  
proof of Proposition 4.17. Namely, in a sequential coloring scheme, whenever we encoun-  
ter an edge e = \(u, v\) that requires a "new" color, we try to adjust the present coloring so  
that a new color is not required for e. A simple rule of this type is to consider two colors,  
say red and blue. Now we try to recolor all of the red and blue edges by red and blue so that  
feasibility is maintained and every edge adjacent to u and v is colored red. Then e can be  
colored blue.  
5. NOTES  
Section 111.2.1  
Matching theory predates mathematical programming. Remarks on the early literature,  
which was primarily concerned with bipartite graphs, appear in Pulleyblank \(1983\) and  
Schrijver \(l983a\). Lovasz and Plummer \(1986\) is a recent book on matching theory that  
emphasizes the graph-theoretic aspects of matching.  
The application to the postman problem was given by Edmonds and Johnson \(1973\).  
Fujii et al. \(1969\) and Coffman and Graham \(1972\) gave an application to a scheduling  
problem. Network flow problems in which an arc can have two heads or two tails can be  
modeled as matching problems \[see Edmonds and Johnson \(1970\)\]. Nemhauser and  
Weber \(1979\) used weighted matching in the solution of set-partitioning problems. Ball,  
Bodin and Dial \(1983\) gave a matching-based algorithm for the scheduling of mass transit  
crews and vehicles.  
Section 111.2.2  
The augmenting-path proposition is due to Berge \(1957\) and Norman and Rabin \(1959\).  
A fast cardinality matching algorithm for bipartite graphs was given by Hopcroft and  
Karp \(1973\).  
The algorithmic aspects of the I-matching problem on general graphs were initiated by  
Edmonds \(1965a\). In this article, he gave a polynomial-time algorithm for the cardinality  
problem. The Hopcroft-Karp algorithm for bipartite graphs was extended to general  
graphs by Even and Kariv \(1975\).  
Section 111.2.3  
The maximum-weight matching algorithm was developed by Edmonds \(1965c\). The  
algorithm given here is a slight variation of the one by Edmonds. Another variation is  
given in Lawler \(1976\).  
Other weighted matching algorithms have been given by Cunningham and Marsh  
\(1978\), Derigs \(1986\), and Grotschel and Holland \(1985\). The latter is a fractional cutting-  
plane approach that uses the simplex method and an efficient separation routine for  
finding violated blossom inequalities. The separation routine is based on a polynomial-  
time algorithm by Padberg and Rao \(1982\) for finding minimum-weighted T-cuts \(see  
Section 111.2.4\).  
Ball and Derigs \(1983\) presented alternative strategies for implementing matching  
algorithms. Burkhard and Derigs \(1980\) gave FORTRAN listings of matching and  
assignment algorithms.  
Pulleyblank and Edmonds \(1975\) characterized the blossom inequalities that are facets  
of the matching polytope.  
Sensitivity analysis in weighted matching has been considered by Weber \(1981\), Derigs  
\(1985\), and Ball and Taverna \(1985\). 6\. Exercises 655  
Avis \(1983\) presented a survey of heuristics for solving weighted matching problems.  
Edmonds and Johnson \(1970\) described an algorithm for weighted b-matching prob-  
lems. The first polynomial-time algorithm for this class of problems is attributed to  
Cunningham and Marsh \(1978\).  
Section 111.2.4  
The nonalgorithmic proof technique given here for the convex hull of I-matchings is due  
to Lovasz \(1979a\). Other nonalgorithmic proofs have been given by Balinski \(1972\),  
Hoffman and Oppenheim \(1978\), and Schrijver \(1983b\). The proof of total dual integrality  
comes from Schrijver \(1983a\). A different proof is given by Cunningham and Marsh  
\(1978\).  
Relationships between matching and edge covering have been studied by Norman and  
Rabin \(1959\) and Balinski \(1970b\).  
The transformations used to obtain the b-matching results come from Schrijver  
\(1983a\), who attributed them to Tutte \(1954\).  
Theorem 4.8 on the b-matching polytope is due to Edmonds and Pulleyblank and  
appears in Pulleyblank \(1973\). Pulleyblank \(1980,1981\) established that this system is TD!.  
Further results regarding a minimal TDI system have been obtained by Cook \(1983b\).  
Cook and Pulleyblank \(1987\) provided a minimal linear inequality representation of the  
convex hull of capacitated b-matchings.  
The reduction of the minimum T-join problem to a perfect matching problem comes  
from Edmonds and Johnson \(1973\). They also used the connection with matchings to  
prove Theorem 4.15. Also see Gastou and Johnson \(1986\) and Johnson and Mosterts  
\(1987\).  
Generalizations of matching problems have been studied by Gerards and Schrijver  
\(1986\), Cornuejols and Hartvigsen \(1986\), and Cornuejols \(1986\).  
The edge-coloring result for bipartite graphs is a classic theorem of Konig. The proof  
given here can be found in many graph theory texts \[e.g., Bondy and Murty \(1976\)\].  
Theorem 4.18 is due to Vizing \(1964\). Marcotte \(1986b\) showed that Vizing's theorem is  
true in the weighted case for a restricted class of graphs.  
Proposition 4.19 and Theorem 4.20 are due to Holyer \(1981\).  
The result cited on the worst-case bounds of edge-coloring heuristics is due to Hoch-  
baum et al. \(1986\).  
6\. EXERCISES  
1\. Find a maximum-cardinality matching in the bipartite graph of Figure 6.1, and give  
a short proof of optimality.  
2. Find a maximum-cardinality matching in the graph of Figure 6.2, and give a short  
proof of optimality.  
3\. A graph G = \(V, E\) is said to have a perfect matching if there exists M \~ E such that  
no node is exposed relative to M. Let P\( U\), U \~ V, denote the number of com-  
ponents with an odd number of nodes in the graph G u induced by V \\ U. Prove that  
G has a perfect matching if and only if P\( U\) \~ I U I for all U \~ V.  
4\. Find a maximum-weight matching in  
i\) the graph of Figure 6.2 with weights as shown,  
ii\) the graph of Figure 6.3 with weights as shown. 656 III.2. Matching  
Figure 6.1  
5. i\) Devise a fast heuristic algorithm to find violated blossom inequalitie\~.  
ii\) Use this in an FCPA to find a maximum-weight matching in the graph of Figure  
6.2.  
6\. Prove that the dual solution is always half-integer in the maximum-weight matching  
algorithm.  
7. For the maximum-weight matching problem, define an augmenting path p, relative  
to M, to be an alternating path or alternating cycle having no edge of M \\ P incident  
to P and having the property  
I Cj - I Cj\> 0,  
ejEP\\M ejEM\\P  
where P is the set of edges contained in the path p. Prove that M is optimal if and  
only if M admits no augmenting path.  
8\. Find an optimal postman route for the graph of Figure 6.2 with the distances as  
shown.  
9\. Show that the system \(4.4\) \(a\), \(b\), \(d\) is not TDI for the complete graph on four  
nodes.  
10. Show that the weighted b-matching problem reduces to a network flow problem  
when bv is even for all v E V.  
Figure 6.2 6. Exercises 657  
18 13 11  
2 3 5  
e3 e5 e6  
10 13  
e2  
e7  
e8 10  
11 e4 10 6  
7 e9 11  
9  
elO  
7  
ell 8  
e12  
5  
8  
Figure 6.3  
11. Solve the weighted b-matching problem on the graph of Figure 6.4 by reducing it to  
a I-matching problem.  
12\. For the graph of Figure 6.2, solve the minimum-weight T-join problem by reducing  
it to a perfect matching problem for\:  
i\) T = \{2, 7\};  
ii\) T = V.  
13\. Prove Theorem 4.13.  
14. Describe an efficient combinatorial algorithm for the minimum-weight T-cut  
problem.  
6  
b2 =4 F-------------\~ b3 = 2  
3  
Figure 6.4 658 III.2. Matching  
15\. Given a graph G, suppose that an efficient combinatorial algorithm is known for the  
separation problem for the I-matching polytope P\(I\) \(see Section III.3.7\). Let  
XLP\(w\) = min\{1y\: yA \~ w, y E R'\:'\}, where A is the matching-edge incidence matrix  
ofG.  
i\) Verify that XLP\( w\) \~ 1 if and only if w lies in the I-matching polytope P\(I\) and  
that  
\{ w\(S\) \}  
XLP\(W\) = mffx \(IS I \_ 1\)/2\: IS I odd, where w\(S\) = eEtS\) We'  
ii\) Verify that C\(w\) = max\{A,\: A,W E P\(I\)\} = l/XLP\(W\).  
iii\) Consider the following algorithm to calculate C\( w\), using as a subroutine an  
efficient separation algorithm for the polytope P\(1\); that is,  
max\{w\(S\) - \(IS I - 1\)/2\: IS I odd\}  
Algorithm\: Choose A,0 with A,°W \~ P\(1\). Set t = O.  
Iteration t\:  
a\) Solve the separation problem for A,tw.  
b\) Stop if A,tw E P\(1\).  
c\) Set A,t+l such that A,t+l w\(st\) = \( I S I - 1\)/2.  
d\) Augment t.  
Verify that  
a\) A,t+l \< A,t,  
b\) \[l/A,t+l-l/A,t\]\(IS t l-l\)/2-\(ISt+1 1-l\)/2»O.  
c\) 1S t I is strictly decreasing.  
d\) The algorithm terminates after, at most, I V I /2 iterations.  
iv\) Use this algorithm to calculate XLP\(G\) andx\(G\) for the graph of Figure 4.8.  
16\. Show that the maximum-weight assignment problem with the following conditions  
can be formulated as a matching problem\: Cii = -00 for all i, ci\} = Cji for all i andj,  
and xi\} = xji for all i andj. 111.3.  
Matroid and Submodular Function  
# Optimization  
1. INTRODUCTION  
Matroids and submodular functions are the foundations for some combinatorial optimi-  
zation problems that generalize both network flow problems and the spanning tree  
problem treated in Chapter 1.3. Matroids can be viewed as prototypes of independence  
systems and 0-1 integer programs with "nice" properties that can be used to obtain  
efficient algorithms for the corresponding optimization problems.  
Definition 1.1\. Let N = \{l, ... , n\} be a finite set, and let *fF* be a set of subsets of N . .f\> =  
*\(N, fF\)* is an independence system if FI E *fF,* and F2 \~ FI implies F2 E *fF.* Elements of *fF*  
are called independent sets, and the remaining subsets of N are called dependent sets.  
Let *fFT* = *\{F* E *fF\: F* \~ n. Then if.f\> = *\(N, fF\)* is an independence *system,.f\>T* = *\(T, fFT\)*  
is an independence system for all *T* \~ *N.*  
Definition 1.2\. Given an independence system .f\> = \(N, *fF\),* we say that FE *fF* is a  
maximal independent set if F U \{j\} \$. *fF* for all\} EN \\ F. A maximal independent set T  
is maximum if IS I \~ I TI for all S E *fF.*  
In describing independence systems, we use the notation  
meT\) = max\{ I S I\: S E *fF\}* for T \~ N  
S\~T  
to denote the size ofa maximum-cardinality independent set in T. Note that meT\) \~ I TI  
and *fF* = \{T \~ N\: meT\) = I T I\}. Hence.f\> can also be specified as.f\> = \(N, m\).  
Matroids are those independence systems for which all maximal independent sets in *T*  
are maximum for any subset *T* \~ *N.*  
Definition 1.3. M = \(N, *fF\)* is a matroid if M is an independence system in which for any  
subset T \~ N, every independent set in T that is maximal in T has cardinality meT\).  
The following proposition is an immediate consequence of the fact that maximal sets  
must be maximum not just in *N* but also for all subsets *T* \~ *N.*  
Proposition 1.1. If M = *\(N, fF\)* is a matroid, then the independence system .f\> *T* = *\(T, fF T\)*  
is a matroid for T \~ *N.*  
659 660 111.3. Matroid and Submodular Function Optimization  
Matroids were originally developed from matrices to generalize the properties oflinear  
independence and bases in a vector space. This generalization has yielded several classes of  
matroids.  
\(a\) Matric Matroids. Let A be an m x n matrix, and let N be the index set of the  
columns of *A.* Define the independence system *\(N,* gjP\) by *F* E gjP if the set of columns  
defined by F is linearly independent. For any submatrixA T with columns *aj* forj E T, it is  
well known that every maximal set of linearly independent columns contains  
m\(T\) = rank\(AT\) columns. Hence *\(N,* gjP\) is a matroid. If *M* is a matroid and there exists a  
matrix A such that the independent sets of M correspond to the linearly independent  
columns of *A,* then M is called a matric matroid.  
\(b\) Graphic M atroids. Let G = *\(V, E\)* be a graph, and let *F* c;; *E* be a subset ofthe edges.  
Let *F* E gjP if *G*p = *\(V, F\)* contains no cycles. For any subset *T* c;; *E,* the cardinality of a  
maximal set of edges that is acyclic in *GT* is m\(T\) = I *VI* - number of connected  
components of G*T .* Hence \(E, gjP\) is a matroid. If M is a matroid and there exists a graph G  
such that the independent sets of M correspond to the acyclic edge sets of G, then M is a  
graphic matroid. We leave it as an exercise to show that graphic matroids are matric.  
\(c\) Partition Matroids. Given m disjoint finite sets Ei for i E I = \{l, ... ,m\}, let  
E = U;\:\:! E i. F c;; E is independent if iF n Ei I \~ 1 for all *i* E I. For any T c;; E, the  
cardinality of a maximal independent set contained in *T* is *LiEf ai,* where ai = 1 if  
*Tn E i "\** 0 and ai = ° otherwise. Hence *\(E,* gjP\) is a matroid.  
The set of matchings in a graph do not form a matroid. For a path *e* j, *e2, e3,* both the sets  
*\{e2\}* and *\{ej, e3\}* are maximal matchings in *\{ej, e2, e3\},* but they differ in cardinality.  
In the context of combinatorial optimization, the most striking property of matroids-  
and indeed, another way to define them-is that, given a weight vector c E *RIEl,* a greedy  
algorithm \(see Section 1.3.3 for trees and Section II.5.3 for general independence systems\)  
always gives an optimal-weight independent set. This will be demonstrated in Section 3.  
Submodular functions are closely related to matroids. We will see that for a matroid,  
the cardinality function m is submodular. Such functions have already appeared in  
Section 11.5.3, where the uncapacitated location problem was shown to be a problem of  
maximizing a submodular function.  
Definition 1.4. ofN.  
Let *N* be a finite set, and letf be a real-valued function on the subsets  
a. fis non decreasing ifj\(S\) \~f\(T\) for S c;; T c;; *N.*  
b. fis submodular iff\(S\) + f\(T\) \~ f\(S U T\) + f\(S n T\) for S, T c;; *N.*  
c. fis supermodular if -fis submodular.  
d. r is a submodular rank function if r\(0\) = 0, r is integer-valued, nondecreasing, and  
submodular, and *r\(U\}\)* \~ 1 for aUj EN.  
Example 1.1. Given a digraph 7iJ = *\(V, sti\)* and weights c E *R'\:I,* for S c;; *V* let  
c\(S\) = I Cij = I Cij'  
*«i,j\)Eb'\(S\)\)* iES  
*jEV\\S*  
The cut function c\(S\) is submodular because  
c\(S\) + c\(T\) - c\(S U T\) - c\(S n T\) = I Cij + I Cij \~ 0.  
*iES\\T* iET\\S  
jET\\S *jES\\T* 1. Introduction 661  
Having introduced both matroids and submodular functions, we now briefly indicate  
some of the other problems to be studied in this chapter. In the next section we will  
establish the equivalence between a matroid *M* = *\(N,* 81'\) and a submodular rank function  
r on *N,* and we will introduce and develop some elementary matroid properties for later  
use.  
In Section 3 we will consider the matroid optimization problem\: An instance is given by  
a matroid M = \(N, 81'\) and a weight vector cERn. The problem is  
max\{ I *Cj\:* S E 8F\}.  
S *jES*  
Formulating this problem as an integer program leads us to study polytopes of the form  
P\(f\) = \{x E R\~\: I Xj \~f\(S\) for S S N\},  
*jES*  
where/is a submodular function.  
An important generalization of the matroid optimization problem is the k-matroid  
intersection problem\: Given k matroids Mi = \(N, 8Fi\) for i = 1, ... , k and a weight vector  
cERn, the problem is  
Thus, feasible solutions correspond to sets that are independent in each of the matroids.  
Remember that a branching in a digraph *rziJ* = *\(V,* .s4\) is a set of arcs .s4' S .s4 such that  
*rziJ'* = *\(V,* .s4'\) is a spanning tree and no more than one arc enters each node. Hence a set of  
arcs forms part of a branching if and only if it is independent in both a partition and a  
graphic matroid. In Sections 4 and 5 we will study efficient algorithms for the 2-matroid  
intersection problem.  
Now consider the arc sets that form part of a branching in a digraph and intersect these  
sets with a second partition matroid specifying that no more than one arc leaves each  
node. The resulting objects of maximum cardinality are Hamiltonian paths. Because it is  
known that the question of whether a graph contains a Hamiltonian path isXg\}l-complete,  
it follows that the k-matroid intersection problem is Xg\}l-hard for all k \~ 3.  
In Sections 6 and 7 we will consider, in more detail, polytopes P\(f\) where / is  
submodular and nondecreasing. It will be shown that the separation problem for P\(f\) is  
equivalent to the problem of minimizing another submodular function; that is,  
min\{j'\(S\)\: S s N\},  
S  
*f'* submodular.  
Thus we study algorithms for this minimization problem and some special cases where *f'*  
has more structure.  
In Section 8 we will study a covering problem of the form\: Given a matroid *M* = *\(N,* 81'\),  
what is the minimum number of independent sets whose union is N? This problem has the  
integer-rounding property and can be solved efficiently.  
Finally, we consider the problem of maximizing a submodular function\:  
max\{j\(S\)\: S s N\},  
S  
/ sub modular. 662 111.3. Matroid and Submodular Function Optimization  
In contrast to the earlier problems of the chapter, this model includes X9Jl-hard problems,  
such as the uncapacitated location problem. Hence we examine different integer program-  
ming formulations and heuristics.  
2. ELEMENTARY PROPERTIES  
There are many ways of defining and viewing both matroids and submodular functions.  
Here we introduce the definitions and the fundamental results that we will use later. First  
we study submodular functions \(see Definition 1.4\).  
Proposition 2.1  
i. *f* is submodular *if* and only *if*  
\(a\) f\(S U \(j\}\) - f\(S\) \~ f\(S U \(j, k\}\) - f\(S U \(k\}\) for j, k E N,j \* k,  
and S s; N \\ \{j, k\}.  
ii. *f* is submodular and nondecreasing *if* and only *if*  
\(b\) fiT\) \~f\(S\) + *L* \[f\(S U \(j\}\) - f\(S\)\] for S, T s; N.  
*JET\\S*  
Proof Definition 1.4.  
i. *Iff* is submodular we obtain \(a\) by setting S .... S U \{j\} and T .... S U \{k\} in  
If\(a\) holds, let S = A n B, A \\ B = \{jl, ... *,j,\},* and B \\ A = \{k b ... , k s\}. Then  
fiB\) - f\(A n B\)  
s  
= *L* \[f\(S U \(k *1,* ••• , kl\}\) - f\(S U \(k *b* ••• , k *l - 1\}\)\]*  
1=1  
s  
\~ L \[f\(S U \{k b ... , k l\} U \(jl\}\) - f\(s U \{k b ... ,kl-1 \} U \(jd\)\]  
1=1  
s  
\~ *L* \[f\(S U \{k b •.. , k l\} U \(jb ... *,j,\}\)* - f\(s U \{k b ••• ,kl \_l \} U \(jb ... *,j,\}\)\]*  
1=1  
s  
= *L* \[f\(A U \(k *b* ••• , k *l \}\)* - f\(A U \(k *b* ••• , k l-*1\}\)\]*  
1=1  
= f\(A U B\) - f\(A\).  
ii. Let *T* \\ S = \{jI. ... *,j,\}.* Then  
f\(T\) \~f\(S U T\) = f\(S\) + \(f\(S U T\) - f\(S\)\}  
,  
= f\(S\) + *L* \{j\(S U \(jI. ... , jl\}\) - f\(S U \(jJ, ... , jt-1\}\)\}  
1=1  
,  
\~f\(S\) + *L* \(j\(S U \(jl\}\) - f\(S\)\},  
1=1  
where the first inequality holds if *f* is nondecreasing, and the second one holds if *f* is  
submodular. Taking *T* = S U *\{j,k\}* and *T* = *S\\\{ k\}* in \(b\) gives the converse. • 2. Elementary Properties 663  
Complex submodular functions are often constructed from simple submodular  
functions.  
Proposition 2.2. The following conditions yield sub modular functions.  
a. If aj E R' for j EN and ao E R', then f\(S\) = ao + LjES aJor S \~ N is submodular  
onN.  
b. Iffis submodularon N, then\]\(S\) = feN \\ S\)for S \~ N is submodularon N.  
c. Iff is submodular on Nand k E R', then f' \(S\) = min\(f\(S\), k\) is submodular on N.  
d. If f, and hare submodular on N, then f\(S\) = f,\(S\) + fZ\<S\) is submodular on N.  
Proposition 2.3. Iffis integer valued, submodular, and nondecreasing withf\(0\) = 0, and  
reS\) = minQss \(j\(Q\) + IS \\ Q I\}, then r is a submodular rankfunction.  
Proof Suppose reS\) = f\(A\) + IS \\ A I and reT\) = feB\) + IT \\ B I. Then  
reS\) + reT\) = f\(A\) + feB\) + IS \\ A I + IT \\ B I  
\~ f\(A U B\) + f\(A n B\) + IS \\ A I + I T \\ B I  
\~ f\(A U B\) + f\(A n B\) + I \(S U T\) \\ \(A U B\) I + I \(S n T\) \\ \(A n B\) I  
*=* f\(A U B\) + I \(S U T\) \\ \(A U B\) I + f\(A n B\) + I \(S n T\) \\ \(A n B\) I  
\~ reS U T\) + reS n T\).  
Hence r is submodular.  
Now suppose  
reS U \{j\}\) = f\(Q\*\) + I \(S U \(j\}\) \\ Q\* I, where Q\* \~ S U \{j\}.  
*Ifj* E Q\*, we obtain  
reS U \(j\}\) \~ f\(Q\* \\ \(j\}\) + IS \\ \(Q\* \\ \(j\}\) I \(sincef\(Q\*\) \~ f\(Q\* \\ \(j\}\)\)  
\~ reS\) \(since Q\* \\ \{j\} \~ S\).  
If *j* \$. Q\*, we obtain  
reS U \(j\}\) = f\(Q\*\) + IS \\ Q\* I + 1 \~ reS\) + 1.  
Hence r is nondecreasing.  
Finally, r\(0\) = f\(0\) = ° and r\(\{j\}\) .\:;;f\(0\) + l\{j\} I = 1 for allj. •  
Theorem 2.4. If M = \(N, *Bf\)* is a matroid, its cardinality function meT\) =  
maXSsT \{I S I\: S E *Bf\}* is submodular. If\(N, *Bf\)* is an independence system whose cardinal-  
ity function meT\) is submodular, then M = \(N, *Bf\)* is a matroid.  
Proof Clearly m\(0\) = 0, m is nondecreasing, and m\(S U \(j\}\) - m\(S\).\:;; 1, since  
*\(N, Bf\)* is an independence system. To prove that *m* is submodular we will show that  
m\(S U \(j\}\) - m\(S\) \~ m\(S U \(j, k\}\) - m\(S U \{k\}\). 664 III.3. Matroid and Submodular Function Optimization  
The inequality is obvious when *m\(S* U \{j\}\) - *m\(S\)* = 1, so suppose that *m\(S* U \{j\}\) =  
*m\(S\)* = t and *m\(S* U \{j, *k\}\)* - *m\(S* U *\{k\}\)* = 1. There are now two cases to consider.  
*Case* 1. *m\(S* U \{j, *k\}\)* = *t* + 2. Then *m\(S* U \{j, *k\}\)* - *m\(S* U \{j\}\) = 2, which is impossi-  
ble.  
*Case* 2. *m\(S* U \{j, *k\}\)* = *t* + 1. Let Q be a maximalindependent set in S. It follows that  
*Q* U \{/\} \$. \~ for all *IE* S \\ *Q.* Also, since *m\(S* U \{j\}\) = *m\(S* U *\{k\}\)* = *t,* we have *Q* U  
\{j\} \$. \~ and *Q* U *\{k\}* \$. \~. Hence *Q* is maximal in S U \{j, *k\}* so that *m\(S* U \{j, *k\}\)* = t,  
which is a contradiction.  
Let *T!\:; N,* and suppose that SI and S2 are maximal independent sets in *T* with  
lSI I \< IS21. Thus *m\(SI\)* = 1St! \< *m\(S2\)* = IS21. Using \(b\) of Proposition 2.1, we have  
*m\(S2\)* .\:\:; *m\(SI\)* + I *\[m\(SI* U \{j\}\) - *m\(SI\)\],*  
jES,\\S,  
which implies that *m\(SI* U \{j\}\) \> *m\(SI\)* for some *j* E S2 \\ SI' Hence *m\(SI* U \{j\}\) =  
I S I U \{j\} I , contradicting the maximality of S I. Therefore *\(N,* \~\) is a matroid. •  
From now on we will represent a matroid *M* as either \(N,\~\) or *\(N, r\),* where *r* is its  
submodular rank function, depending on which is more convenient.  
The last part ofthe proof of Theorem 2.4 establishes an important exchange property of  
matroids that is well known for matrices.  
Proposition 2.5. *If M* = *\(N,* \~\) *is a matroid and* Sb S2 E \~ *satisfy* lSI I \< IS21, *then*  
*there existsj* E S2 \\ SI *such that* SI U \{j\} E \~.  
There are various other important properties of matroids, most of which are familiar  
from matrices.  
Definition 2.1. Let M = \(N, \~\) be a matroid with rank function r.  
a. *A* is a *basis* of the matroid if *A* E \~ and *r\(A\)* = *r\(N\).*  
b. *A* is a *circuit* of the matroid if *A* is a minimal dependent set \(i.e., *A* \$.\~, but  
*A* \\ \{j\} E \~ for allj E *A\).*  
c. For *A* !\:; *N,* the *span or closure* of *A* is the set sp\(A\) = \{j EN\: *r\(A* U \{j\}\) = *r\(A\)\}.*  
Bases are evidently the maximal independent sets in the matroid, all of which are of  
cardinality *r\(N\).* Circuits are minimal dependent sets. Hence if A is a circuit, then  
*r\(A\)* = *IA* I - 1. From the submodularity of the rank function, we observe that sp\(A\) is the  
maximal set *B* 2 *A* for which *r\(A\)* = *r\(B\).*  
For a graphic matroid, bases are the edge sets of spanning trees, circuits are the edge sets  
of cycles, and the span of an edge set *E'* contains *E'* plus any edge that, when added to *E',*  
yields a new cycle.  
One of the most useful properties of a matroid, which we have already seen to be true  
for cycles in a graph, is\:  
Proposition 2.6. *IfF* E \~ *and F* U \{j\} E;l \~, *there exists a unique circuit* C !\:; *F* U \{j\}.  
*This implies F* U \{j\} \\ *\{k\}* E \~ *foral! k* E C\\ \{j\}. 2. Elementary Properties 665  
*Proof* Suppose there exist distinct circuits Ch C2 in *F* U *U\}.* Now C! n C2 E *fJi* by  
the minimality of circuits. Also, *\(C!* U *C*2\) \\ U\} E *fJi* because *\(C!* U *C*2\) \\ U\} s *F.* But  
*r\(Ci \)* = ICil - 1 for *i* = 1,2, *r\(C!* U *C*2\) = IC! U C2 1 - 1, and *r\(C!* n *C*2\) = IC! n C2 1,  
contradicting the submodu1arity of *r.*  
If *\(F* U *U\}\)* \\ *\{k\}* \$. *fJi,* then *\(F* U U\}\) \\ *\{k\}* contains a circuit C' different from C,  
contradicting the uniqueness of C. •  
Example 2.1. Consider the matric matroid M = \(N, *fJi\)* defined by the matrix  
\(  
2 1 4 -1 0 -2\)  
# 113234,  
3 2 7 4 6 8  
where N = \{l, ... , 6\} is the index set for the columns, and S E *fJi* if and only if the set of  
columns indexed by S is linearly independent. The rank function r takes the following  
values\:  
1. *r\(0\)* = 0, *r\(U\}\)* = 1 for allj.  
2. *r\(U, k\}\)* = 2 for allj \* *k* except that *r\(\{4,* 6\}\) = 1.  
3. *r\(S\)* = 3 for all S with I S I ;;. 3, except that *r\(\{1,* 2, 3\}\) = *r\(\{2,* 4, 5\}\) = *r\(\{2,* 4, 5, 6\}\) =  
*r\(\{4,* 6, *k\}\)* = 2 for all kEn, 2, 3, 5\}.  
Since *r\(N\)* = 3, the bases are the independent sets S with I S I = 3. The circuits are  
\{4, 6\}, \{l, 2, 3\}, \{2, 4, 5\}, and all 4-tuples that do not contain any of these circuits. Also,  
sp\(2, 4\) = \{2, 4, 5, 6\}, and so on.  
The last important concept that we introduce is matroid duality.  
Proposition 2.7. *If r* is *the rank function of a matroid M* = *\(N, r\) and rD\(S\)* = lSI +  
*r\(N* \\ S\) - *r\(N\), then* yD is *the rank function of a matroid.*  
*Proof* It follows immediately from Proposition 2.2 that *rD* is submodular. Also  
*rD\(0\)* = 0, ;rod since  
*rD\(S* U U\}\) - *rD\(S\)* = 1 - *\(r\(N* \\ S\) - *r\(N* \\ \(S U *U\}\)\)\),*  
we have 0 \:\:;; *rD\(S* U U\}\) - *rD\(S\)* \:\:;; 1. Thus the result follows from Theorem 2.4. •  
Definition 2.2. *MD* = *\(N, rD \)* is the *dual matroid* associated with *M* = *\(N, r\).*  
It is readily seen that *A* isa basisofMD ifandonlyifN \\ *A* isa basisofM. Moreover, the  
dual of *MD* is again *M.*  
The dual of a graphic matroid is called a *cographic matroid.* Its bases are the comple-  
ments of spanning trees-that is, the maximal sets that do not disconnect the graph. It  
follows that the circuits of this dual matroid are the minimal disconnecting sets, or  
minimal cuts.  
Example 2.2. and its dual *MD.*  
Consider the graphic matroid M associated with the graph in Figure 2.1 666 111.3. Matroid and Submodular Function Optimization  
Figure 2.1  
The circuits of *M* are the cycles such as *\{ej\: i* = 1, 5, 8\), *\{e4, ell\), \{ej\: i* = 1, 2, 3, 4\). The  
bases of M are the spanning trees containing I V I - 1 = 4 edges.  
The circuits of *MD* are the minimal cut-sets such as *\{ej\: i* = 3, 4, 7, 11\),  
*\{ej\: i* = 3, 4, 5, 6, 8, 11\). The bases of *MD* are the complements of the spanning trees \(i.e.,  
*\{ej\: i* = 4,6, 7, 8, 9, 10, 11\), etc.\) containing *IE* I - I *VI* + 1 = 7 edges.  
3. MAXIMUM-WEIGHT INDEPENDENT SETS  
Matroid Representation  
We have already seen two ways to represent matroids\: One is by listing the set *;!Ii* of  
independent sets, and the other is by the rank function r. However, using either *;!Ii* or r,  
*O\(2n\)* sets or values typically must be specified, where n is the number of elements of the  
matroid. This contrasts strongly with the representation of matroids that interest us. For  
example, a graphic matroid on G = *\(V, E\)* is completely described by its graph, so the  
length of the input description is *O\(n\).*  
The reader will see that the algorithms we describe contain *independence tests* of the  
form\: "Is S s; N an independent set in M, or not?"  
We avoid the representation issue by simply reporting the number of independence  
tests in an algorithm as a function of n. Note that answering standard questions such as "Is  
S s; N a basis of M?" or "Given that S E *;!Ii,* S U *\{j\)* \$. *;!Ii,* find the circuit C s; S U *\{j\)."*  
can be answered with *O\(n\)* independence tests.  
The Greedy Algorithm  
Given a matroid M = \(N, *;!Ii\)* and cERn, the problem of finding a maximum-weight  
independent set is  
\(3.1\) max\{ I *Cj\:* S E *;!Ii\}.*  
S jES 3\. Maximum-Weight Independent Sets 667  
The algorithm to find a maximum-weight forest in Section I.3.3 is an instance of the  
greedy algorithm we now describe for solving \(3.1\).  
The Greedy Algorithm for *M* = *\(N, \[ji\)*  
Initialization\: Order the elements of N so that CI ;;;. C2;;;' ... ;;;. Cn. So = 0, t = 1.  
Iteration t\: If C t .s;;; 0, stop. St-I is optimal. If C t \> 0 and S-I U \{t\} E *\[ji,* set st = St-I U \{t\}. If  
C t \> 0 and St-I U \{t\} \$ *\[ji,* set st = St-I. 1ft = n, stop. sn is optimal. 1ft \< n, sett .... t + 1.  
Although for general independence systems the greedy algorithm does not necessarily  
yield an optimal solution, for matroids it does.  
Theorem 3.1. independent set.  
The greedy algorithm for matroids terminates with a maximum-weight  
Proof Let the greedy solution be SG = VI, ... ,jp\} withjl \<h \< ... \<jp' Suppose  
the greedy solution is not optimal, and let SL = \{k *h* ••• ,kq\}, kl \< k2 \< ... \< kq be the  
lexicographically smallest optimal solution. Suppose j I = k h h = k2' ... ,j s-I = ks -I, but  
js \*' ks. From the greedy algorithm, we have js \< ks and hence *Cj,;;;' Ck,* \> O. Now  
SL. U V s\} \$ *\[ji* since otherwise SL is not optimal. Hence SL U V s\} contains a unique circuit  
C with js E C. Also, since \{jh ... ,js-I\} = \{k *h* ••• ,ks-d E *\[ji,* we have k*t* E C for some  
t ;;;. s. But by Proposition 2.6, we have that \(SL u V s\}\) \\ *\{k*t\} E *\[ji,* its value is at least that of  
SL, and it is lexicographically smaller than SL, which is a contradiction. •  
Note that there no more than n independence calls by the algorithm and that the sorting  
of the initialization step requires *O\(n* log *n\)* comparisons. For a specific class ofmatroids,  
we can use this to calculate the running time of the algorithm. For graphic matroids the  
independence test involves testing for a cycle in a graph, which requires *O\(n\)* calculations.  
Hence the running time of the simplistic algorithm given above is O\(n2\).  
Example 3.1\. Given the graph G = *\(V,* E\) shown in Figure 3.1, the problem is to find a  
maximum-weight independent set in the cographic matroid \(i.e., a set of edges whose  
removal does not disconnect the graph\). We have ordered the edges so that CI ;;;.  
C2;;;' ... ;;;. Cg. Applying the greedy algorithm, we obtain SG = \(eh e3, es\), provided that  
Cs \> O. .  
The converse of Theorem 3.1 also holds\:  
Theorem 3.2. If \(N, \~\) is an independence system but not a matroid, there exists a  
weightfunction cERn for which the greedy algorithm does not yield an optimal solution to  
\(3.1\).  
Figure 3.1 668 111.3. Matroid and Submodular Function Optimization  
Proof Since \(N, \~\) is not a matroid, there exists an S s\: N such that all maximal  
independent sets in S are not of the same cardinality. Let A s\: S be a maximal indepen-  
dent set in S of minimum cardinality, and suppose IA I = *k.* Let  
k + 2 *forj* EA  
\{  
Cj = k + 1 for *j* E S \\ A  
o otherwise.  
The greedy algorithm yields the setA of value *k\(k* + 2\). But an optimal solution has value  
at least *\(k* + 1\)2\> *k\(k* + 2\) for *k* \~ 1. •  
Several variants of problem \(3.1\) can be solved by simple modifications of the greedy  
algorithm. These include the problems of finding a maximum-weight basis and a maxi-  
mum-weight independent set of cardinality not greater than *k.* Another useful observation  
is that a maximum-weight basis is the complement of a minimum-weight basis in the dual  
matroid.  
The Matroid Polytope  
Here we consider an integer programming formulation of \(3.1\) and its linear programming  
relaxation. Let x *T* be the characteristic vector of *T* s\: *N.* By the definition of an indepen-  
dence system!fi = *\(N,* \~\) with cardinality function m, it follows that *T* E \~ if and only if  
I *TI* \~ *m\(T\)* if and only if LjES *xJ* = IS n *TI* \~ m\(S\)for all S s\: *N.*  
Let  
*P\(m\)* = *\{x* E R\~\: I *Xj* \~ *m\(S\)* for S s\: N\}.  
jES  
Then an integer programming formulation of the problem of finding a maximum-weight  
independent set in *!fi* is  
*max\{cx\: x* E *P\(m\), x* E En\}.  
We now show that if the independence system is a matroid M = \(N, r\), then P\(r\) is the  
convex hull of the characteristic vectors of its independent sets.  
Consider the linear program  
\(3.2\)  
I *Xj* \~ *r\(S\)* for S s\: *N*  
jES  
xER\~  
and its dual  
\(3.3\)  
I Ys \~ Cj for *j* EN  
S\:S3j  
Ys \~ 0 for S s\: N. 3\. Maximum-Weight Independent Sets 669  
Proposition 3.3. Let SG = \{j\[, ... ,jp\} be the greedy solution to \(3.1\) with the ordering  
Cl \~ C2 \~ •.. \~ Cn· Let ,ft = \{jl\> ... *,jl\}* for t \~ p, and let I\<t = sp\(,ft\). Then an optimal  
solution to \(3.3\) is  
*YK,* = *Cj,* - Cjl+l for t = 1, ... ,p - 1,  
YK = 0 otherwise.  
Proof Clearly the dual solution is nonnegative. If Cj \> 0, *thenj* E *KI* \\ *K 1 - 1* for some  
t \~ *p.* Also, *ifj* E *KI* \\ *K I-h* then Cj \~ *Cj,.* Hence *ifj* E *KI* \\ *K 1- 1,* we obtain  
2\: *Ys* = 2\: *YK,* = *Cj,* \~ *Cj.*  
*S\:S3j 1;'1*  
Now note that since *r\(K*t \) = t, we have  
p-l *P*  
2\: r\(S\)ys = 2\: *t\(Cj,* - *Cj,.\)* + *pCjp* = 2\: *Cj,* = 2\: *Cj,*  
*SsN* t\~l t\~l jESG  
so the primal and dual objective functions are equal.  
We have shown that the linear system  
2\: Xj \~ r\(S\) for S \~ N, *x* E R\~  
jES  
# •  
is totally dual integral.  
Theorem 3.4. P\(r\) is an integral polytope.  
Example 3.2. For a graphic matroid, the associated tree polytope P\(r\) is of the form  
2\: Xe \~ *r\(E'\)* for *E'* \~ *E*  
*eEE'*  
XER\~,  
where, as was shown in Section 1, *r\(E'\)* = I *VI* - number of components *ofG*E, = *\(V, E'\).*  
When *U* is the set of nodes attained by *E',* and G' = *\(U, E'\)* is connected, the  
corresponding inequality is dominated by the inequality with *E'* = *E\(U\).* When G' itself  
has several components, the corresponding inequality is dominated by the inequalities  
from the components. Hence we obtain a polyhedral description of a graphic matroid  
given by  
2\: Xe \~ I *U* I - 1 for *U* \~ *V* with I *U* I \~ 2  
*eEE\(U\)*  
XER\~.  
This example raises the question of which inequalities describing the tree polytope are  
facets and the more general question of describing the facets of any matroid polytope P\(r\). 670 111.3. Matroid and Submodular Function Optimization  
Note that if an inequality does not define a facet, we can delete the corresponding dual  
variable from \(3.3\).  
Definition 3.1. A set S \~ N with S = sp\(S\) is separable \(or disconnected\) ifthere exists a  
partition \(A, B\) of S, that is, A\*, 0, B \*' 0, AU B = S, and A n B = 0, with  
r\(A\) + r\(B\) = r\(S\).  
Now it is easy to see that the inequality L*jES Xj* \:0;;; r\(S\) is dominated by LjESp\(s\) *Xj* \:0;;; r\(S\)  
when S \*' sp\(S\) and that it is the sum of two inequalities when S is separable. These turn  
out to be the only redundant inequalities.  
Proposition 3.5. Suppose \{j\} E g; for alii EN. A minimal description of P\(r\) is given by  
P\(r\) = \{x E R\:\: LjES *Xj* \:0;;; r\(S\)for S \~ N with S = sp\(S\) and S nonseparable\}.  
Example 3.3\. Consider the maximum-weight spanning tree problem on the weighted  
graph shown in Figure 3.2.  
The greedy algorithm with \(3, 6\) preceding \(5, 6\) in the ordering gives the solution  
SG = \{\(3, 4\), \(1,5\), \(1,3\), \(3, 6\), \(6, 7\), \(2, 6\)\} of weight 57.  
The optimal dual solution specified by Proposition 3.3 is  
Ys, = 13 - 11 = 2 with Sj = \{3, 4\}  
YS2 = 11 - 10 = 1 with S2 = \{\(3, 4\), \(1, 5\)\}  
Ys, = 10 - 9 = 1 with S3 = *E\(\{1,* 3, 4, 5\}\)  
Ys, = 9 - 8 = 1 with S4 = *E\(\{1,* 3, 4, 5, 6\}\)  
Ys, = 8 - 6 = 2 with Sj = *E\(\{1,* 3,4, 5, 6, 7\}\)  
Ys. = 6 with *S6=E*  
of value \(2 xl\) + \(1 x 2\) + \(1 x 3\) + \(1 x 4\) + \(2 x 5\) + \(6 x 6\) = 57.  
5  
11  
lO  
Figure 3.2 4\. Matroid Intersection 671  
Using Proposition 3.5, we see that the edge sets \{\(3, 4\), \(1, 5\)\}, *E\(\{1,* 3, 4, 5\}\), and *E\(\{l, 3,*  
4,5, 6\}\) do not define facets. Decomposing each of these edge sets, we have  
\{\(3, 4\), \(1, 5\)\} yields \{3, 4\} and \{l, 5\}  
*E\(\{l,* 3,4, 5\}\) yields \{\(3, 4\)\} and *E\(\{1,* 3, 5\}\)  
*E\(\{1,* 3,4, 5, 6\}\) yields \{\(3, 4\)\} and *E\(\{1,* 3, 5, 6\}\),  
and we obtain the alternative dual solution  
ys = 3 + 1 + 1 + 1 for S = *E\(\{3,* 4\}\)  
ys = 1 ys = 1 for S = *E\(\{1,* 5\}\)  
for S = *E\(\{l,* 3, 5\}\)  
ys = 1  
for S = *E\(\{l,* 3, 5, 6\}\)  
ys= 2  
ys = 6  
for S = *E\(\{1,* 3, 4, 5, 6, 7\}\)  
forS = *E*  
in which only the dual variables associated with facets are positive.  
4\. MATROID INTERSECfION  
We have already seen that the branchings on a digraph can be viewed as the edge sets that  
are independent in two matroids simultaneously. Feasible solutions to matching problems  
on a bipartite graph G = *\(VI, V*2, E\) can also be viewed in this way. Let Mi = \(E,87'\) for  
*i* = 1, 2, be partition matroids where *F* !\: *E* is independent in Mi if there is no more than  
one edge of *F* adjacent to each node ofJl;. *F* isa matching in G if and only *ifF* E 87'1 n 87'2.  
In polyhedral terms, we have  
and  
*P\(r2\)* = *\{x* E R\~\: L Xij \~ 1 forj E Vi\},  
*JEV,*  
and *P\(rl\)* n *P\(r2\)* describes the convex hull ofmatchings in a bipartite graph.  
Here we consider the general *maximum-cardinality matroid intersection problem* for  
the matroids Mi = \(N, 87'i\) for *i* = 1, 2, formulated as  
\(4.1\) z = max\{ISI\: S E 87'1 n 87'2\}.  
s  
Throughout the text, we have stressed the importance of duality. Consider the problem  
\(4.2\) w = min\{rl\(T\) + *r2\(N* \\ T\)\}.  
*T* 672 111.3. Matroid and Submodular Function Optimization  
Proposition 4.1. *Problem* \(4.2\) *is a weak dual of problem* \(4.1\).  
*Proof* Suppose S E *flFI* n *flF1•* Then for any set *T* s\:\:\:\: *N,* we obtain  
ISI= ISnTI+ *IS\\TI=rl\(SnT\)+rl\(S\\T\)*  
\~ *rl\(T\)* + *r2\(N* \\ *T\).* •  
Example 4.1\. Consider the two graphic matroids defined on the graphs shown in Figure  
4.1. Taking S = *\{a,* e\} E *flFI* n *flF2'* we see that *z* \~ 2. Taking *T* = *\{e, d\},* we see that  
*w* \~ *rl\(\{e, d\}\)* + *rz\(\{a, b, e\}\)* = 1 + 1 = 2.  
Hence, using weak duality, we obtain *z* = w = 2, and *\{a,* e\} is a maximum-cardinality set  
independent in both matroids.  
The major aim of this section is to develop an algorithm which shows constructively  
that \(4.2\) is, in fact, a *strong dual* of problem \(4.l\). As is the case for the maximum-flow  
problem and the matching problem, the algorithm is based on finding augmenting paths.  
To motivate and explain this idea, consider the following example involving the two  
graphic matroids of Figure 4.2.  
Example 4.2\. mation we have is that  
We are given an independent set S = *\{a, b,* kb k 2\}. The additional infor-  
and  
A question that we need to answer in searching for common independent sets of greater  
cardinality is\: "Is \(S U \{j b h\}\) \\ *\{k* b *k* 2\} E *flF?"* Note that the answer is "yes" in matroid 1  
but "no" in matroid 2, and observe that \(S U \{j I\}\) \\ *\{k1\}* \$. *flF* in matroid 1.  
The following proposition explains why the answer is "yes" in matroid 1, and it is  
fundamental to what follows. If S E *flF* and S U \{j\} \$. *flF,* let *C\(S,j\)* denote the unique  
circuit contained in S U \{j\}.  
a  
Figure 4.1 4\. Matroid Intersection 673  
Proposition 4.2. Let M = \(N, ,cg;\) be a matroid, let S E ,cg;, and let jj, k j, ... ,jp, kp be a  
sequence of distinct elements of *N* with j 10 ••• ,jp E *N* \\ Sand k j, ••• , kp E S satisfying  
\(a\)  
\(b\)  
then  
S U \{j;\} \$.,cg;, \(S U \{j;\}\) \\ \{kJ E ,cg; \(S U \{j;\}\) \\ *\{k\{\}* \$. ,cg; for i = 1, ... , *p*  
for 1 \~ i \< *I* \~ *p;*  
\(S U \{j;, ... ,j,\}\) \\ \{k;, ... , k,\} E ,cg; for 1 \~ i \< *I* \~ *p.*  
Proof Note first that \(a\) is equivalent to k; E C\(S,j;\), and \(b\) is equivalent to  
k, \$. C\(S,j;\). First we will establish that  
The proof is by induction on the number of pairs in the sequencejj, kj, ... ,jp, kpo  
When *p* = 1, condition \(b\) is void and \(4.3\) reduces to C\(S,jj\) = C\(S,jj\). Now suppose  
that the result holds for all sequences involving *p* \~ t - 1 pairs \(j;, *k;\).* It now suffices to  
show that \(4.3\) holds with *i* = 1 and 1= t.  
Therefore we must show that  
C\(S,jj\) = C\(\(S U \{j2, ... ,jp\}\) \\ \{k2, ••• , kp\},jj\)  
*=* C\(I U *\{h\}\)* \\ \{k2\},jj\),  
where 1= S U \{j3, ... ,jp\} \\ \{k3, ••• , kp\}.  
By the induction hypothesis applied to the sequencejj, kj,h, k 3, ••• ,jp, kp , we have  
C\(S,jj\) = C\(I,jj\). Since k2 \$. C\(S,jj\) by \(b\), we obtain k2 \$. C\(I,jj\) and hence  
C\(I,jj\) £; \(I U \{jj\}\) \\ \{k2\}.  
a  
a  
b  
/  
/  
/ \)2  
Matroid 1  
Matroid 2  
Figure 4.2 674 111.3. Matroid and Submodular Function Optimization  
Now applying the induction hypothesis to the sequenceh, k2' ... ,jp, kp, we get that  
C\(S, h\) = C\(l,h\) and hence \(l U \(j2\}\) \\ \{k2 \} E gjP since k2 E C\(S,h\). But \(l U \(jI,jJ\) \\  
*\{k*2\} contains no more than one circuit. Since it contains the circuits C«\(l U \(j2\}\) \\ *\{k*2\}, j I\)  
and C\(l, *j* I\), they must be identical, so  
C\(S,jl\) = C\(l,jl\) = C«I U \(j2\}\) \\ \(k2\},jl\)  
*=* C«S U \(h, ... ,jp\}\) \\ \(k2, ••• , kp\},jl\)  
and \(4.3\) is established.  
Finally we observe that \(4.3\) and k; E C\(S,j;\) imply S U \{j;, ... ,j,\} \\ \{k;, ... ,  
\~E\~ •  
We will be particularly interested in sequences of odd length.  
Corollary 4.3. Let M = \(N, gjP\) be a matroid, let S E gjP, and letjl, kl' ... ,jp\~1o a sequence of distinct elements of N satisfying  
kP-1o jp be  
\(a\)  
\(b\)  
\(c\)  
S U \{j;\} \$. gjP, \(S U \(j;\}\) \\ \{k,\} \$. gjP  
S U \{jp\} E,gjP,  
\(S U \(j;\}\) \\ \{k;\} E gjP for i = 1, ... *,p* - 1  
for 1 .;; i \< *I* .;; *p* - 1  
then S' = \(S U \(j\], ... ,jp\}\) \\ \{k\], ... , kp \_ l \} E gjP.  
Proof Consider 8 = S U \{jp\} E gjP. Now S U \{ji\} \$. gjP implies 8 U \{j;\} \$. gjP. On the  
other hand, since 8 E gjP, it follows that 8 U \{j;\} contains a unique circuit that must be the  
circuit C\(S,j;\) containing k;. Hence \(8 U *\(j;\}\)* \\ \{k;\} E gjP, and \(a\) holds for 8. Clearly  
\(b\) also holds for 8. Therefore, we can apply Proposition 4.2 to 8 and the sequence  
jt, kI, ... ,jp-t, kp \_ I to conclude that S' = \(8 U \(jI, ... ,jp\_I\}\) \\ \{kt, ... ,kp \_ l\} E gjP. •  
Since I S' I = I S I + 1, Corollary 4.3 provides a scheme for finding a larger cardinality  
independent set in a matroid, but it is obviously unnecessary because 8 = S U \{j p\}  
suffices. However, for the problem of increasing the cardinality of a set S that is a common  
independent set in two matroids, Corollary 4.3 gives us a sufficient condition.  
Proposition 4.4. Given two matroids M; = \(N, gjPi\) for i = 1, 2, a set S s; N with  
S E gjP I n gjP 2, and a sequence j 10 k I, .•. ,jp-1o kp -1o jp of distinct elements with j 10 ••• ,  
jp E N\\ S, kl' ... , kp \_  
*1* E S satisfying  
\(al\)  
\(bl\)  
\(cl\)  
\(a2\)  
\(b2\)  
\(c2\)  
S U \{j;\} \$. gjPI, S U \{j;\} \\ \{k;-a E gjPI for i = 2, ... *,p*  
S U \{j;\} \\ \{k,\} \$. gjPI for 1 .;; *I* \< i-I.;; P - 1  
S U \{jl\} E gjPt  
S U \{j;\} \$. gjP2, S U \{j;\} \\ \{k;\} E gjP2 for i = 1, ... ,p - 1  
S U \{j;\} \\ \{k,\} \$. gjP2 for 1 .;; i \< *I* \<p  
S U \{jp\} E gjP2, 4\. Matroid Intersection 675  
*N\\S* S  
Figure 4.3  
Proof First we apply Corollary 4.3 to the sequence\} t, *k* t, ... ,\} p\_l, *kp\_t,\} p,* observing  
that conditions \(a2\), \(b2\), and \(c2\) are precisely the conditions of the corollary. Therefore  
S' E \:§i2.  
To show that S' E\:§it, we consider the reverse sequence\}p, *kp\_t,* ... ,il, kt,\}I' Condi-  
tions \(al\), \(bl\), and \(cl\) are precisely conditions \(a\), \(b\), and \(c\) of Corollary 4.3 with  
respect to this sequence. Hence \(S U \(jp, ... ,\}I\}\) \\ *\{kp\_I* , ••• , k l\} = S' E \:§il. •  
Now we construct a digraph '2lJ*s* = *\(N* U *\{s,* t\}, *d\)* \(see Figure 4.3\) that will allow us to  
find a sequence of the type described in Proposition 4.4. The arcs are defined as follows\:  
*\(s,\}\)* Ed *\(j,* t\) Ed if S U \{j\} E\:§il  
if S U \{j\} E\:§i2  
*\(j, k\)* Ed *\(k,\)\)* Ed if S U \{j\} \$. \:§i2, \(S U \(j\)\) \\ *\{k\}* E \:§i2  
if S U \{j\} \$. \:Jil , \(S U \(j\}\) \\ *\{k\}* E\:§il  
Note that an arc *\(j, k\),\} EN* \\ S, *k* E S, refers to a replacement of\} by *k* to achieve  
\(S U *\{k\}\)* \\ \{j\} E \:§i2 and that an arc *\(k,\}\), k* E *S,\} EN* \\ S refers to a replacement of\} by  
*k* to achieve \(S U \(j\}\) \\ *\{k\}* E \:§il. This can be interpreted graphically.  
Proposition 4.5. If \(S'\}h kh ... , \}p, t\) is an s-t dipath in qj\)s and qj\)s contains no arcs of  
the form *\(i, k/\)* and *\(k;'\}/+d* for *I\>* i, then \(jb *kb* ... , *\}p\)* is a sequence satisfying the  
conditions of Proposition 4.4.  
In this and the next section we will be interested in the existence of certain dipaths in qj\) s.  
Such dipaths may not exist even though an s-t path may exist in the underlying undirected  
graph. We therefore keep the term dipath \(see Section 1.3.1\). An s-t dipath \(s, II, ... , Ip, t\)  
is node minimal if there is no subsequence *\{ljl'* ; .. *,Ij\)* \~ \{II, ... ,Ip\} with I \~\}I \<  
\}2 \< ... \<\}q \~ P such that *\(s, I*jp ••• , *I*j" *t\)* is an s-t dipath.  
An s-t dipath satisfying the condition of Proposition 4.5 is node minimal \(see Figure  
4.4\). Now we observe that a minimum-length dipath from s to *t* \(i.e, a dipath with a  
minimum number of arcs\) is necessarily node minimal. Hence such a node minimal  
dipath can be found by breadth-first search or a standard shortest-path algorithm. 676 111.3. Matroid and Submodular Function Optimization  
Figure 4.4  
Example 4.3\. Two graphic matroids are exhibited in Figure 4.Sa, and the digraph *r!iJ*s for  
S = \{b, d\} is shown in Figure 4.Sb. Note that \(s, a, d, e, t\) is a node minimal s-t dipath in  
*r!iJs* giving the set S' = *\{a, b,* e\} E *fJi l* n *fJi2•* Note also that the s-t dipath \(s, *a, b,* c, d, e, t\)  
is not node minimal because *\(a, d\)* E *s!l* and does not lead to a larger common indepen-  
dent set since S" = *\{a,* c, *e\}* \$. *fJi*2•  
The final step in developing an algorithm for problem \(4.1\) is to show thatifr!iJs contains  
no s-t dipath, then S is a maximum-cardinality set independent in both matroids. Let  
NL = \{i EN\: there exists a dipath from s to *i* in *r!iJ* s\}, SL = NL n S, NR = N \\ NL, and  
SR =NR ns.  
Proposition 4.6. independent set.  
*JJr!iJ*s contains no s-t dipath, then S is a maximum-cardinality common  
Proof Suppose that *r!iJ* s contains no s -t dipath. We show first that NL S spz\(S *L\).* Since  
SL S SPz\(SL\), we consider\) E NL \\ SL. Now S U \{j\} \$. *fJi*2, because otherwise there would  
be an arc \(j, t\) E *s!l* and an *\(s,* t\) dipath would exist. If k E C2\(S,\) \\ *\{j\},* then *r!iJs* contains  
the arc \(j, *k\)* and so *C*2\(S,\) \\ \{j\} s SL. In other words, \) E spZ\(Sd, and hence  
NL S SP2\(SL\).  
Next we show that NR S SPl\(SR\). We consider\) E NR \\ SR. First we observe that  
S U \{j\} \$. *fJi*2, because otherwise there would be an arc *\(s,\),* which is impossible as  
a  
s  
# I-  
*I* \\  
e  
b  
# \\  
# \\  
# I  
# I  
# I  
# I  
M2  
\\ /  
\\. ... /  
*\(a\)*  
*\(b\)*  
Figure 4.5 4\. Matroid Intersection 677  
\} \$. NL. Ifk E C1\(S,\}\) \\ *\{j\},* then.@scontainsthearc\(k,\}\).Since\} \$. N L, we have k \$. SL,  
and hence C1\(S,\}\) \\ \{j\} £\: SR. ThusNR £\: SP1\(SR\).  
Finally, we use weak duality in problems \(4.1\) and \(4.2\)\:  
z \~ I S I, and w \~ rl\(NR\) + rz\(NL\) \(by Proposition 4.1\)  
\~ rl\(sPl\(SR» + r2\(spz\(SL»  
= ISRI + ISLI = lSI·  
# •  
Theorem 4.7. Problem \(4.2\) is a strong dual o/problem \(4.1\).  
Now we can describe the algorithm for problem \(4.1\).  
Maximum-Cardinality Matroid Intersection Algorithm  
Initialization\: Start with Sl E ;\}fl n *;\}f2.* q = 1.  
Iteration *q\:* Construct the digraph *.@sq.* If there is no s-t dipath in *.@sq,* stop; *sq* is an  
optimal solution. Otherwise, find a shortest s-t dipath \(s,\}t, kt, ... *,\}p, t\).* Set  
Sq+l = *\(sq* u \{it, ... *,\}p\}\)* \\ \{kt, ... ,k*p -*1\} and q ... q + 1.  
Example 4.4. The digraph'@s is shown in Figure 4.7.  
Consider the two graphic matroids shown in Figure 4.6. S = \{el, e2, es\}.  
Since.@s contains no s-t dipath, we obtain  
SL = \{e2, es\}, SR = \{el\}  
NL £\: SPz\(SL\) = \{e2, *e4,* es, *e7, eg\}*  
NR s; SPt\(SR\) = \{et, e3, e4, e6\}  
rt\(NR\) + r2\(NL\) = 3 = lSI.  
eB  
Figure 4.6 678 111.3. Matroid and Submodular Function Optimization  
Figure 4.7  
Finally, we consider the complexity of the cardinality matroid intersection algorithm  
Proposition 4.8. than O\(n3\) independence tests.  
The cardinality matroid intersection algorithm terminates after no more  
Proof There are no more than z \< n iterations. At each iteration the digraph ffis has  
to be constructed. Deciding if \(i, *j\)* is an arc of the digraph requires no more than two  
independence tests, so there are O\(n2\) tests at each iteration. •  
5\. WEIGHTED MATROID INTERSECTION  
Given two matroids Mi = \(N, \[lJPJ for *i* = 1,2 and a weight vector cERn, we consider the  
weighted matroid intersection problem  
\(5.1\)  
It is convenient to introduce the following notation\:  
\[lJPq=\{Sr;;.N\:SE\[lJP, lSI \~q\), \[lJP7=\{Sr;;.N\:SE\[lJPi' lSI \~q\}  
\[lJP'2 = \[lJP, n \[lJP2, \[lJPY2 = \{S E \[lJP'2\: IS I \~ *q\}*  
r'2\(S\) = max\{ I T I\: T E \[lJP'2\}'  
*T-;;S*  
The algorithm we describe actually solves the family of problems  
\(5.2\) zq = max\{ I *Cj\:* S E \[lJPY2\} for q = 0, 1, ... , rl2\(N\).  
S jES 5\. Weighted Matroid Intersection 679  
We will solve \(5.2\) for increasing values of q and base our proof of optimality on the  
following dual pair oflinear programs\:  
\(5.3\)  
I xj\~rl\(A\) *forA* s\:\:\:\:N  
*JEA*  
I *Xj* \~ *riA\)* for *A* s\:\:\:\: *N*  
*JEA*  
Xj\~ 0 forj EN  
and  
A,=N  
A,=N  
\(5.4\)  
IYI\(A\)+IYiA\)+t\~cj forjEN  
A3j A3j  
*YI\(A\), yiA\)* \~ 0 for *A* s\:\:\:\: *N,* t \~ O.  
For all values of q we will show that \(5.3\) has an integral optimal solution by giving an  
integral primal feasible solution and a dual feasible solution of the same value. The primal  
solution is constructed using cost splitting \(see Section II.3.6\).  
Proposition 5.1. Given c, cl  
, c2 ERn with cl + c2 = C, ifSq s\:\:\:\: N is an optimal solution to  
the problems  
\(5.5\) max\{ I c\}\: S E \:¥7 \} for *i* = 1, 2,  
S *jES*  
then sq is an optimal solution to \(5.2\).  
Proof For any S E \:¥f2, we obtain  
I *Cj* = I c\) + I *cJ*  
*jES jES jES*  
\~ I c\} + I *cJ* = I *Cj.*  
*jES' jESq jESq* •  
The greedy algorithm for matroids gives a characterization of an optimal-weight  
solution in \:¥q.  
Proposition 5.2. Given a matroid M = \(N, \:¥\) with weight vector c, a set S with I S I = q is  
optimal in \:¥q if and only *if\:*  
i. *Cj* \~ 0 for j E S;  
ii. ifj \$. Sand S U \{j\} E\:¥, then *Ck* \~ cJor k E S; and  
*111.* ifj \$. Sand S U \{j\} \$.\:¥, then *Ck* \~ cJor k E C \(S,j\) \\ \{j\}. 680 111.3. Matroid and Submodular Function Optimization  
Given c t  
, *c*2  
, and *sq* as in Proposition 5.1, it is simple to give an optimal solution to  
\(5.4\). Consider the problem  
\(5.6\) max\{ I \(C\) - *mi\)\:* S E *f!ft?\}* for *i* = 1,2,  
S *jES*  
where *mi* = max\{cj\: *j* \$. *sq, sq* U \{j\} E f!ftJ and *mi* = - 00 if *sq* is a basis of *Mi.* Its dual is  
\(5.7\) min I *ri\(A\) Yi\(A\)*  
*AsN*  
A3j  
I *Yi\(A\)* \~ C\) - *mi* for *j* EN  
*forA* \~N.  
Proposition 5.3. *If\(a\)* ct  
, *c2*  
*, sq satisfy the conditions of Proposition* 5.1 *with* ISql = *q, \(b\)*  
mt + *m2* \~ 0, *and \(c\) yjisan optimal solution to \(5.7\)for i* = 1, 2, *then an optimal solution*  
*to* \(5.4\) *is Yi* = *yifor i* = 1, 2 *and t* = mt + *m2.*  
*Proof* The proposed solution is feasible to \(5.4\). By hypothesis, *sq* is an optimal  
solution to \(5.5\). Hence, by Proposition 5.2, we have cj \~ *mi* forj E *sq.* It follows that *sq* is  
also an optimal solution to \(5.6\). Hence, equating the optimal values of\(5.6\) and \(5.7\), we  
obtain \~jES' \(cJ - *mi\)* = \~AsN *ri\(A\)y7\(A\).* Now the value of the proposed solution to \(5.4\) is  
I *rt\(A\)YT\(A\)* + I *r2\(A\)Yi\(A\)* + *q\(mt* + *m2\)*  
*AsN AsN*  
= I *\(c\)-mt\)+* I *\(c;-m2\)+q\(mt+ m2\)*  
*jES'* jES'  
The characteristic vector of *sq* is feasible to \(5.3\), so the claim follows. •  
Example 5.1. A digraph is shown in Figure 5.1, along with associated arc weights. We  
wish to find a branching of maximum weight. Thus the underlying edge set must be  
independent in the graphic matroid M t and the partition matroid M 2, where the number  
of arcs entering each node is restricted to be no greater than 1.  
Suppose we have the following split of the arc weights c\), cJ\:  
# 234567891011  
Cj 4 3 4 1 7 2 6 -5 -1 1  
c\) 4 3 1 4 1 5 2 4 -5 -1 1  
cl00000202 0 00  
Observing that S2 = *\{et, e6\}* is optimal in *f!ftI* with weight ct and that S2 is optimal in f!ft\~  
with weight *c2*  
*,* we have by Proposition 5.1 that S2 is optimal in *f!ftI2* with weight c.  
Now we consider the question of how to pass from *sq* to *sq+t,* a maximum-weight  
independent set in f!ftHt. We know that if we construct the digraph *qj\)s.* used in the 5\. Weighted Matroid Intersection 681  
\~ \_\_\_\_\_ -=e4  
Figure 5.1  
cardinality algorithm, then s-t dipaths with a minimum number of edges give us sets  
*Sf* E \:Jirrl. However, because we want to obtain a maximum-weight independent set in  
\:Jiyrl  
, we can only use a subset of the arcs of£iJsq. The following proposition suggests, on the  
basis of the weight vector c, those arcs that should be kept.  
Proposition 5.4. Let Mq = *\(N,* \:Jiq\), let S be an optimal-weight solution in Mq, and let  
\}I, kb ... ,\}p, kp be distinct elements ofN with\}b ... ,\}p EN \\ S, kl' ... , kp E S, and  
a. S U \{jJ tf= \:Ji, \(S U \(jJ\) \\ \{kJ E \:Ji for i = 1, ... ,p  
b. Cji = ckifor i = 1, ... ,p  
c. Cj, = Ch and i \< *I* implies \(S U *\(j;\}\)* \\ \{k\[\} tf= \:Ji.  
Then *Sf* = \(S U \(j I, ... ,\}p\}\) \\ \{k I, ... , kp \} is also an optimal-weight solution in Mq.  
Proof We can reorder the elements\}!\> ... ,\}p so that Cjl \~ ..• \~ Cjp and conditions  
a-c still hold. This is possible because conditions a and b are unaffected by the ordering,  
and condition c only affects pairs such as \(ji'\}\[\) with *i* \< *I* if *cj,* = ck But if the ordering of  
such pairs is preserved in the new ordering, condition c holds.  
We now claim that \(S U \{ji\}\) \\ \{k\[\} tf=\:Jiq for all i \< *I.* Ifnot, S\* = \(S U *\(j;\}\)* \\ \{k\[\} E\:Jiq  
for some i \< *I* with cj, \> Ck,. But then LjES' Cj \> LjES Cj, contradicting the optimality of S.  
Now the conditions of Proposition 4.2 are satisfied and *Sf* E \:Jiq . By condition b the  
weights of S and *Sf* are identical, and hence *Sf* is optimal. •  
Now given S optimal in \:JiY2, cl  
, c2 as in Proposition 5.1 and m h m2 defined below \(5.6\),  
we construct a digraph £iJS\(c l  
, *c*2\) = *\(N* U *\(s,* t\), *d\)* with arcs off our types \(note that these  
are a subset of the arcs of£iJs given in the previous section\)\:  
*\(s,\}\)* Ed if\} tf= S, S U \{j\} E\:Jib and m l = c\)  
\(j, t\) Ed if\} tf= S, S U \{j\} E \:Ji2 , and m2 = c\]  
\(j, k\) Ed if\} \$. S, S U \{j\} tf= \:Ji2 , \(S U \{j\}\) \\ \{k\} E \:Ji2 , and c\] = d  
\(k,\)\) Ed if\} tf= S, S U \{j\} tf=\:Jib \(S U \{j\}\) \\ \{k\} E\:Jib and c\) = d. 682 111.3. Matroid and Submodular Function Optimization  
First we consider what happens when \~S\(CI, *c2 \)* contains an *s-t* dipath.  
Proposition 5.5. *Let* S *be optimalin* \~h *and let s, jl\> klo* ... , *kp -Io jp, t be an s-t dipath*  
*in* \~S\(CI, *optimal in* \~fz I.  
*c2\) with a minimum number* of *arcs. Then* \(S U VI\> ... , *jp\}\)* \\ *\{kJ,* ... , *kp-a is*  
Proof We will apply Proposition 5.4 twice, first to *M2* and then to *MI.* Note first that  
by definition *ofm2'* we have that S\* = S U *Vp\}* is optimal in \~\~+I with weight *c2*  
*.*  
First we apply Proposition 5.4 to S\* E \~\~+I with the sequencejlo *klo* ... , *jp-Io kp\_ I* and  
weights *c2*  
*.* From the construction of \~S\(CI, *c*b\), condition b holds for any sequence  
derived from an *s-t* dipath. Also, S\* U V;\} \$. \~2 and *\(S\** U Vi\}\) \\ *\{ki \}* E \~2' so condition a  
holds for any such sequence.  
Now we use the fact that the dipath has a minimum number of edges and hence is node  
minimal. This means that if c\~ = d, for some *i* \< *I,* there is no arc *Ui' k,\)* and hence  
\(S U VI\}\) \\ *\{k,\}* \$. \~2. It follows that *\(S\** U Vi\}\) \\ *\{k,\}* \$. \~2' and hence condition c holds.  
Therefore *Sf* = \(S U Vio ... *,jp\}\)* \\ *\{k lo* ... *,kp\}* is optimal in \~\~+I with weight *c2*  
*.*  
Taking the path in the reverse order, a similar argument shows that *Sf* is optimal in \~rl  
with weight cl  
. Now by Proposition 5.1, *Sf* is optimal in \~ytl. •  
The other possibility is that there is no *s-t* dipath in \~ *s\(cl*  
*, c2\).* In this case we make a  
*dual change* by changing \(cl  
, *c2 \).* We let NL = V EN\: there exists an *s-j* dipath in  
\~S\(CI *,c2 \)\}* and NR = N \\ NL. .  
The dual change is given by  
where &10 &2 are calculated from \<5i , *i* = 1, ... ,4, which are the minimum-cost changes  
needed to add an arc of each of the four possible types to \~S\(CI, *c2 \).* Their values are  
\<51 = min\{ml - *cJ\:j* \$. S, S U V\} E \~Ioj E NR \}  
\<52 = min\{m2 - *c\}\:j* \$. S, S U V\} E \~2,j E N*L \}*  
\<53 = min\{c\~ - *C\}\:j* \$. S, S U V\} \$. \~2' \(S U V\}\) \\ *\{k\}* E \~2,j \<54 = min\{d - *cJ\:j* \$. S, S U V\} \$. \~Io \(S U V\}\) \\ *\{k\}* E \~2,j E NL , *k* E NR \}  
E NR , *k* E NL \}  
with \<5i = 00 if the corresponding set is empty. Then  
& = min\{ml + *m2,* min\(\<51o \<52 , \<53, \<54\)\}  
&1 = min\{&, mil and &2 = & - &1  
First we check that a real change occurs. 5\. Weighted Matroid Intersection 683  
Proposition 5.6. If m I + m2 \> 0, then 8 \> 0 in the dual change.  
Proof By the definition ofm!\> we have that ml \> c\} if S U *U\}* E \:!fl. Hence 151 \~ O. If  
15*1* = 0, then 0Js\(cl  
, *c2\)* contains an arc *\(s, j\),* contradictingj E N*R •* Hence 151 \> O. A similar  
argument holds for 152,  
If S U *U\}* \$. *\:!f*2, \(S U un \\ \{k\} E *\:!f*2, and de \< cj, then S is not optimal in g;\~ with  
weights c2• Hence 153 \~ O. If 153 = 0, then \~S\(CI, c2\) contains an arc \(j, k\)joiningj E *NL* to  
k E NR , which is impossible. Hence 153 \> O. A similar argument holds for 154, •  
Now we show that the conditions of Proposition 5.1 still apply with the new weights  
\(el  
, CZ\).  
Proposition 5.7. After a dual change based on \~S\(CI, weights eifor i = 1,2.  
c2 \), S is still optimal in *\:!fr* with  
Proof We verify that S satisfies the optimality conditions of Proposition 5.2 with the  
new weights \(el  
, *\(* 2\). We consider matroid M\~ = *\(N, \:!fn.*  
i. The condition ej \~ 0 for j E S holds because m2 \~ m2 - 82 \~ 0, and S is optimal  
before the dual change.  
ii. Suppose j \$. s, S U *U\)* E\:!f, *k* E S, and eTc \< ej. Because de \~ cj, this can only  
happen if ej = cj + 81 and *c't* = de - 82, so that j E *NL* and k E *N*R • But because  
k E S, we obtain de \~ m2. Moreover, since j E N*L ,* we have 8 \~ 152 \~ m2 - cJ.  
Hence  
which is a contradiction.  
iii. Suppose j \$. S, S U *U\}* \$. *\:!f*2, *k* E *C*2*\(S,j\)* \\ *U\},* and de \< cj. This implies that  
ej = cj + 81 and eTc = de - 82 withj 8 *NL* and k E *N*R • But 8 \~ 153, and since j E *NL,*  
k E NR , and \(S U U\}\) \\ \{k\} E *\:!f*2, we obtain 153 \< de - c\}. Hence  
which is a contradiction.  
A similar argument shows that S is optimal in \:!fY with weights el  
. •  
It remains to establish that after a finite number of dual changes, either a larger  
common independent set is found or the algorithm terminates.  
Proposition 5.S. optimal for all q' \~ q.  
After no more than n dual changes, either an s-t dipath isfound, or S is  
Proof We consider the different possibilities for a dual change, together with the  
successive digraphs \~S\(CI, c2\) and \~s\(el, \( 2\). We will establish two claims. First we claim  
that every arc in \~ s\( C I, c2\) with both its head and tail in *NL* is also an arc in \~ s\( el  
, *\(* 2\). Then  
we also show that a new arc appears in \~s\(el, \( 2\) whose tail is in *NL* and whose head is in  
*N*R • Together these imply that NL \:J *N*L , and hence no more than *n* dual changes can occur. 684 111.3. Matroid and Submodular Function Optimization  
To establish the first claim, we examine each type of arc in turn. Let  
*T* = *\{j\:j* \(/\:. S, S U \{j\} E 3'1\}. Consider *\(s, j\)* arcs for which c\) = *mi.* Notethatifj E Tand  
c\) \< *mb* thenj E NR because the only arcsofd enteringanodej E Tare those of the form  
*\(s, j\).* Now we know from the definition of JI that c\) .;;; *m* I - Ji .;;; *m* I - 1\>1' After the dual  
change, we obtain  
*ml* = *max\{C\)\:j* E *T\}*  
= max\{max\{c\) - *I\>I\:j* E NL nT\}, max *\{c\)\:j* E NR n T\}\}  
= max\{ml - I\>b *max\{c\)\:j* E NR n T\}\}  
It follows that if *\(s, j\)* is an arc of 9iJs\(cj, d\), then *\(s, j\)* is an arc of *9iJ*s*\(cl,* c\~\).  
Now consider an arc of the form \(j, *k\)* in 9iJs\(cl, d\), where *j* \(/\:. s, S U \{j\} \(/\:.3'2,  
\(S U un \\ *\{k\}* E 3'2, and c\] = d with both *j, k* E *NL •* After the dual change, we obtain  
*c\]* = c\] - I\> and Ck = d - 1\>, so *c\]* = Ck and the arc is in *9iJ*s*\(cl, cD.* An identical argument  
holds for the \(k, *j\)* arcs based on matroid M\\, and hence the first claim is established.  
To establish the second claim, observe that the algorithm terminates if I\> = *m* I + *m2*  
based on *9iJS\(c l*  
*, c2\),* and an s-t dipath is created if I\> = J2• Hence a dual change can only  
occur if I\> = J\\, J3, or J4• In each of these cases a new arc appears in *9iJS\(cl*  
*, c2\)* whose tail is  
in NL and whose head is in NR. •  
The Weighted Matroid Intersection Algorithm  
*Initialization\:* Start with c*l*  
*,* c*2*  
*, sq* as described in Propositon 5.1, if such a solution is  
known for any q \~ 1. Alternatively let q = 0, let *sq* = 0, and choose any *c l*  
*,* c2 satisfying  
cl + c2 = c. \(The simple choice in this case is cl = c, c2 = 0.\)  
*Step* 1\: Calculate *mb m2.* If *ml* + *m2';;;* 0, stop. *sq* is optimal for all q' \~ q. Otherwise if  
*mi* \< ° for some *i* \(say *i* = 1\), then c\) \<- c\) - *m\\,* c\] \<- c\] + *ml* for all\} EN.  
*Step* 2\: Construct *9iJ* sq\( c I, *c2 \).*  
*Step* 3\: If there is no *s-t* dipath in *9iJsq\(c l*  
*, c2 \),* go to Step 5. Otherwise find a shortest *s-t*  
dipath in *9iJsq\(c l*  
*, c2 \)* and go to Step 4.  
*Step* 4 *\(Augmentation\)\:* Use the *s-t* dipath in *9iJsq\(c l*  
*, c2 \)* to find *Sq+1* optimal in 3'\~+I. Set  
q \<- q + 1. Go to Step 1.  
*Step* 5 *\(Dual Change\)\:* Change *\(c l, c2 \)* as described in \(5.6\). If I\> = *ml* + *m2,* stop. *sq* is  
optimal for q' \~ q. Otherwise, calculate W, *c2\), ml* and *m2,* and construct *9iJ*s*q\(cl*  
*, c2\).*  
Go to Step 3.  
Example 5.1. \(continued\). We apply the weighted matroid intersection algorithm  
starting with S2 = *\{e\\, e6\}* and cl  
, c2 given below.  
2 3 4 5 6 7 8 9 10 11  
Cj 4 3 4 1 7 2 6 -5 -1 1  
ci  
\} 4 3 1 4 1 5 2 4 -5 -1 1  
c2  
\} 0 0 0 0 0 2 0 2 0 0 0 5\. Weighted Matroid Intersection  
685  
Figure 5.2.  
q =2\:  
Step 1\: *ml* = d = 4, mz = d = d = d = d = c§ = eTo = eTl = o. *NL* = \{e6, es\}  
Step 2\: £iJs2\(c 1  
, CZ\) is shown in Figure 5.2.  
Step 5\: \<\>1 = *ml* - d = 4 - 3 = 1, \<\>2 = 00, 03 = 00, 04 = d - d = 1. I\> = 01 = 04 = 1. With  
1\>1 = 1, 1\>2 = 0, and cj \<- *c5,* we obtain  
1 2 3 4 5 6 7 8 9 10 11  
*Cj* 4 3 1 4 1 7 2 6 -5 -1 1  
c1  
\} 4 3 1 4 1 4 2 3 -5 -1 1  
c2  
\} 0 0 0 0 0 3 0 3 0 0 0  
*ml* \<- 3, *m2* \<- O. The new £iJs2\(Cl, C Z \) is shown in Figure 5.3. 686 111.3. Matroid and Submodular Function Optimization  
Figure 5.3  
Two s-t dipaths with a minimum number of edges are found. We use the dipath  
*\(s,* eg, *e6, e4, t\).*  
q=3\:  
*Step* 1\: S3 = *\{eh e4,* eg\}. *ml* = d = 3, *m2* = d = d = d = d = do = dl = O. *figs3\(c l*  
*, c2 \)* is  
shown in Figure 5.4. *NL* = *\{eh e2, e6,* eg\}  
Figure 5.4 5\. Weighted Matroid Intersection  
687  
12345678910 11  
Cj 4 3 4 7 2 6 -5 -1  
c\) 3 2 1 4 1 3 2 2 -5 -1 1  
*cJ* 0 0 0 4 0 4 0 0 0  
Adding the arc *\(e8, e7\),* an *s-t* dipath is found.  
Step 5\: 6, = *m,* - elo = 2,62 = 00,63 = 00, 64 = d - d = d - d = 1,6 = 64 = *m,* + *m2* = 1.  
6,=1,62=0.  
1 2 3 4 5 6 7 8 9 10 11  
Cj 4 3 4 7 2 6 -5 -1 1  
c' \} 2 1 1 4 1 2 1 -5 -1 0  
c2  
*\}* 2 2 0 0 0 5 5 0 0  
m, = ell = 0, m2 = d = d = do = o.  
S = *\{e2, e4, e6, e7\}* is a maximum-weight branching *in!!l1.*  
Now we consider the number of independence tests required in the algorithm. Because  
the set sq does not change through a sequence of dual changes, only *O\(n2\)* independence  
tests are required between augmentations to construct the digraphs *!!l1s'* from which a  
subset of the arcs are selected to give *!!l1s'\(c', c2 \).* Since there are, at most, *n* augmentations,  
no more than *O\(n3\)* independence tests are required.  
Figure 5.5 688 111.3. Matroid and Submodular Function Optimization  
We conclude this section with a polyhedral result. As a consequence of the weighted  
matroid intersection algorithm and Proposition 5.3, we have shown\:  
Theorem 5.9. Given matroids MI = *\(N,* rl\) and M2 = *\(N,* r2\), the polytope  
I Xj.s; rl\(A\) for A s *N*  
JEA  
JEA  
I Xj.s; q  
JEN  
I Xj.s; riA\) for A s *N*  
xER\~  
is the convex hull of the common independent sets of cardinality not greater than q. The  
inequality set is totally dual integral.  
6\. POLYMATROIDS, SEPARATION, AND SUBMODULAR FUNCTION  
MINIMIZATION  
Two of the problems we study in this section are \(1\) the problem of minimizing a  
submodular function and \(2\) the separation problem for a matroid polytope *P\(r\).* To  
understand better the relationship between these problems, we introduce a generalization  
of a matroid.  
Definition 6.1. withf\(0\) = 0, the polytope  
Given a finite set N and a nondecreasing submodular functionf on N  
P\(f\) = \{x E R\~\: I Xj .s;f\(S\) for S s N\}  
jES  
is the polymatroid associated with \(N,j\).  
The property of independence systems generalizes to\:  
If x, y E Z\~, x E P\(f\), and y .s; x, then y E P\(f\).  
Definition 6.2\. Let rp\(f\)\: R\~ .... R\~ be defined by  
rp\(f\) \(a\) = max\{ I Xj\: x E P\(f\), x .s; a\},  
JEN  
rp\(f\) is called the polymatroid rankfunction associated with P\(f\).  
The next proposition shows how the "maximal = maximum" property of matroids  
generalizes.  
Proposition 6.1. Given a polymatroid P\(f\) and a E R\~, P\(f\) n \{x E R\~\: x .s; a\} satisfies LjEN Xj = rp\(j\)\(a\).  
any point x that is maximal in 6\. Polymatroids, Separation, and Submodular Function Minimization 689  
Proof Suppose the claim is false, so there exists a E R\~ and u, v maximal in P\(f\)  
n *\{x* E R\~\: *x* \~ *a\}* with LjEN Uj \< LjEN Vj' Let *V* = *U* EN\: Vj \> Uj\}. Since U is maximal, for  
each k E V there exists Uk with k E Uk such that LjEUk Uj = f\( Uk\).  
Let *U* be a maximal subset of *N* such that LjEU Uj = f\( *U\).* By submodularity, we have for  
each k E V\:  
\~ f\(U\) + f\(Uk\) - f\(U n Uk\)  
\~ f\(U U Uk\).  
But since U E P\(f\), we obtain LjEUUUk Uj \~f\( U U Uk\). Hence LjEUUUk Uj = f\( U U Uk\).  
Since U is maximal, we have Uk £ U for all k E V. Hence V £ U.  
But now  
since LjEN Uj \< LjEN Vj and Uj \~ Vj for *j* EN \\ *V,* and *N* \\ *V\:\:\:2 N* \\ *U.* This contradicts the  
assumption v E P\(f\). •  
Example 6.1\. Suppose a set N of jobs is to be processed on one machine and all  
processing of job *j* must be terminated by the deadline *d*j • If Xj is the machine time  
allocated to job *j,* then x E R\~ is a feasible set of allocation times if and only if  
LjES Xj \~ maXjES \{d\) for all S £ N. It is easily checked that f\(S\) = maXjES \{d\) defines a  
polymatroid function if d E R\~ andf\(0\) = O.  
There is a converse to Proposition 6.1 giving an alternative definition of a polymatroid.  
Proposition 6.2. Suppose P £ R\~ satisfies  
i. ifx E P, y E R\~ and y \~ x, then yEP, and  
ii. for,all a ERn *if* Xl and x2 are maxima! points in P n \{x E R\:\: x ,,;; a\), then \~ 'EN Xl  
\_'" 2 \} \}  
-  
.... jENXj.  
Then P is a po!ymatroid.  
As is the case for matroid polytopes, the greedy algorithm solves the linear optimization  
problem over a polymatroid. Consider the dual pair oflinear programs  
\(6.1\) max\{ I CjXj\: I Xj \~ f\(S\) for S £ N, x E R\~\}  
JEN jES  
and  
\(6.2\) min\{ I f\(S\)ys\: I Ys \~ Cj for *j* EN, Y E R;"\}.  
S",N S3j  
SupposeCl\~C2\~'" \~Ck\>O\~Ck+l\~'" \~cn,andSj=\{1, ... *,j\}forjENwith*  
SO=0. 690 Proposition 6.3. 111.3. Matroid and Submodular Function Optimization  
An optimal solution to \(6.1\) is  
X. = \{!\(Sj\) - !\(Sj-I\) for 1 \~j \~ k  
*\}* 0 for *j* \> *k.*  
An optimal solution to \(6.2\) is  
for 1 \~j \< k  
for\} = k  
otherwise.  
Proof because we have for all *T* \~ *N*  
The proposed x is primal feasible, becausef nondecreasing implies Xj \~ 0, and  
*L* Xj = *L* \[f\(Sj\) - f\(Sj-,\)\]  
JET \{j\:jET. j"k\}  
\~ *L* \{j\:jET,j"k\}  
*=* f\(Sk n T\) - f\(0\) \~f\(T\) \[f\(Sj n T\) - f\(Sj-, n T\)\] \(by the submodularity off\)  
- f\(0\) = f\(T\).  
The proposed y is dual feasible because Ys \~ 0 and because LS3j Ys =  
YSi + ... + YS' = Cj if\} \~ *k,* and LS3j Ys \~ 0 \~ Cj if\} \> *k.*  
The primal objective value is Lj\~1 Cj \(f\(Sj\) - f\(Sj-,», and the dual objective value is  
k-I k  
*L* \(Cj - cj+,\)f\(Si\) + ckf\(Sk\) = *L* Cj \(f\(Sj\) - f\(Sj-,». • j\~1 j\~1  
Note that in the special case wheref = r is the rank function ofa matroid, Proposition  
6.3 gives a different solution than the one given in Proposition 3.3. Also, as for matroids,  
we obtain\:  
Corollary 6.4. The inequality system  
\{ X E R\:\: L Xj \~f\(s\) jES  
for S \~ N\}  
is totally dual integral.  
Example 6.1 \(continued\). Suppose four jobs are to be completed with respective  
deadlines given by *d* = \(10 7 8 4\), and the profit from each job is proportional to the  
time devoted to processing it, with weights C = \(2 3 5 6\). Since C4 \> C3 \> C2 \> CI \> 0,  
the greedy algorithm yields  
X4 = max\{d4\) = 4  
X3 = max\{d3 , d4 \) - max\{d4 \) X2 = max\{dz, d3 , d4 \) - max\{d3 , d4 \) = 4  
= 0  
XI = max\{d" d2, d3 , d4\) - max\{d2 , d3 , d4 \) = 2  
with objective value cx = 48. 6\. Polymatroids, Separation, and Submodular Function Minimization 691  
The integrality of the matroid intersection polyhedron also carries over to poly-  
matroids.  
Theorem 6.5. *If* II and h are two polymatroid functions on N, the linear system  
\{LjEA Xj \~ j;\(A\) for A s Nand i = 1, 2, x E R\~\} is totally dual integral.  
Proof We consider the dual problem  
min I fl\(A\)Yl\(A\) + I f2\(A\)Y2\(A\)  
A\~N A\~N  
\(6.3\) I Yl\(A\) + I ylCA\) ;;. Cj for\} EN,  
A3j A3j  
Let yr, yi be an optimal solution, and let c\) = LA3j y7\(A\) for i = 1, 2 and\} EN. Thus there  
is an optimal solution to \(6.3\) with LA3j Yi\(A\);;. cj for i = 1,2 and\} EN. Hence we can  
decompose \(6.3\) into the problems  
min I \};\(A\)Yi\(A\)  
A\~N  
I Yi\(A\);;. c\) for\} EN  
A3j  
Yi\(A\);;. 0 for A s N  
for *i* = 1, 2. These are duals of polymatroid optimization problems. Hence, by Proposition  
6.2, there exist optimal solutions Yi of the form  
*\{S\: ylS\)* \> O\} = *\{Sf,* ... , *Sl;\},*  
with *Sf* s ... s *sl;* s N, and \(Yb Y2\) is an alternate optimal solution to \(6.3\).  
Now setting Yi\(A\) = 0 for A \* S\\ for some t, 1 \~ t \~ Ii, i = 1,2, \(6.3\) reduces to a  
problem of the form  
\(6.4\) min\{jy\: By ;;. c, Y ;;. O\},  
where the columns of *B* are the characteristic vectors of *\{Sf,* ... ,SD for *i* = 1,2. By  
arranging these columns in the order Sl, Sf, ... , S\~l, S\~2, S\~2-1, ... , S\~, st we see that if  
\} E *S7;* \\ S7;-1 for *i* = 1,2, then\} E stl  
, Stl+1  
, ... , Sjl, S\~" ... , S\~2 but\} is in no other sets.  
So row\} of *B* has the consecutive l's property. Hence *B* is an interval matrix and is totally  
unimodular \(see Definition 2.2 and Corollary 2.10 of Section m.1.2\). So whenever c is  
integer, \(6.3\) has an optimal solution in integers, and hence the given inequality descrip-  
tion of P\(fl\) n P\(f2\) is totally dual integral. • 692 111.3. Matroid and Submodular Function Optimization  
Theorem 6.5 allows us to establish some important properties of polymatroids very  
easily. Generalizing the duality result for maximum-cardinality matroid intersection  
yields\:  
Corollary 6.6 *II P\(h\) and P\(h\) are polymatroids, then*  
*Proof* Take *Cj* = 1 for all\} EN in \(6.3\), and let *\(YI\> Y2\)* be an optimal solution. By  
Theorem 6.5, we can assume the solutions are of the form Yl\(SD = 1 for 1= 1, ... ,  
II\> *Yl\(A\)* = 0 otherwise, Y2\(T\~\) = 1 for *I* = 1, ... ,/*2 , Y2\(A\)* = 0 otherwise. Let S = Ui\~l S\~  
and *T* = U\)\~l T\~. Since *11* and *12* are nondecreasing and submodular, we obtain  
*Il\(S\)* \~ Li\~Jl\(SD and *12\(T\)* \~ Li\~J2\(TD. Hence an alternate optimal solution is  
*Yl\(S\)* = *Y2\(T\)* = 1, *Yl\(A\), YzCA\)* = 0 otherwise. Feasibility implies S U *T* = *N.* Finally,  
since,/; and/2 are nondecreasing, we can take *N* \\ S = *T. •*  
Corollary 6.7. *II P\(f\) is a polymatriod with ranklunction rp, then*  
*rp\(a\)* = min\{/\(T\) + I *aj\}.*  
T,;;,N jEMT  
*Proof* By definition, we have  
*rp\(a\)* = max\{ I *Xj\: x* E *P\(f\)* n *\{x* E R\~\: *x* \~ *a\}\}.*  
JEN  
But *\{x* E R\~\: *x* \~ *a\}* is a polymatroid with underlying submodular function/zCS\) = *LjES aj*  
for all S \~ *N.* Hence the result follows from Corollary 6.6. •  
Example 6.1 \(continued\). Suppose that a = \(1 4 6 2\) gives upper bounds on the  
processing times of the four jobs. Writing out a polyhedral description of  
*P\(f\)* n *\{x\: x* \~ *a\}* and removing the inequalities that are redundant, we obtain  
*X2* + *X3* + *X4* \~ 8  
*Xl* + *X2* + *X3* + *X4* \~ 10  
*Xl* \~  
*X2* \~ 4  
*X3* \~ 6  
*X4* \~ 2  
*xER!.* 6. Polymatroids, Separation, and Submodular Function Minimization 693  
Since x = \(1 Corollary 6.7, we have  
3 3 2\) satisfies all the equalities, we obtain 9 = 1\:.t,1 *Xj* .\:;; rp\(a\). But by  
rp\(a\) = min\{J\(T\) + *L* aj \} = min\{max d*j* + *L* a*j \},*  
*T,;,N jEN\\T T,;,N JET jEN\\T*  
and taking T = \{2, 3, 4\}, we obtain rp\(a\) .\:;; 9. Hence rp\(a\) = 9.  
Now we are ready to tackle the problems mentioned at the beginning of this section-in  
particular, submodular Junction minimization\:  
\(6.5\) min\{j\(S\)\}, withJ submodular,  
*S,;,N*  
polymatroid separation\:  
\(6.6\) Given x" E R\~, is x" E P\(J\)? If not, find S s;;; N so  
that the violation of L *Xj* .\:;;J\(S\) is maximized,  
*jES*  
and polymatroid rank Junction calculation\:  
\(6.7\) Given P\(J\) and a E R\~, calculate rp\(f\)\(a\).  
First we consider problem \(6.5\), whereJis an arbitrary submodular function. By adding  
a constant, we can assume without loss of generality thatJ\(0\) = 0\. Furthermore, since  
J\(S U \(j\}\) - J\(S\) is nonincreasing in S, it follows that ifJ\(N\) - J\(N \\ \(j\}\) \> 0, thenj is not  
contained in any optimal solution of \(6.5\), so the problem reduces to  
min\{j\(S\)\: S s;;; N \\ \{j\}\}. Now define kERn by k*j* = J\(N \\ \(j\}\) - J\(N\) for *j* EN, and  
define a modified functionf" by  
f"\(S\) = J\(S\) + *L* kj for S s;;; N.  
*jES*  
Based on the above remark, we assume without loss of generality that k E R\~.  
It is easily verified that\:  
Proposition 6.8. IJJis submodular, thenJ" is nondecreasing and submodular.  
Theorem 6.9. TheJollowing statements are equivalent\:  
1. S" is an optimal solution oJ\(6.5\).  
2. For the separation problem \(6.6\) with respect to P\(f"\) and the point k E R\~,  
*1\:.jE*S" *Xj* .\:;;f"\(S"\) is a most violated inequality. 694 111.3. Matroid and Submodular Function Optimization  
Proof f\(S\*\) \~f\(T\) for T \~ N if and only if  
I\*\(S\*\) - L *kj* \~I\*\(T\) - L *kj*  
*jES" JET*  
if and only if  
I\*\(S\*\) + *L kj* \~I\*\(T\) + *L kj •*  
*jEN\\S" jEN\\T*  
But the first inequality is equivalent to statement 1, the second inequality is equivalent to  
statement 2, and the third inequality is equivalent to statement 3. •  
Corollary 6.10. Thefollowing statements are equivalent\:  
1. mins\~N f\(S\) = o.  
2. k EP\(f\*\).  
*3. rp\(f.\)\(k\)* = *'LjEN kj •*  
Hence we have shown that problems \(6.5\), \(6.6\), and \(6.7\) are equivalent. In the next  
section we will consider algorithms for these problems.  
Example 6.1 \(continued\). We consider the separation problem withf\(S\) = *maxjES* \{d*j \}*  
and d = \(10 7 8 4\). Isx· = \(1 31 3 2\) E P\(f\)?Ifnot, find a most violated inequal-  
ity.  
Using Theorem 6.9, we have a choice of solving the maximum violation problem  
maxS\~N *\('LjES* xj - f\(S\)\}, or, equivalently, of solving the problem of minimizing a sub-  
modular function, namely, mins\~N \(j\(S\) - *'LjES xj\},* or of calculating  
rp\(f\)\(x\*\) = min \{f\(S\) + *L* x j\}.  
S\~N *jEN\\S*  
Now it is easy to check that  
4  
rp\(f\)\(x\*\) = 8! = \~ax *\{dj \}* + xT \< I xj= 9.  
1-2,3,4 j-l  
Hence x\* \$. P\(f\), and X2 + X3 + X4 \~ 8 is a most violated inequality.  
7. ALGORITHMS TO MINIMIZE A SUBMODULAR FUNCTION  
Here we discuss polynomial-time algorithms for the problem \(6.5\) of minimizing a  
submodular function, and we also discuss the related problem \(6.6\) of separation for  
polymatroids.  
First we consider an important class of sub modular functions that includes many of the  
submodular functions encountered in practical models. 7\. Algorithms to Minimize a Submodular Function Proposition 7.1. If CT, rT \~ 0 for *T* \~ *N,* then  
\(7.1\)  
695  
is a submodular function.  
Proof We have  
f\(S U \{j\}\) -f\(S\) = - I CTU\(j\} + I rT·  
TsS \(T\: TnS=0,jE n  
If S' \~ Sand\) \$ *S',* then *\{T\: T* \~ S\} \~ *\{T\: T* \~ *S'\}* and *\{T\: Tn* S' = 0,\) E T\} \~  
*\{T\: Tn* S = 0,\) E T\}. Hencef\(S U \{j\}\) - f\(S\) is nonincreasing in S. •  
Functions of the form \(7.1\) can be used to represent Boolean functions. In particular,  
consider a quadratic Boolean function  
*g\(x\)* = *dx* - XTQX, *x* E Bn with *d* \~ 0, Q \~ 0, and symmetric,  
and *qii* = 0 for alIi.  
Let X S be the characteristic vector of S. Then  
where  
CT= *\{ qij* + *qji* for *T* = *\{i,\)\}*  
o otherwise,  
r = \{dj for *T* = \{j\}  
T 0 otherwise.  
On a graph G = *\(V, E\)* with weights w E *RI!I* on the edges, the function  
f\(S\) = I S I - I\:eEE\(S\) We for S \~ V can be modeled this way with d*j* = 1 for all\) E *V,*  
*qij* = *qji* = !we for e = *\(i,\),* and *qij* = 0 otherwise. Note that f\(S\) - 1 for S \* 0 is the  
function needed to solve the separation problem for the tree polytope.  
We now show that whenf\(S\) is of the form \(7.1\), problem \(6.5\) can be solved as a  
maximum-flow problem. Consider the digraph \~ = *\(VI* U *V*2 U *\(s,* t\), d\), where  
*VI* = \{S \~ *N\:* Cs \> O\}, *V*2 = *\{T* \~ *N\:* rT\> O\}, and  
.91 = *\{\(S,* T\)\: S E *VI. T* E *V*2, S n *T* \* 0\} U *\{\(s, S\)\:* S E *VI\}* U *\{\(T, t\)\: T* E *Vi\}*  
with capacities de for e Ed, where  
e = \(S, T\) has capacity de = 00  
*e* = *\(s,* S\) has capacity de = Cs  
e = *\(T, t\)* has capacity de = rT  
\(see Figure 7.1\). 696 111.3. Matroid and Submodular Function Optimization  
Figure 7.1  
Now we consider s-t cuts. Let *Wj* £\:\: *Vb W*2 £\:\: *V*2, and *\(fV;, W*2\) be the cut  
The capacity of *\(WI\> Wz\)* is  
*d\( W;, Wz\)* = I *Cs* + I *rT*  
*SEVj\\Wj TEW*2  
if all pairs of sets *\(S, T\)* with *SEW;, T* E *Vi* \\ *Wz* are disjoint; otherwise it is  
*d\(W;, Wz\)* = 00 \(see Figure 7.2\).  
Now for any cut *\(W;, Wz\)* we have\:  
i. Let R = USE *Wj* S. d\( *W;, Wz\)* is finite if and only iff or all T E V*2* with T n R oF 0 we  
have TE *Wz.*  
ii. If S E *Vj \\Wj* and S £\:\: *R,* we can reduce *d\(WI\> Wz\)* by *Cs* by including S in *W;.*  
iii. If *T* E *W2* and *T* £\:\: *N* \\ *R,* we can reduce *d\( Wb W2\)* by *rT* by removing *T* from *Wz.*  
Figure 7.2 7\. Algorithms to Minimize a Submodular Function 697  
Thus we have established\:  
Proposition 7.2. Every minimal capacity s-t cut \(VVj, Jf2\) in *f!lJ* can be characterized by a  
set R s.; N where VVj = \{S E Yt\: S s.; R\} and Jf2 = \{T E \~\: Tn R '" 0\}. The cut has  
capacity  
d\(W\), *Wi\)* = d\(R\) = *L* Cs + *L* rT·  
Sn\(N\\R\)\*0 TnR\*0  
Hence the problem minRsN d\(R\) can be solved by finding a maximum flow in *f!lJ.*  
Since  
d\(R\) = *L* Cs - *L* Cs + *L* rT  
SsN SsR TnR",0  
=f\(R\)+ *L* Cs,  
Sc\:,N  
Proposition 7.2 is applicable to the minimization of submodular fractions of the form  
\(7.1\).  
Theorem 7.3. Iffis a submodular function oftheform \(7.1\), minsc\:,Nf\(S\) can be solved by  
finding a maximum s-tflow in a digraph *f!lJ* with n' + 2 nodes, where  
n'= I\{Ss.;N\:cs\>O\}1 + I\{Ts.;N\:rT\>O\}I.  
Corollary 7.4. lem in a digraph with O\(n2\) nodes.  
minxEB" \(cx - xTQx\) with Q\> 0 can be solved as a maximum-flow prob-  
Example 7.1. We solve the quadratic Boolean problem  
Alternatively, we can solve minsc\:,N f\(S\), where f\(S\) is of the form \(7.1\) with r\{l\) = 9,  
r\(2\) = 4, r\(3\) = 2, r\(4\) = 6, rT = 0 otherwise, and CO,2\) = CO,3\) = C\{2,4\} = 4, C\{2,3\} = 7, C\{I,4\} = 2,  
Cs = 0 otherwise.  
We construct the digraph *f!lJ* shown in Figure 7.3 and solve the maximum-flow problem  
giving the s -t cut indicated with R = \{2, 3\} and d\(R\) = 20\. By Corollary 7.4, it follows that  
Figure 7.3 698 111.3. Matroid and Submodular Function Optimization  
*x* R = \(0 1 1 0\) solves the problem with value g\(XR\) = *d\(R\)* - *Lss;N* Cs = 20 - 21 =-1.  
When f is a general submodular function, the ellipsoid algorithm provides a very  
different approach to the minimization of a submodular function.  
Theorem 7.5. There exists an ellipsoid algorithm for the problem \(6.5\) of minimizing a  
submodular function, requiring a polynomial number of evaluations of the function f  
Proof  
i. By Theorem 6.9, it suffices to give a polynomial-time algorithm for the separation  
problem for polymatroid polytopes.  
ii. By the polynomial equivalence oflinear programming optimization and separation  
\(Theorem 3.3 of Section 1.6.3\), it suffices to give a polynomial-time algorithm for  
the linear programming problem over polymatroid polytopes.  
iii. Proposition 6.2 gives a polynomial-time \(greedy\) algorithm to solve the linear  
programming problem over the family of poly matroid polytopes. •  
Theorem 7.5 motivated the search for a purely combinatorial algorithm for problem  
\(6.5\). An augmenting-path algorithm has been developed for the polymatroid separation  
problem, but the bound on the number of function evaluations is polynomial in nand  
f\(N\). This gives a purely combinatorial separation algorithm with a polynomial number  
offunction evaluations for any matroid polytope P\(r\) because r\(N\) \~ n.  
The final topic of this section is the minimization of a submodular function subject to  
some simple constraints. First suppose that S = 0 is not feasible. This yields the problem  
\(7.2\) min *U\(S\)\},* wherefis submodular.  
*0CSs;N*  
Proposition 7.6. times.  
Proof the problem  
Problem \(7.2\) can be solved by solving problem \(6.5\) no more than INI  
Since S '\* 0, it follows that\) E S for some\) EN. Therefore it suffices to solve  
min \(fJ\(T\)\} for\) EN,  
*Tr;;.Nj*  
where N*j* = N \\\{j\}, andfj\(T\) = f\(T U \(j\}\). Becal\)sefj is submodular, the claim follows .•  
Proposition 7.6 is applicable to the tree polytope on the graph G = *\(V, E\),* namely,  
\{ X E R\~\: I Xe \~ I S I - 1 for S s *V,* I S I ;?; 2\}.  
*eEE\(S\)*  
Corollary 7.7. The separation problem for the tree polytope can be solved by solving no  
more than I VI maximum-flow problems on a digraph with I VI + I E I + 2 nodes. 7\. Algorithms to Minimize a Submodular Function 699  
Proof Let /\(S\) = I S I - l\:eEE\(S\) x;. Then x\* E Rl!llies in the tree polytope if and only  
*ifmin0cs£v /\(S\);;\:.* 1. Since/\(S\) is ofthe form \(7.1\), the claim follows from Theorem 7.3  
and Proposition 7.6. •  
Note that the algorithm given in Section II.6.3 for finding violated subtree elimination  
constraints is a special case of the separation problem for the tree polytope.  
Now consider the problem  
\(7.3\) min *\(f\(S\)\:* IS n TI odd\},  
*0CSCN*  
where/is submodular, and *T* £ *N* with I *T* I even. Problem \(7.3\) is important because odd  
sets arise in the constraints and therefore occur in the separation problems for some  
combinatorial optimization problems. These include the minimum-weight *T-join* prob-  
lem and the matching problem. In matching we take *T* = *N,* and we can always assume  
that *N* is even by adding a dummy node to the graph.  
Let n = INI. We will show how \(7.3\) can be reduced to solving \(6.5\) n3 times. First  
consider the relaxation of \(7 .3\)\:  
\(7.4\) min *\(f\(S\)\:* 1 \~ IS n TI \~ I TI - 1\}.  
*0CSCN*  
Note that when I *T* I = 2, the problems \(7.3\) and \(7.4\) are equivalent.  
Proposition 7.8. Problem \(7.4\) can be reduced to solving \(6.5\) n2 times.  
Proof For JET, let jj\(S\) = /\(S U *j\).* An optimal solution to \(7.4\) is obtained by  
solving  
min fi·\(S\)  
*0£S£N\\\(j, k\)* \]  
for eachj k E T with\} \* k and then taking the best of these solutions. •  
Next we show how to reduce \(7.3\) to solving \(7.4\) n times. Let se be any optimal  
solution to \(7.4\). If I se n *T* I is odd, then se is an optimal solution to \(7.3\). The next result  
imposes restrictions on an optimal solution to \(7.3\) when I se n *T* I is even.  
Proposition 7.9. 1/ se is an optimal solution to \(7.4\) and I se n TI is even, then there  
exists an optimal solution SO to \(7.3\) satisfying one o/the/our following conditions\:  
1. So n T C T \\ se,  
2. So n *T* -\:J *T* \\ se,  
3. SO n *T* -\:J se n *T,*  
4. SO n *T* C se n T.  
Proof If \(SO n se\) n T = 0, then condition 1 holds. Also, if \(SO use\) n T = T, then  
condition 2 holds. 700 111.3. Matroid and Submodular Function Optimization  
Now suppose that \(SO n se\) n T \* 0 and \(SO Use\) n T \* T. Since  
either I \(SO use\) n T I or I \(SO n se\) n T I is odd, but not both. Suppose I \(SO use\) n T I  
is odd.  
By submodularity, we have  
But since se is optimal in \(7.4\) and So n se is feasible in \(7.4\), we have f\(se\) \:5;J\(SO n se\).  
Hence f\(SO\);;;. f\(SO use\). But because So is optimal in \(7.3\) and So U se is feasible in  
\(7.3\), we obtainf\(SO\) \~f\(SO use\). Hencef\(SO\) = f\(SO use\), and SO U se is an alterna-  
tive optimal solution to \(7.3\). Thus there is an optimal solution to \(7.3\) that strictly  
contains se, and condition 3 holds.  
Finally, when I \(SO n se\) n *T* I is odd, a similar argument yields condition 4\. •  
As a consequence of Proposition 7.9, we have reduced \(7.3\) to four subproblems. In  
each of these problems, *T* has been replaced by a smaller even set, either *T'* = *T* \\ se or  
*T"* = *Tn* se.  
Next we show how to recombine the four subproblems into two problems of the form  
\(7.3\), one with T +- *T'* and the other with T +- T".  
If condition 4 holds, the subproblem is  
min \(f\(S\)\: IS n T" I odd\};  
0CSCN\\T'  
and if condition 2 holds, the subproblem is  
min U\(S u *T'\)\:* IS n T" I odd\}.  
0CSCN\\T'  
Now let *n I* represent *T',* let *N'* = *\(N* \\ *T'\)* u *\{n '\},* and for 0 s; S s; *N'* let  
*I* \{f\(S\) if n ' fE S  
f \(S\) = f\(S u *T'\) ifn'* E S.  
It is easily verified that!' is submodular, and  
min \{min\(f\(S\),f\(S U *T'\)\)\:* IS n *T"* I odd\}  
0CSCN\\T'  
is equal to  
\(7.5\) \(7.6\) min \(f/\(S\)\: IS n T" I odd\}.  
0CSCN'  
Similarly, if either conditions 1 or 3 of Proposition 7.9 holds, we obtain the subproblem  
min \(f"\(S\)\: IS n T'I odd\},  
0CSCN" 7. Algorithms to Minimize a Submodular Function 701  
where *n"* represents *T"; N"* = *\(N* \\ *T"\)* U *\{n* "\}; and for 0 \~ S \~ *N",* we have thatf" *\(S\)*  
is the submodular function given by  
f "\(S\) \{f\(S\) ifn"\$S  
= f\(S U T"\) ifn" E S.  
Hence we have reduced \(7.3\) to the smaller problems \(7.5\) and \(7.6\) of the same form  
where\: I *N'* I, I *N"* I \~ n - 2; I T' I + I *T"* I = I *T* I; and I *T'* I, I *T"* I \~ 2 and even. Now  
we proceed recursively by relaxing \(7.5\) and \(7.6\). In each case, either an optimal solution  
is found or the subproblem is decomposed again. Since \(7.3\) and \(7.4\) are equivalent when  
I T I = 2, in the worst case the original problem will finally decompose into! I T I problems  
of the form \(7.4\); and in each of these problems, a feasible solution must contain exactly  
one element from a subset of size 2.  
Theorem 7.10. Problem \(7.3\) can be reduced to solving problem \(7.4\) n3 times.  
Proof Then g\(2\) = 1 and  
Let g\(2k\) be the maximum number of calls of problem \(7.4\) when I T I = 2k.  
g\(2k\) = max \{g\(21\) + g\(2\(k -l\)\}.  
*kkk-I*  
It can be shown that the unique solution isg\(2k\) = 2k - 1. Since 2k \~ n, the result follows  
from Proposition 7.8. •  
Whenf\(S\) represents a quadratic Boolean function, the functionsf' andf" in \(7.5\) and  
\(7.6\) also are of this form. In particular, if  
n \)-1 n  
*g\(x\)* = 2\: *r\)x\)* - 2\: 2\: *cijx,xj, x* E *B* n  
\)\~I '\~I j\~2  
and *T'* = *\{k* + 1, ... , *n\},* then setting *X* n' = *Xk+1* = ... = *X* n , we obtain  
Example 7.1 \(continued\)  
subject to the constraint LJ\~I *Xj* odd.  
Let *T* = *N* = \{l, 2, 3, 4\}. The first step is to solve the relaxation \(7.4\). As shown  
previously, the optimal solution is given by se = \{2, 3\}. Hence T' = \{l, 4\} and *T"* = \{2, 3\}. 702 111.3. Matroid and Submodular Function Optimization  
Now the problem is reduced to solving \(7.5\) and \(7.6\) where \(7.5\) is  
and \(7.6\) is  
The former has optimal solution X3 = 1, X2 = Xs = 0 with f\(3\) = 2, and the latter has  
optimal solution Xl = X6 = 1, X4 = 0 with f\(123\) = O. Hence S = \{l, 2, 3\} is an optimal  
solution to the original problem.  
Both the separation problems for minimum-weight 0-1 b-matchings and minimum-  
weight T-joins correspond to the minimization of a quadratic Boolean function subject to  
an odd set constraint where the corresponding set function is submodular.  
Proposition 7.11. The separation problems for minimum-weight 0-1 b-matchings and  
minimum-weight T-joins on a graph G\(V, *E\)* can be reduced to solving no more than 1 *V* 13  
max-flow problems.  
8. COVERING WITH INDEPENDENT SETS AND MATROID PARTITION  
Here we consider the problem of finding the minimum number of independent sets  
needed to cover each element of a matroid a given number of times. Let *A* be the m x n  
matrix whose rows are the characteristic vectors of the independent sets of the matroid,  
and let w E R\: specify the number of times each element must be covered. Then the  
fractional version ofthis problem can be formulated as  
\(8.1\) Cw = \{min ly\: yA ;;;. w, y E *R';'\},*  
and the integer version can be formulated as  
\(8.2\) Zw = \{min ly\: yA ;;;. w, y E *Z';'\}.*  
First we consider the fractional covering problem \(8.1\).  
Proposition 8.1. fractional covering problem *\(8.1\)\:*  
Given a matroid polytope P\(r\), the following statements are true for the  
1. Cw \~ 1 if and only if wE P\(r\),  
2. Cw = *maXS\<;N \{LjES wj/r\(S\)\},*  
3. w/Cw E P\(r\), and ify\* is an optimal solution to \(8.1\), then y\*/Cw expresses w/Cw as a  
convex combination of points in P\(r\).  
Proof  
1. Since the rows of *A* correspond to the independent sets in the matroid *\(N, r\),* we  
know that the convex hull of these rows is P\(r\). So by Proposition 5.8 of Sec-  
tion 1.4.5, the antiblocker of P\(r\) is nA = \{n E R';'\: An \~ n. Therefore, for w E R\:, 8\. Covering with Independent Sets and Matroid Partition 703  
we have that wE Per\) if and only if wn.s; 1 for all n E ITA or max\{wn\: n E ITA\} .s; l.  
However, by linear programming duality, Cw = max\{wn\: An.s; 1, n \~ O\}, and there-  
fore the claim follows.  
2. Since Per\) = \{x E R\~\: \(1/r\(S» LjES Xj .s; 1 for S c\:\:\:\:; *N\},* the maximal extreme points  
of ITA are of the form nj = 1/r\(S\) for *j* E S, nj = 0 for *j* EN \\ S. Hence Cw =  
maXS\<;;N \{LjES wjr\(S\)\}.  
*3.* min\{ly\: yA \~ wlCw, y E *R';'\}* = 1, and hence by statement 1 we have w/Cw E Per\).  
Letting Ai = y71 Cw, we have that L7!1 Ai = 1, A \~ 0, and AA \~ wi Cw. Since the rows of A  
form an independence system, we can obtain AA = wi Cw by modifying the solution  
by replacing ai E zm by *a*i \< ai if Ai \> O. Thus the claim follows. •  
Statements 1 and 2 suggest that there is a link between \(a\) the separation problem \(6.6\)  
for Per\) and \(b\) problem \(8.1\). In fact, by solving problem \(6.6\) a polynomial number of  
times, we can obtain algorithms to compute the value of Cw, to find the set S maximizing  
LjES wjr\(S\), and to find an optimal solution y\* in \(8.1\).  
For the special case of w = 1, we obtain the following result\:  
Corollary 8.2. For a matroid M = \(N, r\), the minimum number of independent sets  
needed to cover *N* fractionally is maXS\<;;N \{ I S Ilr\(S\)\}.  
Example 8.1\. forests.  
Consider the graph of Figure 8.1 with the values of w as shown. The  
problem is to find a fractional covering of the weighted edges of G with subgraphs that are  
Suppose the feasible solution y\* to \(8.1\) shown in Figure 8.2 has been found. Since  
L yj = 1, it follows that Cw .s; 1. Now taking S = \{e4, e5, e6\}, we see that Cw \~ LjES wjr\(S\) = 1,  
and hence y\* is an optimal solution to the fractional covering problem \(8.1\).  
Now we consider the integer covering problem \(8.2\). To solve this problem, we  
introduce the concepts of matroid union and matroid partition.  
\~----\~----\~ 2  
Figure 8.1 704 111.3. Matroid and Submodular Function Optimization  
y\*= 1  
3  
Figure 8.2  
Definition 8.1\. Given matroids *Mi* = \(N, *gjii\)* for i = 1, ... , *k,* the independence system  
*M\(k\)* = \(N, *gji\(k\)\)* is the matroid union where S E *gji\(k\)* ifand only if there exist *Si* E *gjiJor*  
*i* = 1, ... , *k* such that U7\~1 *Si* = S.  
This definition motivates the matroid partition problem for a matroid union *M\(k\)\:*  
\(8.3\) Given S s *N,* determine whether S E *gji\(k\).*  
Problem \(8.3\) is related to \(8.2\) with w = 1 because when the matroids *Mi* are identical,  
we obtain *Ns gji\(k\)* if and only if *Zl* \~ *k.* Below we will show how the matroid partition  
problem can be solved as a matroid intersection problem, and we will also show how to  
reduce problem \(8.2\) with *w* E Z\~ to a problem of the same type with *w* = 1. This will  
enable us to establish the integer-rounding property for the clutter consisting of the bases  
of a matroid.  
Example 8.2. We are given a set N of jobs, each requiring unit processing time. Job *j* has  
a deadline *dj •* S E *gji* if there is some ordering of the jobs of S such that each job is finished  
by its deadline.  
*Takingf\(S\)* = maXjES *dj* as in Example 6.1, we see that *T* E *gji* if and only if *x T* E Pc!\).  
But by Corollary 6.6 this holds if and only if  
Now it follows from Proposition 2.3 that *\(N, gji\)* is a matroid with rank function  
*reT\)* = minsc;T *U\(S\)* + IT \\ S I\}·  
Thus, two people working together can accomplish the set S of jobs if S = S IUS 2 with  
*Si* E *gji* for *i* = 1, 2, or, in other words, if and only if S is independent in the matroid union  
*M\(2\)* = *\(N, gji\(2\)\).*  
Suppose there are 10 jobs, and the deadlines are as follows\:  
Jobj\: 1 2 3 4 5 6 7 8 9 10  
*dj \:* 1 2 2 3 3 3 3 4  
We have \{l, 4, 6, 1O\}, \{2, 5, 7\} E *gji,* so that \{l, 2, 4, 5,6, 7, 1O\} E *gji\(2\).* On the other  
hand, \{l, 2, 3, 4, 5\} \$. *gji\(2\),* since \{i, *j, k\}* \$. *gji* for any choice of 1 \~ i \< *j* \< *k* \~ 5.  
Finally, note that \{l, 4, 6, 1O\}, \{2, 5, 7\}, \{3, 8, 9\} E *gji.* Hence *N* E *gji\(3\).* Since *N* \$. *gji\(2\),*  
it follows that the optimal value of problem \(8.2\) with w = 1 is *Zl* = 3. 8\. Covering with Independent Sets and Matroid Partition 705  
Given *k* matroids *Mi* = *\(Ni' fJ'i\)* on distinct sets *N i,* their *sum* is *M"* = \(Uf=1 *N i,* U!'\:1 *fJ'j\).*  
It is easily checked that the sum of matroids is a matroid. The partition matroid \(see  
Section 1\) is a simple example of such a sum.  
We now show how the k-matroid partition problem for *M* = *\(N, fJ'\)* can be viewed as a  
matroid intersection problem. Consider the set *N"* = *\{\(i, j\)\: i* E *K, j* E *N\}* where *K =*  
\{l, ... ,k\}. Any subset F" £; *N"* can be written as F" = UiEK UjEFi *\(i,j\),* denoted \(Flo ... ,  
*Fk\),* where *Fi* = \{j\: *\(i,j\)* E *F"\}* £; *N.* We now consider two matroids over the set *N".* The  
first one, *MT* = *\(N",* fJ'r\), is just the sum of *k* copies of the original matroid *M* = *\(N, fJ'\),* so  
*F"* E *fJ'!* if and only if *Fi* E *fJ'* for *i* = 1, ... *,k.* The second one, Mi = *\(N", fJ'2\),* is the  
partition matroid where *F\** E *fJ'i* if and only if *Fi* n *\}j* = 0 for all 1 \~ *i* \< *j* \~ *k.*  
Example 8.1 \(continued\). We are given the graphic matroid *M* = *\(N, fJ'\)* for the graph  
G = *\(V, E\)* of Figure 8.1. Taking the sum of three copies of G = *\(V, E\)* gives the graphic  
matroid *MT* shown in Figure 8.3.  
A set *E\** £; *N"* of edges is independent in the partition matroid Mi if no edge of the same  
type appears more than once; that is, no more than one copy of edge \(1, *2\)-el,1* or *e2,1* or  
*e3,1-*is allowed.  
Now we consider the independence system *\(N",* \~ n *fJ'i\)* of common independent sets  
in the matroids *MT* and Mi, and we investigate how it relates to the matroid union  
*\(N, fJ'\(k»\)* and the matroid partition problem. Let rrbe the rank function of Mrfor i = 1, 2,  
let *m"* be the rank function of *\(N", fJ'T* n *fJ'i\),* and let *mk* be the rank function of the  
matroid union *\(N, fJ'\(k\)\).* By definition of Mrand *M\(k\),* we have\:  
Proposition 8.3. *The following statements are true.*  
1. *r!\(F"\)* = 1\:f=1 *ri\(Fi\).*  
*2. ri\(F"\)* = 1 Uf=1 *Fi I·*  
3. *IfF"* = *\(Flo'* .. , *Fk\)* E *fJ'!* n *fJ'i, then* S = Uf=1 *Fi* E *fJ'\(k\).*  
*4. If* S E *fJ'\(k\), then there exists F\** = \(Flo ... *,Fk\)* E *fJ'T* n *fJ'i such that* Uf=1 *Fi* = S.  
5. *mk\(S\)* = *m\*\(S,* ... , *S\).*  
Now we can show that *M\(k\)* is, in fact, a matroid.  
el.1 e2.1  
....... ----\{ 2 l' r----\~  
i= 1 i=2 ;=3  
Figure 8.3 706 111.3. Matroid and Submodular Function Optimization  
Proposition 8.4. *mk\(S\)* = minn;;sCLtl *ri\(T\)* + IS\\ *TI\}.*  
*The matroid union M\(k\)* = *\(N, fF\(k»* is *a matroid with rank function*  
*Proof* We just consider the case S = *N.* By Proposition 4.9, we have  
*m\*\(N\*\)* = *m\*\(N,* ... , *N\)* = minCrr\(F\*\) + *r;\(N\** \\ *F\*\)\}*  
*PsN·*  
*=* T,lJJ\{\~ *ri\(Fi\)* + li\~ *\(N* \\ *Fi\)l\}*  
= mc in\{± riCF';\) + IN \\ 0 F';I\}  
\~\_N l\~ l\~  
= min\{± ri\(A Fj\) + IN \\ A F';I\}  
*Fi\<\:N* i\~l J\~l ,\~l  
because the *ri* are nondecreasing.  
It follows that the minimum is attained by a set *F\** of the form *F\** = *\(T,* ... , *T\),*  
where *T* = nj\~l Fj. Hence  
*m\*\(N,* ... *,N\)* = tpjJJ \{\~ *r;\(T\)* + IN \\ *Til*  
Now by statement 5 of Proposition 8.3, we have *mk\(N\)* = *m\*\(N,* ... , *N\),* and hence *mk* is  
of the required form. But by Proposition 2.3, *mk* is a submodular rank function; and  
hence by statement \(ii\) of Theorem 2.4, the matroid union is a matroid. •  
By Proposition 8.4, we can solve the matroid partition problem by applying the  
cardinality matroid intersection algorithm to *\(N\*,* fFr n *fF;\),* demonstrating either that  
S E *fF\(k\)* or that there exists a set *T* s Swith L7\~1 *r;\(T\)* \< I *TI.* A more efficient and direct  
algorithm is the matroid partition algorithm.  
Conversely, we can use the matroid partition algorithm to solve the cardinality  
intersection problem formatroidsM1 = *\(N, rl\)andM*2 = *\(N, rz\).* It suffices to consider the  
matroid union *\(M\(2\), m*2\) of *Ml* and *Mf.* Then because the partition algorithm can be  
used to find *m*2*\(N\)* and we have  
the claim follows.  
Now we return to the covering problem \(8.2\). When w = 1, the problem can be restated  
as\:  
Given a matroid *M= \(N, fF\),* determine a minimum number z 1 of  
independent sets whose union is the whole set.  
As we have already observed, matroid unions give a method to solve this problem.  
Taking *k* identical copies of *M* = *\(N, r\),* the matroid union *M\(k\)* has rank function  
*mk\(N\)* = *minCkr\(T\)* + *IN\\ TI\}.*  
*T* 8\. Covering with Independent Sets and Matroid Partition 707  
Proposition 8.5. *Zl* = maxs IISl/r\(S\)l.  
Proof Zt \~ k if and only if N is independent in the matroid union M\(k\) or  
*mk\(N\)* = INI, or  
kr\(T\) + IN \\ TI \~ INI for all T s N.  
Hence *Zl* \~ k ifand onlyifk \~ I TI/r\(T\) for all T s N. •  
In addition, we have seen that either the matroid intersection algorithm applied to M\(k\)  
or the matroid partition algorithm gives the *Z* 1 bases required.  
One approach to problem \(8.2\) for general W E Z\~ is to construct a matroid MW from M  
by duplicating each elementj Wj times for *j* E N. Let NW = *\{\(j, i\)\:j* EN, *i* = 1, ... , w\).  
Given TW s \~, we let Tt = \{i E \{l, ... , Wj\}\: *\(j,* i\) E TWlso that Tt is the set of different  
copies ofj in T W  
, and we write TW = \(Tf, ... , T\:n. Now TW = \{j EN\: I Tt I \~ 1\} is the set  
of elementsj EN of which at least one copy appears in TW.  
We now define an independence system MW = \(NW, 9fW\) such that TW E fFW if I Tt I \~ 1  
for j EN and TW E fF. MW = \(N, rW\) is easily seen to be a matroid with rW\(TW\)\:\:o r\(Tw\).  
By construction of \~, we have\:  
Proposition 8.6. *Zw* is the optimal value of problem \(8.2\) for the matroid M = *\(N,* \~\) *if*  
and only *if Zw* is the optimal value of problem \(8.2\) for the matroid UW = \(N"', fFW\) with a  
right-hand-side vector of 1 *s.*  
Corollary 8.7. *Zw* = *maxT* 1\(l\:jET wj\)/r\(T\)l·  
Proof By Proposition 8.5, *Zw* = *maxTw£;Nw* II TW l/rW\(TW\)l. Since rW\(T"?\);\:; *r\(tw\),* we  
obtain  
*zw=max* 1 W J = max *\_1 \_\_ J •*  
r*l\:'ETW* W'l fl\:'ET W'l  
*TW£;N* r\(T\) *T* r\(T\) •  
Figure 8.4 708 111.3. Matroid and Submodular Function Optimization  
# • •  
Figure 8.5  
From Proposition 8.1, it follows that problem \(8.1\), the linear programming relaxation  
of problem \(8.2\), has value \(w = *maXT \(LjET wj\)/r\(T\).* Hence, since Zw = few\]' the matroid  
covering problem provides an example of the integer-rounding property discussed in  
Section III. 1.6.  
Similar results can be obtained for packing bases. However, for general W E Z\~ the  
above construction does not lead to a polynomial algorithm.  
Example 8.1 \(continued\). The problem is to find an integer covering of the weighted  
edges of G by forests \(see Figure 8.1\).  
Since \(w =\~, we know from the integer-rounding property that Zw = f\(w\] = 3. Now we  
construct an optimal solution. In Figure 8.4, we show the graph GW = *\(V,* EW\) underlying  
the graphic matroid MW = \(EW, g;W\) for which we need to solve problem \(8.2\) with weights  
*we\),!* = 1.  
Now we construct an independence system M\* = *\(N\*,* g;\*\) consisting of three copies of  
MW, such that *E\** = *\(E* 10 *E* 2, *E* 3\) £ *\(EW, EW, EW\)* and *E\** E g;\* if the edge sets *E* 10 *E* 2, *E* 3 are  
disjoint and each edge set E*j* is a forest in GW.  
Applying the cardinality matroid intersection algorithm to *M\** = *\(N\*, g;\*\),* we obtain  
the solution shown in Figure 8.5, which provides an optimal solution \{elo e2, es, e6\},  
\{e2, e3, e4,e6\}, \{e4\} for the covering problem.  
9\. SUBMODULAR FUNCfION MAXIMIZATION  
Whereas there is a polynomial algorithm for the minimization of a sub modular function,  
the problem of maximizing a submodular function is .N"\~-hard. Here we investigate three  
problems that are natural generalizations of problems treated either earlier in this chapter  
or earlier in the book.  
The three problems are\:  
\(1\) maximizing an arbitrary submodular function\:  
\(9.1\) Z I = max\{j\(S\)\} withf submodular,  
s  
*\(2\)* maximizing a non decreasing submodular function subject to a cardinality constraint\:  
\(9.2\) Z2 = max\{j\(S\)\: IS I \~ *p\}* withfsubmodular and nondecreasing,  
s 9\. Submodular Function Maximization 709  
*\(3\)* minimizing a linear function subject to a submodular constraint\:  
\(9.3\) Z3 = min\{ L cj\:f\(S\) =f\(N\)\} withfsubmodular and nondecreasing.  
S jES  
A typical example of problem \(9.l\) is the uncapacitated location problem discussed in  
Chapter II.S with  
f\(S\) = L max cij - L *jj.*  
iEi jES jES  
If capacity constraints limiting the amount of demand that can be met by an open facility  
are added, the problem can still be formulated as the maximization of a submodular  
function.  
The p-facility location problem studied in Section II.S.3, namely,  
max\{L max cij\: IS I \~ p\},  
S iEi jES  
is an example of problem \(9.2\).  
An example of problem \(9.3\) is the integer covering problem  
where *\(A, b\)* is an *m* x *\(n* + 1\) nonnegative integer matrix. Here we set  
Observe thatf\(N\) = L;\:\:l bi andf\(S\) = f\(N\) if and only if LjES aj \~ b. Note that we can  
assume that the *\{x* E *B* n\: *Ax* \~ *b\}* is nonempty by adding, if necessary, an artificial  
variable Xn+l with an+l = band Cn+l \> Lf=l Cj.  
Another important application is the k-matroid intersection problem  
z = max\{ *L* c/ IS I \~ ri\(S\) for i = 1, ... , k\}.  
S jES  
To see that it fits this model, remember from Section 2 that S E g; if and only if *N* \\ S  
contains a basis in the dual matroid. Hence  
z = *L* Cj - min\{ *L* Cj\: rp\(N \\ S\) = rp\(N\) for i = 1, ... , k\}  
JEN S jEN\\S  
= *L* Cj - min\{ *L* cj\:f\(T\) = f\(N\)\} ,  
JEN T JET  
wheref = L\~l rp. 710 111.3. Matroid and Submodular Function Optimization  
Since Problems \(9.1\)-\(9.3\) are ,Ng\>-hard, we consider two approaches. The first is to  
formulate and solve them as integer programs, and the second is to apply heuristic  
algorithms.  
We first consider an integer programming formulation of \(9.1\). We assume f\(0\) = 0;  
then we set f\(S\) = J\*\(S\) - *L*jES kj, where kj = f\(N \\ \{j\}\) - f\(N\) for all *j* E N. Hence f is  
written as the difference of a nondecreasing submodular functionJ\* and a linear function.  
Now consider the polyhedron  
Q\(f\) = *\{\(y!,* x\) E Rl x R\~\: Y! \~J\*\(S\) + I \[f\*\(S U \{j\} - J\*\(S\)\]Xj  
jEN\\S  
- I kjxj for all S £ N\}.  
jEN  
Proposition 9.1. Given *\(y!,* XT\) E Rl X Bn  
, we obtain *\(y!,* XT\) E Q\(j\) *if* and only *if*  
Y! \~f\(T\).  
Proof If *\(y!,* XT\) E Q\(f\), then  
Y! \~J\*\(T\) + I \[f\*\(T U \{j\}\) - J\*\(T\)\]xJ - I kjxJ  
\~MT \~N  
= J\*\(T\) - I kj = f\(T\).  
JET  
Now suppose Y! \~f\(T\). By Proposition 2.1\(b\), we have  
f\(T\) = J\*\(T\) - I kj  
JET  
\~f\*\(S\) + I \[f\*\(S U \{j\}\) - f\*\(S\)\] - I kj  
jET\\S JET  
for all S £ *N,* and hence  
Y! \~f\*\(S\) + I \[f\*\(S U \{j\}\) - f\*\(S\)\]xJ - I kjxJ  
jEMS jEN  
for all S £ *N,* so *\(y!,* x *T\)* E Q\(f\). •  
As a consequence of Proposition 9.1, an alternative formulation for \(9.1\) is  
\(9.4\)  
Since Q\(j\) has an exponential number of constraints, we suggest a cutting-plane algo-  
rithm, similar to that of Benders, for solving \(9.4\) \(see Section 11.5.4\). 9\. Submodular Function Maximization 711  
Example 9.1\. namely,  
The problem is to maximize the quadratic function from Example 7.1,  
or maxs \(j\*\(S\) - \~jES k*j \},* wheref\* is defined above and k = \(1 11 9 0\).  
Generating the constraints of Q\(f\) for S = 0 and S = \{l\}, we obtain the relaxation  
max 17  
17';;; 0 + 9xI + 4X2 + 2X3 + *6*X 4,  
17 .;;; 10 - XI  
# S=0  
S = \{l\}  
with optimal solution 17 = 13, *x\** = \(1 and hence *x\** is optimal.  
0 0 1\). Hence *Z* I .;;; 13. However, f\(\{1,4\}\) = 13,  
A formulation for problem \(9.3\) is derived similarly with  
R\(f\) = \{X E R\~\: I \[f\(S U \{j\}\) - f\(S\)\]xj \~ f\(N\) - f\(S\) for S S N\}.  
*jEN\\S*  
Proposition 9.2  
\(9.5\)  
Proof If x T E B n is the characteristic vector of T and if x T E R\(f\), then  
0= I \[f\(T U \{j\}\) - f\(T\)\]xJ \~ f\(N\) - f\(T\).  
*jEN\\T*  
Hencef\(T\) = f\(N\), and T is feasible in \(9.3\).  
Conversely, if *T* is feasible in \(9.3\), then by submodularity \(Proposition 2.1, statement  
ii\) we have  
f\(N\) = f\(T\) .;;;f\(S\) + I \[f\(S U \{j\}\) - f\(S\)\] for S s N  
*JET\\S*  
or, in other words,  
I \[f\(S U \{j\}\) - f\(S\)\]xJ \~ f\(N\) - f\(S\) for all S s N,  
jEN\\S  
sox*T* ER\(f\). •  
Now we turn to heuristic algorithms. The greedy heuristic algorithm for problem \(9.2\)  
has already been analyzed in Theorem 3.3 of Section 11.5.3. Repeating the theorem, we  
have\: 712 111.3. Matroid and Submodular Function Optimization  
Theorem 9.3. Iff\(0\) = 0, and ZG is the value of a greedy heuristic solution to problem  
\(9.2\), and Z is the value of an optimal solution, then zalz \~ 1 - \[\(p - l\)/p *\]P.*  
The greedy heuristic for the 0-1 covering problem was analyzed in Theorem 2.5 of  
Section 11.6.2. As indicated above, this is a special case of problem \(9.3\). The same result  
holds for the general problem. First we describe the heuristic.  
The Greedy Algorithm for Problem \(9.3\)  
Initialization\: SO = 0, N 1 = N, t = 1.  
Iteration t\: Let  
with the minimum attained at jt EN. Let Nt+l = N \\ Ut\), and let S = St-l 'u Ut\}. If  
f\(S\) = f\(N\), then st is the greedy solution with z G = LjES' Cj. Iff\(st\) \< f\(N\), let t \<- t + 1  
and return.  
Using a relaxation of the formulation \(9.5\), and using a dual heuristic as in the proof of  
Theorem 2.5 of Section 11.6.2, we obtain the following theorem\:  
Theorem 9.4. Let f be integer-valued, let f\(0\) = 0, d = maXjEN *\(f\{J\}\),* and let ZG be the  
value of a greedy heuristic solution to \(9.3\). Then zGlz \~ H\(d\), where H\(d\) = L\~l \(1li\).  
10. NOTES  
Sections 111.3.1 and 111.3.2  
Matroids were introduced by Whitney \(1935\). Further early developments are due to Tutte  
\(1965\). Detailed developments of matroid, polymatroid, and submodular function theory  
are contained in the books by Tutte \(1971\), Welsh \(1976\), and Recski \(1988\). Recski's book  
also gives many applications ofmatroids in the physical sciences and engineering, as does  
the survey article by Iri \(1983\).  
The importance of matroids in combinatorial optimization was established by  
Edmonds \(1965b, 1970, 1971\). Chapters 8 and 9 of the book by Lawler \(1976\) present the  
work done in matroid optimization through the mid-1970s. A survey of matroid results  
tailored to the operations research community was presented by Bixby \(1982\).  
Lovasz \(1983\) surveyed the relationships between submodularity and convexity. Topkis  
\(1978\) studies properties of submodular functions that are of interest in optimization.  
Section 111.3.3  
The optimality of the greedy algorithm was first discovered by Rado \(1957\) and indepen-  
dently by Gale \(1968\), Welsh \(1968\), and Edmonds \(1971\). A more general combinatorial  
structure for which the greedy algorithm works, known as a greedoid, has been studied by  
Korte and Lovasz \(1984\).  
The matroid polytope was studied by Edmonds \(1970, 1971\). 10\. Notes 713  
Sections 111.3.4 and 111.3.5  
The matroid and polymatroid intersection theorems are due to Edmonds \(1970\). Algo-  
rithms for maximum-cardinality matroid intersections have been developed by Lawler  
\(1975\) and Edmonds \(1979\). The algorithm for maximum-weighted matroid intersections  
is based on an algorithm of Frank \(1981\). \[Also see Lawler \(1975\), Cunningham \(1986\), and  
Brezovec et al. \(1986\).\]  
Edmonds \(1967b\) gave an algorithm for minimum-weight branchings. \[Also see Chu  
and Liu \(1965\), Bock \(1971\), Karp \(1971\), Murchland \(1973\), and Tarjan \(1977\).\]  
Fulkerson \(1974\) gave an algorithm for the problem of packing rooted directed cuts in a  
weighted digraph and established a blocking relationship between these cuts and branch-  
ings. His results yielded a TDI system for the convex hull of edge sets that contain a  
branching\:  
Edmonds and Giles \(1977\) studied a model, now known as the submodular flow  
problem, that generalizes both network flows and polymatroid intersection. Min-max  
results and algorithms for this class of problems have been obtained by Frank \(1982,1984\),  
Hassin \(1982\), Lawler and Martel \(1982a,b\), Schrijver \(1984a,b\), and Cunningham and  
Frank \(1985\). An application to a scheduling problem has been given by Martel \(1982\).  
An even more robust model, known as the matroid matching problem or matroid parity  
problem, that generalizes polymatroid intersections and matchings has been studied by  
Lovasz \(1980\\ 1981\). He gave a polynomial-time algorithm for the case of matric matroids  
and showed that the general problem is K2P-hard. Related results were given by Tong et al.  
\(1984\).  
An annotated bibliography on these problems was given by Lawler \(1985\).  
Section 111.3.6  
Polymatroid and submodular rank functions have been studied by Edmonds \(1970\), and  
the role of these functions in combinatorial optimization has been examined by Lovasz  
\(1983\). \[Also see McDiarmid \(1975\).\]  
Section 111.3.7  
The max-flow reduction algorithm for submodular function minimization given in  
Section 7 is due to Rhys \(1970\) \[also see Picard and Ratliff \(1975\)\]. Its applications to  
graphic matroids was given by Picard and Queyranne \(1982\) and Padberg and Wolsey  
\(1983, 1984\).  
Crama \(1986\) gave an efficient recognition algorithm for certain classes of sub modular  
functions representable in the form \(7.1\). A general treatment of Boolean functions,  
primarily of historical interest now, is the book by Hammer and Rudeanu \(1966\).  
The polynomiality of submodular function minimization has been established by  
Grotschel, Lovasz, and Schrijver \(1981, 1984b\). They have also developed the procedure  
for minimizing over odd sets \[Grotschel, Lovasz, and Schrijver \(l984c\)\]. These develop-  
ments and many related results are presented in Grotschel, Lovasz, and Schrijver \(1988\).  
Purely combinatorial algorithms for the separation problem for the matroid polytope  
have been given in Cunningham \(1984\), and for submodular function minimization in  
Cunningham \(1985\).  
Section 111.3.8  
The problem of covering and packing with independent sets was studied by Edmonds and  
Fulkerson \(1965\) and Edmonds \(1965b\). The matroid partition algorithm is due to  
Edmonds \(1965b\) \[also see Cunningham \(1986\)\]. 714 III.J. Matroid and Submodular Function Optimization  
Edmonds \(1973\) considered the packing of branchings, and Cunningham \(1977\)  
described the blocking polyhedron of the convex hull of the common independent sets in  
two matroids. These results were generalized by Baum and Trotter \(1981\).  
A polynomial-time algorithm for problem \(8.1\) with general w is obtained using the  
separation algorithm of Section 7 \[see Cunningham \(1984, 1985\)\].  
Section 111.3.9  
Submodular function maximization has been studied by Nemhauser, Wolsey, and Fisher  
\(1978\), Fisher, Nemhauser and Wolsey \(1978\), Nemhauser and Wolsey \(1979, 1981\),  
Wolsey \(1982a,b\), and Conforti and Cornuejols \(1984\). In Nemhauser and Wolsey \(1979\),  
it was shown that within a large class of algorithms the greedy algorithm is the best possible  
one for problem \(9.2\).  
11. EXEROSES  
1. 2\. Show that graphic matroids are matric.  
Given a family of subsets \{S;\}7!l of a finite set N, we define a transversal of the family  
to be a set *T* = *\{i* 10 ••• , *i* m \} with the following properties\:  
a\) ITI =m;  
b\) ijESjforj= I, ... ,m.  
If R \~ T is a transversal, R is a partial transversal.  
i\) Show that a family has a transversal ifand only ifthe maximum s-t flow in the  
digraph of Figure 11.1, with \(j, *S;\)* Ed ifj E *S;,* has value *m,* where\: *\(s,j\)* Ed  
has capacity 1 forj EN, *\(S;, t\)* Ed has capacity 1 for *i* E \{I, ... , *m\},* and \(j, *S;\)*  
has infinite capacity.  
ii\) Show that the set of partial transversals forms a matroid on *N.*  
3\. Given an m x nO, 1 matrix A, a set *J* \~ N of columns is called dependent if there  
exists a J' \~ *J* such that *I* jEJ, *aij* = O\(mod 2\) for all i.  
i\) Show that the sets of independent columns form a matroid. Such matroids are  
called binary matroids.  
ii\) Show that a graphic matroid is a binary matroid.  
*N*  
Figure 11.1 11\. Exercises  
715  
4\. Show that the following set functions are submodular\:  
i\) *v\(S\)* = max\{LjET *Cj\:* I *T* I ..; *k, T* \~ S\} for S \~ *N;*  
ii\) *v\(S\)* = I\{\) E *V\:\}* E S or *\(i,\}\)* E *E* for some *i* E S\} I for S \~ *V,* where G = *\(V, E\)*  
is a graph \(this is the cardinality of the neighborhood of *S\);*  
iii\) *v\(S\)* = LjES Cj - LiES,jES *qij* for S \~ *N,* where *qij* \~ 0 for 1 ..; *i* \<\} ..; n.  
5. What is the complexity of the greedy algorithm for problem \(3.1\) when *M* is a  
partition matroid?  
6\. Prove Proposition 3.5 directly by showing that if U\} E *fjP* for allj E *N,* every facet of  
the convex hull of independent sets ofa matroid is either of the form  
\(a\)  
*Xj* \~ 0 for\} EN  
or  
\(b\)  
I *Xj* ..; *r\(A\)* for *A* \~ *N.*  
7\. 8. *JEA*  
Hint\: \(i\) Show that if nx ..; no is facet-inducing, either it is the inequality - *Xj* ..; 0, or  
*nj* \~ 0 for all\} EN. \(ii\) Show that ifnj \> 0 for\} E A, and the characteristic vector of  
S E *F* lies on the facet, then IS n *A* I = *r\(A\).*  
Characterize the facets ofthe tree polytope-that is, when is LeEE\(U\) Xe"; I *U* I - 1 a  
facet-defining inequality?  
Show that the following set functions are submodular\:  
i\) Given a matroid M = \(N, *fjP\)* with cERn, let  
*v\(S\)* = max\{ ICe\: T \~ S, T E *fjP\}* for S \~ N.  
eET  
ii\) Given M as in part i, let \{Qi\}iEI be subsets of N and let  
v\(J\) = max\{ ICe\: T \~ U Qi' T E *fjP\}* for J \~ *f.*  
eET" IE!  
\(Qi is the set of elements with color *i.\)*  
iii\)  
*v\(S\)* = max\{ I I *CijYij\:* I *Yij"; ai* for *i* E *f,*  
iEI jES jES  
I *Yij* ..; b*j* for\} E S, *Yij* \~ 0 for *i* E *f,\}* E *s\}.*  
iEI  
\(This function arises in the capacitated facility location problem where S \~ *N* is  
the set of open facilities.\) 716 9. 111.3. Matroid and Submodular Function Optimization  
L\: Rm -+ Rl is *submodular* if  
*L\(u\)* + *L\(v\)* \~ *L\(u* V *v\)* + *L\(u* /\\ *v\)* for *u, v* E Rm,  
where *\(u* V *V\)i* = *max\(ui, Vi\)* and *\(u* /\\ *V\)i* = *min\(ui, Vi\).* Show that if *L\(u, y\)* is  
submodular on *Rm* x *RP,* then *W\(y\)* = minu *L\(u, y\)* is submodular on *RP.*  
10. Consider the clutter of bases of a matroid *M* = *\(N, 8F\).*  
i\) Prove that  
*Q\*=* conv\{x E R\~\: x \~ x*F* for F a basis of M\}  
*= \{x* E R\~\: I *Xj* \~ *r\(N\)* - *r\(s\)\}.*  
jES  
ii\) Show that for the clutter of spanning trees, this gives  
*Q\*=* \{x E R\~\: I I Xe \~ *f* - 1 for all  
\{\(i,j\)\:i\<j\) *eEo\(v,\)nO\(V;\)*  
Vj, ... , Jijthat are disjoint subsets of *V* and allf\> I\}.  
11. Apply the maximum-cardinality matroid intersection algorithm to the pair of  
graphic matroids in Figure 11.2, starting with S = *\{eb e2, e4\}*  
12. Show that the lengths of the shortest s-t dipaths at successive iterations of the  
cardinality matroid intersection algorithm are nondecreasing.  
13\. Apply the weighted matroid intersection algorithm to find a maximum-weight set  
of arcs forming part of a branching \(with no specified root\) in the digraph of  
Figure 11. 3.  
14. i\) Show that the polytope of the arc sets in exercise 13 is given by  
\{ XER\~\: I *Xij\:S;;* 1 *forjE* V, I *Xij\:S;;* ISI-l for0CSs\:; *v\}*  
*iEo-U\)* iES,jES  
ii\) Devise a more efficient algorithm for the maximum-weight branching problem.  
What is the complexity of your algorithm?  
Figure 11.2 11\. Exercises  
717  
Figure 11.3  
15\. For the clutter of rooted branchings, prove that  
*Q\** = \{x E R\~\: I Xu \~ 1 for all S \~ *V* with \{1\} E S\}  
\(i,j\)EJ"\(S\)  
is integral and that the max-min inequality holds strongly where *t5+\(S\)* are the rooted  
dicuts \(see exercise 34 of Section 111.1.8\).  
16\. Show that the matroid intersection polyhedron is box TDI \(see exercise 3 of Sec-  
tion m.1.8\).  
17\. Show that if *P* = *\{x* ERn\: *Ax* \~ *b\}* is box TDI, there exists a 0, 1, -1 matrix *A'* and a  
vector b ' such that P = *\{x* ERn\: A *IX* \~ b'\}. Hint\: Observe that w E P ifand only if  
max\{1x' - *Ix"\: x'* \~ 0, *x"* \~ 0, *x* = w, *Ax* + *Ax'* + *Ax"* = *b\}* \~ 0.  
18\. Prove Proposition 6.2.  
19. Transform the problem  
into a polymatroid separation problem. Write out the polymatroid explicitly.  
20. i\) Show that a polymatroid *Pfis* integral \(has integral extreme points\) iffis integer-  
valued.  
ii\) Show that if P \~ R\~ is an integral polymatroid, *rp\(a\)* E Zl for all *a* E Z\~.  
21. Let *PU\)* = \{x ERn\: LjES Xj \~f\(S\) for S \~ N\} withf\(0\) = 0. Prove the following\:  
i\) For any *T* \~ *N,* there exists x E P\(f\) such that LjET Xj = f\(T\).  
ii\) All maximal points y such that y \~ x, Y E *PU\)* have the same value of LjEN Yh  
denoted by *i'f\(x\).*  
iii\) Iffl andf2 are submodular withfl\(0\) = f2\(0\) = 0, then 718 111.3. Matroid and Submodular Function Optimization  
22\. Solve the problem of exercise 19 by a maximum-flow algorithm.  
23\. Consider the general problem of minimizing a quadratic boolean function z =  
minxEB,J\(x\), where  
J\(X\) = 2\: CjXj + 2\: %XiXj,  
*jEN* \(i,j\)EPUQ  
*P* = *\{\(i,j\)\:* 1 \~ i *\<j* \~ nand *qij* \> O\},  
and  
*Q* = *\{\(i,j\)\:* 1 \~ i *\<j* \~ nand *qij* \< O\}.  
i\) Show that the problem can be reformulated as the mixed-integer program  
z = min 2\: CjXj + 2\: *qijYij* 2\: *qijYij*  
*jEN \(i,j\)EP \(i,j\)EQ*  
*Yij* \~ Xi, *Yij* \~ Xj for *\(i,j\)* E Q  
Xi + Xj - 1 \~ *Yij* for *\(i,j\)* E *P*  
24. 25\. 26. ii\) Show that *J* is submodular if and only if *P* = 0.  
iii\) Show that the problem matrix is *TU* when *P* = 0.  
LetJ\(S\) = LTc;S CT. Show thatJis submodular if and only if  
2\: CTU\{i,j\) \~ 0 for all S \~ N \\ *\{i,j\}* and all *i,j* EN.  
Tc;S  
Show that ifJis cubic \[i.e.,J\(S\) = LTc;s CT, and CT = 0 for I *T* I \> 3\]\:  
i\) There is a polynomial algorithm to test ifj is submodular.  
ii\) Jis submodular if and only ifit can be put in the form \(7.1\).  
Show that the recognition problem\: "Is the quartic function \(f\(S\) = LTc;s CT with  
C *T* = 0 for I *T* I \> 4\) not submodular?" is .KiYl-complete.  
27\. Let *P* be the convex hull of 1-matchings for the complete graph on 5 nodes. Is the  
point shown in Figure 11.4 in P? Ifnot, find a most violated facet-defining inequality  
\(not by inspection\).  
28\. Consider the min-cut problem min0cscv *LeE6\(S\)* C*e* on a graph G = *\(V, E\)* with  
cER\~.  
i\) Show that if S\), S2 are minimum cuts with SI n S2 '\* 0, SI U S2 '\* *V,* then  
SI n S2 and SI U S2 are minimum cuts.  
ii\) Suppose I V I is even, and there exists a minimum cut S\* with I S\* I even. Show  
that there exists a solution So to the problem  
min \{ 2\: Ce\: IS I Odd\}  
*0CSCV eE6\(S\)*  
such that either SO C S\* or So C *N* \\ *S\*.* 11\. Exercises 719  
Figure 11.4  
29\. 30. Describe a polynomial algorithm to \(a\) compute Cw in problem \(8.1\) and \(b\) find an  
optimal solution Sand *y\** as given in Proposition 8.1.  
Consider the fractional packing problem given by  
1'/w = \{max *ly\: yA* \~ W, *Y* E *R'\:'\},*  
where A is the basis-element incidence matrix of a matroid M, and also consider the  
integer packing problem given by  
Cw = \{max *ly\: yA* \~ W, *Y* E *Z,\:,\}.*  
i\) Express 1'/w as the minimum of a set of objects.  
ii\) Give polynomial algorithms to calculate 1'/1 and \~1'  
iii\) Does the integer round-up property hold, that is f1'/w\] = \~w?  
iv\) Apply these results to the graphic matroid of Figure 8.1 with W = 1.  
31\. Solve the max-cut problem maxocscv LeEO\(S\) Ce in the graph of Figure 11.5 using  
formulation Q\(f\) and a cutting-plane algorithm.  
32. i\) Prove Theorem 9.4.  
ii\) Deduce that when *f* is a matroid rank function, R \(f\) is integral. What are its  
extreme points \(see exercise 1O\)?  
3  
9  
Figure 11.5 References  
R. Aboudi and G. L. Nemhauser \(1987\). A Strong Cutting Plane Algorithm for an Assignment Problem with Side  
Constraints, Report J-87-3, Industrial and Systems Engineering, Georgia Institute of Technology.  
N. Agin \(1966\). Optimum Seeking with Branch-and-Bound, Management Science 13, BI76-BI85.  
S. Ahn, C. Cooper, G. Cornuejols, and A. M. Frieze \(1988\). Probabilistic Analysis ofa Relaxation for the  
k-Median Problem, Mathematics of Operations Research 13,1-31.  
A. V. Aho, J. E. Hopcroft, andJ. D. Ullman \(1974\). The Design andAnalysisofComputer Algorithms, Addison-  
Wesley.  
A. I. Ali and H. Thiagarajan \(1986\). A Network Based Enumeration Algorithm for Set Partitioning, Department  
of General Business, University of Texas at Austin.  
R. P. Anstee and M. Farber \(1984\). Characterization of Totally Balanced Matrices, Journal of Algorithms 5,  
215-230.  
J. P. Arabeyre, J. Fearnley, E C. Steiger, and W. Teather \(1969\). The Airline Crew Scheduling Problem\: A Survey,  
Transportation Science 3, 140-163.  
J. Araoz \(1973\). Polyhedral Neopolarities, Doctoral Thesis, University of Waterloo, Waterloo, Ontario.  
J. Araoz and E. L. Johnson \(1981\). Some Results on Polyhedra of Semigroup Problems, SIAM Journal on  
Algebraic and Discrete Methods 3, 244-258.  
D. Avis \(1980\). A Note on Some Computationally Difficult Set Covering Problems, Mathematical Programming  
8,138-145.  
D. Avis \(1983\). A Survey of Heuristics for the Weighted Matching Problem, Networks 13,475-494.  
D. A. Babayev \(1974\). Comments on a Note of Frieze, Mathematical Programming 7,249-252.  
A. Bachem and M. Grotschel \(1982\). New Aspects of Polyhedral Theory, in B. Korte, ed., Modern Applied  
Mathematics, Optimization and Operations Research , North-Holland, pp. 51-106.  
A. Bachem, M. Grotschel, and B. Korte, eds. \(1983\). Mathematical Programming\: The State of the Art, Springer.  
A. Bachem, E. L. Johnson, and R. Schrader \(1982\). A Characterization of Minimal Valid-Inequalities for Mixed  
Integer Programs, Operations Research Letters 1, 63-66.  
A. Bachem and R. Kannan \(1984\). Lattices and the Basis Reduction Algorithm. Report CMU-CS-84-112,  
Department of Computer Science, Carnegie-Mellon University.  
A. Bachem and R. Schrader \(1980\). Minimal Inequalities and Subadditive Duality, SIAM Journal on Control and  
Optimization 18,437 -443.  
E. K. Baker \(1981\). Efficient Heuristic Algorithms for the Weighted Set Covering Problem, Computers and  
Operations Research 8, 303-310.  
E. K. Baker and M. L. Fisher \(1981\). Computational Results for Very Large Air Crew Scheduling Problems,  
Omega 9, 613-618.  
E. Balas \(1965\). An Additive Algorithm for Solving Linear Programs with Zero-One Variables, Operations  
Research 13,517-546.  
E. Balas \(1967\). Discrete Programming by the Filter Method, Operations Research 15,915-957.  
E. Balas \(1975a\). Facets of the Knapsack Polytope, Mathematical Programming 8, 146-164.  
E. Balas \(1975b\). Disjunctive Programming\: Cutting Planes from Logical Conditions, in Nonlinear Program-  
ming 2, O. L. Mangasarian et al., eds., Academic Press, pp. 279-312.  
E. Balas \(1979\). Disjunctive Programming, Annals of Discrete Mathematics 5,3-51.  
721 722 References  
E. Balas \(1980\). Cutting Planes from Conditional Bounds\: A New Approach to Set Covering, Mathematical  
Programming Study, 12, 19-36.  
E. Balas and N. Christofides \(1981\). A Restricted Lagrangean Approach to the Traveling Salesman Problem,  
Mathematical Programming 21,19-46.  
E. Balas and A. Ho \(1980\). Set Covering Algorithms Using Cutting Planes, Heuristics, and Subgradient  
Optimization\: A Computational Study, Mathematical Programming Study 12, 37 -60.  
E. Balas and R. Martin \(1980\). Pivot and Complement\: A Heuristic for 0-1 Programming, Management Science  
26,86-96.  
E. Balas and S. M. Ng \(1985\). On the Set Covering Polytope I\: All Facets with Coefficients in \{O, 1, 2\},  
MSSR-522, Graduate School ofIndustrial Administration, Carnegie-Mellon University.  
E. Balas and M. Padberg \(1972\). On the Set Covering Problem, Operations Research 20, 1152-1161.  
E. Balas and M. Padberg \(1975\). On the Set Covering Problem\: II. An Algorithm for Set Partitioning, Operations  
Research 23, 74-90.  
E. Balas and M. Padberg \(1976\). Set Partitioning\: A Survey, SIAM Review 18, 710-760.  
E. Balas and W. R. Pulleyblank \(1983\). The Perfectly Matchable Subgraph Polytope of a Bipartite Graph,  
Networks 13,486-516.  
E. Balas and M. J. Saltzman \(1986\). Facets of the Three-Index Assignment Polytope, MSRR-529., Graduate  
School ofIndustrial Administration, Carnegie-Mellon University.  
E. Balas and P. Toth \(1985\). Branch and Bound Methods, in Lawler, Lenstra et al., pp. 361-403.  
E. Balas and E. Zemel \(1978\). Facets of the Knapsack Polytope from Minimal Covers, SIAM Journal on Applied  
Mathematics 34,119-148.  
E. Balas and E. Zemel \(1980\). An Algorithm for Large Zero-One Knapsack Problems, Operations Research 28,  
1130-1145.  
E. Balas and E. Zemel \(1984\). Lifting and Complementing Yields all the Facets of Positive Zero-One  
Programming Polytopes, in Mathematical Programming, Proceedings 0/ the International Conference on  
Mathematical Programming, R. W. Cottle et al., eds., pp. 13-24.  
M. L. Balinski \(1965\). Integer Programming\: Methods, Uses, Computation, Management Science 14,253-313.  
M. L. Balinski \(1967\). Some General Methods in Integer Programming, in Nonlinear Programming, J. Abadie,  
ed., Wiley, pp. 221-247.  
M. L. Balinski \(1970a\). On Recent Developments in Integer Programming, in Proceedings 0/ the Princeton  
Symposium on Mathematical Programming, H. Kuhn, ed., pp. 267 -302.  
M. L. Balinski \(1970b\). On Maximum Matching, Minimum Covering and Their Connections, in Proceedings 0/  
the Princeton Symposium on Mathematical Programming, H. Kuhn, ed., pp. 303-312.  
M. L. Balinski \(1972\). Establishing the Matching Polytope, Journal 0/ Combinatorial Theory D13, 1-13.  
M. L. Balinski, ed., \(1974\). Approaches to Integer Programming, Mathematical Programming Study 2.  
M. L. Balinski and A. J. Hoffman, eds. \(1978\). Polyhedral Combinatorics, Mathematical Programming Study 8.  
M. L. Balinski and R. E. Quandt \(1964\). On an Integer Program for a Delivery Problem, Operations Research 12,  
300-304.  
M. L. Balinski and K. Spielberg \(1969\). Methods for Integer Programming\: Algebraic, Combinatorial, and  
Enumerative, in Progress in Operations Research, Relationship Between Operations and the Computer,  
Volume III, J. S. Aranofsky, ed., Wiley, pp. 195-292.  
M. O. Ball, L. D. Bodin and R. Dial \(1983\). A Matching Based Heuristic for Scheduling Mass Transit Crews and  
Vehicles, Transportation Science 17, 4-31.  
M. O. Ball and U. Derigs \(1983\). An Analysis of Alternative Strategies for Implementing Matching Algorithms,  
Networks 13, 517-550.  
M. Ball and M. Magazine \(1981\). The Design and Analysis of Heuristics, Networks 11,215-219.  
M. O. Ball and R. Taverna \(1985\). Sensitivity Analysis for the Matching Problem and Its Use in Solving  
Matching Problems with a Single-Side Constraint, Annals o/Operations Research 4, 25-56.  
F. Barahona, M. Grotschel, and A. R. Mahjoub \(1985\), Facets of the Bipartite Subgraph Polytope, Mathematics  
o/Operations Research 10,340-358.  
F. Barahona and A. R. Mahjoub \(1986\). On the Cut Polytope, Mathematical Programming 36, 157 -173.  
I. Barany, J. Edmonds, and L. A. Wolsey \(1986\). Packing and Covering a Tree by Subtrees, Combinatorica 6,  
245-257. References 723  
I. Barany, T. J. Van Roy, and L. A. Wolsey \(1984\). Un capacitated Lot-Sizing\: The Convex Hull of Solutions,  
Mathematical Programming Study 22, 32-43.  
R. S. Barr, F. Glover, and D. Klingman \(1981\). A New Optimization Method for Large Scale Fixed Charge  
Transportation Problems, Operations Research 29,448-463.  
J. J. Bartholdi III \(1981\). A Good Submatrix is Hard to Find, Operations Research Letters 1,190-193.  
S. Baum and L. E. Trotter, Jr. \(1977\). Integer Rounding and Polyhedral Decomposition of Totally Unimodular  
Systems, in Optimization and Operations Research, R. Henn, B. Korte, and W. Oettli, eds., Lecture Notes in  
Economics and Mathematical Systems 157, Springer, pp. 15-23.  
S. Baum and L. E. Trotter, Jr. \(1981\). Integer Rounding for Polymatroid and Branching Optimization Problems,  
SIAM Journal on Algebraic and Discrete Methods 2,416-425.  
S. Baum and L. E. Trotter, Jr. \(1982\). Finite Checkability for Integer Rounding Properties in Combinatorial  
Programming Problems, Mathematical Programming 22,141-147.  
M. S. Bazarra and J. J. Jarvis \(1977\). Linear Programming and Network Flows, Wiley.  
E. M. L. Beale \(1965\). Survey ofInteger Programming, Operational Research Quarterly 16, 219-228.  
E. M. L. Beale \(1968\). Mathematical Programming in Practice, Wiley.  
E. M. L. Beale \(1979\). Branch and Bound Methods for Mathematical Programming Systems, Annals o/Discrete  
Mathematics 5, 201-219.  
E. M. L. Beale \(1983\). A Mathematical Programming Model for the Long-Term Development of an Off-Shore  
Gas Field. Discrete Applied Mathematics 5, 1-10.  
E. M. L. Beale and J. J. H. Forrest \(1976\). Global Optimization Using Special Ordered Sets, Mathematical  
Programming 10,52-69.  
E. M. L. Beale and J. A. Tomlin \(1970\). Special Facilities in a General Mathematical Programming System for  
Nonconvex Problems using Ordered Sets of Variables, in Proceedings o/the Fifth International Conference on  
Operational Research, J. Lawrence, ed., Tavistock Publications, pp. 447-454.  
J. E. Beasley \(1984\). An Algorithm for the Steiner Problem in Graphs, Networks 14, 147-160.  
D. E. Bell \(1977\). A Theorem Concerning the Integer Lattice. Studies in Applied Mathematics 56, 187-188.  
D. E. Bell and M. L. Fisher \(1975\). Improved Integer Programming Bounds Using Intersections of Corner  
Polyhedra, Mathematical Programming 8,345-368.  
D. E. Bell and J. F. Shapiro \(1977\). A Convergent Duality Theory for Integer Programming, Operations Research  
25,419-434.  
R. Bellman \(1957\). Dynamic Programming, Princeton University Press.  
R. E. Bellman \(1958\). On a Routing Problem, Quarterly of Applied Mathematics 16, 87 -90.  
R. E. Bellman and S. E. Dreyfus \(1962\). Applied Dynamic Programming, Princeton University Press.  
M. Bellmore and J. F. Malone \(1971\). Pathology of Traveling Salesman Subtour Elimination Algorithms.  
Operations Research 19,278-307.  
M. Bellmore and H. D. Ratliff \(1971\). Set Covering and Involutory Bases, Management Science 18, 194-206.  
J. F. BenderS \(1962\). Partitioning Procedures for Solving Mixed Variables Programming Problems, Numerische  
Mathematik 4,238-252.  
M. Benichou, J. M. Gauthier, P. Girodet, G. Hentges, G. Ribiere and O. Vincent \(1971\). Experiments in Mixed  
Integer Linear Programming, Mathematical Programming 1, 76-94.  
M. Benichou, J. M. Gauthie\~, G. Hentges, and G. Ribiere \(1977\). The Efficient Solution of Large Scale Linear  
Programming Problems-Some Algorithmic Techniques and Computation Results, Mathematical Program-  
ming 13, 280-322.  
A. Ben-Israel and A. Charnes \(1962\). On Some Problems of Diophantine Programming, Cahiers du Centre  
dEtudes de Recherche Operationelle 4,215-280.  
C. Berge \(1957\). Two Theorems in Graph Theory, Proceedings o/the National Academy o/Science 43,842-844.  
C. Berge \(1960\). Les Problemes de Colorations en Theorie des Graphes, Publication 0/ the Institute 0/ Statistics,  
University 0/ Paris 9, 123-160.  
C. Berge \(1972\). Balanced Matrices, Mathematical Programming 2, 19-31.  
C. Berge \(1973\). Graphs and Hypergraphs, North-Holland.  
C. Berge and V. Chvatal, eds. \(1984\). Topics on Perfect Graphs \(Annals 0/ Discrete Mathematics 21\).  
D. P. Bertsekas \(1985\). A United Framework for Primal-Dual Methods in Minimum Cost Network Flow  
Problems, Mathematical Programming 32, 125-145. 724 References  
O. Bilde and J. Krarup \(1977\). Sharp Lower Bounds and Efficient Algorithms for the Simple Plant Location  
Problem, Annals of Discrete Mathematics 1, 79-97.  
R. E. Bixby \(1982\). Matroids and Operations Research, in Advanced Techniques in the Practice of Operations  
Research, H. J. Greenberg, F. H. Murphy, and S. H. Shaw, eds., North-Holland, pp. 333-458.  
R. E. Bixby \(1984\). Recent Algorithms for Two Versions of Graph Realization and Remarks on Applications to  
Linear Programming, in Pulleyblank, pp. 39-67.  
R. E. Bixby and W. H. Cunningham \(1980\). Converting Linear Programs to Network Problems, Mathematics of  
Operations Research 5, 321-357.  
C. E. Blair \(1976\). Two Rules for Deducing Valid Inequalities for 0-1 Problems, SIAM Journal of Applied  
Mathematics 31, 614-617.  
C. E. Blair \(1978\). Minimal Inequalities for Mixed Integer Programs, Discrete Mathematics 24, 147 -151.  
C. E. Blair and R. G. Jeroslow \(1977\). The Value Function ofa Mixed Integer Program 1, Discrete Mathematics  
19, 121-138.  
C. E. Blair and R. G. Jeroslow, \(1982\). The Value Function of a Mixed Integer Program, Mathematical  
Programming 23, 237 -273.  
C. E. Blair and R. G. Jeroslow \(1984\). Constructive Characterizations of the Value Function ofa Mixed-Integer  
Program I, Discrete Applied Mathematics 9, 217-233.  
C. E. Blair and R. G. Jeroslow \(1985\). Constructive Characterizations of the Value Function ofa Mixed-Integer  
Program II, Discrete Applied Mathematics 10, 227-240.  
C. E. Blair, R. G. Jeroslow, and J. K. Lowe \(1986\). Some Results and Experiments in Programming Techniques  
for Propositional Logic, School of Management, Georgia Institute of Technology.  
R. G. Bland \(1988\). A Class of Production Planning Problems Solvable by Network Flows, to appear in  
Operations Research.  
R. G. Bland, D. Goldfarb, and M. J. Todd \(1981\). The Ellipsoid Method\: A Survey, Operations Research 29,  
1039-1091.  
R. G. Bland, H. C. Huang, and L. E. Trotter \(1984\). Graphical Properties Related to Minimal Imperfection,  
Annals of Discrete Mathematics 21,181-192.  
R. G. Bland and D. L. Jensen \(1987\). On the Computational Behavior of a Polynomial-Time Network Flow  
Algorithm, School of Operations Research and Industrial Engineering, Cornell University.  
F. Bock \(1971\). An Algorithm to Construct a Minimum Directed Spanning Tree in a Directed Network, in  
Developments in Operations Research, Vol. *1,* B. Avi-Itzhak, ed., Gordon and Breach, pp. 29-44.  
L. Bodin, B. Golden, A. Assad, and M. Ball \(1983\). Routing and Scheduling of Vehicles and Crews\: The State of  
the Art, Computers and Operations Research 10,69-211.  
J. A. Bondy and U. S. R. Murty \(1976\). Graph Theory with Applications, Macmillan.  
E. Bonomi and J. L. Lutton \(1984\). The N-City Travelling Salesman Problem\: Statistical Mechanics and the  
Metropolis Algorithm, SIAM Review 26, 551-568.  
K. H. Borgwardt \(1982a\). Some Distribution-Independent Results about the Asymptotic Order of the Average  
Number of Pivot Steps of the Simplex Method, Mathematics of Operations Research 7,441-462.  
K. H. Borgwardt \(1982b\). The Average Number of Pivot Steps Required by the Simplex-Method Is Polynomial,  
Zeitschrift fur Operations Research 26, 157 -177.  
I. Borosh and L. L. Treybig \(1976\). Bounds on Positive Integral Solutions of Linear Diophantine Equations,  
Proceedings of the American Mathematical Society 55,299-304.  
V. J. Bowman, Jr., and G. L. Nemhauser \(1970\). A Finiteness Proof for Modified Dantzig Cuts in Integer  
Programming, Naval Research Logistics Quarterly 17,309-313.  
S. Boyd, W. R. Pulleyblank, and G. Cornuejols \(1987\). TRAVEL-An Interactive Traveling Salesman Package  
for the IBM Personal Computer. Operations Research Letters 6, 141-144.  
G. H. Bradley, G. G. Brown, and G. W. Graves \(1977\). Design and Implementation of Large Scale Primal  
Transshipment Algorithms, Management Science 24, 1-34.  
G. H. Bradley, P. L. Hammer, and L. A. Wolsey \(1974\). Coefficient Reduction for Inequalities in 0-1 Variables,  
Mathematical Programming 7, 263-282.  
A. L. Brearley, G. Mitra, and H. P. Williams \(1975\). An Analysis of Mathematical Programming Problems Prior  
to Applying the Simplex Method, Mathematical Programming 8, 54-83.  
R. Breu and C. A. Burdet \(1974\). Branch and Bound Experiments in Zero-One Programming, Mathematical  
Programming Study 2,1-50. References 725  
C. Brezovec, G. Cornuejols, and F. Glover \(1986\). Two Algorithms for Weighted Matroid Intersection, Mathe-  
matical Progrmaming 36, 39-53.  
R. Brooks and A. Geoffrion \(1966\). Finding Everett's Lagrange Multipliers by Linear Programming, Operations  
Research 14, 1149-1153.  
G. G. Brown and W. Wright \(1984\). Automatic Identification of Embedded Network Rows in Large Scale  
Optimization Models, Mathematical Programming 29, 41-46.  
C. A. Burdet and E. L. Johnson \(1974\). A Subadditive Approach to the Group Problem ofInteger Programming,  
Mathematical Programming 2,51-71.  
C. A. Burdet and E. L. Johnson \(1977\). A Subadditive Approach to Solve Linear Integer Programs, Annals of  
Discrete Mathematics 1, 117-144.  
R. E. Burkhard \(1984\). Quadratic Assignment Problems, European Journal of Operations Research 15,  
283-289.  
R. E. Burkhard and V. Derigs \(1980\). Assignment and Matching Problems\: Solutions Methods with Fortran  
Programs, Springer.  
M. Burlet and J. Fonlupt \(1984\). Polynomial Algorithm to Recognize a Meyniel Graph, Annals of Discrete  
Mathematics 21,225-252.  
A. V. Cabot and S. S. Erenguc \(1984\). Some Branch-and-Bound Procedures for Fixed-cost Transportation  
Problems, Naval Research Logistics Quarterly 31, 145-154.  
P. M. Camerini, L. Fratta, and F. Maffioli \(1975\). On Improving Relaxation Methods by Modified Gradient  
Techniques, Mathematical Programming 3, 26-34.  
P. Camion \(1965\). Characterization of Totally Unimodular Matrices, Proceedings of the American Mathematical  
Society 16, 1068-1073.  
G. Carpaneto and P. Toth \(1980\). Some New Branching and Bounding Criteria for the Asymmetric Travelling  
Salesman Problem. Management Science 26,736-743.  
J. W. S. Cassels \(1971\). An Introduction to the Theory o/Numbers, Springer.  
L. Chalmet and L. F. Gelders \(1977\). Lagrangean Relaxations for a Generalized Assignment-Type Problem, in  
Advances in Operations Research, M. Roubens, ed., North-Holland, pp. 103-110.  
A. K. Chandra, D. S. Hirshberg, and C. K. Wong \(1976\). Approximate Algorithms for some Generalized  
Knapsack Problems, Theoretical Computer Science 3, 293-304.  
R. Chandrasekaran \(1969\). Total Unimodularity of Matrices, SIAM Journal 11, 1032-1034.  
R. Chandrasekaran \(1981\). Polynomial Algorithms for Totally Dual Integral Systems and Extensions, Annals 0/  
Discrete Mathematics 11, 39-51.  
G. J. Chang and G. L. Nemhauser \(1984\). The k-Domination and k-Stability Problems on Sun-Free Chordal  
Graphs, SIAM Journal on Algebraic and Discrete Methods 5, 332-345.  
G. J. Chang and G. L. Nemhauser \(1985\). Covering, Packing and Generalized Perfection, SIAM Journal on  
Algebraic and Discrete Methods 6, 109-132.  
A. Charnes and W. W. Cooper \(1961\). Management Models and Industrial Application of Linear Programming,  
Vols. I and II, Wiley.  
D. S. Chen and S. Zionts \(1976\). Comparison of Some Algorithms for Solving the Group Theoretic Program-  
ming Problem, Operations Research 24, 1120-1128.  
D. C. Cho, E. L. Johnson, M. W. Padberg, and M. R. Rao \(1983\), On the Uncapacitated Plant Location  
Problem I\: Valid Inequalities and Facets, Mathematics o/Operations Research 8, 579-589.  
D. C. Cho, M. W. Padberg, and M. R. Rao \(1983\), On the Uncapacitated Plant Location Problem II\: Facets and  
Lifting Theorems, Mathematics of Operations Research 8, 590-612.  
N. Christo fides \(1970\). The Shortest Hamiltonian Chain ofa Graph, SIAM Journal of Applied Mathematics 19,  
689-696.  
N. Christofides \(1975a\). Graph Theory\: An Algorithmic Approach, Academic Press.  
N. Christofides \(1975b\). Worst-Case Analysis of a New Heuristic for the Travelling Salesman Problem,  
Report 388, Graduate School of Industrial Administration, Carnegie-Mellon University.  
N. Christofides \(1985a\). Vehicle Routing, in Lawler, Lenstra et al., pp. 431-448.  
N. Christofides \(1985b\). Vehicle Routing, in O'hEigeartaigh et al., pp. 148-163.  
N. Christo fides and S. Korman \(1975\). A Computational Survey of Methods for the Set Covering Problem,  
Management Science 21,591-599.  
N. Christofides, A. Mingozzi, P. Toth, and M. Sandi, eds. \(1979\). Combinatorial Optimization, Wiley. 726 References  
Y. J. Chu and T. H. Liu \(1965\). On the Shortest Arborescence of Directed Graphs, Scientia Sinica 14, 1390-140.  
V. Chvatal \(1973a\). Edmonds Polytopes and a Hierarchy of Combinatorial Problems, Discrete Mathematics 4,  
305-337.  
V. Chvatal \(1973b\). Edmonds Polytopes and Weakly Hamiltonian Graphs, Mathematical Programming 5,  
29-40.  
V. Chvatal \(1975\). On Certain Polytopes Associated with Graphs, Journals of Combinatorial Theory B13,  
138-154.  
V. Chvatal \(1979\). A Greedy Heuristic for the Set Covering Problem, Mathematics of Operations Research 4,  
233-235.  
V. Chvatal \(1980\). Hard Knapsack Problems, Operations Research 28, 1402-1411.  
V. Chvatal \(1983\). Linear Programming, Freeman.  
G. Clarke and J. W. Wright \(1964\). Scheduling of Vehicles from a Central Depot to a Number of Delivery Points,  
Operations Research 12,568-581.  
E. G. Coffman and R. L. Graham \(1972\). Optimal Scheduling for Two-Processor Systems, Acta Informatica 1,  
200-213.  
M. Conforti and G. Cornuejols \(1984\). Submodular Set Functions, Matroids and the Greedy Algorithm\: Tight  
Worst-Case Bounds and Some Generalizations of the Rado-Edmonds Theorem, Discrete Applied Mathe-  
matics 7, 251-274.  
A. R. Conn and G. Cornuejols \(1987\). A Projection Method for the Un capacitated Facility Location Problem,  
WP No. 26-86-87, Graduate School ofIndustrial Administration, Carnegie-Mellon University.  
S. A. Cook \(1971\). The Complexity of Theorem-Proving Procedures, Proceedings of the 3rd Annual ACM  
Symposium on Theory of Computing Machinery, pp. 151-158, ACM.  
W. Cook \(1983a\). Operations that Preserve Total Dual Integrality, Operations Research Letters 2, 31-35.  
W. Cook \(1983b\). A Minimal Totally Dual Integral Defining System for the b-Matching Polyhedron, SIAM  
Journal on Algebraic and Discrete Methods 4, 212-220.  
W. Cook \(1986\). On Box Totally Dual Integral Polyhedra. Mathematical Programming 34, 48-61.  
W. Cook, A. M. H. Gerards, A. Schrijver, and E. Tardos \(1986\). Sensitivity Results in Integer Programming,  
Mathematical Programming 34,251-264.  
W. Cook, L. Lovasz, and A. Schrijver \(1984\). A Polynomial-Test for Total Dual Integrality in Fixed Dimension,  
Mathematical Programming Study 22, 64-69.  
W. Cook and W. R. Pulleyblank \(1987\). Linear Systems for Constrained Matching Problems, Mathematics of  
Operations Research 12, 97 -120.  
G. Cornuejols \(1986\). General Factors of a Graph, Graduate School of Industrial Administration, Carnegie-  
Mellon University.  
G. Cornuejols, M. L. Fisher, and G. L. Nemhauser \(l977a\). Location of Bank Accounts to Optimize Float\: An  
Analytic Study of Exact and Approximate Algorithms, Management Science 23, 789-810.  
G. Cornuejols, M. L. Fisher, and G. L. Nemhauser \(1977b\). On the Uncapacitated Location Problem, Annals of  
Discrete Mathematics 1, 163-177.  
G. Cornuejols, J. Fonlupt, and D. Naddef \(1985\). The Traveling Salesman Problem on A Graph and Some  
Related Integer Polyhedra, Mathematical Programming 33,1-27.  
G. Cornuejols and D. Hartvigsen \(1986\). An Extension of Matching Theory, Journal of Combinatorial Theory  
B40, 285-296.  
G. Cornuejols, D. Naddef, and W. R. Pulleyblank \(1983\). Halin Graphs and the Traveling Salesman Problem,  
Mathematical Programming 26, 287-294.  
G. Cornuejols and G. L. Nemhauser \(1978\). Tight Bounds for Christo fides' Traveling Salesman Heuristic,  
Mathematical Programming 14, 116-121.  
G. Cornuejols, G. L. Nemhauser, and L. A. Wolsey \(I980a\). A Canonical Representation of Simple Plant  
Location Problems and its Applications, SIAM Journal on Algebraic and Discrete Methods 1, 261-272.  
G. Cornuejols, G. L. Nemhauser and L. A. Wolsey \(1980b\). Worst Case and Probabilistic Analysis of Algorithms  
for a Location Problem, Operations Research 28,847 -858.  
G. Cornuejols, G. L. Nemhauser, and L. A. Wolsey \(1984\). The Uncapacitated Facility Location Problem,  
Report No. 605, Operations Research and Industrial Engineering, Cornell University \(to appear in Francis  
and Mirchandini\).  
G. Cornuejols and W. R. Pulleyblank \(1982\). The Travelling Salesman Polytope and \{O, 2\}-Matchings, Annals of  
Discrete Mathematics 16, 27 -55. References 727  
G. Cornuejols and W. R. Pulleyblank \(1983\). Critical Graphs, Matchings and Tours, or a Hierarchy of  
Relaxations for the Traveling Salesman Problem, Combinatorica 3, 35-52.  
G. Cornuejols and A. Sassano \(1986\). On the 0,1 Facets of the Set Covering Polytope, Report 153, Instituto di  
Analisi dei Sistemi ed Informatica del C.N.R., Rome.  
G. Cornuejols and J. M. Thizy \(1982a\). A Primal Approach to the Simple Plant Location Problem, SIAM  
Journal on Algebraic and Discrete Methods 3,504-510.  
G. Cornuejols and J. M. Thizy \(1982b\). Some Facets of the Simple Plant Location Polytope, Mathematical  
Programming 23, 50-74.  
Y. Crama \(1986\). Recognition Problems for Special Classes of Pseudo-Boolean Functions, RUTCOR, Rutgers  
University.  
G. A. Croes \(1958\). A Method for Solving Traveling Salesman Problems, Operations Research 6, 791-812.  
H. P. Crowder and E. L. Johnson \(1973\). Use of Cyclic Group Methods in Branch and Bound, in Mathematical  
Programming, T. C. Hu and S. M. Robinson, eds., Academic Press, pp. 213-216.  
H. P. Crowder, E. L. Johnson, and M. W. Padberg \(1983\). Solving Large-Scale Zero-One Linear Programming  
Problems, Operations Research 31, 803"\:834.  
H. P. Crowder and M. W. Padberg \(1980\). Solving Large-Scale Symmetric Traveling Salesman Problems to  
Optimality, Management Science, 26, 495-509.  
F. H. Cullen, J. J. Jarvis, and H. D. Ratliff\(1981\). Set Partitioning Heuristics for Interactive Routing, Networks  
11,125-143.  
W. H. Cunningham \(1977\). An Unbounded Matroid Intersection Polyhedron, Linear Algebra and Its Applica-  
tions 16, 209-215.  
W. H. Cunningham \(1983\). A Class of Linear Programs Convertible to Network Problems, Operations Research  
32,387-391.  
W. H. Cunningham \(1984\). Testing Membership in Matroid Polyhedra, Journal of Combinatorial Theory 36B,  
161-188.  
W. H. Cunningham \(1985\). On Submodular Function Minimization, Combinatorica 5, 185-192.  
W. H. Cunningham \(1986\). Improved Bounds for Matroid Partition and Intersection Algorithms, SIAM Journal  
on Computing 15, 948-957.  
W. Cunningham and A. Frank \(1985\). A Primal-Dual Algorithm for Submodular Flows, Mathematics of  
Operations Research 10,251-262.  
W. H. Cunningham and A. B. Marsh III \(1978\). A Primal Algorithm for Optimum Matching, Mathematical  
Programming Study 8, 50-72.  
R. J. Dakin \(1965\). A Tree Search Algorithm for Mixed Integer Programming Problems, Computer Journal 8,  
250-255.  
G. B. Dantzig \(1957\). Discrete-Variable Extremum Problems, Operations Research 5,266-277.  
G. B. Dantzig \(1959\). Note on Solving Linear Programs in Integers, Naval Research Logistics Quarterly 6,  
75-76.  
G. B. Dantzig \(1960\). On the Significance of Solving Linear Programming Problems with Some Integer  
Variables, Econometrica 28, 30-44.  
G. B. Dantzig \(1963\). Linear Programming and Extensions, Princeton University Press.  
G. B. Dantzig and D. R. Fulkerson \(1956\). On the Max-Flow Min-Cut Theorem of Networks, in Linear  
Inequalities and Related Systems, H. W. Kuhn and A. W. Tucker, eds., Princeton University Press,  
pp. 215-221.  
G. B. Dantzig, D. R. Fulkerson, and S. M. Johnson \(1954\). Solution of a Large-Scale Traveling Salesman  
Problem, Operations Research 2, 393-410.  
G. B. Dantzig, D. R. Fulkerson, and S. M. Johnson \(1959\). On a Linear-Programming, Combinatorial Approach  
to the Traveling Salesman Problem, Operations Research 7, 58-66.  
G. B. Dantzig, and P. Wolfe \(1960\). Decomposition Principle for Linear Programs, Operations Research 8,  
101-111.  
M. Davis and H. Putnam \(1960\). A Computing Procedure for Quantification Theory, Journal of the Association  
for Computing Machinery 7,201-215.  
G. de Ghellinck and J. P. Vial \(1986\). A Polynomial Newton Method for Linear Programming, Alg\~rithmica 1,  
425-453.  
G. de Ghellinck and J. P. Vial \(1987\). An Extension of Karmarkar's Algorithm for Solving a System of Linear  
Homogeneous Equations on the Simplex, Mathematical Programming 39,79-92. 728 References  
E. V. Denardo \(1982\). Dynamic Programming Models and Applications, Prentice-Hall.  
U. Derigs \(1985\). Postoptimal Analysis for Matching Problems, Methods of Operations Research 49, 215-221.  
U. Derigs \(1986\). Solving Matching Problems Efficiently\: A New Primal Approach, Networks 16, 1-16.  
E. W. Dijkstra \(1959\). A Note on Two Problems in Connection with Graphs, Numerische Mathematik 1,  
269-271.  
G. A. Dirac \(1961\). On Rigid Circuit Graphs, Ahh. Math. Sem., Univ. Hamburg 25, pp. 71-76.  
G. Dobson \(1982\). Worst-Case Analysis of Greedy Heuristics for Integer Programming with Nonnegative Data,  
Mathematics of Operations Research 7, 515-531.  
P. D. Domich, R. Kannan, and L. E. Trotter \(1987\). Hermite Normal Form Computation Using Modulo  
Determinant Arithmetic, Mathematics of Operations Research 12,50-59.  
S. E. Dreyfus and A. M. Law \(1977\). The Art and Theory of Dynamic Programming, Academic Press.  
N. J. Driebeek \(1966\). An Algorithm for the Solution of Mixed Integer Programming Problems, Management  
Science 12,576-587.  
P. Duchet \(1984\). Classical Perfect Graph Conjecture on Special Graphs-A Survey, Annals of Discrete  
Mathematics 21, 67-96.  
M. E. Dyer \(1984\), An O\(n\) Algorithm for the Multiple Choice Knapsack LinearProgram, Mathematical  
Programming 29,57-63.  
W. L. Eastman \(1958\). Linear Programming with Pattern Constraints, Ph.D. Thesis, Harvard University.  
J. Edmonds \(1965a\). Paths, Trees and Flowers, Canadian Journal of Mathematics 17,449-467.  
J. Edmonds \(1965b\). Minimum Partition of a Matroid into Independent Subsets, Journal of Research of the  
National Bureau of Standards 69B, 67 - 72.  
J. Edmonds \(1965c\), Maximum Matching and a Polyhedron with 0-1 Vertices, Journal of Research of the  
National Bureau of Standards 69B, 125-130.  
J. Edmonds \(1967a\). Systems of Distinct Representatives and Linear Algebra, Journal of Research of the National  
Bureau of Standards 71B, 241-245.  
J. Edmonds \(1967b\). Optimum Branchings, Journal of Research of the National Bureau of Standards 71B,  
233-240.  
J. Edmonds \(1970\). Submodular Functions, Matroids and Certain Polyhedra, in Combinatorial Structures and  
Their Applications, R. Guy et al., eds., Gordon and Breach, pp. 69-87.  
J. Edmonds \(1971\). Matroids and the Greedy Algorithm, Mathematical Programming 1, 127 -136.  
J. Edmonds \(1973\). Edge-Disjoint Branchings, in Combinatorial Algorithms, R. Rustin, ed., Academic Press,  
pp.91-96.  
J. Edmonds \(1979\). Matroid Intersection, Annals of Discrete Mathematics 4,39-49.  
J. Edmonds and D. R. Fulkerson \(1965\). Transversals and Matroid Partition, Journal of Research of the National  
Bureau of Standards **69B,** 147-153.  
J. Edmonds and D. R. Fulkerson \(1970\). Bottleneck Extrema, Journal of Combinatorial Theory 8, 299-306.  
J. Edmonds and R. Giles \(1977\). A Min-Max Relation for Submodular Functions on Graphs, Annals of Discrete  
Mathematics 1, 185-204.  
J. Edmonds and R. Giles \(1984\). Total Dual Integrality of Linear Inequality Systems, in Pulleyblank,  
pp.117-129.  
J. Edmonds and E. L. Johnson \(1970\). Matching\: A Well-Solved Class of Integer Linear Programs, in Proceed-  
ings of the Calgary International Conference on Combinatorial Structures and Their Applications, R. K. Guy  
et al. eds., Gordon and Breach, pp. 89-92.  
J. Edmonds and E. L. Johnson \(1973\). Matching, Euler Tours and the Chinese Postman, Mathematical  
Programming 5, 88-124.  
J. Edmonds and R. M. Karp \(1972\). Theoretical Improvements in Algorithmic Efficiency for Network Flow  
Problems, Journal of the Associationfor Computing Machinery 19, 248-264.  
M. A. Efroymson and T. L. Ray \(1966\). A Branch-and-Bound Algorithm for Plant Location, Operations  
Research 14,361-368.  
G. D. Eppen and R. K. Martin \(1988\). Solving Multi-Item Capacitated Lot-Sizing Problems Using Variable  
Redefinition, to appear in Operations Research.  
D. Erlenkotter \(1978\). A Dual-Based Procedure for Uncapacitated Facility Location, Operations Research 26,  
992-1009. References 729  
J. Etcheberry \(1977\). The Set Covering Problem\: A New Implicit Enumeration Algorithm, Operations Research  
25, 760-772.  
S. Even and O. Kariv \(1975\). An O\( n2  
.  
5\) Algorithm for Maximum Matching in General Graphs, Proceedings of  
the 16th Annual Symposium on Foundations of Computer Science, *100-112.*  
H. Everett III \(1963\). Generalized Lagrange Multiplier Method for Solving Problems of Optimum Allocation of  
Resources, Operations Research 11, 399-417.  
M. Farber \(1983\). Characterization of Strongly Chordal Graphs, Discrete Mathematics 43, 173-189.  
M. Farber \(1984\). Domination, Independent Domination and Duality in Strongly Chordal Graphs, Discrete  
Applied Mathematics 7, 115-130.  
D. Fayard and G. Plateau \(1975\). Resolution of the 0-1 Knapsack Problem\: Comparison of Methods, Mathemat-  
ical Programming 3,272-307.  
D. Fayard and G. Plateau \(1982\). An Algorithm for the Solution of the 0-1 Knapsack Problem, Computing 28,  
269-287.  
M. L. Fisher \(1973\). Optimal Solution of Scheduling Problems Using Lagrange Multipliers\: Part I, Operations  
Research 21, 1114-1127.  
M. L. Fisher \(1976\). A Dual Algorithm for the One-Machine Scheduling Problem, Mathematical Programming  
11,229-251.  
M. L. Fisher \(1980\). Worst-Case Analysis of Heuristic Algorithms, Management Science 26, 1-18.  
M. L. Fisher \(1981\). The Lagrangian Relaxation Method for Solving Integer Programming Problems, Manage-  
ment Science 27,1-18.  
M.L. Fisher \(1985\). Interactive Optimization, Annals ojOperations Research 4, 541-556.  
M. L. Fisher, A. Greenfield, R. Jaikumar, and J. T. Lester \(1982\). A Computerized Vehicle Routing Application,  
Interfaces 12,42-52.  
M. L. Fisher and D. S. Hochbaum \(1980\). Probabilistic Analysis of the Planar K-Median Problem, Mathematics  
of Operations Research 5, 27-34.  
M. L. Fisher and R. Jaikumar \(1981\). A Generalized Assignment Heuristic for Vehicle Routing, Networks *11,*  
109-124.  
M. L. Fisher, R. Jaikumar, and L. N. Van Wassenhove \(1986\). A Multiplier Adjustment Method for the  
Generalized Assignment Problem, Management Science 32, 1095-1103.  
M. L. Fisher, B. J. Lageweg, J. K. Lenstra, and A. H. G. Rinnooy Kan \(1983\). Surrogate Duality Relaxation for  
Job Shop Scheduling, Discrete Applied Mathematics 5, 65-76.  
M. L. Fisher, G. L. Nemhauser and L. A. Wolsey \(1978\). Analysis of Approximation Algorithms for Maximizing  
a Submodular Set Function II, Mathematical Programming Study 8,73-87.  
M. L. Fisher, G. L. Nemhauser, and L. A. Wolsey \(1979\). An Analysis of Approximations for Finding a  
Maximum Weight Hamiltonian Circuit, Operations Research 27, 799-809.  
M. L. Fisher, W. D. Northup, and J. F. Shapiro \(1975\). Using Duality to Solve Discrete Optimization Problems\:  
Theory and Computational Experience, Mathematical Programming Study 3, 56-94.  
M. L. Fisher and J. F. Shapiro \(1974\). Constructive Duality in Integer Programming, SIAM Journal on Applied  
Mathematics 27, 31-52.  
M. L. Fisher and L. A. Wolsey \(1982\). On the Greedy Heuristic for Covering and Packing Problems, SIAM  
Journal on Algebraic and Discrete Methods 3, 584-591.  
L. R. Ford and D. R. Fulkerson \(1956\). Maximal flow Through a Network, Canadian Journal of Mathematics  
8,399-404.  
L. R. Ford, Jr. and D. R. Fulkerson \(1962\). Flows in Networks, Princeton University Press.  
J. J. H. Forrest, J. P. H. Hirst, and J. A. Tomlin \(1974\). Practical Solution of Large Mixed Integer Programming  
Problems with UMPIRE, Management Science 20,736-773.  
B. L. Fox, J. K. Lenstra, A. H. G. Rinnooy Kan, and L. E. Schrage \(1978\). Branching from the Largest Upper  
Bound\: Folklore and Facts, European Journal of Operations Research 2, 191-194.  
R. L. Francis and P. Mirchandani, eds. \(1988\). Discrete Location Theory, Wiley.  
A. Frank \(1975\). Some Polynomial Algorithms for Certain Graphs and Hypergraphs, Proceedings of the 5th  
British Combinatorial Conference, pp. 211-226.  
A. Frank \(1981\). A Weighted Matroid Intersection Theorem, Journal of Algorithms 2, 328-336.  
A. Frank \(1982\). An Algorithm for Submodular Functions on Graphs, Annals of Discrete Mathematics 16,  
97-120. 730 References  
A. Frank \(1984\). Submodular Flows, in Pulleyblank, pp. 147-166.  
A. Frank and E. Tardos \(1987\). An Application of Simultaneous Approximation in Combinatorial Optimiza-  
tion, Combinatorica 7,49-66.  
A. M. Frieze \(1974\). A Cost Function Property for Plant Location Problems, Mathematical Programming 7,  
245-248.  
A. M. Frieze \(1976\). Shortest Path Algorithms for Knapsack Type Problems, Mathematical Programming 11,  
150-157.  
A. M. Frieze \(1986\). On the Lagarias-Odlyzko Algorithm for the Subset Sum Problem, SIAM Journal on  
Computing 15, 536-540.  
A. M. Frieze and M. R. B. Clarke \(1984\). Approximation Algorithm for the m-Dimensional 0-1 Knapsack  
Problem\: Worst Case and Probabilistic Analyses, European Journal 0/ Operations Research 15, 100-109.  
A. M. Frieze, G. Galbiati, and E Maffioli \(1982\). On the Worst-Case Performance of Some Algorithms for the  
Asymmetric Traveling Salesman Problem, Networks 12,23-39.  
K. R. Frisch \(1955\). The Logarithmic Potential Method of Convex Programming, Institute of Economics,  
University of Oslo.  
M. Fujii, T. Kasami, and K. Ninomiya \(1969\). Optimal Sequencing of Two Equivalent Processors, SIAM Journal  
0/ Applied Mathematics 17, 784-789.  
S. Fujishige \(1986\). A Capacity-Rounding Algorithm for the Minimum Cost Circulation Problem\: A Dual  
Framework of the Tardos Algorithm, Mathematical Programming 35,298-308.  
D. R. Fulkerson \(1968\). Networks, Frames, Blocking Systems, in Mathematics a/the Decision Sciences\: Part 1,  
G. E Dantzig and A. E Veinott, Jr., eds., American Mathematical Society, pp. 303-334.  
D. R. Fulkerson \(1970a\). Blocking Polyhedra, in Graph Theory and Its Applications, B. Harris, ed., Academic  
Press, pp. 93-112.  
D. R. Fulkerson \(l970b\). The Perfect Graph Conjecture and Pluperfect Graph Theorem, in Proceedings o/the  
Second Chapel Hill Conference on Combinatorial Mathematics and Its Applications, R. C. Bose et al., eds.,  
University of North Carolina Press, pp. 171-175.  
D. R. Fulkerson \(1971\). Blocking and Antiblocking Pairs of Polyhedra, Mathematical Programming 1, 168-194.  
D. R. Fulkerson \(1972\). Antiblocking Polyhedra, Journal o/Combinatorial Theory B12, 56-71.  
D. R. Fulkerson \(1973\). On the Perfect Graph Theorem, Mathematical Programming, T. C. Hu and S. M.  
Robinson, eds., Academic Press, pp. 69-76.  
D. R. Fulkerson \(1974\). Packing Rooted Directed Cuts in a Weighted Directed Graph, Mathematical Program-  
ming 6, 1-13.  
D. R. Fulkerson and D. A. Gross \(1965\). Incidence Matrices and Interval Graphs, Pacific Journal o/Mathemat-  
ics 15, 833-835.  
D. R. Fulkerson, A. J. Hoffman, and R. Oppenheim \(1974\). On Balanced Matrices, Mathematical Programming  
Study 1,120-132.  
D. R. Fulkerson, G. L. Nemhauser, and L. E. Trotter, Jr. \(1974\). Two Computationally Difficult Set Covering  
Problems that Arise in Computing the I-Width ofIncidence Matrices of Steiner Triple Systems, Mathemati-  
cal Programming Study 2, 72-81.  
D. R. Fulkerson and D. B. Weinberger \(1975\). Blocking Pairs of Polyhedra Arising from Network Flows, Journal  
o/Combinatorial Theory B18, 265-283.  
M. L. Furst and R. Kannan \(1987\). Succinct Certificates for Almost All Subset Sum Problems, Computer  
Science Department, Carnegie-Mellon University.  
H. P. Gabow, Z. Galil, T. Spencer, and R. E. Tarjan \(1986\). Efficient Algorithms For Finding Minimum  
Spanning Trees in Undirected and Directed Graphs, Combinatorica 6, 109-122.  
H. P. Gacs and L. Lovasz \(1981\). Khachiyan's Algorithm for Linear Programming, Mathematical Programming  
Study 14, 61-68  
D. Gale \(1968\). Optimal Assignments in an Ordered Set\: An Application of Matroid Theory, Journal 0/  
Combinatorial Theory 4, 176-180.  
G. Gallo and S. Pallottino \(1986\). Shortest Path Methods\: A Unifying Approach, Mathematical Programming  
Study 26, 38-64.  
M. R. Garey and D. S. Johnson \(1979\). Computers and Intractibility\: A Guide to the Theory 0/ J\{\{fP-Complete-  
ness, Freeman. References 731  
R. S. Garfinkel \(1973\). On Partitioning the Feasible Set in a Branch-and-Bound Algorithm for the Asymmetric  
Traveling-Salesman Problem, Operations Research 21,340-343.  
R. S. Garfinkel \(1979\). Branch and Bound Methods for Integer Programming, in Christofides, Mingozzi et aI.,  
pp. 1-20.  
R. S. Garfinkel, A. W. Neebe, and M. R. Rao \(1974\). An Algorithm for the M-median Plant Location Problem,  
Transportation Science 8, 217 -236.  
R. S. Garfinkel and G. L. Nemhauser \(1969\). The Set Partitioning Problem\: Set Covering with Equality  
Constraints, Operations Research 17,848-856.  
R. S. Garfinkel and G. L. Nemhauser \(1972a\). Integer Programming, Wiley.  
R. S. Garfinkel and G. L. Nemhauser \(1972b\). Optimal Set Covering\: A Survey, in Perspectives On Optimiza-  
tion\: A Collection of Expository Articles, A. M. Geoffrion, ed., pp. 164-183.  
R. S. Garfinkel and G. L. Nemhauser \(1973\). A Survey ofInteger Programming Emphasizing Computation and  
Relations Among Models, in Mathematical Programming, T. C. Hu and S. M. Robinson, eds., Academic  
Press, pp. 77-155.  
S. Gass \(1975\). Linear Programming, 4th ed. McGraw-Hill.  
G. Gastou and E. L. Johnson \(1986\). Binary Group and Chinese Postman Polyhedra, Mathematical Program-  
ming 34, 1-33.  
J. M. Gauthier and G. Ribiere \(1977\). Experiments in Mixed-Integer Programming Using Pseudo-Costs,  
Mathematical Programming 12, 26-47.  
F. Gavril \(1972\). Algorithms for Minimum Coloring, Maximum Clique, Minimum Covering by Cliques, and  
Maximum Independent Set of a Chordal Graph, SIAM Journal on Computing 1, 183-191.  
A. M. Geoffrion \(1967\). Integer Programming by Implicit Enumeration and Balas Method, SIAM Review 9,  
178-190.  
A. M. Geoffrion \(1969\). An Improved Implicit Enumeration Approach for Integer Programming, Operations  
Research 17,437 -454.  
A. M. Geoffrion \(1970\). Primal Resource-Directive Approaches for Optimizing Nonlinear Decomposable  
Systems, Operations Research 18,375-403.  
A. M. Geoffrion \(1972\). Generalized Benders Decomposition, Journal of Optimization Theory and Applications  
10, 237-260.  
A. M. Geoffrion \(1974\). Lagrangean Relaxation for Integer Programming, Mathematical Programming Study 2,  
82-114.  
A. M. Geoffrion \(1976\). A Guided Tour of Recent Practical Advances in Integer Linear Programming, Omega 4,  
49-57.  
A. M. Geoffrion and G. Graves \(1974\). Multicommodity Distribution System Design by Benders Decomposi-  
tion, Management Science 20, 822-844.  
A. M. Geoffrion and R. E. Marsten \(1972\). Integer Programming Algorithms\: A Framework and State-of-the-  
Art Survey, Management Science 18, 465-491.  
A. M. Geoffrion and R. McBride \(1978\). Lagrangian Relaxation Applied to Capacitated Facility Location  
Problems, AIlE Transactions 10,40-47.  
A. M. Geoffrion and R. Nauss \(1977\). Parametric and Postoptimality Analysis in Integer Linear Programming,  
Management Science 18, 453-466.  
A. M. Gerards and A. Schrijver \(1986\). Matrices with the Edmonds-Johnson Property, Combinatorica 6,  
365-379.  
A. Ghouila-Houri \(1962\). Caracterisation des Matrices Totalement Unimodulaires, C.R. Academy of Sciences of  
Paris 254, 1192-1194.  
R. Giles \(1978\). A Balanced Hypergraph Defined by Certain Subtrees ofa Tree, ARS Combinatoria 6, 179-183.  
R. Giles and W R. Pulleyblank \(1979\). Total Dual Integrality and Integral Polyhedra, Linear Algebra and Its  
Applications 25, 191-196.  
R. Giles and L. E. Trotter, Jr. \(1981\). On Stable Set Polyhedra for K1.3-Free Graphs, Journal of Combinatorial  
Theory B31, 313-316.  
P. C. Gilmore and R. E. Gomory \(1966\). The Theory and Computation of Knapsack Functions, Operations  
Research 14, 1045-1074.  
P. C. Gilmore, E. L. Lawler, and D. B. Shmoys \(1985\). Well-Solved Special Cases, in Lawler, Lenstra et aI.,  
pp.87-144. 732 References  
E Glover \(1965\). A Multiphase-Dual Algorithm for the Zero-One Integer Programming Problem, Operations  
Research 13,879-919.  
E Glover \(1967\). A Pseudo Primal-Dual Integer Programming Algorithm, Journal of Research of the National  
Bureau of Standards 71B, 187-195.  
E Glover \(1968a\). A New Foundation for a Simplified Primal Integer Programming Algorithm, Operations  
Research 16,727-740.  
E Glover \(1968b\). Surrogate Constraints, Operations Research 16, 741-749.  
E Glover \(1968c\). A Note on Integer Programming and Integer Feasibility, Operations Research 16,1212-1216.  
E Glover \(1969\). Integer Programming over a Finite Additive Group, SIAM Journal of Control 7, 213-231.  
E Glover \(1975\). Surrogate Constraint Duality in Mathematical Programming, Operations Research 23,  
434-451.  
E Glover \(1985\). Future Paths for Integer Programming and Links to Artificial Intelligence, Report 85-8. Center  
for Applied Artificial Intelligence, University of Colorado.  
E Glover, D. Karney, and D. Klingman \(1974\). Implementation and Computational Comparisons of Primal,  
Dual and Primal-Dual Computer Codes for Minimum Cost Network Flow Problems, Networks 4,191-212.  
J. L. Goffin \(1977\). On the Convergence Rates of Sub gradient Optimization Methods, Mathematical Program-  
ming 13, 329-347.  
B. L. Golden and A. A. Assad \(1986\). Perspectives on Vehicle Routing, Exciting New Developments, Operations  
Research 34, 803-810.  
B. L. Golden, L. D. Boden, T. Doyle, and W R. Stewart \(1980\). Approximate Traveling Salesman Algorithms,  
Operations Research 28, 694- 711.  
B. L. Golden and W R. Stewart \(1985\). Empirical Analysis of Heuristics, in Lawler, Lenstra et aI., pp. 207 -250.  
M. C. Golumbic \(1980\). Algorithmic Graph Theory and Perfect Graphs, Academic Press.  
R. E. Gomory \(1958\). Outline of an Algorithm for Integer Solutions to Linear Programs, Bulletin of the  
American Mathematical Society 64,275-278.  
R. E. Gomory \(1960a\). Solving Linear Programming Problems in Integers, in Combinatorial Analysis, R. E.  
Bellman and M. Hall, Jr., eds., American Mathematical Society, pp. 211-216.  
R. E. Gomory \(1960b\). An Algorithm for the Mixed Integer Problem, RM-2597, The Rand Corporation.  
R. E. Gomory \(1963a\). An Algorithm for Integer Solutions to Linear Programs, in Recent Advances in  
Mathematical Programming, R. Graves and P. Wolfe, eds., McGraw-Hill, pp. 269-302.  
R. E. Gomory \(1963b\). An All-Integer Programming Algorithm, in Industrial Scheduling, J. E Muth and G. I.  
Thompson, eds., Prentice-Hall, pp. 193-206.  
R. E. Gomory \(1965\). On the Relation between Integer and Non-Integer Solutions to Linear Programs,  
Proceedings of the National Academy of Science 53.260-265.  
R. E. Gomory \(1967\). Faces of an Integer Polyhedron, Proceedings of the NationalAcademyofScience 57,16-18.  
R. E. Gomory \(1969\). Some Polyhedra Related to Combinatorial Problems, Linear Algebra and Its Applications  
2,451-558.  
R. E. Gomory \(1970\). Properties of a Class of Integer Polyhedra, in Integer and Nonlinear Programming, J.  
Abadie, ed., North-Holland, pp. 353-365.  
R. E. Gomory and A. J. Hoffman \(1963\). On the Convergence of an Integer-Programming Process, Naval  
Research Logistics Quarterly 10, 121-123.  
R. E. Gomory and T. C. Hu \(1961\). Multi-Terminal Network Flows, SIAM Journal 9, 551-570.  
R. E. Gomory and E. L. Johnson \(1972\). Some Continuous Functions Related to Corner Polyhedra, Mathemati-  
cal Programming 3,23-85.  
R. E. Gomory and E. L. Johnson \(1973\). The Group Problem and Subadditive Functions, in Mathematical  
Programming, T. C. Hu and S. M. Robinson, eds., Academic Press, pp. 157-184.  
M. Gondran and M. Minoux \(1984\). Graphs and Algorithms, Wiley-Interscience.  
G. A. Gorry, W D. Northup, and J. E Shapiro \(1973\). Computational Experience with a Group Theoretic  
Integer Programming Algorithm, Mathematical Programming 4, 171-192.  
G. A. Gorry and J. E Shapiro \(1971\). An Adaptive Group Theoretic Algorithm for Integer Programming  
Problems, Management Science 7,285-306.  
G. A. Gorry, J. E Shapiro, and L. A. Wolsey \(1972\). Relaxation Methods for Pure and Mixed Integer Program-  
ming Problems, Management Science 18, 229-239. References 733  
R. L. Graham \(1966\). Bounds for Certain Multiprocessing Anomalies, Bell System Technical Journal 45,  
1563-1581.  
S. C. Graves \(1982\). Using Lagrangean Techniques to Solve Hierarchial Production Planning Problems,  
Management Science 28, 260-274.  
P. Gray \(1971\). Exact Solution of the Fixed-Charge Transportation Problem, Operations Research 19,1529-1538.  
H. Greenberg \(1971\). Integer Programming, Academic Press.  
H. J. Greenberg and W. P. Pierskalla \(1970\). Surrogate Mathematical Programming, Operations Research 18,  
924-939.  
G. R. Grimmett and W. R. Pulleyblank \(1985\). Random Near-Regular Graphs and the Node Packing Problem,  
Operations Research Letters 4, 169-174.  
R. C. Grinold \(1970\). Lagrangean Subgradients, Management Science 17, 185-188.  
R. C. Grinold \(1972\). Steepest Ascent for Large Scale Linear Programs, SIAM Review 14,447 -464.  
M. Grotschel \(1980a\). On the Symmetric Travelling Salesman Problem\: Solution of a 120-City Problem,  
Mathematical Programming Study 12, 61-77.  
M. Grotschel \(1980b\). On the Monotone Symmetric Travelling Salesman Problem\: Hypohamiltonian/Hypo-  
traceable Graphs and Facets, Mathematics o/Operations Research 5,285-292.  
M. Grotschel \(1984\). Developments in Combinatorial Optimization, in Perspectives in Mathematics\: Anniver-  
sary o/Oberwolfach 1984, W. Jager, J. Moser, and R. Remmert, eds., Birkhauser, pp. 249-294.  
M. Grotschel \(1985\). Polyhedral Combinatorics, in O'hEigeartaigh et aI., pp. 1-10.  
M. Grotschel and O. Holland \(1985\). Solving Matching Problems with Linear Programming, Mathematical  
Programming 33,243-259.  
M. Grotschel, M. Junger, and G. Reinelt \(1984\). A Cutting Plane Algorithm for the Linear Ordering Problem,  
Operations Research 32, 1195-1220.  
M. Grotschel, M. Junger and G. Reinelt \(1985a\). On the Acyclic Subgraph Polytope, Mathematical Program-  
ming 33,1-27.  
M. Grotschel, M. Junger, and G. Reinelt \(1985b\). Facets of the Linear Ordering Polytope, Mathematical  
Programming 33, 43-60.  
M. Grotschel, L. Lovasz, and A. Schrijver \(1981\). The Ellipsoid Method and Its Consequences in Combinatorial  
Optimization, Combinatorica 1, 169-197.  
M. Grotschel, L. Lovasz, and A. Schrijver \(1984a\). Polynomial Algorithms for Perfect Graphs, Annals o/Discrete  
Mathematics 21, 325-356.  
M. Grotschel, L. Lovasz, and A. Schrijver \(l984b\). Geometric Methods in Combinatorial Optimization, in  
Pulleyblank \(1984\), pp. 167-184.  
M. Grotschel, L. Lovasz, and A. Schrijver \(1984c\). Corregendum to Our Paper "The Ellipsoid Method and Its  
Consequences in Combinatorial Optimization", Combinatorica 4, 291-295.  
M. Grotschel, L. Lovasz and A. Schrijver \(1988\). Geometric Algorithms and Combinatorial Optimization,  
Springer.  
M. Grotschel and M. W. Padberg \(1975\). Partial Linear Characterizations of the Asymmetric Traveling Salesman  
Polytope, Mathematical Programming 8,378-381.  
M. Grotschel and M. W. Padberg \(1979a\). On the Symmetric Travelling Salesman Problem I\: Inequalities,  
Mathematical Programming 16,265-280.  
M. Grotschel and M. W. Padberg \(1979b\). On the Symmetric Travelling Salesman Problem II\: Lifting Theorems  
and Facets, Mathematical Programming 16, 281-302.  
M. Grotschel and M. W. Padberg \(1985\). Polyhedral Theory, in Lawler, Lenstra et al., pp. 251-302.  
M. Grotschel and W. R. Pulleyblank \(1986\). Clique Tree Inequalities and the Symmetric Travelling Salesman  
Problem, Mathematics o/Operations Research 11,537-569.  
M. Grotschel and Y. Wakabayashi \(1981a\). On the Structure of the Monotone Asymmetric Travelling Salesman  
Polytope I\: Hypohamiltonian Facets, Discrete Mathematics 34, 43-59.  
M. Grotschel and Y. Wakabayashi \(1981b\). On the Structure of the Asymmetric Travelling Salesman Polytope II\:  
Hypotraceable Facets, Mathematical Programming Study 14, 77-97.  
B. Grunbaum \(1967\). Convex Polytopes, Wiley.  
M. Guignard \(1980\). Fractional Vertices, Cuts and Facets ofthe Simple Plant Location Problem, Mathematical  
Programming 12, 150-162. 734 References  
M. Guignard \(1982\). Preprocessing and Optimization in Network Flow Problems with Fixed Charges, Methods  
of Operations Research 45,235-256.  
M. Guignard and K. Spielberg \(1977\). Reduction Methods for State Enumeration Integer Programming, Annals  
of Discrete Mathematics 1, 273-286.  
M. Guignard and K. Spielberg \(1981\), Logical Reduction Methods in Zero-One Programming \(Minimal  
Preferred Variables\), Operations Research 29,49-74.  
G. Gunawardane, S. Hoff, and L. Schrage \(1981\). Identification of Special Structure Constraints in Linear  
Programs, Mathematical Programming 21,90-97.  
G. Hadley \(1962\), Linear Programming, Addison-Wesley.  
B. Hajek \(1985\), A Tutorial Survey of Theory and Applications of Simulated Annealing, Proceedings of the 24th  
IEEE Conference on Decision and Control, 755-760.  
P. R. Halmos \(1959\). Finite Dimensional Vector Spaces, Van Nostrand.  
J. H. Halton and R. Terada \(1982\). A Fast Algorithm for the Euclidean Traveling Salesman Problem, Optimal  
with Probability One, SIAM Journal on Computing 11, 28-46.  
P. L. Hammer, E. L. Johnson, and B. Korte, eds. \(1979a\). Discrete Optimization I \(Annals of Discrete Mathe-  
matics 4\).  
P. L. Hammer, E. L. Johnson, and B. Korte, eds. \(1979b\). Discrete Optimization II \(Annals -of Discrete  
Mathematics 5\).  
P. L. Hammer, E. L. Johnson, B. Korte, and G. L. Nemhauser, eds. \(1977\). Studies in Integer Programming  
\(Annals of Discrete Mathematics 1\).  
P. L. Hammer, E. L. Johnson, and U. N. Peled \(1975\). Facets of Regular 0-1 Polytopes, Mathematical Program-  
ming 8, 179-206.  
P. L. Hammer and S. Rudeanu \(1966\). Boolean Methods in Operations Research and Related Areas, Springer.  
P. Hansen, ed. \(1981\), Studies on Graphs and Discrete Programming \(Annals of Discrete Mathematics 11\).  
R. Hassin \(1982\). Minimum Cost Flow with Set Constraints, Networks 12, 1-21.  
D. Hausmann, ed., \(1978\). Integer Programming and Related Areas\: A Classified Bibliography 1976-1978,  
Springer.  
D. Hausmann, T. A. Jenkins, and B. Korte \(1980\). Worst Case Analysis of Greedy Algorithms for Independence  
Systems, Mathematical Programming Study 12, 120-131.  
D. Hausmann and B. Korte \(1978\). Lower Bounds on the Worst Case Complexity of Some Oracle Algorithms,  
Discrete Mathematics 24, 261-276.  
K. Helbig-Hansen and J. Krarup \(1974\). Improvements of the Held-Karp Algorithm for the Symmetric  
Traveling Salesman Problem, Mathematical Programming 7,87-96.  
M. Held and R. M. Karp \(1970\). The Traveling Salesman Problem and Minimum Spanning Trees, Operations  
Research 18, 1138-1162.  
M. Held and R. M. Karp \(1971\). The Traveling Salesman Problem and Minimal Spanning Trees\: Part II,  
Mathematical Programming 1, 6-25.  
M. Held, P. Wolfe, and H. P. Crowder \(1974\). Validation of Sub gradient Optimization, Mathematical Program-  
ming 6, 62-88.  
I. Heller \(1957\). On Linear Systems with Integral Valued Solutions, Pacific Journal of Mathematics 7,  
1351-1364.  
I. Heller \(1963\). On Unimodular Sets of Vectors, in Recent Advances in Mathematical Programming,  
R. L. Graves and P. Wolfe, eds., McGraw-Hill, pp. 39-53.  
I. Heller and A. J. Hoffman. \(1962\). On Unimodular Matrices, Pacific Journal of Mathematics 12, 1321-1327.  
I. Heller and C. B. Tompkins \(1956\). An Extension of a Theorem ofDantzig, in Linear Inequalities and Related  
Systems, H. W. Kuhn and A. W. Tucker, eds., Princeton University Press, pp. 247-254.  
D. S. Hochbaum \(1982\). Approximation Algorithms for the Set Covering and Vertex Cover Problems, SIAM  
Journal on Computing 11, 555-556.  
D. S. Hochbaum, T. Nishizeki, and D. B. Shmoys \(1986\). A Better than "Best Possible" Algorithm to Edge Color  
Multigraphs, Journal of Algorithms 7, 79-104.  
A. J. Hoffman \(1974\). A Generalization of Max-Flow Min-Cut Theorem, Mathematical Programming 6,  
352-359.  
A. J. Hoffman \(1979\). The Role ofUnimodularity in Applying Linear Inequalities to Combinatorial Theorems,  
Annals of Discrete Mathematics 4,73-84. References 735  
A. J. Hoffman, A. Kolen, and M. Sakarovitch \(1985\). Totally Balanced and Greedy Matrices, SIAM Journal on  
Algebraic and Discrete Methods 6, 721-730.  
A. J. Hoffman and J. B. Kruskal \(1956\). Integral Boundary Points of Convex Polyhedra, in Linear Inequalities  
and Related Systems, H. W. Kuhn and A. W Tucker, eds., Princeton University Press, pp. 223-246.  
A. J. Hoffman and R. Oppenheim \(1978\). Local Unimodularity in the Matching Polytope, Annals oj Discrete  
Mathematics 2,201-209.  
K. Hoffman and M. Padberg \(1985\). LP-based Combinatorial Problem Solving, Annals oJOperations Research  
4,145-194.  
S. Holm and D. Klein \(1984\). Three Methods for Postoptimal Analysis in Integer Linear Programming,  
Mathematical Programming Study 21, 97 -109.  
S. Holm and J. Tind \(1985\). Decomposition in Integer Programming by Superadditive Functions, WP 32-85-86,  
Graduate School ofIndustrial Administration, Carnegie-Mellon University.  
I. Holyer \(1981\). The .N'\~-Completeness of Edge Coloring, SIAM Journal on Computing 10, 718-720.  
1. Hopcroft and R. M. Karp \(1973\). An n2  
.  
5 Algorithm for Maximum Matching in Bipartite Graphs, SIAM  
Journal on Computing 2,223-231.  
W L. Hsu \(1981\). How To Color Claw-Free Perfect Graphs, Annals oj Discrete Mathematics 11, 189-197.  
W L. Hsu \(1984\). The Perfect Graph Conjecture on Special Graphs-A Survey, Annals oJDiscrete Mathematics  
21, 103-114.  
W L. Hsu and G. L. Nemhauser \(1981\). Algorithms for Minimum Coverings by Cliques and Maximum Cliques  
in Claw-Free Perfect Graphs, Discrete Mathematics 37, 181-191.  
W L. Hsu and G. L. Nemhauser \(1982\). A Polynomial Algorithm for the Minimum Weighted Clique Cover  
Problem on Claw-Free Perfect Graphs, Discrete Mathematics 38, 65-71.  
W. L. Hsu and G. L. Nemhauser \(1984\). Algorithms for Maximum Weight Cliques, Minimum Weighted Clique  
Covers and Minimum Colorings of Claw-Free Perfect Graphs, Annals oJDiscrete Mathematics 21, 357 -369.  
T. C. Hu \(1969\). Integer Programming and Network Flows, Addison-Wesley.  
T. C. Hu \(1970\). On the Asymptotic Integer Algorithm, Linear Algebra and Its Applications 3, 279-294.  
H. C. Huang and L. E. Trotter, Jr. \(1980\). A Technique for Determining Blocking and Antiblocking Polyhedral  
Descriptions, Mathematical Programming Study 12,197-205.  
p. Huard \(1967\). Resolution of Mathematical Programming with Nonlinear Constraints by the Method of  
Centers, in Nonlinear Programming, J. Abadie, ed., North-Holland, pp. 209-219.  
T. Ibaraki \(1976\). Theoretical Comparisons of Search Strategies in Branch-and-Bound Algorithms, Journal oj  
Computer and Information Science 5,315-344.  
T. Ibaraki \(1977\). Power of Dominance Relations in Branch-and-Bound Algorithms, Journal oJthe Association  
Jor Computing Machinery 24, 264-279.  
O. H. Ibarra and C. E. Kim \(1975\). Fast Approximation Algorithms for the Knapsack and Sum of Subset  
Problems, Journal oJthe AssociationJor Computing Machinery 22, 463-468.  
Y. Ikura and G. L. Nemhauser \(1985\). Simplex Pivots on the Set Packing Polytope, Mathematical Programming  
33, 123-138.  
Y. Ikura and G. L. Nemhauser \(1986\). Computational Experience with a Polynomial-Time Dual Simplex  
Algorithm for the Transportation Problem, Discrete Applied Mathematics 13,232-248.  
1. P. Ingargiola and J. F. Korsch \(1973\). A Reduction Algorithm for Zero-One Single Knapsack Problems,  
Management Science 20, 460-463.  
M. Iri \(1966\). A Criterion for the Reducibility of a Linear Programming Problem to a Network Flow Problem,  
RAAG Research Notes, Third Series, No. 98.  
M. Iri \(1983\). Applications of Matroid Theory, in Bachem, Grotschel and Korte, pp. 158-201.  
T. A. Jenkins \(1976\). The Efficacy of the Greedy Algorithm, Proceedings oJthe 7th Southeastern Conference on  
-Combinatories, Graph Theory and Computing, F. Hoffman et al., eds., Utilitas Mathematica, pp. 341-350.  
p. A. Jensen and J. W Barnes \(1980\). Network Flow Programming, Wiley.  
R. G. leroslow \(1971\). Comments on Integer Hulls of Two Linear Constraints, Operations Research 19,  
1061-1069.  
R. G. leroslow \(1972\). There Cannot be Any Algorithm for Integer Programming with Quadratic Constraints,  
Operations Research 21, 221-224.  
R. G. leroslow \(1974\). Trivial Integer Programs Unsolvable by Branch-and-Bound, Mathematical Programming  
6, 105-109. 736 References  
R. G. Jeroslow \(1977\). Cutting Plane Theory\: Disjunctive Methods, Annals of Discrete Mathematics 1,  
293-330.  
R. G. Jeroslow \(1978\). Cutting Plane Theory\: Algebraic Methods, Discrete Mathematics 23, 121-150.  
R. G. Jeroslow \(1979a\). An Introduction to the Theory of Cutting Planes, Annals of Discrete Mathematics 5,  
71-95.  
R. G. Jeroslow \(1979b\). Minimal Inequalities, Mathematical Programming 17, 1-15.  
R. G. Jeroslow \(1979c\). The Theory of Cutting-Planes, in Christofides, Mingozzi et aI., pp. 21-72.  
R. G. Jeroslow \(1985\). Representability in Mixed Integer Programming II\: A Lattice of Relaxations, College of  
Management, Georgia Institute of Technology.  
R. G. Jeroslow and K. O. Kortanek \(1971\). On an Algorithm ofGomory, SIAM Journal 21, 55-60.  
R. G. Jeroslow and J. K. Lowe \(1984\). Modelling with Integer Variables, Mathematical Programming Study 22,  
167-184.  
D. S. Johnson \(1974\). Approximation Algorithms for Combinatorial Problems, Journal of Computer and System  
Science 9,256-278.  
D. S. Johnson, A. Demers, J. D. Ullman, M. R. Garey, and R. L. Graham \(1974\). Worst-Case Performance  
Bounds for Simple One-Dimensional Packing Problems, SIAM Journal on Computing, 3, 299-325.  
D. S. Johnson and C. H. Papadimitriou \(1985a\). Computational Complexity, in Lawler, Lenstra et aI.,  
pp.37-86.  
D. S. Johnson and C. H. Papadimitriou \(1985b\). Performance Guarantees for Heuristics, in Lawler, Lenstra  
et aI., pp. 87-144.  
E. L. Johnson \(1973\). Cyclic Groups, Cutting Planes and Shortest Paths, in Mathematical Programming,  
T. C. Hu and S. Robinson, eds., Academic Press, pp. 185-211.  
E. L. Johnson \(1974\). On the Group Problem for Mixed Integer Programming, Mathematical Programming  
Study 2,137-179.  
E. L. Johnson \(1978\). Support Functions, Blocking Pairs, and Anti-Blocking Pairs, Mathematical Programming  
8,167-196.  
E. L. Johnson \(1979\), On the Group Problem and a Subadditive Approach to Integer Programming, Annals of  
Discrete Mathematics 5, 97 -112.  
E. L. Johnson, \(1980a\). Integer Programming-Facets, Subadditivity, and Duality for Group and Semi-Group  
Problems, SIAM Publications.  
E. L. Johnson \(1980b\). Subadditive Lifting Methods for Partitioning and Knapsack Problems, Journal of  
Algorithms 1, 75-96.  
E. L. Johnson \(1981a\). On the Generality of the Subadditive Characterization of Facets, Mathematics of  
Operations Research 6,101-112.  
E. L. Johnson \(1981b\). Characterization of Facets for Multiple Right-Hand Choice Linear Programs, Mathemat-  
ical Programming Study 14, 112-142.  
E. L. Johnson, M. M. Kostreva, and U. H. Suhl \(1985\), Solving 0-1 Integer Programming Problems Arising from  
Large Scale Planning Models, Operations Research 33, 803-819.  
E. L. Johnson and S. Mosterts \(1987\). On Four Problems in Graph Theory, SIAM Journal on Algebraic and  
Discrete Methods 8,163-185.  
E. L. Johnson and M. W. Padberg \(1981\). A Note on the Knapsack Problem with Special Ordered Sets,  
Operations Research Letters 1, 18-22.  
E. L. Johnson and M. W. Padberg \(1983\). Degree-two Inequalities, Clique Facets, and Bipartite Graphs, Annals  
of Discrete Mathematics 16, 169-188.  
E. L. Johnson and U. H. Suhl \(1980\). Experiments in Integer Programming, Discrete Applied Mathematics 2,  
39-55.  
R. Jonker, G. Deleve, J. Vandervelde, and T. Volgenant \(1980\). Rounding Symmetric Traveling Salesman  
Problems with an Asymmetric Assignment Problem, Operations Research 28,623-627.  
R. Jonker, R. Kaas, and T. Volgenant \(1980\). Data-dependent Bounds for Heuristics to find a Minimum Weight  
Hamiltonian Circuit, Operations Research 28, 1219-1221.  
K. O. Jornsten and M. Nasberg \(1986\). A New Lagrangian Relaxation Approach to the Generalized Assignment  
Problem, European Journal of Operations Research 27, 313-323.  
R. Kannan \(1980\). A Polynomial Algorithm for the Two Variable Integer Programming Problem, Journal of the  
Associationfor Computing Machinery 27,118-122. References 737  
R. Kannan \(1983\). Improved Algorithms for Integer Programming and Related Lattice Problems, Proceedings 0/  
the 1983 Symposium on the Theory o/Computing, 193-206.  
R. Kannan \(1987a\). Minkowski's Convex Body Theorem and Integer Programming, Mathematics 0/ Operations  
Research 12,415-440.  
R. Kannan \(1987b\). Algebraic Geometry of Numbers, Report 87453-0R, Institute for Econometrics and  
Operations Research, University of Bonn.  
R. Kannan and A. Bachem \(1979\). Polynomial Algorithms for Computing the Smith and Hermite Normal  
Forms of an Integer Matrix, SIAM Journal on Computing 8,499-507.  
R. Kannan and C. L. Monma \(1978\). On the Computational Complexity of Integer Programming Problems, in  
Optimization and Operations Research, Lecture Notes in Economics and Mathematical Systems 157,  
Springer, pp. 161-172.  
N. Karmarkar \(1984\). A New Polynomial Time Algorithm for Linear Programming, Combinatorica ***4,***  
375-395.  
R. M. Karp \(1971\). A Simple Derivation of Edmonds Algorithm for Optimum Branchings, Networks *1,*  
265-272.  
R. M. Karp \(1972\). Reducibility among Combinatorial Problems, in Complexity 0/ Computer Computations,  
R. E. Miller and J. W. Thatcher, eds., Plenum Press, pp. 85-103.  
R. M. Karp \(1975\). On the Complexity of Combinatorial Problems, Networks 5, 45-68.  
R. M. Karp \(1976\). The Probabilistic Analysis of Some Combinatorial Search Algorithms, in Algorithms and  
Complexity\: New Directions and Recent Results, J. F. Traub, ed., Academic Press, pp. 1-19.  
R. M. Karp \(1977\). Probabilistic Analysis of Partitioning Algorithms for the Traveling Salesman Problem in the  
Plane, Mathematics 0/ Operations Research 2,209-224.  
R. M. Karp \(1979\). A Patching Algorithm for the Nonsymmetric Traveling Salesman Problem, SIAM Journal on  
Computing 8,561-573.  
R. M. Karp, J. K. Lenstra, C. J. H. McDiarmid, and A. H. G. Rinnooy Kan \(1985\). Probabilistic Analysis, in  
O'hEigeartaigh et aI., pp. 52-88.  
R. M. Karp and C. H. Papadimitriou \(1982\). On Linear Characterizations of Combinatorial Optimization  
Problems, SIAM Journal on Computing 11, 620-632.  
R. M. Karp and J. M. Steele \(1985\). Probabilistic Analysis of Heuristics, in Lawler, Lenstra et aI., pp. 207-250.  
M. H. Karwan and R. L. Rardin \(1979\). Some Relationships Between Lagrangian and Surrogate Duality in  
Integer Programming, Mathematical Programming 17, 320-324.  
C. Kastning, ed. \(1976\). Integer Programming and Related Areas, A Classified Bibliography, Lecture Notes in  
Economics and Mathematical Systems 128, Springer.  
J. Kennington and R. V. Helgason \(1980\). Algorithms/or Network Programming, Wiley.  
L. G. Khachian \(1979\). A Polynomial Algorithm in Linear Programming, Soviet Mathematics Doklady 20,  
191-194.  
A. Khintchine \(1930\). Continued Fractions, Noordhoff \(1963\), English translation.  
G. A. P. Kindervater and J. K. Lenstra \(1985\). Parallel Algorithms, in O'hEigeartaigh et aI., pp. 106-128.  
G. A. P. Kindervater and J. K. Lenstra \(1986\). An Introduction to Parallelism in Combinatorial Optimization,  
Discrete Applied Mathematics 14, 135-156.  
S. Kirkpatrick \(1984\). Optimization by Simulated Annealing\: Quantitative Studies, Journal of Statistical Physics  
34,975-986.  
S. Kirkpatrick, C. D. Gelatt, Jr., and M. P. Vecchi \(1983\). Optimization by Simulated Annealing, Science 220,  
671-680.  
V. Klee \(1980\). Combinatorial Optimization\: What Is the State of the Art? Mathematics o/Operations Research  
5,1-26.  
V. Klee and G. J. Minty \(1972\). How Good is the Simplex Algorithm?, in Inequalities 111, O. Shisha, ed.,  
Academic Press, pp. 159-175.  
D. E. Knuth \(1979\). The Art o/Computer Programming, Voll\: Fundamental Algorithms, 2nd ed., Addison-  
Wesley.  
D. E. Knuth \(1981\). The Art o/Computer Programming, Vol. 2\: Seminumerical Algorithms, 2nd ed., Addison-  
Wesley.  
A. Kolen \(1983\). Solving Covering Problems and the Un capacitated Plant Location Problem on Trees, European  
Journal a/Operations Research 12,266-278. 738 References  
A. Kolen, A. H. G. Rinnooy Kan, and H. Trienekeus \(1987\). Vehicle Routing with Time Windows, Operations  
Research 35,266-273.  
P. J. Kolesar \(1967\). A Branch and Bound Algorithm for the Knapsack Problem, Management Science 13,  
723-735.  
B. Korte \(1979\). Approximative Algorithms for Discrete Optimization Problems, Annals of Discrete Mathemat-  
ics 4, 85-120.  
B. Korte and D. Hausmann \(1976\). An Analysis of the Greedy Heuristic for Independence Systems, Annals of  
Discrete Mathematics 2,65-74.  
B. Korte and L. Lovasz \(1984\). Greedoids-A Structural Framework for the Greedy Algorithm, in Pulleyblank,  
pp.221-244.  
B. Korte and R. Schrader \(1980\). On the Existence of Fast Approximation Schemes, in Nonlinear Programming  
4, O. L. Mangasarian, R. R. Meyer, and S. M. Robinson, eds., Academic Press, pp. 415-437.  
J. Krarup and O. Bilde \(1977\). Plant Location, Set Covering, and Economic Lot-Size\: An O\(mn\) Algorithm for  
Structured Problems, in Numerische Methoden bei Optimierungsaufgaben, Band 3\: Optimierung bei Gra-  
phentheoretischen und Ganzzahlligen Problem en , Birkhauser, pp. 155-186.  
J. Krarup and P. M. Pruzan \(1983\). The Simple Plant Location Problem\: Survey and Synthesis, European  
Journal of Operations Research 12, 36-81.  
J. B. Kruskal \(1956\). On the Shortest Spanning Subtree of a Graph and the Traveling Salesman Problem,  
Proceedings of the American Mathematical Society 7, 48-50.  
A. A. Kuehn and M. J. Hamburger \(1963\). A Heuristic Program for Locating Warehouses, Management Science  
9,643-666.  
1. C. Lagarias \(1985\). The Computational Complexity of Simultaneous Diophantine Approximation Problems,  
SIAM Journal on Computing 14, 196-209.  
J. C. Lagarias and A. M. Odlyzko \(1985\). Solving Low-density Subset Sum Problems, Journal of the Association  
for Computing Machinery 32,229-246.  
A. H. Land and A. G. Doig \(1960\). An Automatic Method for Solving Discrete Programming Problems,  
Econometrica 28, 497 -520.  
A. H. Land and S. Powell \(1979\). Computer Codes for Problems of Integer Programming, Annals of Discrete  
Mathematics 5, 221-269.  
G. Laporte, Y. Norbet, and M. Desrochers \(1985\). Optimal Routing under Capacity and Distance Restrictions,  
Operations Research 33, 1050-1073.  
M. Lauriere \(1978\). An Algorithm for the 011 Knapsack Problem, Mathematical Programming 14, 1-10.  
E. L. Lawler \(1972\). A Procedure for Computing the K Best Solutions to Discrete Optimization Problems and Its  
Application to the Shortest Path Problem, Management Science 18,401-405.  
E. L. Lawler \(1975\). Matroid Intersection Algorithms, Mathematical Programming 9,31-56.  
E. L. Lawler \(1976\). Combinatorial Optimization\: Networks and Matroids, Holt, Rinehart and Winston.  
E. L. Lawler \(1979\). Fast Approximation Algorithms for Knapsack Problems, Mathematics of Operations  
Research 4,339-356.  
E. L. Lawler \(1980\). The Great Mathematical Sputnik of 1979, The Sciences, September Issue.  
E. L. Lawler \(1985\). Submodular Functions and Polymatroid Optimization, in O'hEigeartaigh et al., pp. 32-38.  
E. L. Lawler, J. K. Lenstra, A. H. G. Rinnooy Kan, and D. B. Shmoys \(1985\) eds., The Traveling Salesman  
Problem\: A Guided Tour of Combinatorial Optimization, Wiley.  
E. L. Lawler and C. U. Martel \(1982a\). Computing Maximal Polymatroidal Network Flows, Mathematics of  
Operations Research 7,334-347.  
E. L. Lawler and C. U. Martel \(1982b\). Flow Network Formulations of Poly matroid Optimization Problems,  
Annals of Disc rete Mathematics 16, 189-200.  
E. L. Lawler and D. E. Wood \(1966\). Branch-and-Bound Methods\: A Survey, Operations Research 14,699-719.  
A. Lehman \(1979\). On the Width-Length Inequality, Mathematical Programming 16, 245-259; 17, 403-417  
\(with proof corrections\).  
C. E. Lemke, H. M. Salkin, and K. Spielberg \(1971\). Set Covering by Single-Branch Enumeration with Linear  
Programming Subproblems, Operations Research 19,998-1022.  
C. E. Lemke and K. Spielberg \(1967\). Directed Search Algorithms for Zero-One and Mixed-Integer Program-  
ming, Operations Research 15,892-914. References 739  
A. K. Lenstra, H. W. Lenstra, Jr., and L. Lovasz \(1982\). Factoring Polynomials with Rational Coefficients,  
Mathematics Annals 261, 515-534.  
H. W Lenstra, Jr. \(1983\). Integer Programming with a Fixed Number of Variables, Mathematics o/Operations  
Research 8, 538-547.  
H. W Lenstra Jr. \(1984\). Integer Programming and Cryptography, Mathematical Intelligencer 6, 14-21.  
J. K. Lenstra and A. H. G. Rinnooy Kan \(1979\). Computational Complexity of Discrete Optimization Prob-  
lems, Annals of Discrete Mathematics 4, 121-140.  
J. Leung and T. L. Magnanti \(1986\). Valid Inequalities and Facets of the Capacitated Plant Location Problem,  
Working Paper ORI49-86, Operations Research Center, Massachusetts Institute of Technology.  
H. R. Lewis and C. H. Papadimitriou \(1981\). Elements o/the Theory o/Computation, Prentice-Hall.  
S. Lin \(1965\). Computer Solutions of the Traveling Salesman Problem, Bell System Technical Journal 44,  
2245-2269.  
S. Lin \(1975\). Heuristic Programming as an Aid to Network Design, Networks 5, 33-43.  
S. Lin and B. W Kernighan \(1973\). An Effective Heuristic Algorithm for the Traveling Salesman Problem,  
Operations Research 21,498-516.  
J. D. C. Little, K. G. Murty, D. W. Sweeney, and C. Karel \(1963\). An Algorithm for the Traveling Salesman  
Problem, Operations Research 11,972-989.  
J. Lorie and L. J. Savage \(1955\). Three Problems in Capital Rationing, Journal 0/ Business 28, 229-239.  
R. Loulou and E. Michaelides \(1979\). New Greedy-like Heuristics for the Multidimensional 0-1 Knapsack  
Problem, Operations Research 27, 1101-1114.  
L. Lovasz \(1972\). Normal Hypergraphs and the Perfect Graph Conjecture, Discrete Mathematics 2, 253-267.  
L. Lovasz \(1975\). On the Ratio of Optimal Integral and Fractional Covers, Discrete Mathematics 13,383-390.  
L. Lovasz \(1979a\). Graph Theory and Integer Programming, Annals 0/ Discrete Mathematics 4, 141-158.  
L. Lovasz \(1979b\). Combinatorial Problems and Exercises, Akademiai Kiado, p 528.  
L. Lovasz \(1980\). Matroid Matching and Some Applications, Journal o/Combinatorial Theory B28, 208-236.  
L. Lovasz \(1981\). The Matroid Matching Problem, in Algebraic Methods in Graph Theory, L. Lovasz and  
V. T. Sos, eds., North-Holland, pp. 495-517.  
L. Lovasz \(1983\). Submodular Functions and Convexity, in Bachem, Grotschel and Korte, pp. 235-257.  
L. Lovasz and M. D. Plummer \(1986\). Matching Theory, Akademiai Kiado.  
A. Lubiw \(1982\). Gamma-Free Matrices, M. S. Thesis, University of Waterloo.  
M. Lundy and A. Mees \(1986\). Convergence of an Annealing Algorithm, Mathematical Programming 34,  
111-124.  
P. G. MacKeown \(1981\). A Branch-and-Bound Algorithm for Solving Fixed Charge Problems, Naval Research  
Logistics Quarterly 28, 607 -618.  
E Maffioli \(1986\). Randomized Algorithms in Combinatorial Optimization\: A Survey, Discrete Applied  
Mathemat\~cs 14, 157-170.  
E Maffioli, M. G. Speranza, and C. Vercellis \(1985\). Randomized Algorithms, in O'hEigeartaigh et al.,  
pp.89-105.  
M. J. Magazine and M. S. Chern \(1984\). A Note on Approximation Schemes for Multidimensional Knapsack  
Problems, Mathematics o/Operations Research 9,244-247.  
M. J. Magazine and O. Oguz \(1981\). A Fully Polynomial Approximation Algorithm for the 0-1 Knapsack  
Algorithm, European Journal o/Operations Research 8, 270-273.  
T. L. Magnanti, P. Mireault, and R. T. Wong \(1986\). Tailoring Benders' Decomposition for Uncapacitated  
Network Design, Mathematical Programming Study 26,112-154.  
T. L. Magnanti and R. T. Wong \(1981\). Accelerated Benders Decomposition\: Algorithmic Enhancement and  
Model Section Criteria, Operations Research 29,464-484.  
T. L. Magnanti and R. T. Wong \(1984\). Network Design and Transportation Planning\: Models and Algorithms,  
Transportation Science 18, 1-55.  
A. S. Manne \(1964\). Plant Location under Economies of Scale-Decentralization and Computation, Manage-  
ment Science 11, 213-235.  
O. Marcotte \(1985\). The Cutting Stock Problem and Integer Rounding, Mathematical Programming 33,89-92.  
O. Marcotte \(1986a\). An Instance of the Cutting Stock Problem for Which the Rounding Property Does Not  
Hold, Operations Research Letters 4, 239-243. 740 References  
O. Marcotte \(1986b\). On the Chromatic Index of Multigraphs and a Conjecture of Seymour, Journal 0/  
Combinatorial Theory 418,306-331.  
H. M. Markowtiz and A. S. Manne \(1957\). On the Solution of Discrete Programming Problems, Econometrica  
25,84-110.  
R. E. Marsten \(1974\). An Algorithm for Large Set Partitioning Problems, Management Science 20, 774-787.  
R. E. Marsten \(1981\). XMP\: A Structured Library of Subroutines for Experimental Mathematical Programming,  
ACM Transactions on Mathematical Software 7, 481-497.  
R. E. Marsten and E Shepardson \(1981\). Exact Solution of Crew Scheduling Problems Using the Set Partitioning  
Model\: Recent Successful Applications, Networks 11, 165-178.  
C. U. Martel \(1982\). Preemptive Scheduling with Release Times, Deadlines and Due Times, Journal 0/ the  
Association/or Computing Machinery 29, 812-829.  
S. Martello and P. Toth \(1979\). The 0-1 Knapsack Problem, in Christofides, Mingozzi et aI., pp. 237 -279.  
S. Martello and P. Toth \(1981a\). A Branch and Bound Algorithm for the Zero-One Multiple Knapsack Problem,  
Discrete Applied Mathematics 3, 275-288.  
S. Martello and P. Toth \(1981b\). Heuristic Algorithms for the Multiple Knapsack Problem, Computing 27,  
93-112.  
R. K. Martin \(1984\). Generating Alternative Mixed-Integer Linear Programming Models, Graduate School of  
Business, University of Chicago.  
R. K. Martin \(1987\). Using Separation Algorithms to Generate Mixed Integer Model Reformulations, Graduate  
School of Business, University of Chicago.  
R. K. Martin and L. Schrage \(1985\). Subset Coefficient Reduction Cuts for 0-1 Mixed Integer Programming,  
Operations Research 33, 505-526.  
1. E Maurras \(1975\). Some Results on the Convex Hull of Hamiltonian Cycles of Symmetric Complete Graphs,  
in Combinatorial Programming\: Methods and Applications, B. Roy, ed., Reidel, pp. 179-190.  
C. 1. H. McDiarmid \(1975\). Rado's Theorem for Polymatroids, Proceedings 0/ the Cambridge Philosophical  
Society 78, 263-28l.  
C. 1. H. McDiarmid \(1983\). Integral Decomposition in Polyhedra, Mathematical Progrumming 25, 183-198.  
N. Metropolis, A. Rosenbluth, M. Rosenbluth, A. Teller, and E. Teller \(1953\). Equations of State Calculations by  
Fast Computing Machines, Journal o/Chemical Physics 21, 1087-1091.  
R. R. Meyer \(1974\). On the Existence of Optimal Solutions to Integer and Mixed-Integer Programming  
Problems, Mathematical Programming 7,223-235.  
R. R. Meyer \(1975\). Integer and Mixed Integer Programming Models\: General Properties, Journal o/Optimiza-  
tion Theory and Applications 16, 191-206.  
R. R. Meyer and M. L. Wage \(1978\). On the Polyhedrality of the Convex Hull of the Feasible Set of an Integer  
Program, SIAM Journal on Control and Optimization 16,682-687.  
H. Meyniel \(1976\). On the Perfect Graph Conjecture, Discrete Mathematics 16,339-342.  
H. Meyniel \(1984\). The Graphs Whose Odd Cycles Have at Least Two Chords, Annals 0/ Discrete Mathematics  
21, 103-114.  
P. Miliotis \(1976\). Integer Programming Approaches to the Traveling Salesman Problem, Mathematical  
Programming 10, 367 -378.  
P. Miliotis \(1978\). Using Cutting Planes to Solve the Symmetric Travelling Salesman Problem, Mathematical  
Programming 15,177-188.  
C. E. Miller, A. W Tucker, and R. A. Zemlin \(1960\). Integer Programming Formulations and Traveling  
Salesman Problems, Journal o/the Association/or Computing Machinery 7,326-329.  
G. 1. Minty \(1980\). On Maximal Independent Sets of Vertices in a Claw-Free Graph, Journal o/Combinatorial  
Theory 828,284-304.  
G. Mitra \(1973\). Investigations of some Branch and Bound Strategies for the Solution of Mixed Integer Linear  
Programs, Mathematical Programming 4, 155-170.  
L. G. Mitten \(1970\). Branch-and-Bound Methods\: General Formulation and Properties, Operations Research  
18,24-34.  
C. L. Monma, ed. \(1986\). Algorithms and Software/or Optimization-Part I \(Annals o/Operations Research 4\).  
1. G. Morris \(1978\). On the Extent to which Certain Fixed Depot Location Problems Can Be Solved by LP,  
Journal o/the Operational Research Society 29, 71-76. References 741  
J. A. Muckstadt and S. A. Koenig \(1977\). An Application of Lagrangian Relaxation to Scheduling in Power  
Generation Systems\) Operations Research 25\) 387 -403.  
J. M. Mulvey and H. M. Crowder \(1979\). Cluster Analysis\: An Application of Lagrangian Relaxation\) Manage-  
ment Science 25,329-340.  
J. D. Murchland \(1973\). Historical Note on Optimal Spanning Arborescences, Networks 3,287-288.  
K. G. Murty \(1976\). Linear and Combinatorial Programming, Wiley.  
R. M. Nauss \(1979\). Parametric Integer Programming, University of Missouri Press, Columbia.  
A. W. Neebe and M. R. Rao \(1983\). An Algorithm for the' Fixed Charge Assignment of Users to Sources  
Problem, Journal o/the Operational Research Society 34, 1107-1115.  
G. L. Nemhauser \(1966\). Introduction to Dynamic Programming, Wiley.  
G. L. Nemhauser \(1985\). Duality for Integer Optimization, in O\)hEigeartaigh et aI., pp. 11-20.  
G. L. Nemhauser and G. Sigismondi \(1988\). A Constraint Generation Algorithm for Node Packing, School of  
Industrial and Systems Engineering, Georgia Institute of Technology.  
G. L. Nemhauser and L. E. Trotter \(1974\). Properties of Vertex Packing and Independence System Polyhedra,  
Mathematical Programming 6,48-61.  
G. L. Nemhauser and L. E. Trotter \(1975\). Vertex Packings\: Structural Properties and Algorithms, Mathematical  
Programming 8, 232-248.  
G. L. Nemhauser, L. E. Trotter, and R. M. Nauss \(1974\). Set Partitioning and Chain Decomposition, Manage-  
ment Science 20, 1413-1423.  
G. L. Nemhauser and Z. Ullman \(1968\). A Note on the Generalized Multiplier Solution to an Integer  
Programming Problem, Operations Research 16, 450-452.  
G. L. Nemhauser and Z. Ullman \(1969\). Discrete Dynamic Programming and Capital Allocation, Management  
Science 15, 494-505.  
G. L. Nemhauser and G. M. Weber \(1979\). Optimal Set Partitioning, Matchings and Lagrangian Duality, Naval  
Research Logistics Quarterly 26, 553-563.  
G. L. Nemhauser and L. A. Wolsey \(1979\). Best Algorithms for Approximating the Maximum of a Submodular  
Set Function, Mathematics 0/ Operations Research 3, 177 -188.  
G. L. Nemhauser and L. A. Wolsey \(1981\). Maximizing Submodular Set Functions\: Formulations and Analysis  
of Algorithms, Annals o/Discrete Mathematics 11,279-301.  
G. L. Nemhauser and L. A. Wolsey \(1984\). A Recursive Procedure for Generating all Cuts for 0-1 Mixed Integer  
Programs, Core DP 8439, Universite Catholique du Louvain.  
G. L. Nemhauser, L. A. Wolsey, and M. L. Fisher \(1978\). An Analysis of Approximations for Maximizing  
Submodular Set Functions-I, Mathematical Programming 14, 265-294.  
J. P. Norback and R. F. Love \(1979\). Heuristic for the Hamiltonian Path Problem in Euclidean Two Space,  
Operations Research 30, 363-368.  
R. Z. Norman and M. D. Rabin \(1959\). An Algorithm for the Minimum Cover of a Graph, Proceedings o/the  
American Mathematical Society 10, 315-319.  
F. J. Nourie and E. R. Venta \(1982\). An Upper Bound on the Number of Cuts Needed in Gomory's Method of  
Integer Forms, Operations Research Letters 1, 129-133.  
M. O'hEigertaigh, J. K. Lenstra and A. H. G. Rinnooy Kan, eds. \(1985\). Combinatorial Optimization\: Anno-  
tated Bibliographies, Wiley.  
J. Orlin \(1982\). A Polynomial Algorithm for Integer Programming Covering Problems Satisfying the Integer  
Round-Up Property, Mathematical Programming 22,231-235.  
J. B. Orlin \(1984\). Genuinely Polynomial Simplex and Non-Simplex Algorithms for the Minimum Cost Flow  
Problem, WP 1615-84, Sloan School of Management, Massachusetts Institute of Technology.  
J. B. Orlin \(1986\). A Dual Version of Tardos's Algorithm for Linear Programming, Operations Research Letters  
5,221-226.  
M. W. Padberg \(1973\). On the Facial Structure of Set Packing Polyhedra, Mathematical Programming 5,  
199-215.  
M. W. Padberg \(1974\), Perfect Zero-One Matrices, Mathematical Programming 6,180-196.  
M. W. Padberg \(1975a\). A Note on Zero-One Programming, Operations Research 23, 833-837.  
M. W. Padberg \(1975b\). Characterizations of Totally Unimodular, Balanced and Perfect Matrices, in Combina-  
torial Programming\: Methods and Applications, B. Roy, ed., Reidel, pp. 275-284. 742 References  
M. W. Padberg \(1976a\). A Note on the Total Unimodularity of Matrices, Discrete Mathematics 14, 273-278.  
M. W. Padberg \(1976b\). Almost Integral Polyhedra Related to Certain Combinatorial Optimization Problems,  
Linear Algebra and Its Applications 15, 69-88.  
M. W. Padberg \(1977\). On the Complexity of Set Packing Polyhedra, Annals of Discrete Mathematics 1,  
421-434.  
M. W. Padberg \(1979\). Covering, Packing and Knapsack Problems, Annals of Discrete Mathematics 4, 265-287.  
M. W. Padberg, ed. \(1980a\). Combinatorial Optimization, Mathematical Programming Study 12.  
M. W. Padberg \(1980b\). \(1, k\)-Configurations and Facets for Packing Problems, Mathematical Programming 18,  
94-99.  
M. W. Padberg \(1984\). A Characterization of Perfect Matrices, Annals of Discrete Mathematics 21, 169-178.  
M. W. Padberg \(1988\). Total Unimodularity and the Euler-Subgraph Problem. To appear in Operations Research  
Letters.  
M. W. Padberg and M. Grotschel \(1985\). Polyhedral Computations, in Lawler, Lenstra et aI., pp. 307-360.  
M. W. Padberg and S. Hong \(1980\). On the Symmetric Traveling Salesman Problem\: A Computational Study,  
Mathematical Programming Study 12, 78-107.  
M. W. Padberg and M. R. Rao \(1982\). Odd Minimum Cut-Sets and b-Matchings, Mathematics of Operations  
Research 7,67 -80.  
M. W. Padberg and G. Rinaldi \(1987a\). Optimization of a 532-City Traveling Salesman Problem by Branch and  
Cut, Operations Research Letters 6, 1-8.  
M. W. Padberg and G. Rinaldi \(1987b\). Facet Indentification for the Symmetric Traveling Salesman Polytope,  
New York University.  
M. W. Padberg, T. J. Van Roy and L. A. Wolsey \(1985\). Valid Linear Inequalities for Fixed Charge Problems,  
Operations Research 33, 842-861.  
M. W. Padberg and L. A. Wolsey \(1983\). Trees and Cuts, Annals of Discrete Mathematics 17, 511-517.  
M. W. Padberg and L. A. Wolsey \(1984\). Fractional Covers for Forests and Matchings, Mathematical Program-  
ming 29, 1-14.  
e. H. Papadimitriou \(1981a\). On the Complexity of Integer Programming, Journal of the Association for  
Computing Machinery 28,765-768.  
e. H. Papadimitriou \(1981b\). Worst-Case and Probabilistic Analysis of a Geometric Location Problem, SIAM  
Journal on Computing 10, 542-557.  
e. H. Papadimitriou \(1984\). Polytopes and Complexity, in Pulleyblank, pp. 295-306.  
e. H. Papadimitriou \(1985\). Computational Complexity, in O'hEigeartaigh et aI., pp. 39-51.  
e. H. Papadimitriou and K. Stieglitz \(1977\). On the Complexity of Local Search for the Traveling Salesman  
Problem, SIAM Journal on Computing 6,76-83.  
e. H. Papadimitriou and K. Stieglitz \(1978\). Some Examples of Difficult Traveling Salesman Problems,  
Operations Research 26, 434-443.  
C. H. Papadimitriou and K. Stieglitz \(1982\). Combinatorial Optimization\: Algorithms and Complexity, Pren-  
tice-Hall.  
e. H. Papadimitriou and M. Yannakakis \(1984\). The Complexity of Facets \(and Some Facets of Complexity\),  
Journal of Computing and System Science 28, 244-259.  
K. R. Parthasarathy and G. Ravindra \(1976\). The Strong Perfect Graph Conjecture Is True for K,,3-Free Graphs,  
Journal of Combinatorial Theory B21, 212-223.  
e. e. Petersen \(1967\). Computational Experience with Variants of the Balas Algorithm Applied to the Selection  
of Rand D Projects, Management Science 13, 736-750.  
J. e. Picard and M. Queyranne \(1977\). On the Integer-Valued Variables in the Linear Vertex Packing Problem,  
Mathematical Programming 12, 97 -10 1.  
J. C. Picard and M. Queyranne \(1982\). Selected Applications of Minimum Cuts in Networks, INFOR 20,  
394-422.  
J. e. Picard and H. D. Ratliff\(1975\). Minimum Cuts and Related Problems, Networks 5, 357-370.  
J. F. Pierce \(1968\). Application of Combinatorial Programming to a Class of All Zero-One Integer Programming  
Problems, Management Science 13, 736-750.  
J. F. Pierce and J. Lasky \(1973\). Improved Combinatorial Programming Algorithms for a Class of All Zero-One  
Integer Programming Problems, Management Science 19, 528-543. References 743  
y. Pochet \(1988\). Valid Inequalities and Separation for Capacitated Economic Lot Sizing, to appear in  
Operations Research Letters.  
Y. Pochet and L. A. Wolsey \(1988\). Lot-Size Models with Backlogging\: Strong Formulations and Cutting Planes,  
to appear in Mathematical Programming.  
C. N. Potts \(1985\). A Lagrangian Based Branch and Bound Algorithm for Single Machine Scheduling with  
Precedence Constraints to Minimize Total Weighted Completion Time, Management Science 31,1300-1311.  
S. Powell \(1985\). Software, in O'hEigeartaigh et al., pp. 190-194.  
R. C. Prim \(1957\). Shortest Connection Networks and Some Generalizations, Bell System Technological Journal  
36, 1389-1401.  
A. Prodon, T. M. Liebling, and H. Groflin \(1985\). Steiner's Problem on Two-Trees, RO 850315, Departement de  
Mathematiques, Ecole Polytechnique Federale de Lausanne.  
E. Pruul \(1975\). Parallel Processing and a Branch-and-Bound Algorithm, M. S. Thesis, Cornell University.  
E. Pruul, G. L. Nemhauser, and R. Rushmeier \(1988\). Parallel Processing and Branch-and-Bound\: A Historical  
Note, to appear in Operations Research Letters.  
W. R. Pulleyblank \(1973\). Faces of Matching Polyhedra, Ph.D. Thesis, University of Waterloo.  
W. R. Pulleyblank \(1980\). Dual Integrality in b-Matching Problems, Mathematical Programming Study 12,  
176-196.  
W. R. Pulleyblank \(1981\). Total Dual Integrality and b-Matchings, Operations Research Letters 1,28-30.  
W. R. Pulleyblank \(1983\). Polyhedral Combinatorics, in Bachem, Grotschel and Korte, pp. 312-345.  
W. R. Pulleyblank, ed. \(1984\). Progress in Combinatorial Optimization, Academic Press.  
W. R. Pulleyblank and J. Edmonds \(1975\). Facets of I-Matching Polyhedra, in Hypergraph Seminar, C. Berge  
and D. Ray-Chaudhuri, eds., Springer, pp. 214-242.  
M. O. Rabin \(1976\). Probabilistic Algorithms, in Algorithms and Complexity\: New Directions and Recent  
Results, J. E Traub, ed., Academic Press, pp. 21-40.  
R. Rado \(1957\). Note on Independence Functions, Proceedings o/the London Mathematical Society 7, 300-320.  
R. L. Rardin and U. Choe \(1979\). Tighter Relaxations of Fixed Charge Network Flow Problems, Industrial and  
Systems Engineering Report J-79-18, Georgia Institute of Technology.  
H. D. Ratliff and A. S. Rosenthal \(1983\). Order Picking in a Rectangular Warehouse\: A Solvable Case of the  
T. S. P., Operations Research 31, 507 -521.  
A. Recski \(1988\). Matroid Theory and Its Applications, Springer.  
S. Reiter, and D. B. Rice \(1966\). Discrete Optimizing Solution Procedures for Linear and Nonlinear Integer  
Programming Problems, Management Science 12, 829-850.  
S. Reiter and G. Sherman \(1965\). Discrete Optimizing, SIAM Journal 13, 864-899.  
J. M. W. Rhys \(1970\). A Selection Problem of Shared Fixed Costs and Network Flows, Management Science 17,  
200-207.  
C. Ribeiro and M. Minoux \(1985\). Solving Hard Constrained Shortest Path Problems by Lagrangian Relaxation  
and Branth and Bound Algorithms, in Proceedings 0/ X Symposium on Operations Research, M. Beckmann  
et al., eds., Methods of Operations Research 53, Anton Hain, pp. 303-316.  
A. H. G. Rinnooy Kan \(1976\). On Mitten's Axioms for Branch and Bound, Operations Research 24, 1176-1178.  
A. H. G. Rinnooy Kan \(1986\). An Introduction to the Analysis of Approximation Algorithms, Discrete Applied  
Mathematics 14, 111-134.  
T. Rockafellar \(1970\). Convex Analysis, Princeton University Press.  
D. J. Rosenkrantz, R. E. Stearns, and P. M. Lewis \(1977\). An Analysis of Several Heuristics for the Traveling  
Salesman Problem, SIAM Journal on Computing 6,563-581.  
K. Rosling \(1983\). The Dynamic Inventory Model and the Uncapacitated Facility Location Problem, S-581 83,  
Department of Production Economics, Linkoping Institute of Technology.  
G. T. Ross and R. M. Soland \(1975\). A Branch and Bound Algorithm for the Generalized Assignment Problem,  
Mathematical Programming 8, 91-103.  
D. S. Rubin \(1985\). Polynomial Algorithms for *m* x *\(m* + 1\) Integer Programs and *m* x *\(m* + *k\)* Diophantine  
Systems, Operations Research Letters 3, 289-291.  
S. Sahni \(1975\). Approximate Algorithms for the 0-1 Knapsack Problem, Journal of the Association for  
Computing Machinery 22, 115-124.  
S. Sahni \(1977\). General Techniques for Combinatorial Approximation, Operations Research 25, 920-936. 744 References  
S. Sahni and T. Gonzalez \(1976\). P-Complete Approximation Problems, Journal of the Association for  
Computing Machinery 23, 555-565.  
M. Sakarovitch \(1975\). Quasi-Balanced Matrices, Mathematical Programming 8,382-386.  
M. Sakarovitch \(1976\). Quasi-Balanced Matrices-an Addendum, Mathematical Programming 10, 405-407.  
H. M. Salkin \(1975\). Integer Programming, Addison-Wesley.  
H. M. Salkin and C. A. de Kluyver \(1975\). The Knapsack Problem\: A Survey, Naval Research Logistics  
Quarterly 22, 127-144.  
H. M. Salkin and R. D. Koncal \(1973\). Set Covering by an All-Integer Algorithm\: Computational Experience,  
Journal of the Association for Computing Machinery 31,336-345.  
C. Sandi \(1979\). Subgradient Optimization, in Christofides, Mingozzi et al., pp. 73-91.  
A. Sassano \(1985\). On the Facial Structure of the Set Covering Polytope, R139, Instituto di Analisi dei Sistemi ed  
Informatica del CNR. Rome.  
N. Sbihi \(1980\). Algorithme de Recherche d'un Stable de Cardinalite Maximum dans un Graphe sans Etoile,  
Discrete Mathematics 29, 53-76.  
H. E. Scarf \(1981a\). Production Sets with Indivisibilities Part I, Econometrica 49, 1-32.  
H. E. Scarf\(1981b\). Production Sets with Indivisibilities Part II, Econometrica 49,395-423.  
L. Schrage \(1975\). Implicit Representation of Variable Upper Bounds in Linear Programming, Mathematical  
Programming Study, 4, 118-132.  
L. Schrage \(1986\). Linear, Integer and Quadratic Programming with LINDO, Scientific Press.  
L. Schrage and L. A. Wolsey \(1985\). Sensitivity Analysis for Branch and Bound Integer Programming,  
Operations Research 33, 1008-1023.  
A. Schrijver \(1980\). On Cutting Planes, Annals of Discrete Mathematics 9, 291-296.  
A. Schrijver \(1981\). On Total Dual Integrality, Linear Algebra and Its Applications 38, 27 -32.  
A. Schrijver \(1983a\). Min-Max Results in Combinatorial Optimization, in Bachem, Grotschel and Korte,  
pp.439-500.  
A. Schrijver \(1983b\). Short Proofs on the Matching Polytope, Journal of Combinatorial Theory B34, 104-108.  
A. Schrijver \(1984a\). Proving Total Dual Integrality with Cross-Free Families-A General Framework, Mathe-  
matical Programming 29,15-27.  
A. Schrijver \(1984b\). Total Dual Integrality from Directed Graphs, Crossing Families and Sub- and Supermodu-  
lar Functions, in Pulleyblank, pp. 315-362.  
A. Schrijver \(1986a\). Linear and Integer Programming, Wiley.  
A. Schrijver \(1986b\). Polyhedral Proof Methods in Combinatorial Optimization. Discrete Applied Mathematics  
14, 111-134.  
P. D. Seymour \(1977\). The Matroids with the Max-Flow Min-Cut Property, Journal of Combinatorial Theory  
B26, 189-222.  
P. D. Seymour \(1978\). A Two-Commodity Cut Theorem, Discrete Mathematics 23, 177-181.  
P. D. Seymour \(1979\). On Multi-Colourings of Cubic Graphs, and Conjectures of Fulkerson and Tutte,  
Proceedings of the London Mathematical Society 38, 423-460.  
P. D. Seymour \(1980\). Decomposition of Regular Matroids, Journal of Combinatorial Theory B28, 305-359.  
R. Shamir \(1987\). The Efficiency ofthe Simplex Method\: A Survey, Management Science 33, 301-334.  
J. F. Shapiro \(1968a\). Dynamic Programming Algorithms for the Integer Programming Problem 1\: The Integer  
Programming Problem Viewed as a Knapsack Type Problem, Operations Research 16, 103-121.  
J. F. Shapiro \(1968b\). Group Theoretic Algorithms for the Integer Programming Problem-II\: Extensions to a  
General Algorithm, Operations Research 18, 103-121.  
J. F. Shapiro \(1970\). Turnpike Theorems for Integer Programs, Operations Research 18, 432-440.  
J. F. Shapiro \(1971\). Generalized Lagrange Multipliers in Integer Programming, Operations Research 19,68-76.  
J. F. Shapiro \(1977\). Sensitivity Analysis in Integer Programming, Annals of Discrete Mathematics 1,467-477.  
J. F. Shapiro \(l979a\). Mathematical Programming, Structures and Algorithms, Wiley.  
J. F. Shapiro \(l979b\). A Survey of Lagrangian Techniques for Discrete Optimization, Annals of Discrete  
Mathematics 5, 113-138.  
J. F. Shapiro and H. M. Wagner \(1967\). A Finite Renewal Algorithm for the Knapsack and Turnpike Models,  
Operations Research 15,319-341.  
M. L. Shore, L. R. Foulds and P. B. Gibbons \(1982\). An Algorithm for the Steiner Problem in Graphs, Networks  
12, 323-333. References **745**  
P. Sinha and A. A. Zoltners \(1979\). The Multiple-Choice Knapsack Problem, Operations Research 27,503-515.  
S. Smale \(1983a\). The Problem of the Average Speed of the Simplex Method, in Bachem, Grotschel and Korte,  
pp.530-539.  
S. Smale \(1983b\). On the Average Number of Steps of the Simplex Method of Linear Programming, Mathemati-  
cal Programming 27,241-262.  
K. Spielberg \(1969a\). Plant Location with Generalized Search Origin, Management Science 16, 165-178.  
K. Spielberg \(1969b\). Algorithms for the Simple Plant Location Problem with Some Side Conditions, Operations  
Research 17, 85-11l.  
K. Spielberg \(1979\). Enumerative Methods in Integer Programming, Annals 0/ Discrete Mathematics 5,  
139-183.  
J. Stoer and C. Witzgall \(1970\). Convexity and Optimization in Finite Dimensions, Springer.  
G. Strang \(1976\). Linear Algebra and Its Applications, Academic Press.  
U. Suhl \(1978\). Algorithm and Efficient Data Structures for the Binary Knapsack Problem, European Journal 0/  
Operations Research 2, 420-428.  
U. Suhl \(1985\), Solving Large Scale Mixed Integer Programs with Fixed Charge Variables, Mathematical  
Programming 32, 165-182.  
H. A. Taha \(1975\). Integer Programming, Theory, Applications, and Computations, Academic Press.  
A. Tamir \(1976\). On Totally Unimodular Matrices, Networks 6,373-382.  
A. Tamir \(1983\). A Class of Balanced Matrices Arising From Location Problems, SIAM Journal 0/ Algebraic and  
Discrete Methods 4,363-370.  
A. Tamir \(1987\). Totally Balanced and Totally Unimodular Matrices Defined by Center Location Problems,  
Discrete Applied Mathematics 16, 245-264.  
E. Tardos \(1985\). A Strongly Polynomial Minimum Cost Circulation Algorithm, Combinatorica 5, 247-255.  
E. Tardos \(1986\). A Strongly Polynomial Algorithm to Solve Combinatorial Linear Programs, Operations  
Research 34, 250-256.  
E. Tardos, C. A. Tovey, and M. A. Trick \(1986\), Layered Augmenting Path Algorithms, Mathematics 0/  
Operations Research 11,362-370.  
R. E. Tarjan \(1977\). Finding Optimum Branchings, Networks 7, 25-35.  
R. E. Tarjan \(1983\). Data Structures and Network Algorithms, SIAM Publications.  
R. E. Tarjan \(1986\). Algorithms for Maximizing Network Flow, Mathematical Programming 26, 1-11.  
J. Tind \(1974\). Blocking and Antiblocking Sets, Mathematical Programming 6,157-166.  
J. Tind \(1977\). On Antiblocking Sets and Polyhedra, Annals o/Discrete Mathematics 1, 507-515.  
J. Tind \(1979\). Blocking and Antiblocking Polyhedra, Annals 0/ Discrete Mathematics 4, 159-174.  
J. Tind and L. A. Wolsey \(1981\). An Elementary Survey of General Duality Theory in Mathematical Program-  
ming, Mathematical Programming 21, 241-26l.  
M. J. Todd \(1982\). An Implementation of the Simplex Method for Linear Programming with Variable Upper  
Bounds, Mathematical Programming 23,34-49.  
M. J. Todd \(1987\). Polynomial Algorithms for Linear Programming, in Proceedings o/the Optimization Days  
1986, H. A. Eiselt, ed.  
M. J. Todd and B. P. Burrell \(1986\). An Extension of Karmarkar's Algorithm for Linear Programming Using  
Dual Variables, Algorithmica 1,409-424.  
J. A. Tomlin \(1970\). Branch and Bound Methods for Integer and Non-Convex Programming, in Integer and  
Nonlinear Programming, J. Abadie, ed., American Elsevier, 437-450.  
J. A. Tomlin \(1971\). An Improved Branch and Bound Method for Integer Programming, Operations Research  
19, 1070-1075.  
P. Tong, E. L. Lawler, and V. V. Vazirani \(1984\). Solving the Weighted Parity Problem for Gammoids by  
Reduction to Graphic Matching, in Pulleyblank, pp. 363-374.  
D. Topkis \(1978\). Minimizing a Subadditive Function on a Lattice, Operations Research 26, 305-32l.  
M. Trick \(1987\). Networks with Additional Structured Constraints, Ph.D. Thesis, School of Industrial and  
Systems Engineering, Georgia Institute of Technology.  
L. E. Trotter, Jr. \(1975\). A Class of Facet Producing Graphs for Vertex Packing Polyhedra, Discrete Mathematics  
12,373-388.  
L. E. Trotter, Jr. \(1985\). Discrete Packing and Covering, in O'hEigeartaigh et aI., pp. 21-31.  
L. E. Trotter and D. B. Weinberger \(1978\). Symmetric Blocking and Antiblocking Relations for Generalized  
Circulations, Mathematical Programming 8, 141-158. 746 References  
K. Truemper \(1977\). Unimodular Matrices of Flow Problems with Additional Constraints, Networks 7,  
343-358.  
K. Truemper \(1978\). Algebraic Characterizations of Unimodular Matrices, SIAM Journal on Applied Mathe-  
matics 35, 328-332.  
W. T. Tutte \(1954\). A Short Proof ofthe Factor Theorem for Finite Graphs, Canadian Journal of Mathematics 6,  
347-352.  
W T. Tutte \(1965\). Lectures on Matroids, Journal of Research of the National Bureau of Standards 69B, 1-48.  
W T. Tutte \(1971\). Introduction to the Theory of Matroids, American Elsevier.  
p. Van Emde Boas \(1981\). Another Kg\>-Complete Partition Problem and the Complexity of Computing Short  
Vectors in a Lattice, Report 81-04, Mathematical Institute, University of Amsterdam.  
T. J. Van Roy \(1983\). Cross Decomposition for Mixed Integer Programming, Mathematical Programming 25,  
46-63.  
T. J. Van Roy \(1986\). A Cross Decomposition Algorithm for Capacitated Facility Location, Operations Research  
34, 145-163.  
T. J. Van Roy and L. A. Wolsey \(1985\). Valid Inequalities and Separation for Uncapacitated Fixed Charge  
Networks, Operations Research Letters 4, 105 -112.  
T. J. Van Roy and L. A. Wolsey \(1986\). Valid Inequalities for Mixed 0-1 Programs, Discrete Applied Mathematics  
14, 199-213.  
T. J. Van Roy and L. A. Wolsey \(1987\). Solving Mixed 0-1 Programs by Automatic Reformulation, Operations  
Research 35, 45-57.  
M. P. Vecchi and S. Kirkpatrick \(1983\). Global Wiring by Simulated Annealing, IEEE Transactions on  
Computer-Aided Design 2, 215-222.  
A. F. Veinott, Jr. and G. B. Dantzig \(1968\). Integral Extreme Points, SIAM Review 10, 371-372.  
V. G. Vizing \(1964\), On an Estimate of the Chromatic Class of a P-graph Diskretnyi Analiz 3, 25-30 \(in  
Russian\).  
R. von Randow, ed. \(1982\). Integer Programming and Related Areas, A Classified Bibliography 1978-1981,  
Lecture Notes in Economics and Mathematical Systems 197, Springer.  
R. von Randow, ed. \(1985\). Integer Programming and Related Areas, A Classified Bibliography 1981-1984,  
Lecture Notes in Economics and Mathematical Systems 243, Springer.  
J. Von zur Gathen and M. Sieveking \(1978\). A Bound on Solutions of Linear Integer Equalities and Inequalities,  
Proceedings of the American Mathematical Society 72, 155-158.  
H. M. Wagner \(1959\). On a Class of Capacitated Transportation Problems, Management Science, 5, 304-318.  
H. M. Wagner and T. M. Whitin \(1958\). Dynamic Version of the Economic Lot Size Model, Management  
Science 5, 89-96.  
G. M. Weber \(1981\). Sensitivity Analysis of Optimal Matchings, Networks 11,41-56.  
D. B. Weinberger \(1976\). Network Flows, Minimum Coverings, and the Four-Color Conjecture, Operations  
Research 24,272-290.  
H. M. Weingartner and D. N. Ness \(1967\). Methods for the Solution of Multidimensional 0/1 Knapsack  
Problems, Operations Research 15, 83-103.  
D. J. A. Welsh \(1968\). Kruskal's Theorem for Matroids, Proceedings of the Cambridge Philosophical Society 64,  
3-4.  
D. J. A. Welsh \(1976\). Matroid Theory, Academic Press.  
D. J. A. Welsh \(1983\). Randomised Algorithms, Discrete Applied Mathematics 5, 133-146.  
D. J. White \(1969\). Dynamic Programming, Holden-Day.  
W W White \(1961\). On Gomory's Mixed Integer Algorithm, Senior Thesis, Department of Mathematics,  
Princeton University.  
S. H. Whitesides \(1984\). A Classification of Certain Graphs with Minimum Imperfection Properties. Annals of  
DiscreteMathematics 21, 207-218.  
H. Whitney \(1935\). On the Abstract Properties of Linear Dependence, American Journal of Mathematics 57,  
509-533.  
H. P. Williams \(1974\). Experiments in the Formulation of Integer Programming Problems, Mathematical  
Programming Study 2, 180-197.  
H. P. Williams \(1978a\). Model Building in Mathematical Programming, Wiley. References 747  
H. P. Williams \(1978b\). The Reformulation of Two Mixed Integer Programming Problems, Mathematical  
Programming 14,325-331.  
L. A. Wolsey \(1971a\), Group-Theoretic Results in Mixed Integer Programming, Operations Research 19,  
1691-1697.  
L. A. Wolsey \(1971b\). Extensions of the Group Theoretic Approach in Integer Programming, Management  
Science 18, 74-83.  
L. A. Wolsey \(1973\). Generalized Dynamic Programming Methods in Integer Programming, Mathematical  
Programming 4, 222-232.  
L. A. Wolsey \(1975\). Faces for a Linear Inequality in 0-1 Variables, Mathematical Programming **8,165-178.**  
L. A. Wolsey \(1976\). Facets and Strong Valid Inequalities for Integer Programs, Operations Research ***24,***  
367-372.  
L. A. Wolsey \(1977\). Valid Inequalities and Superadditivity for 0-1 Integer Programs, Mathematics of Operations  
Research 2, 66-77.  
L. A. Wolsey \(1980\). Heuristic Analysis, Linear Programming and Branch and Bound, Mathematical Program-  
ming Study 13, 121-134.  
L. A. Wolsey \(1981a\). Integer Programming Duality\: Price Functions and Sensitivity Analysis, Mathematical  
Programming 20,173-195.  
L. A. Wolsey \(1981b\). The b-Hull of an Integer Program, Discrete Applied Mathematics 3,193-201.  
L. A. Wolsey \(198lc\). A Resource Decomposition Algorithm for General Mathematical Programs, Mathematical  
Programming Study 14,244-257.  
L. A. Wolsey \(1982a\). An Analysis of the Greedy Algorithm for the Submodular Set Covering Problem,  
Combinatorica 2, 417-425.  
L. A. Wolsey \(1982b\). Maximizing Real Valued Submodular Functions\: Primal and Dual Heuristics for Location  
Problems, Mathematics of Operations Research 7,410-425.  
L. A. Wolsey \(1987\). Strong Formulations for Mixed Integer Programming\: A Survey, Ecole Polytechnique  
Federale de Lausanne.  
R. T. Wong \(1984\). A Dual Ascent Approach for Steiner Tree Problems on Directed Graphs, Mathematical  
Programming **28,271-287.**  
R. T. Wong \(1985\). Location and Network Design, in O'hEigeartaigh et al., pp. 129-147.  
M. Yannakakis \(1985\). On a Class of Totally Unimodular Matrices, Mathematics of Operations Research 10,  
280-304.  
R. D. Young \(1965\). A Primal \(All Integer\), Integer Programming Algorithm, Journal of Research of the National  
Bureau of Standards 69B, 213-250.  
R. D. Young \(1968\). A Simplified Primal \(All-Integer\) Integer Programming Algorithm, Operations Research *16,*  
750-782.  
W. I. Zangwill \(1966\). A Deterministic Multi-Period Production Scheduling Model with Backlogging, Manage-  
ment Science 13, 105-119.  
E. Zemel \(1978\). Lifting the Facets of 0-1 Polytopes, Mathematical Programming 15, 268-277.  
E. Zemel \(1980\). The Linear Multiple Choice Knapsack Problem, Operations Research **28,** 1412-1423.  
E. Zemel \(1981\). Measuring the Quality of Approximate Solutions to Zero-One Programming Problems,  
Mathematics of Operations Research **6,** 319-332.  
E. Zemel \(1984\). An O\(n\) Algorithm for the Linear Multiple Choice Knapsack Problem and Related Problems,  
Information Processing Letters 18, 123-128.  
E. Zemel \(1986\). On the Computational Complexity of Facets of the Knapsack Problem, Working Paper  
No. 713, Graduate School of Management, Northwestern University. AUTHOR INDEX  
Aboudi, R., 425  
Agin, N., 379  
Ahn, S., 426  
Aho, A. v., 142  
Ali, A. 1.,522  
Anstee, R. P., 600  
Arabeyre, J. P., 523  
Araoz, J., 255  
Assad, A. A., 525  
Avis, D., 522, 655  
Babayev, D. A., 426  
Bachem, A., 109,201,206,256  
Baker, E. K., 523  
Balas, E., 21, 254, 290, 379, 425, 521-526  
Balinski, M. L., 20, 322, 655  
Ball, M. 0.,425,654  
Barahona, F, 425  
Barany, 1., 291, 526  
Barnes, J. S., 82  
Barr, R. S., 525  
Bartholdi, J. J., III, 599  
Baum, S., 601, 714  
Bazarra, M. S., 82  
Beale, E. M. L., 20, 379-380  
Beasley, J. E., 526  
Bell, D. E., 142, 342, 521  
Bellman, R. E., 82, 427  
Bellmore, M., 522-523  
Benders, J. F, 343  
Benichou, M., 380  
Ben-Israel, A., 380  
Berge, C., 82, 600, 654  
Bertsekas, D. P., 82  
Bilde, 0.,21,425  
Bixby, R. E., 599, 712  
Blair, C. E., 254, 256, 342  
Bland, R. G., 82, 180, 599-600  
Bock, F, 713  
Bodin, L., 525, 654  
Bondy, J. A., 82, 655  
Bonomi, E., 426, 524  
Borgwardt, K. H., 142  
Borosh, 1., 142  
Bowman, V. J., Jr., 254,380  
Boyd, S., 525  
Bradley, G. H., 21, 82  
Brearley, A. L., 21  
Breu, R., 380  
Brezovec, c., 713  
Brooks, R., 342  
Brown, G. G., 82, 599  
Burdet, C. A., 255, 380  
Burkhard, R. E., 525, 654  
Burlet, M., 600  
Burrell, B. P., 181  
Cabot, A. v., 525  
Camerini, P. M., 49  
Camion, P., 599  
Carpaneto, G., 525  
Cassels, J. W. S., 201  
Chalmet, L., 426  
Chandra, A. K., 521  
Chandrasekaran, R., 599  
Chang, G. J., 600  
Charnes, A., 49, 254, 380  
Chen, D. S., 521  
Chern, M. S., 521  
Cho, D. c., 425  
Choe, U., 525  
Christofides, N., 20-21, 82, 523-525  
Chu, Y. J., 713  
Chvatal, v., 49, 254-255, 290, 521, 523, 600  
Clarke, G., 524  
Clarke, M. R. B., 521  
Coffman, E. G., 654  
Conforti, M., 714  
Conn, A. R., 425  
Cook, S. A., 142  
Cook, W., 342, 379, 599, 655  
Cooper, W. W., 49, 254  
Cornuejols, G., 291, 425-427, 523-525, 655, 714  
Crama, Y., 713  
Croes, G. A., 524  
Crowder, H. P., 22, 426, 521-522, 525  
Cullen, F. H., 525  
Cunningham, W. H., 599, 654-655, 713-714  
749 750  
Dakin, R. J., 379  
Dantzig, G. B., 21, 49, 254, 284, 290, 342, 427,  
523-525, 527, 598-599  
Davis, M., 254  
De Ghellinck, G., 180-181  
de Kluyver, D. A., 521  
Deleve, G., 523  
Denardo, E. V., 427  
Derigs, U., 654  
Dial, R., 654  
Dijkstra, E. W., 82  
Dirac, G. A., 600  
Dobson, G., 523  
Doig, A. G., 379  
Domich, P. D., 201  
Dreyfus, S. E., 427  
Driebeek, N. J., 379  
Duchet, P., 600  
Dyer, M. E., 521  
Eastman, W. L., 523  
Edmonds, J., 82,142,201,598-599,601,654-  
655, 712-714  
Efroymson, M. A., 424  
Eppen, G. D., 526  
Erenguc, S. S., 525  
Erlenkotter, D., 425  
Etcheberry, J., 522  
Even, S., 654  
Everett, H., 341-342  
Farber, M., 600  
Fayard, D., 521  
Fisher, M. L., 342-343, 379, 425-426, 521, 523-  
525, 714  
Fonlupt, J., 291, 600  
Ford, L. R., 82, 598  
Forrest, J. J. H., 380  
Fox, B. L., 380  
Francis, R. L., 425  
Frank, A., 181, 343, 600, 713  
Frieze, A. M., 426, 521, 524, 526  
Frisch, K. R., 181  
Fujii, M., 654  
Fujishige, S., 181  
Fulkerson, D. R., 82, 109,254,290,522-525,  
598-601, 713  
Furst, M. L., 526  
Gabow, H. P., 82  
Gacs, H. P., 180  
Galbiati, G., 524  
Gale, D., 712  
Galil, Z., 82  
Gallo, G., 82  
Garey, M. R., 142, 426  
Garfinkel, R. S., 20, 254, 379, 427, 522-523, 525  
Gass, S., 49  
Gastou, G., 655  
Gauthier, J. M., 380  
Author Index  
Gavril, F., 600  
Gelders, L. F., 426  
Geoffrion, A. M., 20, 341-343,379,426-427,522  
Gerards, A. M., 655  
Gerards, A. M. H., 342, 379  
Ghouila-Houri, A., 599  
Giles, R., 109,201,290,598-601,713  
Gilmore, P. c., 342, 520, 525  
Glover, F., 82, 343, 380, 521-522  
Goffin, J. L., 49  
Golden, B. L., 426, 524-525  
Goldfarb, D., 180  
Golumbic, M. c., 600  
Gomory, R. E., 254-255, 290, 342, 380, 520-521,  
525  
Gondran, M., 20  
Gonzales, T., 524  
Gorry, G. A., 521  
Graham, R. L., 426, 521, 654  
Graves, G. W., 82, 427  
Graves, S. c., 426  
Gray, P., 525  
Greenberg, H. J., 20, 343  
Greenfield, A., 525  
Grimmett, G. R., 523  
Grinold, R. c., 49,427  
Gross, D. A., 599  
Grotschel, M., 20, 109, 180, 201, 290-291, 380,  
425, 524-525, 600, 654, 713  
Grunbaum, *B., 109*  
Guignard, M., 22, 425, 525  
Gunawardane, G., 599  
Hadley, G., 49  
Hajek, B., 426  
Halmos, P. R., 109  
Halton, J. H., 524  
Hamburger, M. M., 425  
Hammer, P. L., 20-21, 290, 713  
Hansen, P., 21  
Hartvigsen, D., 655  
Hassin, R., 713  
Hausmann, D., 20, 426  
Helbig-Hansen, D., 524  
Held, M., 49, 342, 524  
Helgason, R. V., 82  
Heller, 1., 599  
Ho, A., 522  
Hochbaum, D. S., 426, 523, 655  
Hoffman, A. J., 20, 380, 598-600, 655  
Hoffman, K., 522  
Holland, 0., 425, 654  
Holm, S., 343, 379  
Holyer, 1., 655  
Hong, S., 380, 525  
Hopcroft, J. E., 654  
Hsu, W. L., 601  
Hu, T. c., 20, 521, 525, 601  
Huang, H. c., 600-601  
Huard, P., 181 Author Index  
Ibaraki, T., 380  
Ibarra, O. H., 521  
Ikura, y., 82, 522  
Ingargiola, J. P., 521  
Iri, M., 599, 712  
Jaikumar, R., 425-426, 525  
Jarvis, J. J., 82, 525  
Jenkins, T. A., 426  
Jensen, D. L., 82  
Jensen, P. A., 82  
Jeroslow, R. G., 21, 142,251,254-256,342,380  
Johnson, D. S., 142,426  
Johnson, E. L., 20-22, 255-256, 290, 342, 520-  
525, 601, 654-655  
Johnson, S. M., 254, 290  
Jonker, R., 523-524  
Jornsten, K. 0.,343  
Junger, M., 425  
Kaas, R., 524  
Kannan, R., 142,201,526  
Kariv, 0., 654  
Karmarkar, N., 180  
Karney, D., 82  
Karp, R. M., 49, 82, 142-143, 180,342,426,524-  
526, 654, 713  
Karwan, M. H., 343  
Kastning, c., 20  
Kennington, J., 82  
Kernighan, B. S., 524  
Khachian, L. G., 180  
Khintchine, A., 201  
Kim, C. E., 521  
Kindervater, G. A. P., 379  
Kirkpatrick, S., 426  
Klee, V., 20, 142, 181  
Klein, D., 379  
Klingman, D., 82, 525  
Knuth, D. E., 142, 201  
Koenig, S. A., 426  
Kolen, A., 525,600  
Kolesar, P. J., 521  
Koncal, R. D., 522  
Korman, S., 523  
Korsch, J. F., 521  
Kortanek, K. 0.,254  
Korte, B., 20-21, 426, 712  
Kostreva, M. M., 22, 522  
Kramp, J., 21,425,524  
Kmskal, J. B., 82, 425, 599  
Kuehn, A. A., 425  
Lagarias, J. C., 201, 526  
Lageweg, B. J., 343  
Land, A. H., 379-380  
Laporte, G., 525  
Lasky, J., 522  
Lauriere, M., 521  
751  
Law, A. M., 427  
Lawler, E. L., 20, 82, 180,379,521,523,654,  
712-713  
Lehman, A., 601  
Lemke, C. E., 343, 523  
Lenstra, A. K., 201  
Lenstra, H. W., 526  
Lenstra, J. K., 20,142,343,379,523  
Lester, J. T., 525  
Leung,J., 241, 291, 425  
Lewis, H. R., 142  
Lin, S., 524-525  
Little, J. C. D., 379, 523  
Liu, T. H., 713  
Lorie, J., 342  
Loulou, R., 521  
Lovasz, L., 20,180,201,523,599-600,654-655,  
712-713  
Love, R. F., 524  
Lowe, J. K., 21, 254  
Lubiw, A., 600  
Lundy, M., 426  
Lutton, J. L., 426, 524  
McBride, R., 426  
McDiarmid, C. J. H., 601, 713  
MacKe own , P. G., 525  
Mattioli, F., 426, 524  
Magazine, M. J., 425, 521  
Magnanti, T. L., 291, 343, 425, 427, 525  
Mahjoub, A. R., 425  
Malone, J. F., 523  
Manne, A. S., 254, 425  
Marcotte, 0., 601, 655  
Markowitz, H. M., 254  
Marsh, A. B., III, 654-655  
Marsten, R. E., 20, 341, 379-380, 522-523  
Martel, C. U., 713  
Martello, S., 521  
Martin, R. K., 291, 521-522, 526  
Maurras, J. F., 290  
Mees, A., 426  
Metropolis, N., 426  
Meyer, R. R., 109  
Meyniel, H., 600  
Michaelides, E., 521  
Miliotis, P., 525  
Miller, C. E., 21  
Mingozzi, A., 21  
Minoux, M., 20, 343  
Minty, G. J., 142, 180,601  
Mirchandani, P., 425  
Mitra, G., 380  
Mitten, L. G., 379  
Monma, C. L., 21, 142  
Morris, J. G., 425  
Mosterts, S., 655  
Muckstadt, J. A., 426  
Mulvey, J. M., 426  
Murchland, J. D., 713 752  
Murty, K. G., 49  
Murty, U. S. R., 82,655  
Naddef, D., 291, 524  
Nasberg, M., 343  
Nauss, R. M., 379, 522, 599  
Neebe, A. W., 426-427, 525  
Nemhauser, G. L., 20, 82, 254-255, 290, 341-  
342, 380, 425-427, 522-525, 599-601, 654,  
714  
Ness, D. N., 427  
Ng, S. M., 523  
Norback, J. P., 524  
Norman, R. Z., 654-655  
Northup, W. D., 426, 521  
Nourie, E J., 380  
Odlyzko, A. M., 526  
Oguz, 0.,521  
O'hEigertaigh, M., 20  
Oppenheim, R., 599-600, 655  
Orlin, J., 181,601  
Padberg, M. W., 21-22, 290, 380, 521-525, 599-  
601, 654, 713  
Pallotino, S., 82  
Papadimitriou, C. H., 20,142-143,180,291,426,  
524  
Parthasarathy, K. R., 601  
Peled, U. N., 290  
Petersen, C. C, 523, 713  
Picard, J. C, 523, 713  
Pierce, J. E, 522  
Pierskalla, W. P., 343  
Plateau, G., 521  
Plummer, M. D., 654  
Pochet, Y., 291, 526  
Potts, C. N., 426  
Powell, S., 380  
Prim, R. C, 82  
Prodon, A., 526  
Pruul, E., 379  
Pruzan, P. M., 425  
Pulleyblank, W. R., 20-21, 109,290-291,523-  
526, 599, 654-655  
Putnam, H., 254  
Quandt, R. E., 522  
Queyranne, M., 523, 713  
Rabin, M. D., 426, 654-655  
Rado, R., 712  
Rao, M. R., 426-427, 525, 654  
Rardin, R. L., 343, 525  
Ratliff, H. D., 522, 525, 713  
Ravindra, G., 601  
Ray, T. L., 424  
Recski, A., 712  
Reinelt, G., 425  
Reiter, S., 425  
Author Index  
Rhys, J. M. W., 713  
Ribeiro, c., 343  
Ribiere, G., 380  
Rice, D. B., 425  
Rinaldi, G., 524-525  
Rinnooy Kan, A. H. G., 142, 343, 380, 425, 523  
Rockafellar, T., 109  
Rosenkrantz, D. J., 524  
Rosenthal, A. S., 525  
Rosling, K., 526  
Ross, G. T., 426  
Rubin, D. S., 526  
Rudeanu, S., 713  
Sahni, S., 426, 521, 524  
Sakarovitch, M., 600  
Salkin, H. M., 20, 255, 521-522  
Saltzman, M. J., 425  
Sandi, C, 49  
Sassano, A., 523  
Savage, L. J., 342  
Sbihi, N., 601  
Scarf, H. E., 526  
Schrader, R., 256, 521  
Schrage, L., 291, 425, 380  
Schrijver, A., 20, 180, 201, 254-255, 291, 342,  
379, 599-600, 654-655, 713  
Seymour, P. D., 599, 601  
Shamir, R., 142  
Shapiro, J. E, 49, 342, 345, 379, 426, 520-521  
Shepardson, E, 522-523  
Sherman, G., 425  
Shmoys, D. B., 523  
Shore, M. L., 526  
Sieveking, M., 142  
Sigismondi, G., 523  
Sinha, P., 521  
Smale, S., 142  
Soland, R. M., 426  
Spielberg, K., 20, 22, 343, 379, 424-425, 522  
Steele, J. M., 426, 524-525  
Steiglitz, K., 20, 524  
Stewart, W. R., 426, 524  
Stoer, J., 109  
Strang, G., 109  
Suhl, U., 22, 380, 521-522, 525  
Taha, H. A., 20  
Tamir, A., 599-600  
Tardos, E., 82, 142, 181, 342, 379  
Tarjan, R. E., 82, 713  
Taverna, R., 654  
Terada, R., 524  
Thiagarajan, H., 522  
Thizy, J. M., 425, 427  
Tind, J., 109, 342-343, 601  
Todd, M. J., 180-181,425  
Tomlin, J. A., 379  
Tompkins, C. B., 599  
Tong, P., 713 Author Index  
Topkis, D., 712  
Toth, P., 21, 521, 523, 525  
Treybig, L. L., 142  
Trick, M., 343, 345  
Trotter, L. E., 201, 290, 522-523, 599-601, 714  
Truemper, K., 599  
Tutte, W. T., 599, 655, 712  
Ullman, Z., 342, 427  
Van Emde Boas, P., 201  
Van Roy, T. J., 291, 343, 427, 521-522, 525  
Van Wassenhove, L. N., 426  
Vecchi, M. P., 426  
Veinott, A. F., Jr., 599  
Venta, E. R., 380  
Vizing, V. G., 655  
Volgenant, T., 524  
von Randow, R., 20  
Von Zur Gathen, J., 142  
Wage, M. L., 109  
Wagner, H. M., 427, 520  
Wakabayashi, Y., 291  
753  
Weber, G. M., 343, 522, 654  
Weinberger, D. B., 601  
Weingartner, H. M., 427  
Welsh, D. J. A., 426, 712  
White, D. J., 255, 426-427  
Whitesides, S. H., 600  
Whitin, T. M., 427  
Whitney, H., 712  
Williams, H. P., 20-21  
Witzgall, c., 109  
Wolfe, P., 49, 342  
Wolsey, L. A., 255, 290-291, 342-343, 379, 426-  
427, 521-526, 713-714  
Wong, R. T., 343, 425, 427, 525-526  
Wood, D. E., 379  
Wright, J. W., 524, 599  
Yannakakis, M., 20, 143,291,599  
Young, R. D., 380  
Zangwill, W. I., 427, 599  
Zemel, E., 290, 426, 521  
Zionts, S., 521  
Zoltners, A. A., 521 SUBJECT INDEX  
Adjacency matrix, 52, 118  
Affine\:  
function, 325  
independence, 192  
transformation, 150  
Algorithm, see also specific entries  
balanced matrix, totally, recognition, 570  
basis, reduced, 514  
of lattice, 199  
Bellman-Ford, 58  
branch-and-bound, 355, 363, 482, 484, 497  
constraint generation, 411, 413  
cutting-plane, 498, 710  
/branch-and-bound, 388, 459, 463, 485, 489,  
502  
fractional, 351, 367, 373, 386  
general, 367  
Gomory,368  
mixed-integer, 374  
primal, 374  
strong, 386  
decomposition, 409-417  
for DFC and FC for row inclusion matrices, 566  
for DFNP and FNP for chordal graphs, 577-585  
dual desc.ent, greedy, 397-388  
dual simplex, 37  
lexicographic, 371  
efficiency, 117  
ellipsoid, 124, 147-162,698  
euclidean, 184-187  
feasibility, 179  
fixed-charge, branch-and-bound, 497  
greedy, 60, 666-667, 679, 689, 712  
group, increasing, 448  
Hermite normal form, 193  
heuristic, 393-409  
analysis, 399  
probabilistic, 408  
worst-case, 399  
dual, 400  
dual descent, 397  
greedy, see Greedy, heuristic  
interchange, 394, 397, 406-407, 477-478  
nearest insertion, 477-478  
nearest neighbor, 475, 477, 494  
primal, 452  
primal-dual, 395  
randomized, 407-409  
scaling/rounding, 442-443  
simplex-based, for BIP, 457-459  
knapsack problem, 0-1 separation, 462  
linear programming, polynomial-time, 146-181  
ellipsoid, 147-160  
projective, 164-172  
strongly polynomial for combinatorial  
problems, 172-180  
linear programming relaxation, 451-452  
matching\:  
cardinality, 618, 625  
bipartite, 612  
weighted, 628  
matrix, totally balanced, recognition, 570  
matroid, greedy, for maximum-weight  
independent sets, 666-671  
intersection\:  
maximum cardinality, 677, 706  
weighted, 684  
mixed-integer, cutting-plane, 374  
minimum-weight path, Dijkstra's, 56-58  
network flow problem, primal simplex, 76-81  
nondeterministic, 128, 131  
objective rounding, 177  
path\:  
augmenting, 65  
minimum-cost, 75-76  
minimum-weight, Dijkstra's, 56-58  
polynomial-time, 119  
strong, combinatorial for linear programs,  
172-180  
primal, simplex, 33, 76  
primal-dual, 69-70  
projective, 164-172  
for recognizing an EPT matrix, 555-561  
for recognizing TB matrices, 570-573  
reduced basis of lattice, 199  
relaxation, 349, 482  
assignment problem/branch-and-bound, 482-  
483  
1-tree, sub gradient optimization, branch-and-  
bound,484-485  
FCP Ibranch-and-bound, 485  
separation, 412, 462, 487  
755 756  
Algorithm \(Continued\)  
simplex\:  
dual, 37  
primal,33  
simple upper bounds, 39  
sliding objective function approximate ellipsoid,  
155\. See also Ellipsoid  
spanning-tree, construction, 60-61  
special purpose, 383, 433  
subgradient, 46, 410, 484  
sub modular , function maximization, greedy,  
712  
superadditive, dual, 435  
transportation problem, primal-dual, 68-76  
Alphabet, 118  
Annealing, see Simulated annealing  
Antiblocker, 102  
Antiblocking\:  
clutter, 594  
matrix, 102  
Antihole, 575  
Approximation scheme\:  
fully polynomial, 401  
polynomial, 402  
Are, 54  
forward, 64  
reverse, 64  
saturated, 63  
slack, 309  
variable *j,* 309, 313  
Artificial variable, 36  
Ascent, steepest, 43  
Assignment problem, 5, 68, 332, 482  
generalized, 346  
Augmenting path, see Path  
Backtracking, 358  
Balanced matrix, 563, 564, 573  
totally, 563-565, 570-573, 576  
recognition algorithm, 570  
Basic solution, 30  
feasible, 31  
Basis, 30  
adjacent, 31  
degenerate, 32  
dual feasible, 31, 37, 321  
matroid, see Matroid  
nondegenerate, 34  
orthogonal, 196  
reduced, see Reduced basis of lattice  
Bellman-Ford algorithm, 58  
Benders' decomposition, 412, 508  
Benders' reformulation, 337-341, 710  
Binary\:  
alphabet, 118  
digits, 157  
representation, 72  
search, 128  
string, 119  
variable, 5-13  
Bipartite, 50, 54, 544, 575, 593, 612, 651  
Subject Index  
Blocker, 101  
Blocking, 586  
clutter, 587, 650  
matrix, 103  
pair, 103, 590  
polyhedron, 101, 586-598  
Blossom, 616  
base of, 616  
shrinking, 616  
Boolean\:  
function, 695  
implications, 215  
Bounded, 107  
Bounds, tightening, 18  
Branch-and-bound, 354-367, 454  
algorithm, 355, 363, 482, 484, 497  
node selection, 358  
adaptive rules, 358  
best estimate, 359  
best upper bound, 359  
quick improvement, 359  
a priori rules, 358  
variable selection, 359  
degradation, 359  
integer infeasibility, 360  
penalties, 359  
Branching, 55, 661  
problem, 532, 680  
scheme, 365  
Breadth-first search, see Search  
Capital budgeting, 3  
Certificate of feasibility, 114, 128  
Certificate of optimality, 114, 124  
C-G\:  
function, 306  
inequality, 210, 220-225, 228, 232  
Characteristic vector, 118, 259  
Checking stage, 129  
Chord, 575  
Chordal graph, 576-578  
strong, 605  
Chromatic\:  
index, 651  
number, 582  
Claw, 585  
Claw-free, 585  
Clique, 260, 573  
cover, 298  
matrix, 574, 576  
perfect graph, 583  
problem, maximum weight, 163  
Closest vector problem, 182  
Closure, 664  
elementary, 225  
Clutter, 562, 574, 576, 583, 587  
Coloring, 582, 651  
Column operation, elementary, 192  
Comb,277  
inequality, 277-280  
generalized, 280 Subject Index  
Combinatorial optimization problem, 4  
Complementarity condition, 330  
Complementary slackness, 29, 305  
Complexity, 114, 117, 139  
Component, 53, 555  
Concave function, 11,42-43  
Cone, 86, 156, 164  
polyhedral,99  
Connected, 53, 552  
strongly, 55  
CoNP, 130, 141  
Constraint generation algorithm, 411, 413  
Constraints\:  
adding, 39, 358  
disjunctive, 12  
complicating, 323, 329, 337, 512  
generalized upper-bound, 365  
nice, 323  
redundant, 19  
Continued fractions, 187-189  
Convergence, finite, 370, 378  
Convex, 150  
combination, 83  
function, 11, 329  
hull, 83, 106, 125, 127, 206, 241  
set, 86  
Cost-splitting, 334  
Cover, 299  
inequality, 459  
extended, 461, 498  
minimal,463  
Covering, 6  
by edges, 538, 586, 588, 639  
fractional, 103  
by independent sets, 702, 707  
problem, 6, 144, 464, 571, 589, 702, 709  
fractional, 562, 566  
greedy heuristic, 466  
Cramer's rule, 123  
Cryptography, 513  
Cut,62 .  
capacity, 63  
clutter, 587  
edge, 551  
proper, 551  
function, 660  
Gomory fractional, 368  
minimum weight, 62, 63, 586-587, 592  
primal, 376  
-set equality, 486  
T-, see T-cut  
Cutting-plane algorithm, 498, 710  
Ibranch-and-bound, 388, 459, 463, 485, 489, 502  
fractional, 351, 367, 373, 386  
general, 367  
Gomory, 368  
mixed-integer, 374  
primal, 374  
strong, 386  
Cycle, 52, 78  
chordless, 261  
# 757  
directed, 54  
negative weight, 59  
odd, 583  
Cycling, 34  
Data, 4, 115  
Data structures, 82  
Davis-Putnam procedure, 256  
Decision problem, 127  
Decomposition, 556  
algorithms, 409-417  
integral, 595  
Degree, 51, 608  
constraints, 272, 470  
Demand nodes, 596  
Dependent set, 265, 283, 659  
extension, 266  
minimal, 266  
Depth-first search, see Search  
Determinant, 123, 196, 540  
Dicut, 535  
Digraph, 54, 320, 496, 546  
Dijkstra's algorithm, 56, 446, 592  
Dimension, 86, 92, 108  
full, 86-87  
D-inequality, 213, 218, 220  
Diophantine approximation, 184, 187, 189,200  
Dipath, 55, 535, 675, 682  
Disjunctive\:  
constraints, 12, 212  
procedure, 213  
Distribution, 3  
Division, 352, 355, 356  
Dominance, 207, 247  
Dual, 28, 296-341  
cost-splitting, 334  
feasible basis, 371  
gap, 299, 329  
heuristic, 400  
integral, totally, 537-539, 562, 638, 690-691  
Lagrangian, 323-337, 409-411, 484, 494  
linear programming, 28-30  
matroid, 665  
optimal solution, 175, 176  
problem, 28, 97  
restricted, 336  
simplex algorithm, 37  
lexicographic, 371  
strong, 29, 299, 301, 305, 672, 677  
superadditive, 304-312  
general integer programming, 304  
mixed-integer programming, 308  
surrogate, 334  
weak, 28, 299, 304, 672  
Dynamic programming, 417-424, 433, 440  
Edge, 50  
coloring, 651-654  
coverings, 639  
end, 551  
shrinking, 487 **758**  
Ellipsoid, 147  
algorithm, 124, 147-162, 698  
property, 148  
volume, 148  
Enumeration tree, 352  
Epsilon-approximate solution, 174, 331  
EPT matrix, see Tree  
Equality-constrained subgraph, 627  
Equality set, 86, 91  
Euclidean\:  
algorithm, 184-187  
distance, 45  
Euler cycle, 478, 609  
Exchange property, see Matroid  
Exponential, 120  
Extreme point, 93, 95, 123, 125, 158  
Extreme ray, 94-95, 123  
Face, 88, 108  
proper, 88  
Facet, 89-91, 127  
validity, 141  
Facet -defining inequality, 158  
Facility location, 3, 7, 15, 17, 287  
capacitated, 8, 347  
median problem, 408  
uncapacilated, 8, 340, 384, 416, 496, 509, 709  
dual descent, 397  
FCPA with separation, 387  
greedy heuristic, 393  
Lagrangian dual, 409  
p-, 402-406, 411  
primal-dual heuristic, 398  
strong formulation, 384  
weak formulation, 385  
Farkas' lemma, 30, 97  
integer version, 191  
FCPA, see Cutting-plane algorithm, fractional  
Feasibility\:  
algorithm, 179  
problem, 127  
homogeneous, 164  
integer programming, 127-129, 133  
linear equations integer variables, 182  
lower bound, 127, 139, 141  
property, strict, 148  
Feasible\:  
region, 4  
solution, 4  
Fibonacci number, 186  
Finitely generated, 104  
Fixed-charge\:  
network flow problem, 8, 18, 423, 495-513  
branch-and-bound algorithm, 497  
fractional cutting-plane algorithm, 498  
multi-source, 506, 508  
single-source, 496, 506  
uncapacitated, 496  
transportation problem, 502  
Fixed cost, 18, 496  
Subject Index  
Flow, 62  
blocking, 63  
cover inequalities generalized, 499  
extended, 501  
feasible integral, 596  
maximum, 62-68, 487, 695  
minimum-cut theorem, 63  
problem with budget constraints, 332  
Forest, 53, 648  
Formulation, 15,217, 338, 384  
Fourier-Motzkin elimination, 111  
Function, separable, 11  
Gaussian elimination, 121, 179  
gcd, see Greatest common divisor  
Generalized upper-bound, see GUB  
Gomory cut, 212  
fractional, 227-229, 236  
mixed integer, 249-250  
Good characterization, 124  
Gradient, 43  
Gram-Schmidt orthogonalization, 196  
Graph, 50-82  
acyclic, 52  
bipartite, see Bipartite  
chordal, see Chordal graph  
comparability, 606  
complement, 52, 578  
complete, 51, 270  
component, 53  
connected, 53  
directed, see Digraph  
intersection, 117  
interval, 605  
line, 584  
perfect, see Perfect  
planar, 582  
random, 408  
reduced, 488, 616  
simple, 51  
underlying, 54  
Greatest common divisor, 183  
Greedy\:  
algorithm, 60, 666-667, 679, 689  
heuristic, 393, 397, 400, 403, 428, 440, 452, 466,  
476, 712  
Group, 315  
algorithm, increasing, 448  
problem, 312-322, 444  
GUB \(generalized upper-bound\)\:  
constraints, 365  
dichotomy, 356  
Hadamard inequality, 197  
Hamiltonian cycle, 129, 270  
problem, 129  
directed, 144  
Hermite normal form, 184, 189-195  
algorithm, 193 Subject Index  
Heuristic algorithms, 393-409  
analysis, 399  
probabilistic, 408  
worst-case, 399  
dual,400  
dual-descent, 397  
greedy, see Greedy, heuristic  
interchange, 394, 397, 406-407, 477-478  
nearest insertion, 477-478  
nearest neighbor, 475, 477, 494  
primal-dual, 395  
randomized, 407-409  
Hole, 575  
odd, 261, 575  
Hypersphere, 147, 158, 517  
Identity transformation, 132  
Imperfect, 584  
Implicit enumeration, 354  
Impossible to solve, 121  
Incidence matrix, 76  
neighborhood subtree-neighborhood subtree,  
573  
node-edge, 51  
node-arc, 54  
Incidence vector, see Characteristic vector  
Independence\:  
affine, 84, 108  
linear, 83, 84  
system, 237, 659, 663  
test, 666  
Independent set, 265, 281, 659  
Inequality\:  
dominating, 207  
equivalent, 91  
max-min, 103  
min-max, 104  
set, 86  
stronger, 207  
valid, see Valid inequality  
Inner point, 86-87  
Instance, 4, 115  
feasible, 127  
size, 118  
Integer programming feasibility problem, 129,  
515-520  
Integer programming problem, 4, 104, 115, 125-  
128  
0-1, 456-469  
FCP Ibranch-and-bound, 459  
simplex heuristic, 457  
fixed number of variables, 520  
Integer round down, 595-598, 708  
Integer rounding, see Rounding  
Interior point, 86-87, 101  
Interval matrix, 544, 549  
Job processing, 689-694, 704  
Join, see T-join  
759  
Knapsack problem\:  
0-1, 5, 265-270, 418, 420-422, 450-464  
branch-and-bound, 454  
linear programming relaxation, 451  
primal heuristic, 452  
separation algorithm, 462  
integer, 125,312  
dynamic programming, 433  
heuristics, 440, 442  
multiple choice, 527  
superadditive dual, 435  
lower bound feasibility, 136  
multidimensional, 5  
Lagrangian\:  
dual, 323-337, 409  
relaxation, 323-337  
Lattice, 182, 518  
basis of, 190, 197  
Leaf, 53  
Lexicographic, 34, 371-373  
totally reverse, 568  
LIFO \(last in, first out\), 358  
Lifting, 261-267  
heuristic for, 461  
Linear algebra, 83  
Linear equations problem, 121  
integer feasibility, 182  
Linear programming, 4, 27-49, 115, 122-124, 131  
polynomial-time algorithms, 146-181  
ellipsoid algorithm, 147-160  
projective algorithm, 164-172  
strongly polynomial for combinatorial  
problems, 172-180  
Line Graph, 584  
Logical inequalities, 19  
Lot-sizing, 16  
capacitated, 347  
uncapacitated, 218, 288-290, 418, 422-424,  
508-513  
backlogging, 431  
Matching, 226, 608-657  
1-, 608  
2-,273,469, 641, 647  
fractional, 471, 494  
inequality, 276, 490, 495  
integer, 471, 483  
algorithm\:  
cardinali ty, 618, 625  
bipartite, 612  
weighted,628  
b-, 608, 640-648  
0-1, 608, 647, 702  
integer, 608, 643  
perfect, 644  
bonds, 144  
perfect, 480, 638, 649  
polytope, 633, 636-640 760  
Matching \(Continued\)  
problem, 5, 400, 608  
cardinality, 608, 611-626  
weighted, 124, 608, 627-636  
separation, 658  
Matroid, 659-703  
basis, 664  
binary, 714  
circui t, 664  
cographic, 665, 667  
covering with independent sets, 702  
dual, 665  
exchange property, 664  
graphic, 660, 665  
greedy algorithm for maximum-weight  
independent sets, 666-671  
intersection, 671-678, 692  
algorithm\:  
maximum cardinality, 677, 706  
weighted, 684  
k-, 661, 709  
polytope, 688  
problem, 671  
weighted, 678-688  
matric, 660, 665  
optimization problem, 661  
partition, 660, 704  
polytope, 668  
rank function, 665  
representation, 666  
span, 664  
sum, 704  
union, 705-706  
Max-min equality, 591  
strong, 592  
Membership problem, 139, 141  
extreme point, 141  
strict, 148  
Minkowski's theorem, 96  
MIR inequality, see Mixed-integer, rounding  
Mixed-integer\:  
cutting-plane algorithm, 374  
programming problem, 3, 115, 338, 374,  
413  
dual, 308  
rounding, 244-246  
set, 242-254  
Model formulation, 5-17  
choices in, 14  
valid inequalities, 217  
Modular arithmetic, 212, 236, 312  
Multigraph, 478  
Neighborhood, 407  
subtree, 571  
Neighbors, 576  
Network\:  
design, 3  
Subject Index  
flow problem, 8, 62, 76, 549  
fixed-charge, see Fixed-charge  
primal simplex algorithm, 76-81  
matrix, 546-561  
Node, 50  
conservation, 62  
cover, 144  
covering by edges, 608  
demand,62  
duplication, 582  
even, 612  
exposed, 611  
induced subgraph, 52, 579-580  
intermediate, 52  
isolated, 53  
minimal, 675  
odd,612  
partition, 537  
selection, see Branch-and-bound  
simplicial, 576  
supply, 62  
Node-arc incidence matrix, 54  
Node-edge incidence matrix, 51  
Node-induced subgraph, 575, 582  
Node-neighborhood subtree incidence matrix, 572  
Node packing, 115-117, 216, 297, 307, 467-468,  
573-585, 594  
polytope, 141, 163, 259-261, 574  
unweighted, lower bound feasibility, 133  
Node-star incidence matrix, 571  
Nondecreasing, 229, 660  
Nondeterministic\:  
algorithm, 128, 131  
polynomial, 127, 129, 141  
Nondifferentiable optimization, 41  
Nonlinear function, 10  
Nonsingular, 150  
NP, see Nondeterministic  
NP-complete, 131  
strongly, 138  
NP-hard, 138  
Objective function, 4  
nonlinear, 11  
Objective rounding algorithm, 177  
Objective value, 107  
Odd-set constraints, 124, 610  
O\( \) notation, 57  
Optimal\:  
solution, 4, 94-95, 536  
value, 4, 107  
Optimality conditions, 296  
Optimization, 161  
Orthonormal matrix, 150  
Packing, 6, 115, 117, 299, 394, 401, 464  
fractional, 103, 562, 571  
Partition, 6, 352, 704  
feasibility, 134-135 Subject Index  
Path, 52  
alternating, 611  
augmenting, 64, 66, 75, 611-612  
algorithm, 65  
shortest, 67  
clutter, 587  
directed, 54  
maximum weight, 308, 320  
minimum weight, 56, 58, 121, 444, 511, 535,  
586-587, 592  
shortest, 55-59  
Perfect, 573-585  
elimination scheme, 576  
graph,574  
conjecture, 576  
theorem, 582  
matching, see Matching  
Performance guarantee, 399, 477  
Perturbation lemma, 168  
Phase I, 36, 78  
Piecewise linear function, 11, 329, 366  
Pivot element, 33  
Polar, 99, 139, 206  
1-, 100, 163, 239  
Polarity, 98-104  
Polyhedron, 85-98, 139, 161  
bounded, 86  
dual, 108  
full-dimensional, 86  
integral, 535-607, 638, 669  
minimal representation, 91, 539  
projection of, 97-98  
rational, 85  
volume, 148  
Polymatroid, 688-694  
polytope, 690  
rank function, 688  
calculation of, 693  
separation, 693  
Polynomial\:  
equivalence, 161, 163, 387, 592  
function, 127  
normalization, 176  
reduction, 132  
space, 121  
-time, 146, 162-163  
algorithm, 119  
strong, 172  
combinatorial algorithm for linear  
programs, 172-180  
transformation, 131  
Polytope, 86  
full-dimensional, 100  
integral, 562  
master, 238, 241  
round, 515  
Portfolio analysis, 3  
Positive definite, 147, 150  
Positive homogeneity, 247  
761  
Postman problem, 609, 648  
Precision, finite, 157, 159  
Preprocessing, 17-20  
Primal\:  
-dual algorithm, 69-70  
heuristic, 452, 475  
problem, 28, 97  
simplex algorithm, 33, 76  
Principle of optimality, 419  
Priorities, 359  
Probabilistic analysis, 408  
Problem, 115  
Production planning, 290  
Production scheduling, 3, 16  
Projection, 49, 85, 98, 164  
Projective algorithm, 164-172  
Pruning, 352-357  
Pseudonode, 616  
Pseudopolynomial, 137, 421  
Pure integer programming problem, see Integer  
programming problem  
Quadratic Boolean function, 695, 697, 701, 711  
Radius, 571  
Rank, 76, 84, 226  
bounded, 227  
Rational\:  
approximation problem, 183  
number, 120, 183  
polyhedron, 85  
Ray, 93, 164  
epsilon-approximate, 166  
extreme, see Extreme ray  
integer, 104  
Recognition problem, 127, 550, 555, 565, 570  
Reduced basis of a lattice, 184, 195-201, 518  
algorithm, 199  
applications, 513  
heuristic, 514  
Reduced price, 20, 32, 392  
Reduced problem, 453  
Reduction, 132  
Reformulation, see Formulation  
Relative error, 399  
Relatively prime, 183  
Relaxation, 296-300, 353  
adding variables, 299  
algorithm, 349, 482  
dual, 300  
choice of, 383  
group, 298  
Lagrangian, 298, 323-337, 410  
linear programming, 298, 355, 451  
state space, 431  
surrogate, 334  
Representation, 88  
Restriction, 344  
Rotation, 151 762  
Round,515  
Rounding, 160, 227  
heuristic, 440  
integer, 209, 232-233, 594-598  
method, 210  
Row inclusion matrix, 565-566  
Row intersection graph, 550  
Running time, 119  
Satisfiability problem, 133  
Scaling, 72  
heuristic, 441  
Scheduling, 3, 13, 287, 347  
Search\:  
breadth-first, 67, 359  
depth-first, 358  
feasibility, 457  
local improvement, 458  
Separable, see Function, separable; Set, separable  
Separation, 161-164  
algorithm, 412, 462, 487  
problem, 161, 387, 459, 499, 693  
Sequential decision process, 417  
Set\:  
dependent, 265, 283, 659  
extension, 266  
minimal, 266  
independent, 265, 281, 659  
separable, 670  
Set-covering problem, see Covering  
Set-function maximization, 393  
Set-packing problem, see Packing  
Set-partitioning problem, see Partition  
Short, 124  
Shortest path problem, see Path, shortest  
Shortest vector problem, 182, 197, 200  
Shrinking, 487, 616  
Simplex, 516  
algorithm\:  
dual,37  
primal, 33  
simple upper bounds, 39  
Simplex-based heuristic, 457  
Simplicial, 576  
Simulated annealing, 407  
Single source problem, 496, 506  
Size of problem instance, 118  
Sliding objective function, 155  
Smith normal form, 195, 319  
Space requirements, 121  
Sphere, 147  
Star, 551  
minimum weight, 586  
State, 417  
Steepest ascent, 43  
Steiner branching problem, 496, 507  
Subdifferential, 45  
Subgradient, 45, 409  
algorithm, 46, 410, 484  
optimization, 41-49, 409  
Subject Index  
Subgraph, 52  
induced,52  
spanning, 52  
Submodular, 144, 403, 660, 662-663  
function maximization, 708-712  
greedy algorithm, 712  
function minimization, 693-702  
Subset sum problem, 136, 513  
Subspace, 85  
orthogonal, 85  
Subtour, 10, 273  
elimination constraints, 273-275, 470, 487  
Superadditive, 229-237, 300, 316, 320  
dual algorithm, 435  
duality, 304-312  
dual problem, 304  
valid inequality, 230, 237-242  
mixed-integer, 246-254  
Supermodular, 660  
Supply-demand vector, 596  
Supply nodes, 596  
Support, 88  
TB, see Balanced matrix  
T-cut, 649-651  
TDI, see Dual, integral  
T-join, 648-651, 702  
Tour, 10, 270, 469, 588  
Transformation, 131  
Transportation problem, 62, 68, 122  
primal-dual algorithm, 68-76  
Transpose, 540, 564  
Transversal, 714  
Traveling salesman\:  
polytope, 270-281  
dimension, 272  
Traveling salesman problem, 9, 16, 586  
symmetric, 469-495  
branch-and-bound, 482  
I-tree, subgradient, 484  
assignment problem, 482  
FCP, 485  
heuristics, 475  
double spanning tree, 479  
greedy feasible, 476  
interchange, 477  
nearest insertion, 477  
nearest neighbor, 475  
spanning tree, perfect matching, 480  
relaxations, 469  
2-matching, 469, 475  
I-tree, 470, 473  
fractional, 471  
integer, 471  
Tree, 53  
1-, 470, 484, 494  
directed, 55, 546  
edge-path incidence matrix, 550-559  
recognition algorithm, 555  
polytope, 669, 698 Subject Index  
spanning, 55, 77, 546  
minimum weight, 60-61  
Triangle inequality, 478  
TU, see Unimodular matrix  
UFL, see Facility location, uncapacitated  
ULS, see Lot-sizing, uncapacitated  
Unary, 137  
Unbounded, 4, 95, 107  
Unimodular matrix, 189,319  
totally, 540-546, 549, 561, 574  
Valid inequality, 88, 205-295  
generating all, 217-227  
knapsack, 0-1, 265-270  
mixed integer sets, 242-246  
node packing, 259-261  
rank,226-227  
representing a face, 88  
strong, 259-290, 386  
superadditive, 230, 237-242, 249, 252  
supporting, 88  
traveling salesman, 270-281  
variable upper bound flow model, 281-  
290  
Validity problem, 139, 141  
Value dominance, 352  
# 763  
Value function, 300  
Variable, 4  
adding of, 39  
artificial, 36  
bounded, 222  
branching, see Branch-and-bound  
complicating, 323  
decision, 417  
dichotomy, 356  
fixing, 19, 452, 468  
regular, 457  
slack, 37  
selection in branching, 359  
Variable upper bound, 281  
Vizing's theorem, 652  
Walk, 52, 313  
closed, 52  
directed, 54  
even, 54  
length of, 52  
odd, 54  
Weyt's theorem, 98  
Wheel, 110  
Worst-case\:  
analysis, 119, 399  
performance, 399 WILEY-INTERSCIENCE  
SERIES IN DISCRETE MATHEMATICS AND OPTIMIZATION  
ADVISORY EDITORS  
RONALD L. GRAHAM  
AT & T Laboratories, Florham Park, New Jersey, US.A.  
JAN KAREL LENSTRA  
Department of Mathematics and Computer Science,  
Eindhoven University of Technology, Eindhoven, The Netherlands  
ROBERT E. TARJAN  
Princeton University, New Jersey, and  
NEC Research Institute. Princeton, New Jersey, US.A.  
AARTS AND KORST • Simulated Annealing and Boltzmann Machines\: A Stochastic Approach to  
Combinatorial Optimization and Neural Computing  
AARTS AND LENSTRA • Local Search in Combinatorial Optimization  
ALON, SPENCER, AND ERDOS • The Probabilistic Method  
ANDERSON AND NASH • Linear Programming in Infinite-Dimensional Spaces\: Theory and  
Application  
AZENCOTT • Simulated Annealing\: Parallelization Techniques  
BARTHELEMY AND GUENOCHE • Trees and Proximity Representations  
BAZARRA, JARVIS, AND SHERALI • Linear Programming and Network Flows  
CHANDRU AND HOOKER. Optimization Methods for Logical Inference  
CHONG AND ZAK • An Introduction to Optimization  
COFFMAN AND LUEKER • Probabilistic Analysis of Packing and Partitioning Algorithms  
COOK, CUNNINGHAM, PULLEYBLANK, AND SCHRIJVER • Combinatorial Optimization  
DASKIN • Network and Discrete Location\: Modes, Algorithms and Applications  
DINITZ AND STINSON • Contemporary Design Theory\: A Collection of Surveys  
ERICKSON • Introduction to Combinatorics  
GLOVER, KLINGHAM, AND PHILLIPS • Network Models in Optimization and Their Practical  
Problems  
GOLSHTEIN AND TRETY AKOV • Modified Lagrangians and Monotone Maps in Optimization  
GONDRAN AND MINOUX • Graphs and Algorithms \(Translated by S. Vajda\)  
GRAHAM, ROTHSCHILD, AND SPENCER • Ramsey Theory, Second Edition  
GROSS AND TUCKER· Topological Graph Theory  
HALL • Combinatorial Theory, Second Edition  
JENSEN AND TOFT • Graph Coloring Problems  
KAPLAN. Maxima and Minima with Applications\: Practical Optimization and Duality  
LA WLER, LENSTRA, RINNOOY KAN, AND SHMOYS, Editors • The Traveling Salesman  
Problem\: A Guided Tour of Combinatorial Optimization  
LAYWINE AND MULLEN • Discrete Mathematics Using Latin Squares  
LEVITIN. Perturbation Theory in Mathematical Programming Applications  
MAHMOUD • Evolution of Random Search Trees  
MARTELLO AND TOTH • Knapsack Problems\: Algorithms and Computer Implementations  
McALOON AND TRETKOFF • Optimization and Computational Logic  
MINC • Nonnegative Matrices  
MINOUX • Mathematical Programming\: Theory and Algorithms \(Translated by S. Vajda\)  
MIRCHANDANI AND FRANCIS, Editors • Discrete Location Theory  
NEMHAUSER AND WOLSEY • Integer and Combinatorial Optimization  
NEMIROVSKY AND YUDIN • Problem Complexity and Method Efficiency in Optimization  
\(Translated bv *E.* R. Dawson\)  
PACH AND AGARWAL. Combinatorial Geometry  
PLESS • Introduction to the Theory of Error-Correcting Codes, Third Edition ROOS AND VIAL • Ph. Theory and Algorithms for Linear Optimization\: An Interior Point Approach  
SCHEINERMAN AND ULLMAN • Fractional Graph Theory\: A Rational Approach to the Theory of  
Graphs  
SCHRIJVER • Theory of Linear and Integer Programming  
TOMESCU • Problems in Combinatorics and Graph Theory \(Translated by R. A. Melter\)  
TUCKER· Applied Combinatorics, Second Edition  
WOLSEY • Integer Programming  
YE • Interior Point Algorithms\: Theory and Analysis  
